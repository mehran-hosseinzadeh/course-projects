[{"date": "2018", "abstract": "Recent work on neural network pruning indicates that, at training time, neural networks need to be significantly larger in size than is necessary to represent the eventual functions that they learn. This paper articulates a new hypothesis to explain this phenomenon. This conjecture, which we term the \"lottery ticket hypothesis,\" proposes that successful training depends on lucky random initialization of a smaller subcomponent of the network. Larger networks have more of these \"lottery tickets,\" meaning they are more likely to luck out with a subcomponent initialized in a configuration amenable to successful optimization. \nThis paper conducts a series of experiments with XOR and MNIST that support the lottery ticket hypothesis. In particular, we identify these fortuitously-initialized subcomponents by pruning low-magnitude weights from trained networks. We then demonstrate that these subcomponents can be successfully retrained in isolation so long as the subnetworks are given the same initializations as they had at the beginning of the training process. Initialized as such, these small networks reliably converge successfully, often faster than the original network at the same level of accuracy. However, when these subcomponents are randomly reinitialized or rearranged, they perform worse than the original network. In other words, large networks that train successfully contain small subnetworks with initializations conducive to optimization. \nThe lottery ticket hypothesis and its connection to pruning are a step toward developing architectures, initializations, and training strategies that make it possible to solve the same problems with much smaller networks.", "authors": ["Jonathan Frankle", "Michael Carbin"], "id": "f90720ed12e045ac84beb94c27271d6fb8ad48cf", "title": "The Lottery Ticket Hypothesis: Training Pruned Neural Networks", "references": ["397de65a9a815ec39b3704a79341d687205bc80a", "e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724", "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "049fd80f52c0b1fa4d532945d95a24734b62bdf3", "2dfef5635c8c44431ca3576081e6cfe6d65d4862", "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "642d0f49b7826adcf986616f4af77e736229990f", "34f25a8704614163c4095b3ee2fc969b60de4698", "cc46229a7c47f485e090857cbab6e6bf68c09811", "b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d"]}, {"date": "2019", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. \nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "references": ["93b8da28d006415866bf48f9a6e06b5242129195", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "27e98e09cf09bc13c913d01676e5f32624011050", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "8c1b00128e74f1cd92aede3959690615695d5101", "687bac2d3320083eb4530bf18bb8f8f721477600", "6e795c6e9916174ae12349f5dc3f516570c17ce8"]}, {"date": "2017", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need", "references": ["032274e57f7d8b456bd255fe76b909b2c1d7458e", "735d547fc75e0772d2a78c46a1cc5fad7da1474c", "510e26733aaff585d65701b9f1be7ca9d5afc586", "cea967b59209c6be22829699f05b8b1ac4dc092d", "13d9323a8716131911bfda048a40e2cde1a76a46", "b60abe57bc195616063be10638c6437358c81d1e", "43428880d75b3a14257c3ee9bda054e61eb869c0", "d76c07211479e233f7c6a6f32d5346c983c5598f", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "98445f4172659ec5e891e031d8202c102135c644"]}, {"date": "2015", "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, We introduce a three stage pipeline: pruning, quantization and Huffman encoding, that work together to reduce the storage requirement of neural networks by 35\u00d7 to 49\u00d7 without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman encoding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9\u00d7 to 13\u00d7; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35\u00d7, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG16 by 49\u00d7 from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory, which has 180\u00d7 less access energy.", "authors": ["Song Han", "Huizi Mao", "William J. Dally"], "id": "397de65a9a815ec39b3704a79341d687205bc80a", "title": "A Deep Neural Network Compression Pipeline: Pruning, Quantization, Huffman Encoding", "references": ["fbeaa499e10e98515f7e1c4ad89165e8c0677427", "efb5032e6199c80f83309fd866b25be9545831fd", "27a99c21a1324f087b2f144adc119f04137dfd87", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "e7bf9803705f2eb608db1e59e5c7636a3f171916", "2a4117849c88d4728c33b1becaa9fb6ed7030725", "e5ae8ab688051931b4814f6d32b18391f8d1fa8d", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"]}, {"date": "2013", "abstract": "Dropout is a relatively new algorithm for training neural networks which relies on stochastically \"dropping out\" neurons during training in order to avoid the co-adaptation of feature detectors. We introduce a general formalism for studying dropout on either units or connections, with arbitrary probability values, and use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks. For deep neural networks, the averaging properties of dropout are characterized by three recursive equations, including the approximation of expectations by normalized weighted geometric means. We provide estimates and bounds for these approximations and corroborate the results with simulations. Among other results, we also show how dropout performs stochastic gradient descent on a regularized error function.", "authors": ["Pierre Baldi", "Peter Sadowski"], "id": "cc46229a7c47f485e090857cbab6e6bf68c09811", "title": "Understanding Dropout", "references": ["1366de5bb112746a555e9c0cd00de3ad8628aea8", "0688fbcbfb08d7b91238bc90589209b31f97290f", "7b0db6135b8dd3e2a9efa86163e91c0cd0fdf660", "5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "327d3df8ea2020882827d6bace1e26c9d24309c2", "ba15f09796d53adfbe9e78cf79182e59b6045543", "fc6b1ff29f2da985cccfa644652bb320d7720d59", "7ab5ceb40c0e267ea6fcdbcaedd327d7b263bb8e"]}, {"date": "2015", "abstract": "We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity and thus implicitly enforcing regularization. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches.", "authors": ["Zelda Mariet", "Suvrit Sra"], "id": "2dfef5635c8c44431ca3576081e6cfe6d65d4862", "title": "Diversity Networks: Neural Network Compression Using Determinantal Point Processes", "references": ["a42954d4b9d0ccdf1036e0af46d87a01b94c3516", "ec46bcbced500820521e9f65b0f9ffef5a83ae11", "efb5032e6199c80f83309fd866b25be9545831fd", "87d810fcea61068e8b29f2b75fa1cbb00c190bea", "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "d7bfd8a283a7ed8b43255cfd04909484cd3ade28", "31f88db95eb5c66b95cd7335b0cd4f27f0f271f2", "8ba555d9587688bd3225d71ef9d686dad288e1f1", "48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016", "b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d"]}, {"date": "2015", "abstract": "Deep Neural nets (NNs) with millions of parameters are at the heart of many state-of-the-art computer vision systems today. However, recent works have shown that much smaller models can achieve similar levels of performance. In this work, we address the problem of pruning parameters in a trained NN model. Instead of removing individual weights one at a time as done in previous works, we remove one neuron at a time. We show how similar neurons are redundant, and propose a systematic way to remove them. Our experiments in pruning the densely connected layers show that we can remove upto 85\\% of the total parameters in an MNIST-trained network, and about 35\\% for AlexNet without significantly affecting performance. Our method can be applied on top of most networks with a fully connected layer to give a smaller network.", "authors": ["Suraj Srinivas", "R. Venkatesh Babu"], "id": "b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d", "title": "Data-free Parameter Pruning for Deep Neural Networks", "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "82b9099ddf092463f497bd48bb112c46ca52c4d1", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "eb42cf88027de515750f230b23b1a057dc782108", "2a4117849c88d4728c33b1becaa9fb6ed7030725", "e8650503ab80ad7299f0845b1843abf3a97f313a", "34f25a8704614163c4095b3ee2fc969b60de4698", "0c908739fbff75f03469d13d4a1a07de3414ee19", "cd85a549add0c7c7def36aca29837efd24b24080"]}, {"date": "2017", "abstract": "We propose an efficient and unified framework, namely ThiNet, to simultaneously accelerate and compress CNN models in both training and inference stages. We focus on the filter level pruning, i.e., the whole filter would be discarded if it is less important. Our method does not change the original network structure, thus it can be perfectly supported by any off-the-shelf deep learning libraries. We formally establish filter pruning as an optimization problem, and reveal that we need to prune filters based on statistics information computed from its next layer, not the current layer, which differentiates ThiNet from existing methods. Experimental results demonstrate the effectiveness of this strategy, which has advanced the state-of-the-art. We also show the performance of ThiNet on ILSVRC-12 benchmark. ThiNet achieves 3.31 x FLOPs reduction and 16.63\u00d7 compression on VGG-16, with only 0.52% top-5 accuracy drop. Similar experiments with ResNet-50 reveal that even for a compact network, ThiNet can also reduce more than half of the parameters and FLOPs, at the cost of roughly 1% top-5 accuracy drop. Moreover, the original VGG-16 model can be further pruned into a very small model with only 5.05MB model size, preserving AlexNet level accuracy but showing much stronger generalization ability.", "authors": ["Jian-Hao Luo", "Jianxin Wu", "Weiyao Lin"], "id": "049fd80f52c0b1fa4d532945d95a24734b62bdf3", "title": "ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression", "references": ["e15cf50aa89fee8535703b9f9512fca5bfc43327", "60ae4f18cb53efff0174e3fea7064049737e1e67", "e7bf9803705f2eb608db1e59e5c7636a3f171916", "eb42cf88027de515750f230b23b1a057dc782108", "7601b995303f953955004db7b9b8b206c0e02ff8", "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "642d0f49b7826adcf986616f4af77e736229990f", "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"]}, {"date": "2016", "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.", "authors": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "id": "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "title": "Pruning Filters for Efficient ConvNets", "references": ["397de65a9a815ec39b3704a79341d687205bc80a", "8ad35df17ae4064dd174690efb04d347428f1117", "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "7d39283a0fce1c96f57eb20046d09bd95ccc56d7", "3ed94217fbf29b86d5f1baec90dc33adacb40b58", "b64601d509711468f5d085261d463846f36785b2", "d5b4721c8188269b120d3d06149a04435753e755", "d559dd84fc473fca7e91b9075675750823935afa", "751c8884c1e857e675d85d8594c5f9b608005ed5", "021fc345d40d3e6332cd2ef276e2eaa5e71102e4"]}, {"date": "2016", "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.", "authors": ["Song Han", "Huizi Mao", "William J. Dally"], "id": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding", "references": ["fbeaa499e10e98515f7e1c4ad89165e8c0677427", "efb5032e6199c80f83309fd866b25be9545831fd", "27a99c21a1324f087b2f144adc119f04137dfd87", "6bdb186ec4726e00a8051119636d4df3b94043b5", "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "e7bf9803705f2eb608db1e59e5c7636a3f171916", "2a4117849c88d4728c33b1becaa9fb6ed7030725", "a6373454105df0c5511ca5f6cae4d20c48214272", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"]}, {"date": "1993", "abstract": "The use of information from all second-order derivatives of the error function to perform network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and, in some cases, enable rule extraction, is investigated. The method, Optimal Brain Surgeon (OBS), is significantly better than magnitude-based methods and Optimal Brain Damage, which often remove the wrong weights. OBS, permits pruning of more weights than other methods (for the same error on the training set), and thus yields better generalization on test data. Crucial to OBS is a recursion relation for calculating the inverse Hessian matrix H/sup -1/ from training data and structural information of the set. OBS deletes the correct weights from a trained XOR network in every case.<<ETX>>", "authors": ["Babak Hassibi", "David G. Stork", "Gregory J. Wolff"], "id": "e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724", "title": "Optimal Brain Surgeon and general network pruning", "references": ["a42954d4b9d0ccdf1036e0af46d87a01b94c3516", "57dc98cfb48247b400cc8decb93380e022864905", "de996c32045df6f7b404dda2a753b6a9becf3c08", "5887de8eed53c444b2ef93d8ab9c8cc685cd7ac5", "111fd833a4ae576cfdbb27d87d2f8fc0640af355", "1b29884885401d12299a01b0eae099f425dd32e1"]}, {"date": "2018", "abstract": "For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.", "authors": ["Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "id": "93b8da28d006415866bf48f9a6e06b5242129195", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "references": ["afc2850945a871e72c245818f9bc141bd659b453", "ceb7dddbd0c51f511c4ba97d328b48fd10d2a7fc", "e242ba1a62eb2595d89afbec2657f33d9ab4abe3", "93b4cc549a1bc4bc112189da36c318193d05d806", "8472e999f723a9ccaffc6089b7be1865d8a1b863", "2997b26ffb8c291ce478bd8a6e47979d5a55c466", "8f1c9b656157b1d851563fb42129245701d83175", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "ade0c116120b54b57a91da51235108b75c28375a"]}, {"date": "2015", "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.", "authors": ["Song Han", "Jeff Pool", "John Tran", "William J. Dally"], "id": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network", "references": ["fbeaa499e10e98515f7e1c4ad89165e8c0677427", "e7bf9803705f2eb608db1e59e5c7636a3f171916", "081651b38ff7533550a3adfc1c00da333a8fe86c", "2a4117849c88d4728c33b1becaa9fb6ed7030725", "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "642d0f49b7826adcf986616f4af77e736229990f", "34f25a8704614163c4095b3ee2fc969b60de4698", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d"]}, {"date": "2014", "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.", "authors": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "id": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting", "references": ["db869fa192a3222ae4f2d766674a378e47013b1b", "5d90f06bb70a0a3dced62413346235c02b1aa086", "5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd", "d124a098cdc6f99b9a152fcf8afa9327dac583be", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "ec92efde21707ddf4b81f301cd58e2051c1a2443", "3c20df69865df6a627cc45c524869ccc0297048f", "de75e4e15e22d4376300e5c968e2db44be29ac9e", "8978cf7574ceb35f4c3096be768c7547b28a35d0"]}, {"date": "2018", "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.", "authors": ["Romain Paulus", "Caiming Xiong", "Richard Socher"], "id": "032274e57f7d8b456bd255fe76b909b2c1d7458e", "title": "A Deep Reinforced Model for Abstractive Summarization", "references": ["f77a604410d88307ec5c6331c8b6133272fbaa10", "7a67159fc7bc76d0b37930b55005a69b51241635", "5ab72d44237533534de8402e30f3ccce25ce30de", "1bc49abe5145055f1fa259bd4e700b1eb6b7f08d", "2f160ce71f01ac2043de67536ff0e413ff6f58c5", "489955574c435169abd72285cfe2f055f538a401", "668db48c6a79826456341680ee1175dfc4cced71", "cea967b59209c6be22829699f05b8b1ac4dc092d", "f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "5082a1a13daea5c7026706738f8528391a1e6d59"]}, {"date": "2015", "abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice.", "authors": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler"], "id": "6e795c6e9916174ae12349f5dc3f516570c17ce8", "title": "Skip-Thought Vectors", "references": ["0ca7d208ff8d81377e0eaa9723820aeae7a7322d", "ebd1f6822d1dbb13bb813ff83a3490e0439fc9e4", "d41cfe9b2ada4e09d53262bc75c473d8043936fc", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "0b544dfe355a5070b60986319a3f51fb45d1348e", "cea967b59209c6be22829699f05b8b1ac4dc092d", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "687bac2d3320083eb4530bf18bb8f8f721477600", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "ae5e6c6f5513613a161b2c85563f9708bf2e9178"]}, {"date": "2019", "abstract": "LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.", "authors": ["Rami Al-Rfou", "Dokook Choe", "Noah Constant", "Mandy Guo", "Llion Jones"], "id": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention", "references": ["4db8cd9117254d21c9c828b8ba2aea58e57ee2c4", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "f9a1b3850dfd837793743565a8af95973d395a4e", "55cf59bfbb25d6363cab87cb747648ebe8a096e5", "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb", "27981998aaef92952eabef2c1490b926f9150c4f", "84ca430856a92000e90cd728445ca2241c10ddc3", "88caa4a0253a8b0076176745ebc072864eab66e1", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "58c6f890a1ae372958b7decf56132fe258152722"]}, {"date": "2016", "abstract": "We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.", "authors": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "A\u00e4ron van den Oord", "Alex Graves", "Koray Kavukcuoglu"], "id": "98445f4172659ec5e891e031d8202c102135c644", "title": "Neural Machine Translation in Linear Time", "references": ["93499a7c7f699b6630a86fad964536f9423bb6d0", "5b791cd374c7109693aaddee2c12d659ae4e3ec0", "acec46ffd3f6046af97529127d98f1d623816ea4", "733b821faeebe49b6efcf5369e3b9902b476529e", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "0b544dfe355a5070b60986319a3f51fb45d1348e", "cea967b59209c6be22829699f05b8b1ac4dc092d", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "b60abe57bc195616063be10638c6437358c81d1e", "944a1cfd79dbfb6fef460360a0765ba790f4027a"]}, {"date": "2013", "abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.", "authors": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "id": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "references": ["2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "d9b0190b06ac7270e9052895f8592beb4959ccfd", "6af58c061f2e4f130c3b795c21ff0c7e3903278f", "81b3b3fe994a9eda6d3f9d2149aa4492d1933975", "2063745d08868c928455f422202b72146a1960fb", "553fb89d5858826c02f26e94262e8958debc777e", "da5cd00115f7ec108de8eebf071c5f3f19807df4", "cfa2646776405d50533055ceb1b7f050e9014dcb", "27e38351e48fe4b7da2775bf94341738bc4da07e", "57458bc1cffe5caa45a885af986d70f723f406b4"]}, {"date": "2018", "abstract": "Current end-to-end machine reading and question answering (Q\\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\\&A architecture called QANet, which does not require recurrent networks: Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.", "authors": ["Adams Wei Yu", "David Dohan", "Minh-Thang Luong", "Rui Zhao", "Kai Chen", "Mohammad Norouzi", "Quoc V. Le"], "id": "8c1b00128e74f1cd92aede3959690615695d5101", "title": "QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension", "references": ["93499a7c7f699b6630a86fad964536f9423bb6d0", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "de0c30321b22c56d637e7c29cb59180f157272a8", "c25a67ad7e8629a9d12b9e2fc356cd73af99a060", "e94697b98b707f557436e025bdc8498fa261d3bc", "adc276e6eae7051a027a4c269fb21dae43cadfed", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "12e20e4ea572dbe476fd894c5c9a9930cf250dd2"]}, {"date": "2014", "abstract": "We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2 x, while keeping the accuracy within 1% of the original model.", "authors": ["Emily L. Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus"], "id": "e5ae8ab688051931b4814f6d32b18391f8d1fa8d", "title": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation", "references": ["fbeaa499e10e98515f7e1c4ad89165e8c0677427", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "e8650503ab80ad7299f0845b1843abf3a97f313a", "05cc38e249a6f642363b5a5cbd71cda67cea5893", "d743430cb2329caa5d446c17fc9ec07f5e916ab0", "a7621b4ec18719b08f3a2a444b6d37a2e20227b7", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "1109b663453e78a59e4f66446d71720ac58cec25", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "021fc345d40d3e6332cd2ef276e2eaa5e71102e4"]}, {"date": "2012", "abstract": "Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory. In contrast to traditional structured models like Markov random fields, which become intractable and hard to approximate in the presence of negative correlations, DPPs offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. While they have been studied extensively by mathematicians, giving rise to a deep and beautiful theory, DPPs are relatively new in machine learning. Determinantal Point Processes for Machine Learning provides a comprehensible introduction to DPPs, focusing on the intuitions, algorithms, and extensions that are most relevant to the machine learning community, and shows how DPPs can be applied to real-world applications like finding diverse sets of high-quality search results, building informative summaries by selecting diverse sentences from documents, modeling non-overlapping human poses in images or video, and automatically building timelines of important news stories. It presents the general mathematical background to DPPs along with a range of modeling extensions, efficient algorithms, and theoretical results that aim to enable practical modeling and learning.", "authors": ["Alex Kulesza", "Ben Taskar"], "id": "48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016", "title": "Determinantal Point Processes for Machine Learning", "references": ["ec46bcbced500820521e9f65b0f9ffef5a83ae11", "15acca25f75076b80b0bd24c5710c70733308c11", "96a5867d0b9b997108633ff3da314edf69b0122c", "e0ab5b00d4fb0ef319093f94d5024008b6000381", "325ea1f2022ee3886a5810df76dcfbe4010ad439", "702c2fde33ccb4328be06405c11e208a4b3ee347", "733edbe153056edb62f3c3cdec975db85a072906", "b91afd46236a9c9eda9056bf4e70fe9235867571", "c87f5836627a4ea7d928ff1aecf1b7cdebaf1302", "6c4ab2b7bf202e621dcb722d2e7cf421415cc3ed"]}, {"date": "2015", "abstract": "While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.", "authors": ["Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio"], "id": "cd85a549add0c7c7def36aca29837efd24b24080", "title": "FitNets: Hints for Thin Deep Nets", "references": ["184ac0766262312ba76bbdece4e7ffad0aa8180b", "5d90f06bb70a0a3dced62413346235c02b1aa086", "eb42cf88027de515750f230b23b1a057dc782108", "e7bf9803705f2eb608db1e59e5c7636a3f171916", "ccf415df5a83b343dae261286d29a40e8b80e6c6", "355d44f53428b1ac4fb2ab468d593c720640e5bd", "523b12db4004b89284387f978c2af8ae0e79d54b", "0c908739fbff75f03469d13d4a1a07de3414ee19", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "021fc345d40d3e6332cd2ef276e2eaa5e71102e4"]}, {"date": "1991", "abstract": "We propose and empirically evaluate a method for the extraction of expert-comprehensible rules from trained neural networks. Our method operates in the context of a three-step process for learning that uses rule-based domain knowledge in combination with neural networks. Empirical tests using real-worlds problems from molecular biology show that the rules our method extracts from trained neural networks: closely reproduce the accuracy of the network from which they came, are superior to the rules derived by a learning system that directly refines symbolic rules, and are expert-comprehensible.", "authors": ["Geoffrey G. Towell", "Jude W. Shavlik"], "id": "1b29884885401d12299a01b0eae099f425dd32e1", "title": "Interpretation of Artificial Neural Networks: Mapping Knowledge-Based Neural Networks into Rules", "references": ["fdfad550280c6a850d92424b6075e7fb58e8e415", "111fd833a4ae576cfdbb27d87d2f8fc0640af355", "a57c6d627ffc667ae3547073876c35d6420accff", "a080a28ff7fb3b58fa8cd7123a473c5e75bf46e1", "d14670a0c65a007912b37e2436ee2d7caf70fd76", "b981cb3324d69a079c8f934c67c1a8ce14358f17", "1e560e13357d180dda2507ef572961d9de913c6c", "d8c9a210221e3c925b4119a4ab90aa8b57bb31fc", "de75e4e15e22d4376300e5c968e2db44be29ac9e", "d2f6ac8b70044a437a7d5444252d6af6037b46c2"]}, {"date": "2015", "abstract": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.", "authors": ["Geoffrey E. Hinton", "Oriol Vinyals", "Jeffrey Dean"], "id": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network", "references": ["1366de5bb112746a555e9c0cd00de3ad8628aea8", "30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9", "31868290adf1c000c611dfc966b514d5a34e8d23", "c8d90974c3f3b40fa05e322df2905fc16204aa56", "3127190433230b3dc1abd0680bb58dced4bcd90e", "34f25a8704614163c4095b3ee2fc969b60de4698", "8d25d04051074be7590cbe5e4e34c45bb26674e1", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "a0456c27cdd58f197032c1c8b4f304f09d4c9bc5"]}, {"date": "2014", "abstract": "The focus of this paper is speeding up the application of convolutional neural networks. While delivering impressive results across a range of computer vision and machine learning tasks, these networks are computationally demanding, limiting their deployability. Convolutional layers generally consume the bulk of the processing time, and so in this work we present two simple schemes for drastically speeding up these layers. This is achieved by exploiting cross-channel or filter redundancy to construct a low rank basis of filters that are rank-1 in the spatial domain. Our methods are architecture agnostic, and can be easily applied to existing CPU and GPU convolutional frameworks for tuneable speedup performance. We demonstrate this with a real world network designed for scene text character recognition [15], showing a possible 2.5\u00d7 speedup with no loss in accuracy, and 4.5\u00d7 speedup with less than 1% drop in accuracy, still achieving state-of-the-art on standard benchmarks.", "authors": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman"], "id": "021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "title": "Speeding up Convolutional Neural Networks with Low Rank Expansions", "references": ["fbeaa499e10e98515f7e1c4ad89165e8c0677427", "8b25a44f617c1ed3ed52c6655b0d456ff1c565bd", "b3d8dffb73bc93de239998548386c84177caa2ad", "4dbc68cf2e14155edb6da0def30661aca8c96c22", "a7621b4ec18719b08f3a2a444b6d37a2e20227b7", "1e80f755bcbf10479afd2338cec05211fdbd325c", "c08f5fa876181fc040d76c75fe2433eee3c9b001", "e5ae8ab688051931b4814f6d32b18391f8d1fa8d", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"]}, {"date": "2015", "abstract": "Deep convolutional neural networks have shown promising results in image and speech recognition applications. The learning capability of the network improves with increasing depth and size of each layer. However this capability comes at the cost of increased computational complexity. Thus reduction in hardware complexity and faster classification are highly desired. This work proposes an optimization method for fixed point deep convolutional neural networks. The parameters of a pre-trained high precision network are first directly quantized using L2 error minimization. We quantize each layer one by one, while other layers keep computation with high precision, to know the layer-wise sensitivity on word-length reduction. Then the network is retrained with quantized weights. Two examples on object recognition, MNIST and CIFAR-10, are presented. Our results indicate that quantization induces sparsity in the network which reduces the effective number of network parameters and improves generalization. This work reduces the required memory storage by a factor of 1/10 and achieves better classification results than the high precision networks.", "authors": ["Sajid Anwar", "Kyuyeon Hwang", "Wonyong Sung"], "id": "a6373454105df0c5511ca5f6cae4d20c48214272", "title": "Fixed point optimization of deep convolutional neural networks for object recognition", "references": ["162d958ff885f1462aeda91cd72582323fd6a1f4", "a4db2d26b5d169de6b64de361dc7d4fd5b1f61a3", "e277762804aa4615b2258fbd367d91326c00b90e", "5d90f06bb70a0a3dced62413346235c02b1aa086", "5d21006fa32ff69f6b0a646f26ce0db84f2f4d33", "5562a56da3a96dae82add7de705e2bd841eb00fc", "0abb49fe138e8fb7332c26b148a48d0db39724fc", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "c3c82b476162d2d006e02180530875a64af18154"]}, {"date": "2012", "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.", "authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet Classification with Deep Convolutional Neural Networks", "references": ["82b9099ddf092463f497bd48bb112c46ca52c4d1", "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "398c296d0cc7f9d180f84969f8937e6d3a413796", "5d90f06bb70a0a3dced62413346235c02b1aa086", "f354310098e09c1e1dc88758fca36767fd9d084d", "c43025c429b1fbf6f1379f61801a1b40834d62e7", "1e80f755bcbf10479afd2338cec05211fdbd325c", "5562a56da3a96dae82add7de705e2bd841eb00fc", "bea5780d621e669e8069f05d0f2fc0db9df4b50f", "3a4a53fe47036ac89dad070ab87a9d8795b139b1"]}, {"date": "2018", "abstract": "A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. We train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations.", "authors": ["Sandeep Subramanian", "Adam Trischler", "Yoshua Bengio", "Christopher Joseph Pal"], "id": "afc2850945a871e72c245818f9bc141bd659b453", "title": "Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning", "references": ["26e743d5bd465f49b9538deaf116c15e61b7951f", "cea967b59209c6be22829699f05b8b1ac4dc092d", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "e44da7d8c71edcc6e575fa7faadd5e75785a7901", "395044a2e3f5624b2471fb28826e7dbb1009356e", "d76c07211479e233f7c6a6f32d5346c983c5598f", "3f1802d3f4f5f6d66875dac09112f978f12e1e1e", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "ade0c116120b54b57a91da51235108b75c28375a", "6e795c6e9916174ae12349f5dc3f516570c17ce8"]}, {"date": "2006", "abstract": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.", "authors": ["Geoffrey E. Hinton", "Simon Osindero", "Yee Whye Teh"], "id": "8978cf7574ceb35f4c3096be768c7547b28a35d0", "title": "A Fast Learning Algorithm for Deep Belief Nets", "references": ["709b4bfc5198336ba5d70da987889a157f695c1e", "162d958ff885f1462aeda91cd72582323fd6a1f4", "2077d0f30507d51a0d3bbec4957d55e817d66a59", "14d2d9b2e4c29fe105bfbb31f9749b60690303a7", "9f87a11a523e4680e61966e36ea2eac516096f23", "a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5", "5562a56da3a96dae82add7de705e2bd841eb00fc", "9360e5ce9c98166bb179ad479a9d2919ff13d022", "2184fb6d32bc46f252b940035029273563c4fc82", "b95799a25def71b100bd12e7ebb32cbcee6590bf"]}, {"date": "1992", "abstract": "One way of simplifying neural networks so they generalize better is to add an extra term to the error function that will penalize complexity. Simple versions of this approach include penalizing the sum of the squares of the weights or penalizing the number of nonzero weights. We propose a more complicated penalty term in which the distribution of weight values is modeled as a mixture of multiple gaussians. A set of weights is simple if the weights have high probability density under the mixture model. This can be achieved by clustering the weights into subsets with the weights in each cluster having very similar values. Since we do not know the appropriate means or variances of the clusters in advance, we allow the parameters of the mixture model to adapt at the same time as the network learns. Simulations on two different problems demonstrate that this complexity term is more effective than previous complexity terms.", "authors": ["Steven J. Nowlan", "Geoffrey E. Hinton"], "id": "de75e4e15e22d4376300e5c968e2db44be29ac9e", "title": "Simplifying Neural Networks by Soft Weight-Sharing", "references": ["59fa47fc237a0781b4bf1c84fedb728d20db26a1", "4a42b2104ca8ff891ae77c40a915d4c94c8f8428", "82fa37d5be8e747131a5857992cc33bb95469ce3", "d9b824794b2df1ae5708b8a4ba9c6c69c8eef93a", "e7297db245c3feb1897720b173a59fe7e36babb7", "25406e6733a698bfc4ac836f8e74f458e75dad4f", "c83684f6207697c12850db423fd9747572cf1784", "2cee043045b529fceda7964a70e626d45657245a", "f8830ea439ca695e7dd848275e534f1024c2fe8a", "6f3175b3930d0c71495a52a7bccb3889e5f33520"]}, {"date": "2011", "abstract": "Paraphrase detection is the task of examining two sentences and determining whether they have the same meaning. In order to obtain high accuracy on this task, thorough syntactic and semantic analysis of the two statements is needed. We introduce a method for paraphrase detection based on recursive autoencoders (RAE). Our unsupervised RAEs are based on a novel unfolding objective and learn feature vectors for phrases in syntactic trees. These features are used to measure the word- and phrase-wise similarity between two sentences. Since sentences may be of arbitrary length, the resulting matrix of similarity measures is of variable size. We introduce a novel dynamic pooling layer which computes a fixed-sized representation from the variable-sized matrices. The pooled representation is then used as input to a classifier. Our method outperforms other state-of-the-art approaches on the challenging MSRP paraphrase corpus.", "authors": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennington", "Andrew Y. Ng", "Christopher D. Manning"], "id": "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection", "references": ["c6afe8a8aa13de8e3f2710ef07b22ce86a005419", "cadc3dbd73f0cbbe04b2a66f832c3cf34c877b41", "7acfdc905f734abf966aed58abb983bc015ff7fe", "81b3b3fe994a9eda6d3f9d2149aa4492d1933975", "6d1792f1871a99cb89dcf713abd24592630f32b7", "4f65a57f4511629109544a9ff7d326e03e56c35e", "9c0ddf74f87d154db88d79c640578c1610451eec", "cfa2646776405d50533055ceb1b7f050e9014dcb", "57458bc1cffe5caa45a885af986d70f723f406b4", "10b8f21e57b3392ce623c374c2c039f811ce5f69"]}, {"date": "2014", "abstract": "We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.", "authors": ["Min Lin", "Qiang Chen", "Shuicheng Yan"], "id": "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "title": "Network In Network", "references": ["b3d8dffb73bc93de239998548386c84177caa2ad", "822f3b9a392a9abccdaa7ef5ae4183d2d4d3d6db", "5d90f06bb70a0a3dced62413346235c02b1aa086", "38f35dd624cd1cf827416e31ac5e0e0454028eca", "5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "523b12db4004b89284387f978c2af8ae0e79d54b", "f9def788d4ae040edb8bde18b8aeea635444a4d1", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "0abb49fe138e8fb7332c26b148a48d0db39724fc"]}, {"date": "2018", "abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.", "authors": ["Stephen Merity", "Nitish Shirish Keskar", "Richard Socher"], "id": "58c6f890a1ae372958b7decf56132fe258152722", "title": "Regularizing and Optimizing LSTM Language Models", "references": ["d1275b2a2ab53013310e759e5c6878b96df643d4", "7dba53e72c182e25e98e8f73a99d75ff69dda0c2", "f0b6c1ffed9984317050d0c1dfb005cb65582f13", "efbd381493bb9636f489b965a2034d529cd56bcd", "63e39cdf1ad884da6bc69096bb3413b5b1100559", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652", "67d968c7450878190e45ac7886746de867bf673d", "424aef7340ee618132cc3314669400e23ad910ba", "fc18e99f918d8906ec44be3f7d90d8f9ebabae96"]}, {"date": "2016", "abstract": "The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which are very successful in computer vision. We present a new architecture for text processing which operates directly on the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report significant improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to NLP.", "authors": ["Alexis Conneau", "Holger Schwenk", "Lo\u00efc Barrault", "Yann LeCun"], "id": "84ca430856a92000e90cd728445ca2241c10ddc3", "title": "Very Deep Convolutional Networks for Natural Language Processing", "references": ["1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "5423e418053a585bbc35180f5719c25b36e160b7", "f9a1b3850dfd837793743565a8af95973d395a4e", "eb42cf88027de515750f230b23b1a057dc782108", "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "cea967b59209c6be22829699f05b8b1ac4dc092d", "bc1022b031dc6c7019696492e8116598097a8c12", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "57458bc1cffe5caa45a885af986d70f723f406b4"]}, {"date": "2017", "abstract": "Machine comprehension(MC) style question answering is a representative problem in natural language processing. Previous methods rarely spend time on the improvement of encoding layer, especially the embedding of syntactic information and name entity of the words, which are very crucial to the quality of encoding. Moreover, existing attention methods represent each query word as a vector or use a single vector to represent the whole query sentence, neither of them can handle the proper weight of the key words in query sentence. In this paper, we introduce a novel neural network architecture called Multi-layer Embedding with Memory Network(MEMEN) for machine reading task. In the encoding layer, we employ classic skip-gram model to the syntactic and semantic information of the words to train a new kind of embedding layer. We also propose a memory network of full-orientation matching of the query and passage to catch more pivotal information. Experiments show that our model has competitive results both from the perspectives of precision and efficiency in Stanford Question Answering Dataset(SQuAD) among all published results and achieves the state-of-the-art results on TriviaQA dataset.", "authors": ["Boyuan Pan", "Hao Li", "Zhou Zhao", "Bin Cao", "Deng Cai", "Xiaofei He"], "id": "12e20e4ea572dbe476fd894c5c9a9930cf250dd2", "title": "MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension", "references": ["f37e1b62a767a307c046404ca96bc140b3e68cb5", "b1e20420982a4f923c08652941666b189b11b7fe", "0f2ea810c16275dc74e880296e20dbd83b1bae1c", "35b91b365ceb016fb3e022577cec96fb9b445dc5", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "d2ad6ae5f57844c4474cd3f318c3cdc469994fae", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "525f65936c331b0b766c7aea0eae64c595704c50", "e94697b98b707f557436e025bdc8498fa261d3bc", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19"]}, {"date": "2017", "abstract": "The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.", "authors": ["Yann Dauphin", "Angela Fan", "Michael Auli", "David Grangier"], "id": "88caa4a0253a8b0076176745ebc072864eab66e1", "title": "Language Modeling with Gated Convolutional Networks", "references": ["0dc9eb7d17f2def56ad930945f2521653f04c3fa", "efbd381493bb9636f489b965a2034d529cd56bcd", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "759956bb98689dbcc891528636d8994e54318f85", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "2d7782c225e0fc123d6e227f2cb253e58279ac73", "b7cfccf123f86785476a06c8039889a2eb1e2d73", "9819b600a828a57e1cde047bbe710d3446b30da5", "12a5b7190b981bf478b4c9c04d3c0d41f13b9023"]}, {"date": "2016", "abstract": "We explore the effect of introducing prior information into the intermediate level of neural networks for a learning task on which all the state-of-the-art machine learning algorithms tested failed to learn. We motivate our work from the hypothesis that humans learn such intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favor of this hypothesis. In our experiments, a two-tiered MLP architecture is trained on a dataset with 64x64 binary inputs images, each image with three sprites. The final task is to decide whether all the sprites are the same or one of them is different. Sprites are pentomino tetris shapes and they are placed in an image with different locations using scaling and rotation transformations. The first part of the two-tiered MLP is pre-trained with intermediate-level targets being the presence of sprites at each location, while the second part takes the output of the first part as input and predicts the final task's target binary event. The two-tiered MLP architecture, with a few tens of thousand examples, was able to learn the task perfectly, whereas all other algorithms (include unsupervised pre-training, but also traditional algorithms like SVMs, decision trees and boosting) all perform no better than chance. We hypothesize that the optimization difficulty involved when the intermediate pre-training is not performed is due to the {\\em composition} of two highly non-linear tasks. Our findings are also consistent with hypotheses on cultural learning inspired by the observations of optimization problems with deep learning, presumably because of effective local minima.", "authors": ["\u00c7aglar G\u00fcl\u00e7ehre", "Yoshua Bengio"], "id": "523b12db4004b89284387f978c2af8ae0e79d54b", "title": "Knowledge Matters: Importance of Prior Information for Optimization", "references": ["8de174ab5419b9d3127695405efd079808e956e8", "184ac0766262312ba76bbdece4e7ffad0aa8180b", "aaaea06da21f22221d5fbfd61bb3a02439f0fe02", "0d2336389dff3031910bd21dd1c44d1b4cd51725", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "05fd1da7b2e34f86ec7f010bef068717ae964332", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "b8ef1230a5cc9ea7cd8358f1ae7d1af97813ba14", "355d44f53428b1ac4fb2ab468d593c720640e5bd", "679f7231bd7960d4c4fae577e224bb5e4b93d530"]}, {"date": "2014", "abstract": "Deep neural network (DNN) obtains significant accuracy improvements on many speech recognition tasks and its power comes from the deep and wide network structure with a very large number of parameters. It becomes challenging when we deploy DNN on devices which have limited computational and storage resources. The common practice is to train a DNN with a small number of hidden nodes and a small senone set using the standard training process, leading to significant accuracy loss. In this study, we propose to better address these issues by utilizing the DNN output distribution. To learn a DNN with small number of hidden nodes, we minimize the Kullback\u2013Leibler divergence between the output distributions of the small-size DNN and a standard large-size DNN by utilizing a large number of un-transcribed data. For better senone set generation, we cluster the senones in the large set into a small one by directly relating the clustering process to DNN parameters, as opposed to decoupling the senone generation and DNN training process in the standard training. Evaluated on a short message dictation task, the proposed two methods get 5.08% and 1.33% relative word error rate reduction from the standard training method, respectively.", "authors": ["Jinyu Li", "Rui Zhao", "Jui-Ting Huang", "Yifan Gong"], "id": "8d25d04051074be7590cbe5e4e34c45bb26674e1", "title": "Learning small-size DNN with output-distribution-based criteria", "references": ["fbeaa499e10e98515f7e1c4ad89165e8c0677427", "ecd4bc32bb2717c96f76dd100fcd1255a07bd656", "42ba5f74fe1301c9c5ea1292157daccb7813721b", "23752f55103a1a0e94992c81075f31c9b6d170f5", "6658bbf68995731b2083195054ff45b4eca38b3a", "d7174b0cf599408fb723e6702504e27dc9d6c203", "7d1e282f1613c161585dfc9dd077282cb37b5b89", "5cea23330c76994cb626df20bed31cc2588033df", "a4eb9f4fad5c5a1935c6d0532e2c765ee29b0b37", "6d9429b96d9bb40e2d0d9c3f57d0b97f61db8503"]}, {"date": "2012", "abstract": "We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a deep sparse autoencoder on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200\u00d7200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting from these learned features, we trained our network to recognize 22,000 object categories from ImageNet and achieve a leap of 70% relative improvement over the previous state-of-the-art.", "authors": ["Quoc V. Le", "Marc'Aurelio Ranzato", "Rajat Monga", "Matthieu Devin", "Gregory S. Corrado", "Kai Chen", "Jeffrey Dean", "Andrew Y. Ng"], "id": "72e93aa6767ee683de7f001fa72f1314e40a8f35", "title": "Building high-level features using large scale unsupervised learning", "references": ["e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "5d90f06bb70a0a3dced62413346235c02b1aa086", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "65d994fb778a8d9e0f632659fb33a082949a50d3", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "202cbbf671743aefd380d2f23987bd46b9caaf97", "be9a17321537d9289875fe475b71f4821457b435", "1e80f755bcbf10479afd2338cec05211fdbd325c", "51e93552fe55be91a5711ff2aabc04b742503e68", "b3852f0113fcf8a3913c55ae92393ae6ccde347e"]}, {"date": "2000", "abstract": "Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.", "authors": ["Thomas G. Dietterich"], "id": "a0456c27cdd58f197032c1c8b4f304f09d4c9bc5", "title": "Ensemble Methods in Machine Learning", "references": ["2ad696f9e93d31a96b735606ade4ed3c2217033b", "a5d052cd0afd360883e31cd151d6f6d229aeefc2", "ba94b9ece413f3fac3b817578cae6a5ea3276825", "ad574fa9347bdeb5e940312f238c07f825ac0ed2", "d1ee87290fa827f1217b8fa2bccb3485da1a300e", "27aa6e81d6a0e3e5f08100683830be95f76de7aa", "8b8347e57708f09f84eabe235d3a41e375564146", "e18394e4181adb52ba40e2fbaf6749424c15c7a4", "b26f919ef69be0cd9cfc6364e8ddb13f9876e4f5", "54801c260df5221a9de533d371d3edcc358b4050"]}, {"date": "2017", "abstract": "The success of neural network methods for computing word embeddings has motivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs. Surprisingly, Wieting et al (ICLR'16) showed that such complicated methods are outperformed, especially in out-of-domain (transfer learning) settings, by simpler methods involving mild retraining of word embeddings and basic linear regression. The method of Wieting et al. requires retraining with a substantial labeled dataset such as Paraphrase Database (Ganitkevitch et al., 2013). \n\nThe current paper goes further, showing that the following completely unsupervised sentence embedding is a formidable baseline: Use word embeddings computed using one of the popular methods on unlabeled corpus like Wikipedia, represent the sentence by a weighted average of the word vectors, and then modify them a bit using PCA/SVD. This weighting improves performance by about 10% to 30% in textual similarity tasks, and beats sophisticated supervised methods including RNN's and LSTM's. It even improves Wieting et al.'s embeddings. This simple method should be used as the baseline to beat in future, especially when labeled training data is scarce or nonexistent. \n\nThe paper also gives a theoretical explanation of the success of the above unsupervised method using a latent variable generative model for sentences, which is a simple extension of the model in Arora et al. (TACL'16) with new \"smoothing\" terms that allow for \nwords occurring out of context, as well as high probabilities for words like and, not in all contexts.", "authors": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma"], "id": "3f1802d3f4f5f6d66875dac09112f978f12e1e1e", "title": "A Simple but Tough-to-Beat Baseline for Sentence Embeddings", "references": []}, {"date": "2010", "abstract": "In this paper we present a scalable hardware architecture to implement large-scale convolutional neural networks and state-of-the-art multi-layered artificial vision systems. This system is fully digital and is a modular vision engine with the goal of performing real-time detection, recognition and segmentation of mega-pixel images. We present a performance comparison between a software, FPGA and ASIC implementation that shows a speed up in custom hardware implementations.", "authors": ["Cl\u00e9ment Farabet", "Berin Martini", "Polina Akselrod", "Sel\u00e7uk Talay", "Yann LeCun", "Eugenio Culurciello"], "id": "c3c82b476162d2d006e02180530875a64af18154", "title": "Hardware accelerated convolutional neural networks for synthetic vision systems", "references": ["162d958ff885f1462aeda91cd72582323fd6a1f4", "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "a90998e0023db48b207cee3b39b0441b3935aaa7", "71e3d9fc53ba14c2feeb7390f0dc99076553b05a", "f354310098e09c1e1dc88758fca36767fd9d084d", "07956c7cf9bf4267b86d52aa4143c17a4aa5d0d6", "b8bc656a1935f07e894833b608cc4671b9fa828f", "6dbaff29d3898cf60f63f5a34cb9610ebb75220c", "ccd52aff02b0f902f4ce7247c4fee7273014c41c"]}, {"date": "2017", "abstract": "Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.", "authors": ["Bryan McCann", "James Bradbury", "Caiming Xiong", "Richard Socher"], "id": "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "title": "Learned in Translation: Contextualized Word Vectors", "references": ["f37e1b62a767a307c046404ca96bc140b3e68cb5", "4c7e85ff37dd8b99d8f443eabd3b163ff8b71538", "032e9974cedb31f5c6e354626760e54e5ebf1e3c", "cea967b59209c6be22829699f05b8b1ac4dc092d", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "452059171226626718eb677358836328f884298e", "395044a2e3f5624b2471fb28826e7dbb1009356e", "687bac2d3320083eb4530bf18bb8f8f721477600", "649d03490ef72c5274e3bccd03d7a299d2f8da91", "9f46d7793995677d15a3fa10c3cb2605a44610e8"]}, {"date": "1998", "abstract": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.", "authors": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "id": "162d958ff885f1462aeda91cd72582323fd6a1f4", "title": "Gradient-based learning applied to document recognition", "references": ["92a9311686e48d5d20fbfcdc21362251b121096c", "0bdbc5a8a5ccb718007f0c1999ea7546deb0b473", "464e8d981df7f326c3af6e9d7bd627f83e438816", "3aa4c691289f56f9af6cf543633cfb3917274281", "4f807c11196e524d421706f2e471575389c73623", "847d6ece37d22430a0d9e061b5dc1d1b8c679055", "32315b101afe9bacb725cf80944884e7ac053245", "86890c82b589e24007c56e1f40c5f928a0e04183", "4fb52984078d75ec5655962dc94dc7848182286b", "065b0af1bc05ea4f1fbd2afc50a96b0ef1698c8d"]}, {"date": "2003", "abstract": "We present a new way of extending independent components analysis (ICA) to overcomplete representations. In contrast to the causal generative extensions of ICA which maintain marginal independence of sources, we define features as deterministic (linear) functions of the inputs. This assumption results in marginal dependencies among the features, but conditional independence of the features given the inputs. By assigning energies to the features a probability distribution over the input states is defined through the Boltzmann distribution. Free parameters of this model are trained using the contrastive divergence objective (Hinton, 2002). When the number of features is equal to the number of input dimensions this energy-based model reduces to noiseless ICA and we show experimentally that the proposed learning algorithm is able to perform blind source separation on speech data. In additional experiments we train overcomplete energy-based models to extract features from various standard data-sets containing speech, natural images, hand-written digits and faces.", "authors": ["Yee Whye Teh", "Max Welling", "Simon Osindero", "Geoffrey E. Hinton"], "id": "b95799a25def71b100bd12e7ebb32cbcee6590bf", "title": "Energy-Based Models for Sparse Overcomplete Representations", "references": ["2805537bec87a6177037b18f9a3a9d3f1038867b", "42d906c733f273109c0ed716a5ef6e2a379beb26", "a87e0d75a8c17e464cf8e95a0466533e14b97c5e", "2307fd6058ab4f7554a0b1f188507150ddb5b9a2", "8012c4a1e2ca663f1a04e80cbb19631a00cbab27", "7e2b06b4fcd3f806c014fb05ca642cc6fa3cc664", "c1337a15f45f8d1b0a99727c5c0e3d652ee88159", "1ec3f33d81ee1b795cacb102ee1a05f36b7c7971", "94565856cc7f2fb3a1e046eec3c805a1c84b85ef", "be3c3b8091e09d8569508b08bd3240153a02cfb2"]}, {"date": "2014", "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.", "authors": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Micha\u00ebl Mathieu", "Rob Fergus", "Yann LeCun"], "id": "1109b663453e78a59e4f66446d71720ac58cec25", "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks", "references": ["398c296d0cc7f9d180f84969f8937e6d3a413796", "79ef1a3843a2dc01bde67c3a9a17c6deb352e285", "140d2acd4cdbc30b102dac34f4c68f279ace6a26", "fd790b061082571e20be7892ce4a97e156497c9f", "9ab0de951cc9cdf16887b1f841f8da6affc9c0de", "48adff169c044c674e7cbcc033c81d77c7ac9b43", "38b6540ddd5beebffd05047c78183f7575559fb2", "68a859142ef42196e6a56305b8c6ac4cb2c9326e", "f566b1f24e63151ddae652826638af054973a27f", "a1306ce652f556fbb9e794d91084a29294298e6d"]}, {"date": "2003", "abstract": "We address the text-to-text generation problem of sentence-level paraphrasing --- a phenomenon distinct from and more difficult than word- or phrase-level paraphrasing. Our approach applies multiple-sequence alignment to sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patterns represented by word lattice pairs and automatically determines how to apply these patterns to rewrite new sentences. The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems.", "authors": ["Regina Barzilay", "Lillian Lee"], "id": "10b8f21e57b3392ce623c374c2c039f811ce5f69", "title": "Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment", "references": ["cc37959066f68d1bf9b67a3e309d6da4a3f8acb8", "7c8b067649f09b9260569d6e398c5c1b16fa2712", "1b947f31028c542bbbc75f64773f5e8488977109", "b48af24cd360d6b0a3dd25424550c28bf97bc1ce", "8d4fa9ef512ac562f5ca081bdf2b62510ff7dc5b", "6ea4193bc13b1bdc1afcdbd53b4242f789f8aec5", "8d7a692d8763a38283db54e19f1dcc694a34e706", "25f51f4132626a645924b3c8b3edcbdcc35c48a3", "0666c5355311f7a110157ef5f6ee5f2cc1018b0a", "16fcf74245d98ff681c336edbf6f2af5c18664a8"]}, {"date": "1989", "abstract": "Abstraet--A new approach to unsupervised learning in a single-layer linear feedforward neural network is discussed. An optimality principle is proposed which is based upon preserving maximal information in the output units. An algorithm for unsupervised learning based upon a Hebbian learning rule, which achieves the desired optimality is presented, The algorithm finds the eigenvectors of the input correlation matrix, and it is proven to converge with probability one. An implementation which can train neural networks using only local \"synaptic\" modification rules is described. It is shown that the algorithm is closely related to algorithms in statistics (Factor Analysis and Principal Components Analysis) and neural networks (Self-supervised Backpropagation, or the \"encoder\" problem). It thus provides an explanation of certain neural network behavior in terms of\" classical statistical techniques. Examples of the use of a linear network for solving image coding and texture segmentation problems are presented. Also, it is shown that the algorithm can be used to find \"visual receptive fields'\" which are qualitatively similar to those found in primate retina and visual cortex.", "authors": ["Terence D. Sanger"], "id": "709b4bfc5198336ba5d70da987889a157f695c1e", "title": "Optimal unsupervised learning in a single-layer linear feedforward neural network", "references": ["de7cf7c01258719cc3be4321f780db378831f2f4", "16d70e8af45ca0ae2c1bb73f3be6628518d40b8f", "3975117d907c0e582a35c34137231c87956aa93b", "6739c73a8e0d9ccaf7a2d00eb2dd20e02f97d236", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "9552ac39a57daacf3d75865a268935b5a0df9bbb", "9f22cf81654dd50b95e65b86b1125cfe6803a67b", "b8778bb692cf105254fe767ef11a3a8afac4a068", "dbed69808aeccb697d125c049a6554bfac76493d", "06f5a18780d7332ed68a9c786e1c597b27a8e0f6"]}, {"date": "2017", "abstract": "Recurrent neural networks (RNNs) serve as a fundamental building block for many sequence tasks across natural language processing. Recent research has focused on recurrent dropout techniques or custom RNN cells in order to improve performance. Both of these can require substantial modifications to the machine learning model or to the underlying RNN configurations. We revisit traditional regularization techniques, specifically L2 regularization on RNN activations and slowness regularization over successive hidden states, to improve the performance of RNNs on the task of language modeling. Both of these techniques require minimal modification to existing RNN architectures and result in performance improvements comparable or superior to more complicated regularization techniques or custom cell architectures. These regularization techniques can be used without any modification on optimized LSTM implementations such as the NVIDIA cuDNN LSTM.", "authors": ["Stephen Merity", "Bryan McCann", "Richard Socher"], "id": "fc18e99f918d8906ec44be3f7d90d8f9ebabae96", "title": "Revisiting Activation Regularization for Language RNNs", "references": ["7dba53e72c182e25e98e8f73a99d75ff69dda0c2", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "f84d5add20d4df0a6c89c47a920354c272cbdbd8", "d46b81707786d18499f911b4ab72bb10c65406ba", "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652", "cf76789618f5db929393c1187514ce6c3502c3cd", "67d968c7450878190e45ac7886746de867bf673d", "424aef7340ee618132cc3314669400e23ad910ba", "9819b600a828a57e1cde047bbe710d3446b30da5", "e9c771197a6564762754e48c1daafb066f449f2e"]}, {"date": "2016", "abstract": "We propose BlackOut, an approximation algorithm to efficiently train massive recurrent neural network language models (RNNLMs) with million word vocabularies. BlackOut is motivated by using a discriminative loss, and we describe a new sampling strategy which significantly reduces computation while improving stability, sample efficiency, and rate of convergence. One way to understand BlackOut is to view it as an extension of the DropOut strategy to the output layer, wherein we use a discriminative training loss and a weighted sampling scheme. We also establish close connections between BlackOut, importance sampling, and noise contrastive estimation (NCE). Our experiments, on the recently released one billion word language modeling benchmark, demonstrate scalability and accuracy of BlackOut; we outperform the state-of-the art, and achieve the lowest perplexity scores on this dataset. Moreover, unlike other established methods which typically require GPUs or CPU clusters, we show that a carefully implemented version of BlackOut requires only 1-10 days on a single machine to train a RNNLM with a million word vocabulary and billions of parameters on one billion words. Although we describe BlackOut in the context of RNNLM training, it can be used to any networks with large softmax output layers.", "authors": ["Shihao Ji", "S. V. N. Vishwanathan", "Nadathur Satish", "Michael J. Anderson", "Pradeep Dubey"], "id": "12a5b7190b981bf478b4c9c04d3c0d41f13b9023", "title": "BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies", "references": ["3aaa1e4974800767fcbd2c24c2f2af42bf412f97", "8250ecbaef057bdb5390ef4e4be798f1523a23f6", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "0cea6b034ee949d10604ba163270b699e711ded8", "5b0d644f5c4b9880cbaf79932c0a4fa98996f068", "8ba93e77b83bca5f90b7d697041ded442986890b", "757f517f1952addc1716ea56f912f2e4a2803f7a", "34f25a8704614163c4095b3ee2fc969b60de4698", "ac973bbfd62a902d073a85ca621fd297e8660a82", "699d5ab38deee78b1fd17cc8ad233c74196d16e9"]}, {"date": "2017", "abstract": "Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.", "authors": ["Hakan Inan", "Khashayar Khosravi", "Richard Socher"], "id": "424aef7340ee618132cc3314669400e23ad910ba", "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling", "references": ["d1275b2a2ab53013310e759e5c6878b96df643d4", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "efbd381493bb9636f489b965a2034d529cd56bcd", "63e39cdf1ad884da6bc69096bb3413b5b1100559", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "687bac2d3320083eb4530bf18bb8f8f721477600", "9819b600a828a57e1cde047bbe710d3446b30da5", "49899bd9e5a7f59aa14e6d21ed501e3c3acd5852", "f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "5762b7deff7e95febe193196d548379ff34b34f1"]}, {"date": "2006", "abstract": "Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.", "authors": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "id": "355d44f53428b1ac4fb2ab468d593c720640e5bd", "title": "Greedy Layer-Wise Training of Deep Networks", "references": ["995a3b11cc8a4751d8e167abc4aa937abc934df0", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "29a37cc8b6866e5809074cac2f7ce134aa763c4b", "19bb461ebc18b43d44b3589659a2e450fff74c32", "b7d471970467a99bec4bce34c7dba5ef6745ad06", "758b1d823ac975720e6e81e375cd4432009e5bca", "6fdb77260fc83dff91c44fea0f31a2cb8ed13d04", "9360e5ce9c98166bb179ad479a9d2919ff13d022", "646ff15fbd38f8c4e2b099ad09e4570179709c73", "8978cf7574ceb35f4c3096be768c7547b28a35d0"]}, {"date": "2013", "abstract": "Recently proposed deep neural network (DNN) obtains significant accuracy improvements in many large vocabulary continuous speech recognition (LVCSR) tasks. However, DNN requires much more parameters than traditional systems, which brings huge cost during online evaluation, and also limits the application of DNN in a lot of scenarios. In this paper we present our new effort on DNN aiming at reducing the model size while keeping the accuracy improvements. We apply singular value decomposition (SVD) on the weight matrices in DNN, and then restructure the model based on the inherent sparseness of the original matrices. After restructuring we can reduce the DNN model size significantly with negligible accuracy loss. We also fine-tune the restructured model using the regular back-propagation method to get the accuracy back when reducing the DNN model size heavily. The proposed method has been evaluated on two LVCSR tasks, with context-dependent DNN hidden Markov model (CD-DNN-HMM). Experimental results show that the proposed approach dramatically reduces the DNN model size by more than 80% without losing any accuracy. Index Terms: deep neural network, singular value decomposition, model restructuring", "authors": ["Jian Xue", "Jinyu Li", "Yifan Gong"], "id": "6d9429b96d9bb40e2d0d9c3f57d0b97f61db8503", "title": "Restructuring of deep neural network acoustic models with singular value decomposition", "references": ["fbeaa499e10e98515f7e1c4ad89165e8c0677427", "ecd4bc32bb2717c96f76dd100fcd1255a07bd656", "c25f3a963f62165a8fc46bc63865e6bec1477e59", "d2b62f77cb2864e465aa60bca6c26bb1d2f84963", "23752f55103a1a0e94992c81075f31c9b6d170f5", "2446e8f2012f23176ff602be633c0ed2b956d66c", "473f0739666af2791ad6592822118240ed968b70", "008e9e2d3908c964d5b1c408c478215709dbea10", "d7174b0cf599408fb723e6702504e27dc9d6c203", "1c2c11e60755fc1df95f57edc37875e2b43cd946"]}, {"date": "1996", "abstract": "The use of previously learned knowledge during learning has been shown to reduce the number of examples required for good generalization, and to increase robustness to noise in the examples. In reviewing various means of using learned knowledge from a domain to guide further learning in the same domain, two underlying classes are discerned. Methods which use previous knowledge to initialize a learner (as an initialization bias), and those that use previous knowledge to constrain a learner (as a search bias). We show such methods in fact exploit the same domain knowledge diierently, and can complement each other. This is shown by presenting a combined approach which both initializes and constrains a learner. This combined approach is seen to outperform the individual methods under the conditions that accurate previously learned domain knowledge is available, and that there are irrelevant features in the domain representation.", "authors": ["Joseph O'Sullivan"], "id": "679f7231bd7960d4c4fae577e224bb5e4b93d530", "title": "Integrating Initialization Bias and Search Bias in Neural Network Learning", "references": ["fedfc9fbcfe46d50b81078560bce724678f90176", "75e50717070e82cdf3945265a75def6960b55a9d", "ffb199e36de4f34ea233a30d392fdcf0c3b25a14", "a6deddb3ed3cc92613e3da82d23accf741ced0a5", "5fc0c7afc6bb27fb3752eae0ea5869413b1259b7", "90ed32fa521b9e85f1c9efe356619814a2e79961", "48e1de7d085808004d5f0493d486669a3d2930b5", "a24508e65e599b5b20c33af96dbe7017d5caca37", "210da45e57f86a50c04bdd7b37d498c8ecc288da", "b67be1aee59478e2ef06b68e0c5aa1da601ac145"]}, {"date": "2014", "abstract": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.", "authors": ["Yoon Kim"], "id": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "title": "Convolutional Neural Networks for Sentence Classification", "references": ["1366de5bb112746a555e9c0cd00de3ad8628aea8", "941f318e41147773ae69d9da4f8de9b8dbea70f4", "bc1022b031dc6c7019696492e8116598097a8c12", "687bac2d3320083eb4530bf18bb8f8f721477600", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "66ff19af88c81e0f3582ac65359a0543a16e1ac8", "cfa2646776405d50533055ceb1b7f050e9014dcb", "dc0975ae518a5b30e60fde23a41c74bafd7c6f8c", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"]}, {"date": "2000", "abstract": "In the past decade, many researchers have employed various methodologies to combine decisions of multiple classifiers in order to order to improve recognition results. In this article, we will examine the main combination methods that have been developed for different levels of classifier outputs - abstract level, ranked list of classes, and measurements. At the same time, various issues, results, and applications of these methods will also be considered, and these will illustrate the diversity and scope of this research area.", "authors": ["Ching Y. Suen", "Louisa Lam"], "id": "b26f919ef69be0cd9cfc6364e8ddb13f9876e4f5", "title": "Multiple Classifier Combination Methodologies for Different Output Levels", "references": []}, {"date": "2012", "abstract": "The use of Deep Belief Networks (DBN) to pretrain Neural Networks has recently led to a resurgence in the use of Artificial Neural Network Hidden Markov Model (ANN/HMM) hybrid systems for Automatic Speech Recognition (ASR). In this paper we report results of a DBN-pretrained context-dependent ANN/HMM system trained on two datasets that are much larger than any reported previously with DBN-pretrained ANN/HMM systems 5870 hours of Voice Search and 1400 hours of YouTube data. On the first dataset, the pretrained ANN/HMM system outperforms the best Gaussian Mixture Model Hidden Markov Model (GMM/HMM) baseline, built with a much larger dataset by 3.7% absolute WER, while on the second dataset, it outperforms the GMM/HMM baseline by 4.7% absolute. Maximum Mutual Information (MMI) fine tuning and model combination using Segmental Conditional Random Fields (SCARF) give additional gains of 0.1% and 0.4% on the first dataset and 0.5% and 0.9% absolute on the second dataset.", "authors": ["Navdeep Jaitly", "Patrick Nguyen", "Andrew W. Senior", "Vincent Vanhoucke"], "id": "a4eb9f4fad5c5a1935c6d0532e2c765ee29b0b37", "title": "Application of Pretrained Deep Neural Networks to Large Vocabulary Speech Recognition", "references": ["cd0568b4faa03910ae3c07d00c627666f404305d", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "26cf16673269bdb0979bc601a340083448e5ad44", "d2b62f77cb2864e465aa60bca6c26bb1d2f84963", "6658bbf68995731b2083195054ff45b4eca38b3a", "473f0739666af2791ad6592822118240ed968b70", "be53d4def5e0601f2416e9345babc7ef1b30a664", "8770b4a5ca7734c88e5755f9558f79e93229c023", "2443dc59cf3d6cc1deba6d3220d61664b1a7eada", "90b63e917d5737b06357d50aa729619e933d9614"]}, {"date": "1998", "abstract": "We develop a common theoretical framework for combining classifiers which use distinct pattern representations and show that many existing schemes can be considered as special cases of compound classification where all the pattern representations are used jointly to make a decision. An experimental comparison of various classifier combination schemes demonstrates that the combination rule developed under the most restrictive assumptions-the sum rule-outperforms other classifier combinations schemes. A sensitivity analysis of the various schemes to estimation errors is carried out to show that this finding can be justified theoretically.", "authors": ["Josef Kittler", "Mohamad Hatef", "Robert P. W. Duin", "Juan E. Sala Matas"], "id": "54801c260df5221a9de533d371d3edcc358b4050", "title": "On Combining Classifiers", "references": ["35c4fc6205ac1708e86d162dffdfd003d93ffacf", "5c85308415a544df7f9ac0657971f996fafff99a", "a2c44bd1b53e3183d356a3d6ed9e1a4a23fff0f7", "0a12808ec97adc39f1fdf8036d1b9781a4824f75", "ec25da04ef7f09396ca00da3f9b5f2d9670cb6fc", "2d6d5cfa8e99dd53e50bf870e24e72b0be7f7aeb", "cfe29860f5c544504d72a5234d1b6328c47ae723", "2d57e848b78a167a41206a44d3cb10e5de3e4f75", "d3234a24b9af2920d15d5bb813e33683bfe13b1d", "41955a29e382d608ef58582497fc94c24537ff3e"]}, {"date": "1992", "abstract": "A neural network with 136000 connections for recognition of handwritten digits has been implemented using a mixed analog/digital neural network chip. The neural network chip is capable of processing 1000 characters/s. The recognition system has essentially the same rate (5%) as a simulation of the network with 32-b floating-point precision.", "authors": ["Eduard S\u00e4ckinger", "Bernhard E. Boser", "Jane Bromley", "Yann LeCun", "Lawrence D. Jackel"], "id": "b8bc656a1935f07e894833b608cc4671b9fa828f", "title": "Application of the ANNA neural network chip to high-speed character recognition", "references": ["adf724f637afdb300426df8d2ff4c4342f1e7528", "86ab4cae682fbd49c5a5bedb630e5a40fa7529f6", "3d79f9bea2be35f4708f2313242d01a62c7f5dab", "c91d07e3a0da4ee494c741b387bb65f8ca66c5cc", "59f0ac84552ea111ff39b1f6d06e628210340bd7", "c2c5940ff6f12d7fc907c9af6ce7854f9f60082d", "cd62c9976534a6a2096a38244f6cbb03635a127e", "2c25cb7cb7c0be41510efeaef5cb96dea339823f"]}, {"date": "2009", "abstract": "Convolutional Networks (ConvNets) are biologicallyinspired hierarchical architectures that can be trained to perform a variety of detection, recognition and segmentation tasks. ConvNets have a feed-forward architecture consisting of multiple linear convolution filters interspersed with pointwise non-linear squashing functions. This paper presents an efficient implementation of ConvNets on a low-end DSPoriented Field Programmable Gate Array (FPGA). The implementation exploits the inherent parallelism of ConvNets and takes full advantage of multiple hardware multiplyaccumulate units on the FPGA. The entire system uses a single FPGA with an external memory module, and no extra parts. A network compiler software was implemented, which takes a description of a trained ConvNet and compiles it into a sequence of instructions for the ConvNet Processor (CNP). A ConvNet face detection system was implemented and tested. Face detection on a 512 \u00d7 384 frame takes 100ms (10 frames per second), which corresponds to an average performance of 3.4\u00d7109 connections per second for this 340 million connection network. The design can be used for low-power, lightweight embedded vision systems for micro-UAVs and other small robots.", "authors": ["Cl\u00e9ment Farabet", "Cyril Poulet", "Jefferson Y. Han", "Yann LeCun"], "id": "07956c7cf9bf4267b86d52aa4143c17a4aa5d0d6", "title": "CNP: An FPGA-based processor for Convolutional Networks", "references": ["f9e65fcb0e04174577f211d702d3f837e3624c5b", "bb42f32f6c81f4c11ad06d2b24d1620dece170d9", "82795cf04f5631e82413b1952625a5a6f21b68ea", "040c23e5a409fbdedd5032263dfcb1a4d7dfd200", "f42b865e20e61a954239f421b42007236e671f19", "0a1c0303d0b9643c717ddedff06caa97c7a29933", "b8bc656a1935f07e894833b608cc4671b9fa828f", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63", "68a859142ef42196e6a56305b8c6ac4cb2c9326e"]}, {"date": "1993", "abstract": "The authors applied an automatic structure optimization (ASO) algorithm to the optimization of multistate time-delay neural networks (MSTDNNs), an extension of the TDNN. These networks allow the recognition of sequences of ordered events that have to be observed jointly. For example, in many speech recognition systems the recognition of words is decomposed into the recognition of sequences of phonemes or phonemelike units. In handwritten character recognition the recognition of characters can be decomposed into the joined recognition of characteristic strokes, etc. The combination of the proposed ASO algorithm with the MSTDNN was applied successfully to speech recognition and handwritten character recognition tasks with varying amounts of training data.<<ETX>>", "authors": ["Ulrich Bodenhausen", "Stefan Manke"], "id": "92a9311686e48d5d20fbfcdc21362251b121096c", "title": "Connectionist architectural learning for high performance character and speech recognition", "references": ["adf724f637afdb300426df8d2ff4c4342f1e7528", "2cee043045b529fceda7964a70e626d45657245a", "cd62c9976534a6a2096a38244f6cbb03635a127e", "25406e6733a698bfc4ac836f8e74f458e75dad4f", "c7bfd3dff5f6f0c541bf334b53f63577056c951d"]}, {"date": "2001", "abstract": "Previous work has shown that various flavours of Independent Component Analysis, when applied to natural images, all result in broadly similar localised, oriented band-pass feature detectors, which have been likened to wavelets or edge detectors. In this paper, we present a similar analysis of \u2018natural\u2019 sounds drawn from two radio stations: one broadcasting mainly speech; the other mainly classical music. Many of the resulting basis vectors are quite wavelet-like, and can easily be characterised in terms of their position and spread in the time-frequency plane. Some of them, however, particularly from the set trained on music, do not fit that interpretation very well. The Wigner-Ville Distribution can be used to gain a clearer picture of time-frequency localisation of these basis vectors. We conclude by suggesting that these results be compared with other widely used auditory representations such as short-term Fourier transforms, wavelet transforms, and physiologically derived models based on the auditory filterbank. 1. REDUNDANCY REDUCTION AS A GOAL OF PERCEPTION It has been suggested [2, 1, 7] that the processing of sensory data in biological perceptual systems is best understood in the language of information theory. The wealth of structure present in natural phenomena means that sensory signals are highly redundant; characterising this structure in order to develop efficient, non-redundant representations might be an effective processing strategy. In a distributed code, a major source of redundancy is statistical dependency between units; independent or factorial coding will be an important tool in dealing with this. ICA is Reduncany Reduction via Linear Transformation. If we restrict ourselves to instantaneous linear methods, then the best we can do is aim for a matrix operation e-mail: samer.abdallah@kcl.ac.uk e-mail: mark.plumbley@kcl.ac.uk that results in a vector whose elements are as independent as possible\u2014that is, precisely the ICA problem. If the observed data is represented as an -element vector , then we wish to find the linear transformation", "authors": ["Samer A. Abdallah", "Mark D. Plumbley"], "id": "be3c3b8091e09d8569508b08bd3240153a02cfb2", "title": "IF THE INDEPENDENT COMPONENTS OF NATURAL IMAGES ARE EDGES, WHAT ARE THE INDEPENDENT COMPONENTS OF NATURAL SOUNDS?", "references": ["ff1152582155acaa0e9d0ccbc900a4641504256d", "db57d5b714f7fe565361550fbcf2c40eadb9df08", "b5241dd7d74602186dd65fe05435fc65eae797e4", "8012c4a1e2ca663f1a04e80cbb19631a00cbab27", "0479efee5558edd13115f37dac41ccd9272138d4", "8c77c78b76355679562fa341fc6ea8eec3f1a2f3", "7f2c612aadeaf6b8dc62301ad6c596ec014386c9", "d62bcde418144411068d5b09952090962fbc05f6", "ca1d23be869380ac9e900578c601c2d1febcc0c9", "00aacd0fd799bea4925da1a8ac6dbad0106422e9"]}, {"date": "2013", "abstract": "Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-the-art and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.", "authors": ["Pierre Sermanet", "Koray Kavukcuoglu", "Soumith Chintala", "Yann LeCun"], "id": "a1306ce652f556fbb9e794d91084a29294298e6d", "title": "Pedestrian Detection with Unsupervised Multi-stage Feature Learning", "references": ["8b25a44f617c1ed3ed52c6655b0d456ff1c565bd", "5b4b43f10c5779d67ccee15d8d0be10ed036971b", "7fe1a8ca95b63f5c5d60f929c5822bfa7d5ac8e5", "78cea77517dcc8e0f3a4c28ec4d4606ca5b20af5", "932c2a02d462abd75af018125413b1ceaa1ee3f4", "9ab0de951cc9cdf16887b1f841f8da6affc9c0de", "34e0ba2daabfa4d3d22913ade8265aff50b5f917", "fdaf2792b841fd5eced16cef9d77cc3197cb3bf0", "0a072cbdee54b83c8df43a431065f009d2cd2e70", "e79272fe3d65197100eae8be9fec6469107969ae"]}, {"date": "2007", "abstract": "Convolutional networks have achieved a great deal of success in high-level vision problems such as object recognition. Here we show that they can also be used as a general method for low-level image processing. As an example of our approach, convolutional networks are trained using gradient learning to solve the problem of restoring noisy or degraded images. For our training data, we have used electron microscopic images of neural circuitry with ground truth restorations provided by human experts. On this dataset, Markov random field (MRF), conditional random field (CRF), and anisotropic diffusion algorithms perform about the same as simple thresholding, but superior performance is obtained with a convolutional network containing over 34,000 adjustable parameters. When restored by this convolutional network, the images are clean enough to be used for segmentation, whereas the other approaches fail in this respect. We do not believe that convolutional networks are fundamentally superior to MRFs as a representation for image processing algorithms. On the contrary, the two approaches are closely related. But in practice, it is possible to train complex convolutional networks, while even simple MRF models are hindered by problems with Bayesian learning and inference procedures. Our results suggest that high model complexity is the single most important factor for good performance, and this is possible with convolutional networks.", "authors": ["Viren Jain", "Joseph F. Murray", "Fabian Roth", "Srinivas C. Turaga", "Valentin P. Zhigulin", "Kevin L. Briggman", "Moritz Helmstaedter", "Winfried Denk", "H. Sebastian Seung"], "id": "f566b1f24e63151ddae652826638af054973a27f", "title": "Supervised Learning of Image Restoration with Convolutional Networks", "references": ["496c3d75b81b336411e53da1ac632a8139655604", "a8e8f3c8d4418c8d62e306538c9c1292635e9d27", "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657", "ed91ce023b6500c586802de7d23d8f8f01e5aa1b", "b9701ad65e256bd8841c4f80ced09b4ca1d5e331", "6c62fdf1e6a520d9fee8ca9981fb588d07f2c6fa", "dd5061631a4d11fa394f4421700ebf7e78dcbc59", "1c71164b88a016516d99be72ebf49056daea842a", "3120324069ec20eed853d3f9bbbceb32e4173b93", "133809cf62bf67f0a63b35e5ef5180d20c9aec19"]}, {"date": "2011", "abstract": "This paper introduces a new neural network language model (NNLM) based on word clustering to structure the output vocabulary: Structured Output Layer NNLM. This model is able to handle vocabularies of arbitrary size, hence dispensing with the design of short-lists that are commonly used in NNLMs. Several softmax layers replace the standard output layer in this model. The output structure depends on the word clustering which uses the continuous word representation induced by a NNLM. The GALE Mandarin data was used to carry out the speech-to-text experiments and evaluate the NNLMs. On this data the well tuned baseline system has a character error rate under 10%. Our model achieves consistent improvements over the combination of an n-gram model and classical short-list NNLMs both in terms of perplexity and recognition accuracy.", "authors": ["Hai Son Le", "Ilya Oparin", "Alexandre Allauzen", "Jean-Luc Gauvain", "Fran\u00e7ois Yvon"], "id": "3aaa1e4974800767fcbd2c24c2f2af42bf412f97", "title": "Structured Output Layer neural network language model", "references": ["a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "a5f8135cf356a80e13241b5b36a5836eaad85fd1", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "e41498c05d4c68e4750fb84a380317a112d97b01", "6769e78616bc3a332a1829d1a3c2220e5b94555d", "47e3d8a1f8e92923e739ca34bea17004a40514e9", "9819b600a828a57e1cde047bbe710d3446b30da5", "ed23c461535afc492e80c63ee8d1ed55b8a176e1", "84b533115bfe031aaef4722bae8f54e1a39db01e", "3de5d40b60742e3dfa86b19e7f660962298492af"]}, {"date": "2006", "abstract": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting \"spatial pyramid\" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba\u2019s \"gist\" and Lowe\u2019s SIFT descriptors.", "authors": ["Svetlana Lazebnik", "Cordelia Schmid", "Jean Ponce"], "id": "6dbaff29d3898cf60f63f5a34cb9610ebb75220c", "title": "Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories", "references": ["bd232cf2ab28cc0ba06942875f14206f04ebbae0", "a737c107623bcffefa0bac20f1b64677f6a1255a", "12c7fc38debaf3589e712973642246bd54fe63b3", "35e855b0c1af4cc7bf62a8eb459c949776fbe7ee", "fc3098cff5469c55c3e81dc127563afe6dbadf22", "ba6417baed41a8f0fd4cab342aa214704389dcf9", "0f45a46dedadf599c12874b22645d596205ed8d5", "869171b2f56cfeaa9b81b2626cb4956fea590a57", "ee3391a1c0288ccf150b7d4c883918cfedb655bc", "0a32f6f23c05827e466580647467a322b7db9f7d"]}, {"date": "2016", "abstract": "This paper presents a novel approach to recurrent neural network (RNN) regularization. Differently from the widely adopted dropout method, which is applied to \\textit{forward} connections of feed-forward architectures or RNNs, we propose to drop neurons directly in \\textit{recurrent} connections in a way that does not cause loss of long-term memory. Our approach is as easy to implement and apply as the regular feed-forward dropout and we demonstrate its effectiveness for Long Short-Term Memory network, the most popular type of RNN cells. Our experiments on NLP benchmarks show consistent improvements even when combined with conventional feed-forward dropout.", "authors": ["Stanislau Semeniuta", "Aliaksei Severyn", "Erhardt Barth"], "id": "cf76789618f5db929393c1187514ce6c3502c3cd", "title": "Recurrent Dropout without Memory Loss", "references": ["f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "9665247ea3421929f9b6ad721f139f11edb1dbb8", "84069287da0a6b488b8c933f3cb5be759cb6237e", "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "d46b81707786d18499f911b4ab72bb10c65406ba", "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652", "a946fa85ade4306d8912c75ed0186a83404ada0a", "c0b624c46b51920dfec5aa02cc86323c0beb0df5", "4a5e68033940785468cbe34bd6249106c311acfb", "40be3888daa5c2e5af4d36ae22f690bcc8caf600"]}, {"date": "2004", "abstract": "This paper examines whether temporal difference methods for training connectionist networks, such as Sutton's TD(\u03bb) algorithm, can be successfully applied to complex real-world problems. A number of important practical issues are identified and discussed from a general theoretical perspective. These practical issues are then examined in the context of a case study in which TD(\u03bb) is applied to learning the game of backgammon from the outcome of self-play. This is apparently the first application of this algorithm to a complex non-trivial task. It is found that, with zero knowledge built in, the network is able to learn from scratch to play the entire game at a fairly strong intermediate level of performance, which is clearly better than conventional commercial programs, and which in fact surpasses comparable networks trained on a massive human expert data set. This indicates that TD learning may work better in practice than one would expect based on current theory, and it suggests that further analysis of TD methods, as well as applications in other complex domains, may be worth investigating.", "authors": ["Gerald Tesauro"], "id": "646ff15fbd38f8c4e2b099ad09e4570179709c73", "title": "Practical issues in temporal difference learning", "references": ["e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772", "fb5fcf491219f24e91b128ac81543aaea782c6c8", "c606d130b2a6c653f2e84047d79201a820b7ab56", "97f2c9832f1b32c7f49f68b24bfb6d760ff2a6b8", "6ce57ab17fcd507b856a79874063b59555c76b3a", "61f27eca9a47174ca3b4b882503793d1c8bcda7d", "9418fbe5656d5b5f18a9b3bc4a9c427da97fd438", "8a8aea51f5a911e0964d51ac764dc04d5900b7b7", "8a7acaf6469c06ae5876d92f013184db5897bb13", "0b5dbf114ca38a9bc73bf575c5763275941ceddc"]}, {"date": "2016", "abstract": "Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very long-term dependencies.", "authors": ["Mart\u00edn Arjovsky", "Amar Shah", "Yoshua Bengio"], "id": "e9c771197a6564762754e48c1daafb066f449f2e", "title": "Unitary Evolution Recurrent Neural Networks", "references": ["d0be39ee052d246ae99c082a565aba25b811be2d", "0d6203718c15f137fda2f295c96269bc2b254644", "84069287da0a6b488b8c933f3cb5be759cb6237e", "d46b81707786d18499f911b4ab72bb10c65406ba", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "bc1022b031dc6c7019696492e8116598097a8c12", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "99c970348b8f70ce23d6641e201904ea49266b6e", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"]}, {"date": "2012", "abstract": "Recently, we developed context-dependent deep neural network (DNN) hidden Markov models for large vocabulary speech recognition. While reducing errors by 33% compared to its discriminatively trained Gaussian-mixture counterpart on the switchboard benchmark task, DNN requires much more parameters. In this paper, we report our recent work on DNN for improved generalization, model size, and computation speed by exploiting parameter sparseness. We formulate the goal of enforcing sparseness as soft regularization and convex constraint optimization problems, and propose solutions under the stochastic gradient ascent setting. We also propose novel data structures to exploit the random sparseness patterns to reduce model size and computation time. The proposed solutions have been evaluated on the voice-search and switchboard datasets. They have decreased the number of nonzero connections to one third while reducing the error rate by 0.2-0.3% over the fully connected model on both datasets. The nonzero connections have been further reduced to only 12% and 19% on the two respective datasets without sacrificing speech recognition performance. Under these conditions we can reduce the model size to 18% and 29%, and computation to 14% and 23%, respectively, on these two datasets.", "authors": ["Dong Yu", "Frank Seide", "Gang Li", "Li Deng"], "id": "1c2c11e60755fc1df95f57edc37875e2b43cd946", "title": "Exploiting sparseness in deep neural networks for large vocabulary speech recognition", "references": ["46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "a42954d4b9d0ccdf1036e0af46d87a01b94c3516", "7599dfed1de67c726f9e4fd372cc9ef03d2cf3e9", "9b2e2c520ecb48be9dfe1594bcd0d93fd5a6c339", "6658bbf68995731b2083195054ff45b4eca38b3a", "473f0739666af2791ad6592822118240ed968b70", "a08c99425ad94eed67d059813511fe9ca55e73eb", "e7297db245c3feb1897720b173a59fe7e36babb7", "eec1451c8964d6e5311ef63c4e2228188e7cdccd", "607802a4067cb7738bac85d3ca3386f859e637b9"]}, {"date": "1994", "abstract": "Hinton [6] proposed that generalization in artificial neural nets should improve if nets learn to represent the domain's underlying regularities. Abu-Mustafa's hints work [1] shows that the outputs of a backprop net can be used as inputs through which domain-specific information can be given to the net. We extend these ideas by showing that a backprop net learning many related tasks at the same time can use these tasks as inductive bias for each other and thus learn better. We identify five mechanisms by which multitask backprop improves generalization and give empirical evidence that multitask backprop generalizes better in real domains.", "authors": ["Rich Caruana"], "id": "210da45e57f86a50c04bdd7b37d498c8ecc288da", "title": "Learning Many Related Tasks at the Same Time with Backpropagation", "references": ["77dcffce3fb186fb971ac48a3bd2a9cc364e4a07", "dfe7dbbd6e8d5d3720f58c2c9a0b9bec040a8ef8", "ed0c5a44d2e1f9a3a284ee0e6bcff4978c8654bc", "052f4d936ceaccbce8d7a3ad2449fb7d7676eb0c", "406033f22b6a671b94bcbdfaf63070b7ce6f3e48", "c67f0f00eaab360db1b9bd377e783c27c922dc86", "e3cd36c092abd65d6ac8e648f3468eeee90ee1fc", "9cf3250f13d98bd2bd1b23ff81a333119f82ccc3", "bdc3d618db015b2f17cd76224a942bfdfc36dc73"]}, {"date": "1993", "abstract": "Adaptive generalisation is the ability to use prior knowledge in the performance of novel tasks. Thus, if we are to model intelligent behaviour with neural nets, they must be able to generalise across task domains. Our objective is to elucidate the aetiology of transfer of information between connectionist nets. First, a method is described that provides a standardised score for the quantification of how much task structure a net has extracted, and to what degree knowledge has been transferred between tasks. This method is then applied in three simulation studies to examine Input-to-Hidden (IH) and Hidden-to-Output (HO) decision hyperplanes as determinants of transfer effects. In the first study, positive transfer is demonstrated between functions that require the vertices of their spaces to be divided similarly, and negative transfer between functions that require decision regions of different shapes. In the other two studies, input and output similarity are varied independently in a series of paired associate learning tasks. Further explanation of transfer effects is provided through the use of a new technique that permits better visualisation of the entire computational space by showing both the relative position of inputs in Hidden Unit space, and the HO decision regions implemented by the set of weights.", "authors": ["Noel E. Sharkey", "Amanda J. C. Sharkey"], "id": "b67be1aee59478e2ef06b68e0c5aa1da601ac145", "title": "Adaptive generalisation", "references": ["c213af6582c0d518a6e8e14217611c733eeb1ef1", "4bd3fe9c4157d7407f0e3c7c643bf8aaea53faa1", "ff0a05a63787af381d7c489be55966e1e331642e", "c8407caffadc58222920151b845741841ed4d4b5", "4d11c652fc2e4c175b60b143b0b0a59c13bdf9f0"]}, {"date": "2007", "abstract": "One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), reasoning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, with minimal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally limited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very inefficient in terms of required number of computational elements and examples. Second, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learning) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more abstract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence.", "authors": ["Yoshua Bengio", "Yann LeCun"], "id": "6fdb77260fc83dff91c44fea0f31a2cb8ed13d04", "title": "Scaling learning algorithms towards AI", "references": ["162d958ff885f1462aeda91cd72582323fd6a1f4", "125842668eab7decac136db8a59d392dc5e4e395", "f354310098e09c1e1dc88758fca36767fd9d084d", "07b54bac0159028aed116dbdbc2b747f723e585e", "852c6f55f3df7c7118fd4165f25ce8dc1ceab0ab", "b7d471970467a99bec4bce34c7dba5ef6745ad06", "25ca8792a1c183f4dd88dcebcf8a54202b483bb0", "5562a56da3a96dae82add7de705e2bd841eb00fc", "9360e5ce9c98166bb179ad479a9d2919ff13d022", "8978cf7574ceb35f4c3096be768c7547b28a35d0"]}, {"date": "1990", "abstract": "Semantic Scholar extracted view of \"A time delay neural network character recognizer for a touch terminal\" by Isabelle Guyon et al.", "authors": ["Isabelle Guyon", "P. Albrecht", "Yann LeCun", "John S. Denker", "W. Hubbard"], "id": "2c25cb7cb7c0be41510efeaef5cb96dea339823f", "title": "A time delay neural network character recognizer for a touch terminal", "references": []}, {"date": "2010", "abstract": "Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the first-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonal-covariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), first introduced for modeling natural images, is a much more representationally efficient and powerful way of modeling the covariance structure of speech data. Every configuration of the precision units of the mcRBM specifies a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5%, which is superior to all published results on speaker-independent TIMIT to date.", "authors": ["George E. Dahl", "Marc'Aurelio Ranzato", "Abdel-rahman Mohamed", "Geoffrey E. Hinton"], "id": "90b63e917d5737b06357d50aa729619e933d9614", "title": "Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine", "references": ["e084bbb9cbbce7c0d282df263cf70cba4042f067", "8f613e5481d8d567573b0ffa7b8e1c5b07d33a04", "e7c64258997838087c9ba4e87225627b015122b2", "3034afcd45fc190ed71982828b77f6e4154bdc5c", "28c322dc17c8d1dcf1a868f210e0276b288aaa34", "df5b82595a29724467a98eed4d7e2a45e804579e", "182c9ba291d97dc8d7482533044416869cb15f23", "f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "e7f56c88b49cd348cfcc5b34c9792ed2d968fb36", "8f72052f0d67a63756b79fe12d7e36ad338b616c"]}, {"date": "2009", "abstract": "Acoustic models used in hidden Markov model/neural-network (HMM/NN) speech recognition systems are usually trained with a frame-based cross-entropy error criterion. In contrast, Gaussian mixture HMM systems are discriminatively trained using sequence-based criteria, such as minimum phone error or maximum mutual information, that are more directly related to speech recognition accuracy. This paper demonstrates that neural-network acoustic models can be trained with sequence classification criteria using exactly the same lattice-based methods that have been developed for Gaussian mixture HMMs, and that using a sequence classification criterion in training leads to considerably better performance. A neural network acoustic model with 153K weights trained on 50 hours of broadcast news has a word error rate of 34.0% on the rt04 English broadcast news test set. When this model is trained with the state-level minimum Bayes risk criterion, the rt04 word error rate is 27.7%.", "authors": ["Brian Kingsbury"], "id": "2443dc59cf3d6cc1deba6d3220d61664b1a7eada", "title": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling", "references": ["b6c94cc324f585bd6c004f2b99b5589568643e45", "acf4e90062ca28e12f9e3a8c8b117030469d3e4b", "ce30f4e9c5be7be945f22e1d0ed7cd625331fe0f", "2a753eddc1533077e5bcfaf05b79f5fe245c51d8", "0509bf552a0d1fe895c019e4e8f1b1599c7112e4", "de8ceb72bf54293959813c101c4f7ce54fbd3a20", "0687573a482d84385ddd55e708e240f3e303fab9", "5e9082caea65c76bfd23b8763872804473ee7872", "d531dc3d20d35b69b9962c2bedeb60fda89e8a72", "067120574d64e37be5fa66591a6d0115d9a6d561"]}, {"date": "1993", "abstract": "Highly structured artificial neural networks can be optimized in many ways, and must be optimized for optimal performance. A highly structured approach is the multistate time delay neural network (MSTDNN) which uses shifted input windows and allows the recognition of sequences of ordered events that have to be observed jointly. An automatic structure optimization (ASO) algorithm is proposed and applied to MSTDNN-type networks. The ASO algorithm optimizes all relevant parameters of MSTDNNs automatically and is successfully tested with three different tasks and varying amounts of training data.<<ETX>>", "authors": ["Ulrich Bodenhausen", "Alex H. Waibel"], "id": "c7bfd3dff5f6f0c541bf334b53f63577056c951d", "title": "Application oriented automatic structuring of time-delay neural networks for high performance character and speech recognition", "references": ["18f355d7ef4aa9f82bf5c00f84e46714efa5fd77", "28b0563fcd3364077dfc39f42c9684ec00dcd249", "25406e6733a698bfc4ac836f8e74f458e75dad4f", "cd62c9976534a6a2096a38244f6cbb03635a127e", "7e0dab4fe4299bc2f8b4b18f82702af717cf3924", "a34e35dbbc6911fa7b94894dffdc0076a261b6f0", "e7297db245c3feb1897720b173a59fe7e36babb7"]}, {"date": "1996", "abstract": "The present in this paper the architecture and implementation of the Virtual Image Processor (VIP) which is an SIMD multiprocessor build with large FPGAs. The SIMD architecture, together with a 2D torus connection topology, is well suited for image processing, pattern recognition and neural network algorithms. The VIP board can be programmed on-line at the logic level, allowing optimal hardware dedication to any given algorithm.", "authors": ["Jocelyn Cloutier", "Eric Cosatto", "Steven Pigeon", "Fran\u00e7ois R. Boyer", "Patrice Y. Simard"], "id": "0a1c0303d0b9643c717ddedff06caa97c7a29933", "title": "VIP: an FPGA-based processor for image processing and neural networks", "references": ["1aa23bb997804153a2ae4d3a7d903e23ec68bcb2", "b8bc656a1935f07e894833b608cc4671b9fa828f", "fbdf08fb10d92518b7c0571e1ae907de2c20ff76"]}, {"date": "2003", "abstract": "We consider situations where training data is abundant and computing resources are comparatively scarce. We argue that suitably designed online learning algorithms asymptotically outperform any batch learning algorithm. Both theoretical and experimental evidences are presented.", "authors": ["L\u00e9on Bottou", "Yann LeCun"], "id": "133809cf62bf67f0a63b35e5ef5180d20c9aec19", "title": "Large Scale Online Learning", "references": ["ded14685a23df94d93e8662578d4132c9f4aa1c7", "3380f30e85577f67f7e178b70bf9f120ec16a3bc", "fc6b1ff29f2da985cccfa644652bb320d7720d59", "5a767a341364de1f75bea85e0b12ba7d3586a461", "233e47fb2f97ec8f587c62c3f84c7971fae8e568"]}, {"date": "1989", "abstract": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%. >", "authors": ["Alexander H. Waibel", "Toshiyuki Hanazawa", "Geoffrey E. Hinton", "Kiyohiro Shikano", "Kevin J. Lang"], "id": "cd62c9976534a6a2096a38244f6cbb03635a127e", "title": "Phoneme recognition using time-delay neural networks", "references": ["df53e0dc66eb13bb51c6e4803ceae56d3ebe6f23", "853994ab9227c6c682e4494d1a8b2e065f876d1a", "9049fcce47379e88cbba32b1ccbefd7f7c7545f7", "1aa31d5deb45f477a6de45b3b75b62c7f4a213e7", "cab01ddd19b011e184845300d4bb43c82e082a4c", "b8fe93d3e5205a450fdd8a9fb94cea0ab73b067f", "87d79c0c5255bce9dacaf4dab07d00c682200f2e", "1be23f406a1ee7b6a45b7f98be17e7f562bdd48b", "c86590e947c28e8791d1e8bab8fc8ab53302341f", "06486d05481b20a115f6bf01a5630abfb438ce9c"]}, {"date": "1989", "abstract": "Abstract A class of connectionist networks is described that has learned to play backgammon at an intermediate-to-advanced level. The networks were trained by back-propagation learning on a large set of sample positions evaluated by a human expert. In actual match play against humans and conventional computer programs, the networks have demonstrated substantial ability to generalize on the basis of expert knowledge of the game. This is possibly the most complex domain yet studied with connectionist learning. New techniques were needed to overcome problems due to the scale and complexity of the task. These include techniques for intelligent design of training set examples and efficient coding schemes, and procedures for escaping from local minima. We suggest how these techniques might be used in applications of network learning to general large-scale, difficult \u201creal-world\u201d problem domains.", "authors": ["Gerald Tesauro", "Terrence J. Sejnowski"], "id": "0b5dbf114ca38a9bc73bf575c5763275941ceddc", "title": "A Parallel Network that Learns to Play Backgammon", "references": ["1ec0e85eaf619fdb21ef3d41e234828b7d1a1230", "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657", "e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772", "de996c32045df6f7b404dda2a753b6a9becf3c08", "a57c6d627ffc667ae3547073876c35d6420accff", "58821c2fde1ec9f42feda075d5e034379870a7a7", "ed6f076369825e36254c2554824544f840663768", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "6ce57ab17fcd507b856a79874063b59555c76b3a", "948e0898db672a86841b10567afe3a15355531b1"]}, {"date": "2001", "abstract": "This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the \"integral image\" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a \"cascade\" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.", "authors": ["Paul A. Viola", "Michael J. Jones"], "id": "dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63", "title": "Rapid object detection using a boosted cascade of simple features", "references": ["ccf5208521cb8c35f50ee8873df89294b8ed7292", "76f560991d56ad689ec32f9e9d13291e0193f4cf", "9f7c640ea1fe32e017c68005ef5e18969039b3f4", "2c7349eed0d13a1740286196db7bc83f40a88e85", "8b22e1751f75be137b7b210981baccc1b9ab9222", "9008cdacbdcff8a218a6928e94fe7c6dfc237b24", "8930f62a4b5eb1cbabf224cf84aa009ea798cfee", "4d19272112b50547614479a0c409fca66e3b05f7", "9b535f4edc4cbf8d4fb6182ec6b5c54db3c1cccb", "3565c5a65842f26091578b9d71d496cc1561239d"]}, {"date": "2008", "abstract": "Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non-hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.", "authors": ["Andriy Mnih", "Geoffrey E. Hinton"], "id": "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "title": "A Scalable Hierarchical Distributed Language Model", "references": ["3d6036af971c1f11ab712cc41487376a94e63673", "bf32a271a17c9c3376127d287f746e4876779d49", "e41498c05d4c68e4750fb84a380317a112d97b01", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "bd7d93193aad6c4b71cc8942e808753019e87706", "d4e8bed3b50a035e1eabad614fe4218a34b3b178", "09c76da2361d46689825c4efc37ad862347ca577", "3de5d40b60742e3dfa86b19e7f660962298492af"]}, {"date": "2015", "abstract": "Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.", "authors": ["Andrej Karpathy", "Johanna E. Johnson", "Li Fei-Fei"], "id": "40be3888daa5c2e5af4d36ae22f690bcc8caf600", "title": "Visualizing and Understanding Recurrent Networks", "references": ["d38e8631bba0720becdaf7b89f79d9f9dca45d82", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081", "adfcf065e15fd3bc9badf6145034c84dfb08f204", "cea967b59209c6be22829699f05b8b1ac4dc092d", "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "533ee188324b833e059cb59b654e6160776d5812", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "5b8364c21155d3d2cd38ea4c8b8580beba9a3250", "b0b33aaed1d408d04fadf9ff2a080e47ef8cb7b1", "71ae756c75ac89e2d731c9c79649562b5768ff39"]}, {"date": "1987", "abstract": "Results are presented the demonstrate the learning and fine-tuning of search strategies using connectionist mechanisms. Previous studies of strategy learning within the symbolic, production-rule formalism have not addressed fine-tuning behavior. Here a two-layer connectionist system is presented that develops its search from a weak to a task-specific strategy and fine-tunes its performance. The system is applied to a simulated, real-time, balance-control task. We compare the performance of one-layer and two-layer networks, showing that the ability of the two-layer network to discover new features and thus enhance the original representation is critical to solving the balancing task.", "authors": ["Charles W. Anderson"], "id": "8a8aea51f5a911e0964d51ac764dc04d5900b7b7", "title": "Strategy Learning with Multilayer Connectionist Representations", "references": []}, {"date": "1983", "abstract": "It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.", "authors": ["Andrew G. Barto", "Richard S. Sutton", "Charles W. Anderson"], "id": "8a7acaf6469c06ae5876d92f013184db5897bb13", "title": "Neuronlike adaptive elements that can solve difficult learning control problems", "references": ["3c6f34131ad83fda26a3d8ca9892a6705fd40d11", "e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772", "6587a531da06b9cef73e93b6b7627e466ad51d1b", "041db552562217841768ef4669d2d7f4430ab1c0", "b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b", "cacfb77e3dc8faa3cae25b3128f3b3c4c44fc266", "b419910ab2914fcb2cf75bc66708146125ae686c", "0c1accd2ef7218534a1726a8de7d6e7c14271a75", "0f2d0e9c57d268fc1d05ce657eaf64eaaeb323c7", "60944c5243db70a687a320a2622d3bd1610802a8"]}, {"date": "1991", "abstract": "The authors discuss how a methodology called problem decomposition can be applied to an AP-net, a neural network for mapping acoustic spectra to phoneme classes. The network's task is to recognize phonemes from a large corpus of multiple-speaker, continuously spoken sentences. The authors review previous AP-net systems and present results from a decomposition study in which smaller networks trained to recognize subsets of phonemes are combined into a larger network for the full signal-to-phoneme mapping tasks. It is shown that, by using this problem decomposition methodology, comparable performance can be obtained in significantly fewer arithmetic operations.<<ETX>>", "authors": ["Lorien Y. Pratt", "Candace A. Kamm"], "id": "4d11c652fc2e4c175b60b143b0b0a59c13bdf9f0", "title": "Improving a phoneme classification neural network through problem decomposition", "references": []}, {"date": "1999", "abstract": "In this paper we address the problem of minimizing a large class of energy functions that occur in early vision. The major restriction is that the energy function's smoothness term must only involve pairs of pixels. We propose two algorithms that use graph cuts to compute a local minimum even when very large moves are allowed. The first move we consider is an /spl alpha/-/spl beta/-swap: for a pair of labels /spl alpha/,/spl beta/, this move exchanges the labels between an arbitrary set of pixels labeled a and another arbitrary set labeled /spl beta/. Our first algorithm generates a labeling such that there is no swap move that decreases the energy. The second move we consider is an /spl alpha/-expansion: for a label a, this move assigns an arbitrary set of pixels the label /spl alpha/. Our second algorithm, which requires the smoothness term to be a metric, generates a labeling such that there is no expansion move that decreases the energy. Moreover, this solution is within a known factor of the global minimum. We experimentally demonstrate the effectiveness of our approach on image restoration, stereo and motion.", "authors": ["Yuri Boykov", "Olga Veksler", "Ramin Zabih"], "id": "3120324069ec20eed853d3f9bbbceb32e4173b93", "title": "Fast approximate energy minimization via graph cuts", "references": ["d1a636654dea9109946c06da03735d87c2c561f9", "4fc956c8d8b41c1e9c499cad0cf882debf45ca64", "006328c8add2ce30c186048c89097560d2661c27", "6c62fdf1e6a520d9fee8ca9981fb588d07f2c6fa", "a73f23484f890fafff6dd1e79ae25b33de1e666b", "4ec28212d571da349ac886039f8dcd2243c8d57c", "8c3e33fb1d7ffae33d4c97d4cc82e002d772e0a5", "d0794c1a57cad9c35028427e6c084642346c720f", "23844026bf30e29e2edf6f00dd1f167260d4db1e", "c5634533a7daecde097a230484df5f0913dd9410"]}, {"date": "1987", "abstract": "Semantic Scholar extracted view of \"A Mean Field Theory Learning Algorithm for Neural Networks\" by Carsten Peterson et al.", "authors": ["Carsten Peterson", "James R. Anderson"], "id": "607802a4067cb7738bac85d3ca3386f859e637b9", "title": "A Mean Field Theory Learning Algorithm for Neural Networks", "references": []}, {"date": "1942", "abstract": "Semantic Scholar extracted view of \"Interference with recall of original responses after learning new responses to old stimuli.\" by B. R. Bugelski", "authors": ["B. R. Bugelski"], "id": "ff0a05a63787af381d7c489be55966e1e331642e", "title": "Interference with recall of original responses after learning new responses to old stimuli.", "references": []}, {"date": "1989", "abstract": "Publisher Summary Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.", "authors": ["Michael McCloskey", "Neal J. Cohen"], "id": "c213af6582c0d518a6e8e14217611c733eeb1ef1", "title": "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem", "references": []}, {"date": "2007", "abstract": "We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces \u201cstroke detectors\u201d when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps", "authors": ["Bernhard Sch\u00f6lkopf", "John Platt", "Thomas Hofmann"], "id": "25ca8792a1c183f4dd88dcebcf8a54202b483bb0", "title": "Efficient Learning of Sparse Representations with an Energy-Based Model", "references": []}, {"date": "1987", "abstract": "We st ud y th e amount of ti me needed to learn a fixed t rain\u00ad ing se t in the \"back-pro pagation\" proced ure for learning in multi-layer ne ural network models. The task chosen was 32-bit parity, a hi gh\u00ad order fu nct ion for wh ich memor iza ti on o f specific in p u t- out put pairs is necessary. For small t raining sets, the learning time is consistent with a ~-power law depen dence on the nu mber of patterns in the t ra ining set. For lar ger training set s, t he learn ing t ime dive rges at a critical t ra ining set size which appears to be related to the st orage capacity of t he network.", "authors": ["Gerald Tesauro"], "id": "948e0898db672a86841b10567afe3a15355531b1", "title": "Scaling Relationships in Back-Propagation Learning: Dependence on Training Set Size", "references": ["a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657", "b300e1bbb6ad0d513db2eeb64a2508b4fafb9da6", "a26cd182756bc8bd29c3f02d2faa1db471c83d1d", "58821c2fde1ec9f42feda075d5e034379870a7a7", "0120eefaf05bfad5293e87f56d2e787c05f78cf7", "ff2c2e3e83d1e8828695484728393c76ee07a101", "8592e46a5435d18bba70557846f47290b34c1aa5", "7dd89edfc437069ce6a2bd98559cec2eeb62650e", "0b5dbf114ca38a9bc73bf575c5763275941ceddc", "111fd833a4ae576cfdbb27d87d2f8fc0640af355"]}, {"date": "1994", "abstract": "Two NET32K neural-network chips are integrated on a board system with an SBus interface, to serve as a high speed image analysis platform. The system is optimized for convolutional networks. Up to 64 Kernels of size 16/spl times/16 pixels are scanned simultaneously over an image. In this way, simple geometric shapes are extracted from an image, representing its content in a compact form. A standard processor can then do the high level interpretation. To prevent I/O bottlenecks between board and host, several high speed programmable logic devices are included on the board to implement tapped delay lines and compression/decompression algorithms. The board can process 20 frames per second, achieving over 100 GC/s (billion connections per second). The SBus interface makes it possible to directly \"plug\" the board into a SUN Sparcstation, providing a compact and low cost solution for complex image analysis tasks. Several document processing applications are described.", "authors": ["Eric Cosatto", "H. P. Graf"], "id": "1aa23bb997804153a2ae4d3a7d903e23ec68bcb2", "title": "NET32K high speed image understanding system", "references": ["4b0305d9dd44892b1faedc71013cb78de97caf79", "03d42bfef1926122a3c1bed035009fb1aeb2bedc"]}, {"date": "1978", "abstract": "Abstract : A systolic system is a network of processors which rhythmically compute and pass data through the system. Physiologists use the work 'systole' to refer to the rhythmically recurrent contraction of the heart and arteries which pulses blood through the body. In a systolic computing system, the function of a processor is analogous to that of the heart. Every processor regularly pumps data in and out, each time performing some short computation, so that a regular flow of data is kept up in the network. Many basic matrix computations can be pipelined elegantly and efficiently on systolic networks having an array structure. As an example, hexagonally connected processors can optimally perform matrix multiplication. Surprisingly, a similar systolic array can compute the LU-decomposition of a matrix. These systolic arrays enjoy simple and regular communication paths, and almost all processors used in the networks are identical. As a result, special purpose hardware devices based on systolic arrays can be built inexpensively using the VLSI technology. (Author)", "authors": ["H. T. Kung", "Charles E. Leiserson"], "id": "fbdf08fb10d92518b7c0571e1ae907de2c20ff76", "title": "Systolic Arrays for (VLSI).", "references": []}, {"date": "", "abstract": "COL. LYNCH'S complaint of ill-usage to his book in the review in NATURE amounts to a charge that the reviewer has failed to appreciate the originality and the scientific importance of the author's system of psychology. This charge is true. All I can do is to assure your readers that I wrote without consciousness of prejudice, and only after a thoughtful reading of the book and sincere attempt to discover the author's meaning. I respect the author and had no intention of giving offence.", "authors": ["James Earle Deese"], "id": "c8407caffadc58222920151b845741841ed4d4b5", "title": "Principles of Psychology", "references": []}, {"date": "1999", "abstract": "Abstract Learning is a flexible and effective means of extracting the stochastic structure of the environment. It provides an effective method for blind separation and deconvolution in signal processing. Two different types of learning are used, namely batch learning and on-line learning. The batch learning procedure uses all the training examples repeatedly so that its performance is compared to the statistical estimation procedure. On-line learning is more dynamical, updating the current estimate by observing a new datum one by one. On-line learning is slow in general but works well in the changing environment. The present paper gives a unified framework of statistical analysis for batch and on-line learning. The topics include the asymptotic learning curve, generalization error and training error, over-fitting and over-training, efficiency of learning, and an adaptive method of determining learning rate.", "authors": ["Noboru Murata", "Shun-ichi Amari"], "id": "3380f30e85577f67f7e178b70bf9f120ec16a3bc", "title": "Statistical analysis of learning dynamics", "references": ["882b6899d07ce3498a04195885150ef87c02d1c3", "b700d6ccb942bceb6fe8c071e8ca95e9bba12422", "30fa4c9ecaeebcda60c5bf7d48c8d58d4aa85497", "656a33c1db546da8490d6eba259e2a849d73a001", "2498a4e1755f047accc06a6e0fab0b0eb1b37ae0", "99851c4909057223bf0e161e4e1724dfef4d16c8", "fad05ed37159201af2212f239b3d2a2bfb262f75", "7e0dab4fe4299bc2f8b4b18f82702af717cf3924", "b7a64bc8947a1889e8f827917e3bca5babc4369f", "58df58ec0ebe600f0e15d8595785415134ca3c79"]}, {"date": "1992", "abstract": "Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals.", "authors": ["Stuart Geman", "Elie Bienenstock", "Ren\u00e9 Doursat"], "id": "a34e35dbbc6911fa7b94894dffdc0076a261b6f0", "title": "Neural Networks and the Bias/Variance Dilemma", "references": ["c8e8b54f87f43c4a6f85695712dff55e0edec760", "8b7d60f49e7a5368920457c885c813a912c34997", "77fdd39ab366b65a617015a72fe8dc9d0b394d64", "656a33c1db546da8490d6eba259e2a849d73a001", "4abd4e51705e74f1739bd3a1e47ac10e45f6468b", "8da1dda34ecc96263102181448c94ec7d645d085", "b10440620da8a43a1b97e3da4b1ff13746306475", "9552ac39a57daacf3d75865a268935b5a0df9bbb", "72d761afbe35634213849419ff63fad5bc9fabeb", "6f3175b3930d0c71495a52a7bccb3889e5f33520"]}, {"date": "2000", "abstract": "In this paper, we describe a statistical method for 3D object detection. We represent the statistics of both object appearance and \"non-object\" appearance using a product of histograms. Each histogram represents the joint statistics of a subset of wavelet coefficients and their position on the object. Our approach is to use many such histograms representing a wide variety of visual attributes. Using this method, we have developed the first algorithm that can reliably detect human faces with out-of-plane rotation and the first algorithm that can reliably detect passenger cars over a wide range of viewpoints.", "authors": ["Henry Schneiderman", "Takeo Kanade"], "id": "3565c5a65842f26091578b9d71d496cc1561239d", "title": "A statistical method for 3D object detection applied to faces and cars", "references": ["ccf5208521cb8c35f50ee8873df89294b8ed7292", "14e53403a0055dbe5faaf9f1f3be96ca0e692a4d", "23234a0f211a44d9706b2570d474427b8f899ec1", "bbe488bb190d75f4b665d43e306bcab1ab228890", "088eb2d102c6bb486f5270d0b2adff76961994cf", "a2531a801a1df4e65f53794bb56b52718b6dc472"]}, {"date": "2013", "abstract": "Time series often have a temporal hierarchy, with information that is spread out over multiple time scales. Common recurrent neural networks, however, do not explicitly accommodate such a hierarchy, and most research on them has been focusing on training algorithms rather than on their basic architecture. In this pa- per we study the effect of a hierarchy of recurrent neural networks on processing time series. Here, each layer is a recurrent network which receives the hidden state of the previous layer as input. This architecture allows us to perform hi- erarchical processing on difficult temporal tasks, and more naturally capture the structure of time series. We show that they reach state-of-the-art performance for recurrent networks in character-level language modelling when trained with sim- ple stochastic gradient descent. We also offer an analysis of the different emergent time scales.", "authors": ["Michiel Hermans", "Benjamin Schrauwen"], "id": "b0b33aaed1d408d04fadf9ff2a080e47ef8cb7b1", "title": "Training and Analysing Deep Recurrent Neural Networks", "references": []}, {"date": "1995", "abstract": "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weightupdate Littlestone Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in R. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line. ] 1997 Academic Press", "authors": ["Yoav Freund", "Robert E. Schapire"], "id": "ccf5208521cb8c35f50ee8873df89294b8ed7292", "title": "A decision-theoretic generalization of on-line learning and an application to boosting", "references": ["e711e7084d3591c9f4de70213a89e5caabf388ee", "68c1bfe375dde46777fe1ac8f3636fb651e3f0f8", "a1dfeb731fc0c79e04523cd655413c223f6fa102", "f7103090b0bc74106025ac96b21b6ce5d1fb569c", "25406e6733a698bfc4ac836f8e74f458e75dad4f", "14658bf6b693af9f30920c70fad14563f6b0cc10", "888c09de60ce427669fe5a264fa3e787803eb9d2", "8facf54fbb5cc49d3df9674b3facca5d92acbdc0", "d221bbcbd20c7157e4500f942de8ceec490f8936", "ff027adc26fdbe6faa929e500f5fe3136077ffa7"]}, {"date": "2015", "abstract": "The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM's architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. \n \nIn this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thorough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM's forget gate closes the gap between the LSTM and the GRU.", "authors": ["Rafal J\u00f3zefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "id": "5b8364c21155d3d2cd38ea4c8b8580beba9a3250", "title": "An Empirical Exploration of Recurrent Network Architectures", "references": ["0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081", "adfcf065e15fd3bc9badf6145034c84dfb08f204", "d0be39ee052d246ae99c082a565aba25b811be2d", "aa7bfd2304201afbb19971ebde87b17e40242e91", "8c571314311f507731296b21b56ab2c326b97392", "9665247ea3421929f9b6ad721f139f11edb1dbb8", "0d6203718c15f137fda2f295c96269bc2b254644", "cea967b59209c6be22829699f05b8b1ac4dc092d"]}, {"date": "2014", "abstract": "In this paper, we explore different ways to extend a recurrent neural network (RNN) to a \\textit{deep} RNN. We start by arguing that the concept of depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, however, we find three points of an RNN which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. Based on this observation, we propose two novel architectures of a deep RNN which are orthogonal to an earlier attempt of stacking multiple recurrent layers to build a deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide an alternative interpretation of these deep RNNs using a novel framework based on neural operators. The proposed deep RNNs are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNNs benefit from the depth and outperform the conventional, shallow RNNs.", "authors": ["Razvan Pascanu", "\u00c7aglar G\u00fcl\u00e7ehre", "Kyunghyun Cho", "Yoshua Bengio"], "id": "533ee188324b833e059cb59b654e6160776d5812", "title": "How to Construct Deep Recurrent Neural Networks", "references": ["c12fefe264e42e77f1275ce56fb3e905347761a3", "aa7bfd2304201afbb19971ebde87b17e40242e91", "0d6203718c15f137fda2f295c96269bc2b254644", "1b3aab4ff8f77b81501f271877321609cc1a1a2b", "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "9819b600a828a57e1cde047bbe710d3446b30da5", "ba0c34d72dd42ef78c1f1514d92c2c22b9c0d454", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "b0b33aaed1d408d04fadf9ff2a080e47ef8cb7b1"]}, {"date": "2015", "abstract": "Abstract: We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.", "authors": ["Jason Weston", "Sumit Chopra", "Antoine Bordes"], "id": "71ae756c75ac89e2d731c9c79649562b5768ff39", "title": "Memory Networks", "references": ["2b776119a1347e1455dc498ff5078b3a94029ed9", "a584211768d49f80192f13b8ed2fda9c058dec34", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "564257469fa44cdb57e4272f85253efb9acfd69d", "9819b600a828a57e1cde047bbe710d3446b30da5", "c0be2ac2f45681f1852fc1d298af5dceb85834f4", "97cc2fab13bf20d130de19dfffe6670aac2076b5"]}, {"date": "1981", "abstract": "Many adaptive neural network theories are based on neuronlike adaptive elements that can behave as single unit analogs of associative conditioning. In this article we develop a similar adaptive element, but one which is more closely in accord with the facts of animal learning theory than elements commonly studied in adaptive network research. We suggest that an essential feature of classical conditioning that has been largely overlooked by adaptive network theorists is its predictive nature. The adaptive element we present learns to increase its response rate in anticipation of increased stimulation, producing a conditioned response before the occurrence of the unconditioned stimulus. The element also is in strong agreement with the behavioral data regarding the effects of stimulus context, since it is a temporally refined extension of the Rescorla-Wagner model. We show by computer simulation that the element becomes sensitive to the most reliable, nonredundant, and earliest predictors of reinforcement . We also point out that the model solves many of the stability and saturation problems encountered in network simulations. Finally, we discuss our model in light of recent advances in the physiology and biochemistry of synaptic mechanisms.", "authors": ["Richard S. Sutton", "Andrew G. Barto"], "id": "60944c5243db70a687a320a2622d3bd1610802a8", "title": "Toward a modern theory of adaptive networks: expectation and prediction.", "references": ["3c6f34131ad83fda26a3d8ca9892a6705fd40d11", "256d99537c3eef1766bce990c986f75d03acac7e", "11bbf037139cebcc34a24e0eda27705832847dd3", "504c53c667fb50a322b5bb0192920918c0cf8435", "afaf65883ff75cc19926f61f181a687927789ad1", "bd00186bd3d555163b2b30d519edd335e78abdb8", "dce624b5b88e11dcb94eca0cc20ae982214bb321", "32c9f75a7751a7bf47472ccc3cf2703b1cb132a3", "b5c4130e8abc9e070f427f4415460a7a293eaea2", "ac145edb50780eadaa53b4528fc9cd1fac0fa8c8"]}, {"date": "1986", "abstract": "Inverse problems, such as the reconstruction problems that arise in early vision, tend to be mathematically ill-posed. Through regularization, they may be reformulated as well-posed variational principles whose solutions are computable. Standard regularization theory employs quadratic stabilizing functionals that impose global smoothness constraints on possible solutions. Discontinuities present serious difficulties to standard regularization, however, since their reconstruction requires a precise spatial control over the smoothing properties of stabilizers. This paper proposes a general class of controlled-continuity stabilizers which provide the necessary control over smoothness. These nonquadratic stabilizing functionals comprise multiple generalized spline kernels combined with (noncontinuous) continuity control functions. In the context of computational vision, they may be thought of as controlled-continuity constraints. These generic constraints are applicable to visual reconstruction problems that involve both continuous regions and discontinuities, for which global smoothness constraints fail.", "authors": ["Demetri Terzopoulos"], "id": "23844026bf30e29e2edf6f00dd1f167260d4db1e", "title": "Regularization of Inverse Visual Problems Involving Discontinuities", "references": ["34236f79180ff2d8aff48498927c7780ec3cc17f", "508282d199f7aec41c29c1dea51ea4e7963d4695", "bb59464f5322bd122cd982aa98eafdeb6beae0d3", "503989518696578f2b4e389709ae5158d7388f39", "3dd870bdb3ba1886787266680475e2b24e322ce3", "5f75d859e750961d1d094f166fc3b564d9cfe99b", "1821de90c72c557f5bd874a4f7420c07d6220363", "daafb0ccc53b15973ae26992922dd22b1102628b", "8dde5183e612cc27d64efdc522f92f34a23ff849", "44924f2551a7db7c57e829b710576f14197eb76e"]}, {"date": "2011", "abstract": "Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence problems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF) by applying them to character-level language modeling tasks. The standard RNN architecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or \"gated\") connections which allow the current input character to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character-level language modeling \u2013 a hierarchical non-parametric sequence model. To our knowledge this represents the largest recurrent neural network application to date.", "authors": ["Ilya Sutskever", "James Martens", "Geoffrey E. Hinton"], "id": "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "title": "Generating Text with Recurrent Neural Networks", "references": ["a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "d0be39ee052d246ae99c082a565aba25b811be2d", "0d6203718c15f137fda2f295c96269bc2b254644", "893c8f527dadbaed9cc66aaf6d6f8a83c50722a1", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "c08d0525bd42fa1c24f9f5df72f4c8fcf7063b22", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "9819b600a828a57e1cde047bbe710d3446b30da5", "346fbcffe4237aa60e8bcb3d4294a8b99436f1d0", "c6629770cb6a00ad585918e71fe6dbad829ad0d1"]}, {"date": "1999", "abstract": "In a traditional classification problem, we wish to assign one of k labels (or classes) to each of n objects, in a way that is consistent with some observed data that we have about the problem. An active line of research in this area is concerned with classification when one has information about pairwise relationships among the objects to be classified; this issue is one of the principal motivations for the framework of Markov random fields, and it arises in areas such as image processing, biometry: and document analysis. In its most basic form, this style of analysis seeks a classification that optimizes a combinatorial function consisting of assignment costs-based on the individual choice of label we make for each object-and separation costs-based on the pair of choices we make for two \"related\" objects. We formulate a general classification problem of this type, the metric labeling problem; we show that it contains as special cases a number of standard classification frameworks, including several arising from the theory of Markov random fields. From the perspective of combinatorial optimization, our problem can be viewed as a substantial generalization of the multiway cut problem, and equivalent to a type of uncapacitated quadratic assignment problem. We provide the first non-trivial polynomial-time approximation algorithms for a general family of classification problems of this type. Our main result is an O(log k log log k)-approximation algorithm for the metric labeling problem, with respect to an arbitrary metric on a set of k labels, and an arbitrary weighted graph of relationships on a set of objects. For the special case in which the labels are endowed with the uniform metric-all distances are the same-our methods provide a 2-approximation.", "authors": ["Jon M. Kleinberg", "\u00c9va Tardos"], "id": "d0794c1a57cad9c35028427e6c084642346c720f", "title": "Approximation algorithms for classification problems with pairwise relationships: metric labeling and Markov random fields", "references": ["4c5820002582c302736ff6a370904ff685962e3a", "47865b56fee61d9c9ff477f7c79f090cc6663d3a", "d7d9408070d32cf75ad0e2aedd0a353c9cb07862", "a57520b68e73b5e1fc3668b443daf74ebe957cc7", "1cf64c2bdd4f1c384a55910606a64c8d831a96ba", "d131c0813e4ee72e3449236ccf4faa4a014c67dc", "b951b9f78b98a186ba259027996a48e4189d37e5", "3261bb81085f59efae1e1c72453c47daaee777ac", "6c9817b90cfc7cd78143f3749e602febd84d2a81", "b543d70d3e4fe6673a2e39832f005fa7ebc79cec"]}, {"date": "1990", "abstract": "A statistical framework is used for finding boundaries and for partitioning scenes into homogeneous regions. The model is a joint probability distribution for the array of pixel gray levels and an array of labels. In boundary finding, the labels are binary, zero, or one, representing the absence or presence of boundary elements. In partitioning, the label values are generic: two labels are the same when the corresponding scene locations are considered to belong to the same region. The distribution incorporates a measure of disparity between certain spatial features of block pairs of pixel gray levels, using the Kolmogorov-Smirnov nonparametric measures of difference between the distributions of these features. The number of model parameters is minimized by forbidding label configurations, which are assigned probability zero. The maximum a posteriori estimator of boundary placements and partitionings is examined. The forbidden states introduce constraints into the calculation of these configurations. Stochastic relaxation methods are extended to accommodate constrained optimization. >", "authors": ["Donald Geman", "Stuart Geman", "Christine Graffigne", "Ping Dong"], "id": "8c3e33fb1d7ffae33d4c97d4cc82e002d772e0a5", "title": "Boundary Detection by Constrained Optimization", "references": ["5ff8226b8961265fdc98e522d694a416503d6cfc", "fa3794165cfea264cfa171ee65abf1b47bfe8d48", "61008a07996a7daa84d4923de293bffc5f50f4d1", "47865b56fee61d9c9ff477f7c79f090cc6663d3a", "918383d7a7e2541fea21012bbea7399d0b9b5e96", "14e01d75aa673d0a4c6019877983fbbc5d1f875c", "4ed9722202cf3df0dfbec27123a117c283e59250", "0cce258708200c6d45f6e895ebee7395d1de4bf3", "865db314a5d86bb41aa84a5b8263526370367da1", "f738f054fd30d84137011cecc60ebafee1e56269"]}, {"date": "1995", "abstract": "From the Publisher: \nMarkov random field (MRF) theory provides a basis for modeling contextual constraints in visual processing and interpretation. It enables us to develop optimal vision algorithms systematically when used with optimization principles. This book presents a comprehensive study on the use of MRFs for solving computer vision problems. The book covers the following parts essential to the subject: introduction to fundamental theories, formulations of MRF vision models, MRF parameter estimation, and optimization algorithms. Various vision models are presented in a unified framework, including image restoration and reconstruction, edge and region segmentation, texture, stereo and motion, object matching and recognition, and pose estimation. This book is an excellent reference for researchers working in computer vision, image processing, statistical pattern recognition, and applications of MRFs. It is also suitable as a text for advanced courses in these areas.", "authors": ["Stan Z. Li"], "id": "c5634533a7daecde097a230484df5f0913dd9410", "title": "Markov Random Field Modeling in Computer Vision", "references": ["4018bb79e4ac87aee95400c0310f67b045cd1ffe", "ef7171535635556416275fc8bfdd8b868f78d1bc", "843beaaa56f6de56eff986f4c779ddedce874889", "fcfd56c6fc2a1632da285a0cbd1f657956b23cea", "a2e15a859d57f0803733d34beb4c5265e79cfa16", "9c992f5f70325a432663bd04feaca041d3823de3", "dc67a48a9bb2ca8037f47245667a67601d2c5366", "1df90c2c75882107e0a69959721ab1c41fcd9712", "515966ebc3adec9a7740526fbee997937aaa234f", "4c5820002582c302736ff6a370904ff685962e3a"]}, {"date": "1996", "abstract": "Describing a video sequence in terms of a small number of coherently moving segments is useful for tasks ranging from video compression to event perception. A promising approach is to view the motion segmentation problem in a mixture estimation framework. However, existing formulations generally use only the motion, data and thus fail to make use of static cues when segmenting the sequence. Furthermore, the number of models is either specified in advance or estimated outside the mixture model framework. In this work we address both of these issues. We show how to add spatial constraints to the mixture formulations and present a variant of the EM algorithm that males use of both the form and the motion constraints. Moreover this algorithm estimates the number of segments given knowledge about the level of model failure expected in the sequence. The algorithm's performance is illustrated on synthetic and real image sequences.", "authors": ["Yair Weiss", "Edward H. Adelson"], "id": "a73f23484f890fafff6dd1e79ae25b33de1e666b", "title": "A unified mixture framework for motion segmentation: incorporating spatial coherence and estimating the number of models", "references": ["3d22d58f6dc9bdf7a82ba17e0fe3f3ed33d06077", "b61ff5c7d6e6d6c28042870e4715e9c9b193b45f", "ab1cbc81305b382f5f9d572397ee9d2855f05c70", "8d5a193fdbf9d34118f136935cfd07a81b3e0d77", "088898bc4caf25d179533afe0335aeb52dd6f723", "6d7f2205a7bc593089b3e151dd79d7f14d5b3349", "56ab8eebda89b251418ed39b794706c9652b0067", "eaf31005e3e66eda87fda48b369e90a0a95c5320", "d237b135d9cb6c5ea76faa421fa461d3128b61e8", "bdfb57141b2141095ed942b28be24808aeba8d54"]}, {"date": "2000", "abstract": "We present a new image segmentation algorithm based on graph cuts. Our main tool is separation of each pixel p from a special point outside the image by a cut of a minimum cost. Such a cut creates a group of pixels C/sub p/ around each pixel. We show that these groups C/sub p/ are either disjoint or nested in each other and so they give a natural segmentation of the image. In addition this property allows an efficient implementation of the algorithms because for most pixels p the computation of C/sub p/ is not performed on the whole graph. We inspect all C/sub p/ and discard those which are not interesting, for example if they are too small. This procedure automatically groups small components together or merges them into nearby large clusters. Effectively, our segmentation is performed by extracting significant non-intersecting closed contours. We present interesting segmentation results on real and artificial images.", "authors": ["Olga Veksler"], "id": "4ec28212d571da349ac886039f8dcd2243c8d57c", "title": "Image segmentation by nested cuts", "references": ["006328c8add2ce30c186048c89097560d2661c27", "aa4bddbd10eafd8e1b54338517eedfee408f03ae", "df65c38f24b8010ddb030d4d88ec4d7bbe8d36cd", "250748b4494cec56abd55ae049bdd38f4d42e5c8", "a57520b68e73b5e1fc3668b443daf74ebe957cc7", "e9d7f589a3d368a3701832e28d90ca09ec9e5577", "b94c7ff9532ab26c3aedbee3988ec4c7a237c173", "23d31d6c75a3c62ee2cedd22308c5c0ba34217e7", "6c9817b90cfc7cd78143f3749e602febd84d2a81", "c9b3cc3187155ac760babe28634e987441cb27b8"]}, {"date": "1990", "abstract": "An analog CMOS neural net with a programmable architecture containing 32 K connections is discussed. The objective of packing as large a network as possible on a chip leads to the choice of an analog approach. Analog signals are used only inside the network. All the input and output data are digital. The reconfigurable network consists of building blocks that can be joined to form various network architectures. The circuit can be programmed to implement single-layer networks or multilayer networks with binary or analog connections.<<ETX>>", "authors": ["H.P. Graf", "Dennis R. Henderson"], "id": "03d42bfef1926122a3c1bed035009fb1aeb2bedc", "title": "A reconfigurable CMOS neural network", "references": []}, {"date": "1993", "abstract": "The benefits of neural networks and the types of application for which they are suited are outlined. Four representative applications are described in enough detail to show how they work. They are character recognition, function estimation, financial forecasting, and process control. Factors that have slowed the acceptance of neural networks are discussed. Hardware is briefly considered.<<ETX>>", "authors": ["Dan Hammerstrom"], "id": "4b0305d9dd44892b1faedc71013cb78de97caf79", "title": "Neural networks at work", "references": []}, {"date": "1989", "abstract": "The problem of learning a general input-output relation using a layered neural network is discussed in a statistical framework. By imposing the consistency condition that the error minimization be equivalent to a likelihood maximization for training the network, the authors arrive at a Gibbs distribution on a canonical ensemble of networks with the same architecture. This statistical description enables them to evaluate the probability of a correct prediction of an independent example, after training the network on a given training set. The prediction probability is highly correlated with the generalization ability of the network, as measured outside the training set. This suggests a general and practical criterion for training layered networks by minimizing prediction errors. The authors demonstrate the utility of this criterion for selecting the optimal architecture in the continuity problem. As a theoretical application of the statistical formalism, they discuss the question of learning curves and estimate the sufficient training size needed for correct generalization, in a simple example.<<ETX>>", "authors": ["Naftali Tishby", "Esther Levin", "Sara A. Solla"], "id": "b10440620da8a43a1b97e3da4b1ff13746306475", "title": "Consistent inference of probabilities in layered networks: predictions and generalizations", "references": ["33fdc91c520b54e097f5e09fae1cfc94793fbfcf", "7e1ca8d081fc07e6190a3bf5e3156569d8e9c96b", "dd5061631a4d11fa394f4421700ebf7e78dcbc59", "445ad69010658097fc317f7b83f1198179eebae8"]}, {"date": "1989", "abstract": "Convergence properties of empirically estimated neural networks are examined. In this theory, an appropriate size feedforward network is automatically determined from the data. The networks studied include two- and three-layer networks with an increasing number of simple sigmoidal nodes, multiple-layer polynomial networks, and networks with certain fixed structures but an increasing complexity in each unit. Each of these classes of networks is dense in the space of continuous functions on compact subsets of d-dimensional Euclidean space, with respect to the topology of uniform convergence. It is shown how, with the use of an appropriate complexity regularization criterion, the statistical risk of network estimators converges to zero as the sample size increases. Bounds on the rate of convergence are given in terms of an index of the approximation capability of the class of networks.<<ETX>>", "authors": ["Andrew R. Barron"], "id": "72d761afbe35634213849419ff63fad5bc9fabeb", "title": "Statistical properties of artificial neural networks", "references": ["e85a68602abf92fcc1efb8b7aa90d27d141a80c2", "1d9e82ab55414e9e86e46996800ffd4226c545e2", "589b8659007e1124f765a5d1bd940b2bf4d79054", "249ce7a85b158c16ba108451070c07aa1156e7eb", "4c154b63659e656d0402cfc06e8aa8c24f804d08"]}, {"date": "1976", "abstract": "According to the informon theory there must be variable and fixed synapses in a neurone for conditioning to occur. For a variable synapse to behave like an informon pathway its conductivity needs to depend only on the average values of its presynaptic potential and of the internal state of the neurone. Eight predictions are made about the detailed functioning of such a synapse. In a minimal hypothesis all fixed synapses are inhibitory; but sign reversals are considered. Let one unit A in the receptive field of a neurone drive it through a fixed synapse, and all other units, e.g. B, drive variable synapses; then the theory predicts that the conductivity of the B synapse becomes proportional to the mutual information function between the signals at A and B; so inputs which tend to occur with the A signal become connected positively to the neurone. Applied to visual pathways this principle leads to the formation of edge and grating detectors. If X and Y cells excite variable and fixed synapses respectively, simple and complex cells should be driven by both X and Y cells, the latter being inhibitory. The two-pathway theory resolves two apparent conflicts between experimental facts.", "authors": ["Albert M. Uttley"], "id": "ac145edb50780eadaa53b4528fc9cd1fac0fa8c8", "title": "Neurophysiological predictions of a two-pathway informon theory of neural conditioning", "references": []}, {"date": "1954", "abstract": "Continuing the story begun in The Hobbit, this is the first part of Tolkien's epic masterpiece, The Lord of the Rings, featuring a striking black cover based on Tolkien's own design, the definitive text, and a detailed map of Middle-earth. Sauron, the Dark Lord, has gathered to him all the Rings of Power - the means by which he intends to rule Middle-earth. All he lacks in his plans for dominion is the One Ring - the ring that rules them all - which has fallen into the hands of the hobbit, Bilbo Baggins. In a sleepy village in the Shire, young Frodo Baggins finds himself faced with an immense task, as his elderly cousin Bilbo entrusts the Ring to his care. Frodo must leave his home and make a perilous journey across Middle-earth to the Cracks of Doom, there to destroy the Ring and foil the Dark Lord in his evil purpose. Part of a set of three paperbacks, this popular edition is once again available in its classic black livery designed by Tolkien himself.", "authors": ["J. R. R. Tolkien"], "id": "97cc2fab13bf20d130de19dfffe6670aac2076b5", "title": "The Fellowship of the Ring: Being the first part of The Lord of the Rings", "references": []}, {"date": "1989", "abstract": "We study the construction of prediction algorithms in a situation in which a learner faces a sequence of trials, with a prediction to be made in each, and the goal of the learner is to make few mistakes. We are interested in the case that the learner has reason to believe that one of some pool of known algorithms will perform well, but the learner does not know which one. A simple and effective method, based on weighted voting, is introduced for constructing a compound algorithm in such a circumstance. We call this method the Weighted Majority Algorithm. We show that this algorithm is robust w.r.t. errors in the data. We discuss various versions of the Weighted Majority Algorithm and prove mistake bounds for them that are closely related to the mistake bounds of the best algorithms of the pool. For example, given a sequence of trials, if there is an algorithm in the pool A that makes at most m mistakes then the Weighted Majority Algorithm will make at most c(log n + m) mistakes on that sequence, where c is fixed constant. One version of the Weighted Majority algorithm deals with pools that are countably infinite. \n \nThe Weighted Majority algorithm is: \n \nan efficient and robust method for selecting good predictive performance from a pool algorithms; \n \na powerful tool getting upper bounds on learning problems while ignoring computational efficiency. \n \nAn extended abstract will appear in the Proceedings of the 30th Annual Symposium on the Foundations of Computer Science, Research Triangle, North Carolina, October 30 - November 1, 1989.", "authors": ["Nick Littlestone"], "id": "ff027adc26fdbe6faa929e500f5fe3136077ffa7", "title": "The Weighted Majority Algorithm", "references": []}, {"date": "1989", "abstract": "In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.", "authors": ["George Cybenko"], "id": "8da1dda34ecc96263102181448c94ec7d645d085", "title": "Approximation by superpositions of a sigmoidal function", "references": ["29b6251c84def0cbd35397c71fada0d22cd9409c", "386cbc45ceb59a7abb844b5078e5c944f17723b4", "25406e6733a698bfc4ac836f8e74f458e75dad4f", "8e4bd5422c82009290a5cd71457388f0780530d6", "993455b7cdc0e84e65abb90a5ed570e7c51a5883", "d1382a29539b3de419d567f679b5f28cee459a49", "b8778bb692cf105254fe767ef11a3a8afac4a068", "b0f09280ba01ab2e2c60e9450bef332d183ba2f3", "c42c1305ce33c628ffc5401d5de2b0347f50ac78", "10ddb646feddc12337b5a755c72e153e37088c02"]}, {"date": "1995", "abstract": "We investigate the problem of {\\it model\\ selection} in the setting of supervised learning of boolean functions from independent random examples. More precisely, we compare methods for finding a balance between the complexity of the hypothesis chosen and its observed error on a random training sample of limited size, when the goal is that of minimizing the resulting generalization error. We undertake a detailed comparison of three well-known model selection methods \u2014 a variation of Vapnik\u2018s {\\it Guaranteed\\ Risk\\ Minimization} (GRM), an instance of Rissanen\u2018s {\\it Minimum\\ Description\\ Length\\ Principle} (MDL), and (hold-out) cross validation (CV). We introduce a general class of model selection methods (called {\\it penalty-based} methods) that includes both GRM and MDL, and provide general methods for analyzing such rules. We provide both controlled experimental evidence and formal theorems to support the following conclusions: \n \n\\bulletEven on simple model selection problems, the behavior of the methods examined can be both complex and incomparable. Furthermore, no amount of \u201ctuning\u201d of the rules investigated (such as introducing constant multipliers on the complexity penalty terms, or a distribution-specific \u201ceffective dimension\u201d) can eliminate this incomparability. \n \n\\bulletIt is possible to give rather general bounds on the generalization error, as a function of sample size, for penalty-based methods. The quality of such bounds depends in a precise way on the extent to which the method considered automatically limits the complexity of the hypothesis selected. \n \n\\bulletFor {\\it any} model selection problem, the additional error of cross validation compared to {\\it any} other method can be bounded above by the sum of two terms. The first term is large only if the learning curve of the underlying function classes experiences a phase transition\u201d between (1-\\gamma)m andm examples (where \\gamma is the fraction saved for testing in CV). The second and competing term can be made arbitrarily small by increasing\\gamma . \n \n\\bulletThe class of penalty-based methods is fundamentally handicapped in the sense that there exist two types of model selection problems for which every penalty-based method must incur large generalization error on at least one, while CV enjoys small generalization error on both.", "authors": ["Michael Kearns", "Yishay Mansour", "Andrew Y. Ng", "Dana Ron"], "id": "8facf54fbb5cc49d3df9674b3facca5d92acbdc0", "title": "An experimental and theoretical comparison of model selection methods", "references": []}, {"date": "2013", "abstract": "We study question answering as a machine learning problem, and induce a function that maps open-domain questions to queries over a database of web extractions. Given a large, community-authored, question-paraphrase corpus, we demonstrate that it is possible to learn a semantic lexicon and linear ranking function without manually annotating questions. Our approach automatically generalizes a seed lexicon and includes a scalable, parallelized perceptron parameter estimation scheme. Experiments show that our approach more than quadruples the recall of the seed lexicon, with only an 8% loss in precision.", "authors": ["Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni"], "id": "c0be2ac2f45681f1852fc1d298af5dceb85834f4", "title": "Paraphrase-Driven Learning for Open Question Answering", "references": ["b70b2e38cc882192305891ec4d4bb0534e440973", "c1052027ddbacc24fc4a244d2c073f86aca62747", "92fb5e045bb23f13c11d8bb277925013b24b5930", "bd298c1bcefcd7feb108111cd72758c265d16ee6", "eb04bd49bdc2c9218624435eb277be2a973b21a5", "2a5a179a70e98ee1c4b4b8b54f6c8aba629c02e4", "9c99620d7511c83a402ff3b4b3a2348a669e61e3", "d48edf9e81653f4c3da716b037b0b50d54c5b034", "74fe7ec751cd50295b15cfd46389a8fefb37c414", "10b8f21e57b3392ce623c374c2c039f811ce5f69"]}, {"date": "1989", "abstract": "Image models are useful in quantitatively specifying natural constraints and general assumptions about the physical world and the imaging process. This review paper explains how Gibbs and Markov random field models provide a unifying theme for many contemporary problems in image analysis. Random field models permit the introduction of spatial context into pixel labeling problems, such as segmentation and restoration. Random field models also describe textured images and lead to algorithms for generating textured images, classifying textures and segmenting textured images. In spite of some impressive model-based image restoration and texture segmentation results reported in the literature, a number of fundamental issues remain unexplored, such as the specification of MRF models, modeling noise processes, performance evaluation, parameter estimation, the phase transition phenomenon and the comparative analysis of alternative procedures. The literature of random field models is filled with great promise, but...", "authors": ["Richard C. Dubes", "Anil K. Jain"], "id": "4c5820002582c302736ff6a370904ff685962e3a", "title": "Random field models in image analysis", "references": []}, {"date": "1994", "abstract": "This paper presents an application of recurrent networks for phone probability estimation in large vocabulary speech recognition. The need for efficient exploitation of context information is discussed; a role for which the recurrent net appears suitable. An overview of early developments of recurrent nets for phone recognition is given along with the more recent improvements that include their integration with Markov models. Recognition results are presented for the DARPA TIMIT and Resource Management tasks, and it is concluded that recurrent nets are competitive with traditional means for performing phone probability estimation.", "authors": ["Anthony J. Robinson"], "id": "c6629770cb6a00ad585918e71fe6dbad829ad0d1", "title": "An application of recurrent nets to phone probability estimation", "references": ["cd0568b4faa03910ae3c07d00c627666f404305d", "ea705422d5e2292bccb766e9047c9f25880a50c0", "c91c2ac02e33caff601b2e4d62a6841b33ca3929", "c1116b32168ca91607d81e8aa6be64ee7b539449", "758eae04fc9f4331b0ceab797387c8fc9f00db58", "fd6387ca1949d61356adee35708dcdbee1e4fd05", "df682aa90fbbbf665a8b273a57ca87d6cea9ff99", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "a08c99425ad94eed67d059813511fe9ca55e73eb", "ee50abb5aff3e5c43a38f24396b9552d593a9ae0"]}, {"date": "1998", "abstract": "We propose a method for segmenting gray-value images. By segmentation, we mean a map from the set of pixels to a small set of levels such that each connected component of the set of pixels with the same level forms a relatively large and \"meaningful\" region. The method finds a set of levels with associated gray values by first finding junctions in the image and then seeking a minimum set of threshold values that preserves the junctions. Then it finds a segmentation map that maps each pixel to the level with the closest gray value to the pixel data, within a smoothness constraint. For a convex smoothing penalty, we show the global optimal solution for an energy function that fits the data can be obtained in a polynomial time, by a novel use of the maximum-flow algorithm. Our approach is in contrast to a view in computer vision where segmentation is driven by intensity, gradient, usually not yielding closed boundaries.", "authors": ["Hiroshi Ishikawa", "Davi Geiger"], "id": "6c9817b90cfc7cd78143f3749e602febd84d2a81", "title": "Segmentation by grouping junctions", "references": ["47865b56fee61d9c9ff477f7c79f090cc6663d3a", "006328c8add2ce30c186048c89097560d2661c27", "d7d9408070d32cf75ad0e2aedd0a353c9cb07862", "fcf9fc4e23b45345c2404ce7d6cb0fc9dea2c9ec", "8d5a193fdbf9d34118f136935cfd07a81b3e0d77", "b94c7ff9532ab26c3aedbee3988ec4c7a237c173", "a79836ffd64bb3111f5247567a1cb43a8ad6e621", "5be5d22f4300bd59fc42b1c6dcea6ef64ebacc76", "01e5d824826515e5ae0fd69031e0d1c19abe8079", "b543d70d3e4fe6673a2e39832f005fa7ebc79cec"]}, {"date": "1998", "abstract": "Markov Random Fields (MRFs) can be used for a wide variety of vision problems. In this paper we focus on MRFs with two-valued clique potentials, which form a generalized Potts model. We show that the maximum a posteriori estimate of such an MRF can be obtained by solving a multiway minimum cut problem on a graph. We develop efficient algorithms for computing good approximations to the minimum multiway, cut. The visual correspondence problem can be formulated as an MRF in our framework; this yields quite promising results on real data with ground truth. We also apply our techniques to MRFs with linear clique potentials.", "authors": ["Yuri Boykov", "Olga Veksler", "Ramin Zabih"], "id": "b543d70d3e4fe6673a2e39832f005fa7ebc79cec", "title": "Markov random fields with efficient approximations", "references": ["ccb09e3a0ac30cc904d3eb879ad1569e1843b437", "4aeea86e589383e2ec2d4214e919ebda9277c452", "523b9604863d9dd9282723d927b6efb7bad1bcb0", "c8d45226a133ce224699a1dda45e97104594b506", "006328c8add2ce30c186048c89097560d2661c27", "d7d9408070d32cf75ad0e2aedd0a353c9cb07862", "7037d38b71636ee1f6e9156214c4c1a89dcc8cdb", "b232e3426e0014389ea05132ea8d08789dcc0566", "c5634533a7daecde097a230484df5f0913dd9410", "6c9817b90cfc7cd78143f3749e602febd84d2a81"]}, {"date": "1994", "abstract": "This paper presents a Markov random field (MRF) model for object recognition in high level vision. The labeling state of a scene in terms of a model object is considered as an MRF or couples MRFs. Within the Bayesian framework the optimal solution is defined as the maximum a posteriori (MAP) estimate of the MRF. The posterior distribution is derived based on sound mathematical principles from theories of MRF and probability, which is in contrast to heuristic formulations. An experimental result is presented.<<ETX>>", "authors": ["Stan Z. Li"], "id": "4018bb79e4ac87aee95400c0310f67b045cd1ffe", "title": "A Markov random field model for object matching under contextual constraints", "references": ["ccb09e3a0ac30cc904d3eb879ad1569e1843b437", "8bf730243ed967afd5349bef053641a6043517a0", "26acc72a47ae2e7aa9f6f4049b4a88e2a2ee461b", "4fc956c8d8b41c1e9c499cad0cf882debf45ca64", "936b9c7462d96ce2084b7384e5959605635789e2", "4329a92f956e2a6553763248c87ca6a3ba7beba2", "515966ebc3adec9a7740526fbee997937aaa234f", "a99c226dd262a3fccd752f213ce54fc6b2bd1218"]}, {"date": "1995", "abstract": "Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k > 2 values (i.e., k \"classes\"). The definition is acquired by studying collections of training examples of the form (xi, f(xi)). Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that--like the other methods--the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems.", "authors": ["Thomas G. Dietterich", "Ghulum Bakiri"], "id": "d221bbcbd20c7157e4500f942de8ceec490f8936", "title": "Solving Multiclass Learning Problems via Error-Correcting Output Codes", "references": ["7a0bcbcdf6d069682432bddd26b3d75a0d7495d4", "32a86f4c74d69fc9dc7d43a6b561d39d6bfa8292", "830ccb44084d9d6cdcb70d623df5012ae4835142", "a8e8f3c8d4418c8d62e306538c9c1292635e9d27", "595640253ffdfd12e04ac57bd78753f936a7cfad", "a57c6d627ffc667ae3547073876c35d6420accff", "8a6d820385527df2183a36ae1615f426ba894c5d", "f0bd2bc9946405188fc3ae0fadac126d241b0925", "eb0427eeeda79a97e90392c16b40e51e41c5f9d2", "8725185b4910234f0f25be8066d00f4c2d1ca102"]}, {"date": "1991", "abstract": "In order to recover an accurate representation of a scene containing multiple moving objects, one must use estimation methods that can recover both model parameters and segmentation at the same time. Traditional approaches to this problem rely on an edge-based discontinuity model, and have problems with transparent phenomena. The authors introduce a layered model of scene segmentation based on explicitly representing the support of a homogeneous region. The model employs parallel robust estimation techniques, and uses a minimal-covering optimization to estimate the number of objects in the scene. Using a simple direct motion model of translating objects, they successfully segment real image sequences containing multiple motions.<<ETX>>", "authors": ["Trevor Darrell", "A. Pentland"], "id": "3d22d58f6dc9bdf7a82ba17e0fe3f3ed33d06077", "title": "Robust estimation of a multi-layered motion representation", "references": ["e7c49a201daf302b8a7fd70729f1b08fcf4c1b90", "c7cf4e26a6b6fd0b62a452781abdd424fb7749b3", "f349b9dfa765fef54ac2308fd703082683bed32c", "b1e99c01e2f1f5783d476b78c8d0503623f3891c", "9053e06f2f031bbc16b2ab3755fd975483a6117f", "d54af6d5ccd2f6bf4b75772cc23d48b39cf8cae8", "e88791964c01c293f9b647b449b07b8d97b29bd9", "1e112e8bb31ebcf2fc019796cdabec1e5f34f480"]}, {"date": "1999", "abstract": "We present a stochastic clustering algorithm which uses pairwise similarity of elements, based on a new graph theoretical algorithm for the sampling of cuts in graphs. The stochastic nature of our method makes it robust against noise, including accidental edges and small spurious clusters. We demonstrate the robustness and superiority of our method for image segmentation on a few synthetic examples where other recently proposed methods (such as normalized-cut) fail. In addition, the complexity of our method is lower. We describe experiments with real images showing good segmentation results.", "authors": ["Yoram Gdalyahu", "Daphna Weinshall", "Michael Werman"], "id": "c9b3cc3187155ac760babe28634e987441cb27b8", "title": "Stochastic image segmentation by typical cuts", "references": ["9c4f25dc2739b8fff579264374e6ded0679958b5", "b07ce649d6f6eb636872527104b0209d3edc8188", "aa4bddbd10eafd8e1b54338517eedfee408f03ae", "250748b4494cec56abd55ae049bdd38f4d42e5c8", "3a58c3eafcc642ffa2e571e069e53f20bb1d1150", "b94c7ff9532ab26c3aedbee3988ec4c7a237c173", "5164366217cc54edef6fe6017775ad3bf41a453e", "05d976c6c88c9677071c8778feb4e6dcbacc20b3", "23d31d6c75a3c62ee2cedd22308c5c0ba34217e7"]}, {"date": "1989", "abstract": "A frequency detection circuit having an input transistor connected to the primary winding of a coupling transformer. In the secondary winding circuit of the transformer there is connected a diode which cooperates with a parallel arrangement of a resistor and a capacitor arranged so that the current flowing through the resistor depends solely on the frequency of the input signal applied to the input transistor. The frequency detection circuit can simultaneously carry out a number of control functions in response to a corresponding number of frequencies of the input signal.", "authors": ["Andrew R Barron", "Roger L. Barron"], "id": "4c154b63659e656d0402cfc06e8aa8c24f804d08", "title": "Statistical learning networks: A unifying view", "references": []}, {"date": "1998", "abstract": "The foreground group in a scene may be \u2018discovered\u2019 and computed as a factorized approximation to the pairwise affinity of the elements in the scene. A pointwise approximation of the pairwise affinity information may in fact be interpreted as a \u2018saliency\u2019 index, and the foreground of the scene may be obtained by thresholding it. An algorithm called \u2018affinity factorization\u2019 is thus obtained which may be used for grouping.", "authors": ["Pietro Perona", "William T. Freeman"], "id": "23d31d6c75a3c62ee2cedd22308c5c0ba34217e7", "title": "A Factorization Approach to Grouping", "references": ["8c21b3372894b25ed595a73ff058ec5876976ca6", "17f7edd1c71a402baad0732dc4e74dddc709505a", "c501afc481d50a7610ab021ec703a6a196b7e520", "2af3fbcd7f4a6a542ae5962e6c2e869e0ddb7bed", "e3a0a456829c137c02b82c7143893c9dbceef349", "d26faa6f33270cbc4c6e7d20d142b92fc675b793"]}, {"date": "1988", "abstract": "On presente une methode generale pour approximer le biais et la variance dans une echelle de normes hilbertiennes", "authors": ["Dennis D. Cox"], "id": "1d9e82ab55414e9e86e46996800ffd4226c545e2", "title": "Approximation of least squares regression on nested subspaces", "references": []}, {"date": "1986", "abstract": "1. Very rarely, a book is published which not only advances our knowledge of a particular topic, but fundamentally recasts our methods of investigating and thinking about large tracts of the map of learning. Linguists remember 1957 as the publication year of Noam Chomsky's Syntactic structures-a book whose ostensible subjects were the structure of English grammatical rules and the goals of grammatical description, but which can be seen with hindsight as the first shot in an intellectual revolution which ended by radically changing the texture of day-to-day research activity and discourse throughout almost all of linguistics, and in substantial parts of other cognition-related disciplines. In decades to come, perhaps 1986 will be remembered by academics as the year of publication of the pair of volumes reviewed here: they constitute the first large-scale public statement of an intellectual paradigm fully as revolutionary as the generative paradigm ever was (there have been scattered journal articles in the preceding four or five years). I would go further and suggest that, if the promises of this book can be redeemed, the contrast in linguistics and neighboring disciplines between the 1990's and the 1970's will be significantly greater than the contrast between the 1970's and the 1950's. (I need hardly add, of course, that it is one thing to fire an opening salvo, but another to achieve ultimate predominance.) The new paradigm is called Parallel Distributed Processing by the sixteen writers who contributed to this book, many of whom work either at the University of California, San Diego, or at Carnegie-Mellon University in Pittsburgh. Some other researchers (e.g. Feldman 1985) use the term 'connectionism' for the same concept. These two volumes comprise 26 chapters which, among them, (i) explain the over-all nature and aims of PDP/connectionist models, (ii) define a family of specific variants of the general paradigm, and (iii) exemplify it by describing experiments in which PDP models were used to simulate human performance in various cognitive domains. The experiments, inevitably, treat their respective domains in a simplified, schematic way by comparison with the endless complexity found in any real-life cognitive area; but simplification in this case does not mean trivialization. There are also auxiliary chapters on relevant related topics; thus Chap. 9, by M. I. JORDAN, is a tutorial on linear algebra, a branch of mathematics having special significance for the PDP paradigm. (Each chapter is attributed to a particular author or", "authors": ["David E. Rumelhart", "James L. McClelland"], "id": "249ce7a85b158c16ba108451070c07aa1156e7eb", "title": "Parallel Distributed Processing: Explorations in the microstructures of cogni-", "references": []}, {"date": "1966", "abstract": "Abstract : The use of a linearized mathematical spline for interpolation between given points occasionally yields extraneous inflection points which for some applications, notably the fairing of ship lines, are not acceptable. To avoid this difficulty, a spline in tension is considered and the value of tension which is sufficient to remove all the extraneous inflection points is determined. The slopes of the interpolating curve at each of the given stations are considered as the unknowns and can be calculated without the necessity of numerical matrix inversion.", "authors": ["D. G. Schweikert"], "id": "44924f2551a7db7c57e829b710576f14197eb76e", "title": "An Interpolation Curve Using a Spline in Tension", "references": []}, {"date": "1988", "abstract": "A test sequence is used to select the best rule from a class of discrimination rules defined in terms of the training sequence. The Vapnik-Chervonenkis and related inequalities are used to obtain distribution-free bounds on the difference between the probability of error of the selected rule and the probability of error of the best rule in the given class. The bounds are used to prove the consistency and asymptotic optimality for several popular classes, including linear discriminators, nearest-neighbor rules, kernel-based rules, histogram rules, binary tree classifiers, and Fourier series classifiers. In particular, the method can be used to choose the smoothing parameter in kernel-based rules, to choose k in the k-nearest neighbor rule, and to choose between parametric and nonparametric rules. >", "authors": ["Luc Devroye"], "id": "e85a68602abf92fcc1efb8b7aa90d27d141a80c2", "title": "Automatic Pattern Recognition: A Study of the Probability of Error", "references": ["b6c835ed3f011f1221006586989a73998a75fbcb", "0c4a39e43d22b9e39507cde0ccae50859d447a7e", "f53c22ed4f130cf1e703af4cb212de4518d8b405", "66ee4bb5d704856f43d751c259a7b5de9c77b764", "a4f9442c9c8c835e069dc9cb812e2748541590e2", "0efb841403aa6252b39ae6975c1cc5410554ef7b", "942f37ae066e1db5b6660241c84e689d2f6b5c20", "b0eae4c19136a014517a57faa414324079698ffa", "e60ccd8a6aa8759f9af77796526e451bdbe899eb", "80acea2684b1a04ec06360cf7966ebafd311cfc3"]}, {"date": "1981", "abstract": "Abstract A new method for nonparametric multiple regression is presented. The procedure models the regression surface as a sum of general smooth functions of linear combinations of the predictor variables in an iterative manner. It is more general than standard stepwise and stagewise regression procedures, does not require the definition of a metric in the predictor space, and lends itself to graphical interpretation.", "authors": ["Jerome H. Friedman", "Werner Stuetzle"], "id": "589b8659007e1124f765a5d1bd940b2bf4d79054", "title": "Projection Pursuit Regression", "references": ["d9179a0064ad058953121f1ce445c89304c59f37", "4ad1ab02f14ce07861ed1678065a35c5991c1802", "755c4d6ca13e371e2f684c06caa348162190004b"]}, {"date": "1965", "abstract": "This paper develops the separating capacities of families of nonlinear decision surfaces by a direct application of a theorem in classical combinatorial geometry. It is shown that a family of surfaces having d degrees of freedom has a natural separating capacity of 2d pattern vectors, thus extending and unifying results of Winder and others on the pattern-separating capacity of hyperplanes. Applying these ideas to the vertices of a binary n-cube yields bounds on the number of spherically, quadratically, and, in general, nonlinearly separable Boolean functions of n variables. It is shown that the set of all surfaces which separate a dichotomy of an infinite, random, separable set of pattern vectors can be characterized, on the average, by a subset of only 2d extreme pattern vectors. In addition, the problem of generalizing the classifications on a labeled set of pattern points to the classification of a new point is defined, and it is found that the probability of ambiguous generalization is large unless the number of training patterns exceeds the capacity of the set of separating surfaces.", "authors": ["Thomas M. Cover"], "id": "445ad69010658097fc317f7b83f1198179eebae8", "title": "Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition", "references": ["2920f6b916007e39aed3dffebebe4b4442e49f0d", "2934355c03faa1094eb413241bb9de6753b48988", "c3caf34c1c86633b6e80dca29e3cb2b6367a0f93", "7ce11916234117cf9e1d9bd7eae9a24fe68c8c01", "4c1632e3fbe569450978f901ec9c6fccecc0b76b", "fb9f07dc246ec7b3fa2d8491a1a8c4da7cc77985", "e8871e2641df536074612f73e26815bf324c0ba9"]}, {"date": "2005", "abstract": "This paper addresses the problem of mapping natural language sentences to lambda\u2013calculus encodings of their meaning. We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence. We apply the method to the task of learning natural language interfaces to databases and show that the learned parsers outperform previous methods in two benchmark database domains.", "authors": ["Luke Zettlemoyer", "Michael Collins"], "id": "74fe7ec751cd50295b15cfd46389a8fefb37c414", "title": "Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars", "references": ["1df68a034ccc23d297e898730854ef2466377190", "f2ffd8273d01948310feda2771943673aae4fe6f", "49a8ae5eb474e7f6ce3669f9c55efaee1d43cdec", "b7c0e47f8b768258b7d536c21b218e6c46ab8791", "19da3a7bc4fd32b697bab29fdbc6fe27bf3b33fb", "f60d8dd8ca3a7dfa7d0a14988af73084ad93619d", "cbd3bb68861808eadd8c0ef2734fd0cd666305d9", "1c7a1d0b183d0db47a438299d023684491461eac", "1e19a94d547ee023837c14c361139185e2353fc0", "e9c1f510bcf5933d3cf8ec8108a04a9ba601a843"]}, {"date": "2002", "abstract": "We describe the architecture of the AskMSR question answering system and systematically evaluate contributions of different system components to accuracy. The system differs from most question answering systems in its dependency on data redundancy rather than sophisticated linguistic analyses of either questions or candidate answers. Because a wrong answer is often worse than no answer, we also explore strategies for predicting when the question answering system is likely to give an incorrect answer.", "authors": ["Eric Brill", "Susan T. Dumais", "Michele Banko"], "id": "9c99620d7511c83a402ff3b4b3a2348a669e61e3", "title": "An Analysis of the AskMSR Question-Answering System", "references": ["af6339448844835ddc75f426b0b8620781288737", "da7d8f3d6f6e09d2be7235bd167909291ae053ac", "122ab8ab9a7661748ff8719c5f90df6ab279f738", "03a1cf16053deadf60801ee7808d31d2ee0c45ea", "7cb56a8518eeb15604f908cdad18660f11b55417", "38afe960fb869a6e3a458d327f0300cfa23e597b", "016e9cc85c658c6a69710b4c617609ad2a5d3a74", "93cd0a09eec889d28f3ac1e7f1dabee0b7178b37", "12fc6f855f58869cf81743b9be0df1380c17f4d0", "44e4081054990b613347d2d607ed218deee40c85"]}, {"date": "1988", "abstract": "The statistical use of a particular classic form of a connectionist system, the multilayer perceptron (MLP), is described in the context of the recognition of continuous speech. A discriminant hidden Markov model (HMM) is defined, and it is shown how a particular MLP with contextual and extra feedback input units can be considered as a general form of such a Markov model. A link between these discriminant HMMs, trained along the Viterbi algorithm, and any other approach based on least mean square minimization of an error function (LMSE) is established. It is shown theoretically and experimentally that the outputs of the MLP (when trained along the LMSE or the entropy criterion) approximate the probability distribution over output classes conditioned on the input, i.e. the maximum a posteriori probabilities. Results of a series of speech recognition experiments are reported. The possibility of embedding MLP into HMM is described. Relations with other recurrent networks are also explained. >", "authors": ["Herv\u00e9 Bourlard", "Christian Wellekens"], "id": "ee50abb5aff3e5c43a38f24396b9552d593a9ae0", "title": "Links Between Markov Models and Multilayer Perceptrons", "references": ["1e4193d03eb3c5a695a3d8b3506f80704f9dfc19", "de996c32045df6f7b404dda2a753b6a9becf3c08", "a57c6d627ffc667ae3547073876c35d6420accff", "247698d0a716f0d99c0645050d049525e0b08ec2", "1aa31d5deb45f477a6de45b3b75b62c7f4a213e7", "b8778bb692cf105254fe767ef11a3a8afac4a068", "cd62c9976534a6a2096a38244f6cbb03635a127e", "b5f09ce0dd760857e0d0e4879f6e2543f04c5d33", "e66aade2a97005b9f0638357421b1f72f770f75f", "a55d31784aca11871985096644a025f036633569"]}, {"date": "1992", "abstract": "An image is segmented into a collection of disjoint regions that form the nodes of an adjacency graph, and image interpretation is achieved through assigning object labels (or interpretations) to the segmented regions (or nodes) using domain knowledge, extracted feature measurements, and spatial relationships between the various regions. The interpretation labels are modeled as a Markov random field (MRF) on the corresponding adjacency graph, and the image interpretation problem is then formulated as a maximum a posteriori (MAP) estimation rule, given domain knowledge and region-based measurements. Simulated annealing is used to find this best realization or optimal MAP interpretation. This approach also provides a systematic method for organizing and representing domain knowledge through appropriate design of the clique functions describing the Gibbs distribution representing the pdf of the underlying MRF. A general methodology is provided for the design of the clique functions. Results of image interpretation experiments on synthetic and real-world images are described. >", "authors": ["James W. Modestino", "Jun Zhang"], "id": "4329a92f956e2a6553763248c87ca6a3ba7beba2", "title": "A Markov Random Field Model-Based Approach to Image Interpretation", "references": []}, {"date": "1990", "abstract": "A network is described that recognizes objects from uncertain image-derivable descriptions. The network handles uncertainty by making the recognition and segmentation decisions simultaneously, in a cooperative way. Both problems are posed as labeling problems, and a coupled Markov random field (MRF) is used to provide a single formal framework for both. Prior domain knowledge is represented as weights within the MRF network and interacts with the evidence to yield a labeling decision. The domain problem is the recognition of structured objects composed of simple junction and link primitives. Implementation experiments demonstrate the parallel segmentation and recognition of multiple objects in noisy ambiguous scenes with occlusion.<<ETX>>", "authors": ["Paul R. Cooper"], "id": "a99c226dd262a3fccd752f213ce54fc6b2bd1218", "title": "Parallel structure recognition with uncertainty: coupled segmentation and matching", "references": []}, {"date": "2011", "abstract": "Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web's natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multi-instance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint --- for example they cannot extract the pair Founded(Jobs, Apple) and CEO-of(Jobs, Apple). \n \nThis paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Free-base. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level.", "authors": ["Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S. Weld"], "id": "d48edf9e81653f4c3da716b037b0b50d54c5b034", "title": "Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations", "references": ["0c236e611a90018e84d9de23d1cff241354079be", "9c7f4412b8f0310a91334aed79b8553b2ad70908", "8dff21517f7ac744089a260dbc3e2f48649e3119", "6c8898cda9a1f13607e24306f6f64f20e0ff2ae7", "bd298c1bcefcd7feb108111cd72758c265d16ee6", "e7e7b9a731678bf0494fe29cbebb42a822224cc6", "a3fa819575c78be3cbcc8aa394fd21a182dce669", "233d861338cfcd479b1d21897453fcc66418d5e1", "498bb0efad6ec15dd09d941fb309aa18d6df9f5f", "d84b57362e2010f6f65357267df7e0157af30684"]}, {"date": "1996", "abstract": "A stereo algorithm is presented that optimizes a maximum likelihood cost function. The maximum likelihood cost function assumes that corresponding features in the left and right images are normally distributed about a common true value and consists of a weighted squared error term if two features are matched or a (fixed) cost if a feature is determined to be occluded. The stereo algorithm finds the set of correspondences that maximize the cost function subject to ordering and uniqueness constraints. The stereo algorithm is independent of the matching primitives. However, for the experiments described in this paper, matching is performed on the $cf4$individual pixel intensities.$cf3$ Contrary to popular belief, the pixel-based stereo appears to be robust for a variety of images. It also has the advantages of (i) providing adensedisparity map, (ii) requiringnofeature extraction, and (iii)avoidingthe adaptive windowing problem of area-based correlation methods. Because feature extraction and windowing are unnecessary, a very fast implementation is possible. Experimental results reveal that good stereo correspondences can be found using only ordering and uniqueness constraints, i.e., withoutlocalsmoothness constraints. However, it is shown that the original maximum likelihood stereo algorithm exhibits multiple global minima. The dynamic programming algorithm is guaranteed to find one, but not necessarily the same one for each epipolar scanline, causing erroneous correspondences which are visible as small local differences between neighboring scanlines. Traditionally, regularization, which modifies the original cost function, has been applied to the problem of multiple global minima. We developed several variants of the algorithm that avoid classical regularization while imposing several global cohesiveness constraints. We believe this is a novel approach that has the advantage of guaranteeing that solutions minimize the original cost function and preserve discontinuities. The constraints are based on minimizing the total number of horizontal and/or vertical discontinuities along and/or between adjacent epipolar lines, and local smoothing is avoided. Experiments reveal that minimizing the sum of the horizontal and vertical discontinuities provides the most accurate results. A high percentage of correct matches and very little smearing of depth discontinuities are obtained. An alternative to imposing cohesiveness constraints to reduce the correspondence ambiguities is to use more than two cameras. We therefore extend the two camera maximum likelihood toNcameras. TheN-camera stereo algorithm determines the \u201cbest\u201d set of correspondences between a given pair of cameras, referred to as the principal cameras. Knowledge of the relative positions of the cameras allows the 3D point hypothesized by an assumed correspondence of two features in the principal pair to be projected onto the image plane of the remainingN? 2 cameras. TheseN? 2 points are then used to verify proposed matches. Not only does the algorithm explicitly model occlusion between features of the principal pair, but the possibility of occlusions in theN? 2 additional views is also modeled. Previous work did not model this occlusion process, the benefits and importance of which are experimentally verified. Like other multiframe stereo algorithms, the computational and memory costs of this approach increase linearly with each additional view. Experimental results are shown for two outdoor scenes. It is clearly demonstrated that the number of correspondence errors is significantly reduced as the number of views/cameras is increased.", "authors": ["Ingemar J. Cox", "Sunita L. Hingorani", "Satish Rao", "Bruce M. Maggs"], "id": "b232e3426e0014389ea05132ea8d08789dcc0566", "title": "A Maximum Likelihood Stereo Algorithm", "references": ["ffacc9e5144f0efed41fd49b2a388939afff5db9", "278618251450aaeb334227ec82bacabfc1c266de", "79f6afbff9d42d9de9fe6a6ff2fc391834a1fb82", "4fb1358e846b7d441df25a167f190beb7fb3e240", "008c82c45adeec76c9351c5e84751540daad31f3", "a99c14098f69922a17ecaad38e56744ded00eba7", "a46b4b9ea5513590ac4966c9b3e665f85c162dea", "11996252bec281d26e7b9c9b0eb3a834d194ad9d", "53e606d468feedc9d581434b4853904e79083333", "7db5db57bdea8c347b202230f43655c8a247008f"]}, {"date": "1997", "abstract": "We present a new approach to clustering, based on the physical properties of an inhomogeneous ferromagnet. No assumption is made regarding the underlying distribution of the data. We assign a Potts spin to each data point and introduce an interaction between neighboring points, whose strength is a decreasing function of the distance between the neighbors. This magnetic system exhibits three phases. At very low temperatures, it is completely ordered; all spins are aligned. At very high temperatures, the system does not exhibit any ordering, and in an intermediate regime, clusters of relatively strongly coupled spins become ordered, whereas different clusters remain uncorrelated. This intermediate phase is identified by a jump in the order parameters. The spin-spin correlation function is used to partition the spins and the corresponding data points into clusters. We demonstrate on three synthetic and three real data sets how the method works. Detailed comparison to the performance of other techniques clearly indicates the relative success of our method.", "authors": ["Marcelo Blatt", "Shai Wiseman", "Eytan Domany"], "id": "05d976c6c88c9677071c8778feb4e6dcbacc20b3", "title": "Data Clustering Using a Model Granular Magnet", "references": ["ed223a6a3f853a6720406d6c22680e02e3953b9c", "f7d04596a7a0e6e6b8c9199ce4aba2d2a6f3dbc6", "eb4d0956a53aa4d201fc4f26f8564c9d789e5006", "39022a3b31b4df16ee8fc58a559e84aca48ea153", "4d9e8851be945d79568c802d545bb829f14c3ccf", "bfc81a348dfd095f8381274fe6ac8cf9702362d2", "d1b12eeda37a21bdaa11850131c115589a9fc0ad", "f91e2cfeca9234c08ddbccb1bf43f280a12b534a", "ad8f1d2a6616465ae8a21dc5ab2c655d3329311d", "e4672ebfc82a4d6b5c145959d88b03e16fbb1b2e"]}, {"date": "1983", "abstract": "Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of statistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them.", "authors": ["Lalit R. Bahl", "Frederick Jelinek", "Robert L. Mercer"], "id": "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "title": "A Maximum Likelihood Approach to Continuous Speech Recognition", "references": ["8291be2289154cf1dcd5a4009222c1899533e253", "df452affecc2c0342ee84925d1a89f0452c14a87", "bdb3f20fe41bb95f6bc9d162e827de8db3f952d7", "3f6db1b368ebaeace796f78d1ee02807051487d4", "1fd7e526e4a7ccf50cfad47e94425f144aec818b", "2069762a92a0efee1f09206ee4f82b7d28a40856", "2543dea11cbfce22c38fc2f57bd1dc3c502794b1", "338b4a29b872fcd7b5d12f3de0652da84dd38478", "c180f387357d9302a558bcd643209831744c639b", "609d9316f6d933bdef464e222699662ca9b2eb29"]}, {"date": "1998", "abstract": "We present a stochastic clustering algorithm based on pairwise similarity of datapoints. Our method extends existing deterministic methods, including agglomerative algorithms, min-cut graph algorithms, and connected components. Thus it provides a common framework for all these methods. Our graph-based method differs from existing stochastic methods which are based on analogy to physical systems. The stochastic nature of our method makes it more robust against noise, including accidental edges and small spurious clusters. We demonstrate the superiority of our algorithm using an example with 3 spiraling bands and a lot of noise.", "authors": ["Yoram Gdalyahu", "Daphna Weinshall", "Michael Werman"], "id": "5164366217cc54edef6fe6017775ad3bf41a453e", "title": "A Randomized Algorithm for Pairwise Clustering", "references": ["9c4f25dc2739b8fff579264374e6ded0679958b5", "b07ce649d6f6eb636872527104b0209d3edc8188", "aa4bddbd10eafd8e1b54338517eedfee408f03ae", "ac2eeb625c78116a61fb8307fbd5a0fba26c99fa", "250748b4494cec56abd55ae049bdd38f4d42e5c8", "b94c7ff9532ab26c3aedbee3988ec4c7a237c173", "3a58c3eafcc642ffa2e571e069e53f20bb1d1150", "d10da32c39ac3703064d99735982327af6631793", "05d976c6c88c9677071c8778feb4e6dcbacc20b3", "bf584e89bac9daa74e77a35ee65e63deaf8aadf6"]}, {"date": "1974", "abstract": "For the average error probability Pe associated with the Bayes recognition procedures for two possible patterns, using no context, new upper and lower bounds and approximations are obtained. Results are given in terms of simple functions of feature \"reliability\" and a priori probabilities of the patterns. Two kinds of feature \"reliability\" are considered, i.e., distance between probability distributions and error probabilities without the use of a priori probabilities. Computational advantages offered by those bounds and approximations are pointed out. The question as to how close they are to Peis examined. In some special cases, they are perfect. Numerical examples show that the differences are in general about 5-10 percent, and comparisons with certain known results are quite favorable. Possible applications are discussed. Extension is also made to m possible patterns arranged in a hierarchy with two elements at each branching.", "authors": ["J. T. Chu"], "id": "80acea2684b1a04ec06360cf7966ebafd311cfc3", "title": "Some New Error Bounds and Approximations for Pattern Recognition", "references": []}, {"date": "1997", "abstract": "Partitioning a data set and extracting hidden structure from the data arises in different application areas of pattern recognition, speech and image processing. Pairwise data clustering is a combinatorial optimization method for data grouping which extracts hidden structure from proximity data. We describe a deterministic annealing approach to pairwise clustering which shares the robustness properties of maximum entropy inference. The resulting Gibbs probability distributions are estimated by mean-field approximation. A new structure-preserving algorithm to cluster dissimilarity data and to simultaneously embed these data in a Euclidian vector space is discussed which can be used for dimensionality reduction and data visualization. The suggested embedding algorithm which outperforms conventional approaches has been implemented to analyze dissimilarity data from protein analysis and from linguistics. The algorithm for pairwise data clustering is used to segment textured images.", "authors": ["Thomas Hofmann", "Joachim M. Buhmann"], "id": "3a58c3eafcc642ffa2e571e069e53f20bb1d1150", "title": "Pairwise Data Clustering by Deterministic Annealing", "references": ["af77de34c7d5435f3658cf106d53ccd72e24c89d", "7a40f8a315772e15966b48df1ede981f848fd870", "79ec67bbbcea16a55afeb7ca782cbd7672f3bd9f", "8d5a193fdbf9d34118f136935cfd07a81b3e0d77", "08b67692bc037eada8d3d7ce76cc70994e7c8116", "8c3e33fb1d7ffae33d4c97d4cc82e002d772e0a5", "2ee14dd35886c44c87d66f8490528fa58c19fc25", "bf584e89bac9daa74e77a35ee65e63deaf8aadf6", "cd75b16be178aa69ad24dc222c109dacac82f44c", "9899003369af99d02f699cbcbf48b79019666158"]}, {"date": "1988", "abstract": "A system for analyzing an RF signal to determine the noise and modulation components of the signal is disclosed there. The system is capable of analyzing an unknown signal to determine the random AM, FM and PM noise components as well as the intentional AM, FM and PM modulation components. The system includes a phase lock loop through which controls the frequency of a signal which is mixed with the signal to be analyzed such that the resulting signal is within a frequency band which can be easily analyzed.", "authors": ["A. Sha", "S. Ullman Ashua"], "id": "e3a0a456829c137c02b82c7143893c9dbceef349", "title": "Structural saliency: the detection of globally salient structures using a locally connected network", "references": []}, {"date": "1982", "abstract": "The classification of large dimensional data sets arising from the merging of remote sensing data with more traditional forms of ancillary data causes a significant computational problem. Decision tree classification is a popular approach to the problem. This type of classifier is characterized by the property that samples are subjected to a sequence of decision rules before they are assigned to a unique class. If a decision tree classifier is well designed, the result in many cases is a classification scheme which is accurate, flexible, and computationally efficient. This correspondence provides an automated technique for effective decision tree design which relies only on a priori statistics. This procedure utilizes canonical transforms and Bayes table look-up decision rules. An optimal design at each node is derived based on the associated decision table. A procedure for computing the global probability of correct classification is also provided. An example is given in which class statistics obtained from an actual Landsat scene are used as input to the program. The resulting decision tree design has an associated probability of correct classification of 0.75 compared to the theoretically optimum 0.79 probability of correct classification associated with a full dimensional Bayes classifier. Recommendations for future research are included.", "authors": ["Peter Argentiero", "R. T. Chin", "Paul Beaudet"], "id": "e60ccd8a6aa8759f9af77796526e451bdbe899eb", "title": "An Automated Approach to the Design of Decision Tree Classifiers", "references": []}, {"date": "1996", "abstract": "Extra-receptive eld contextual modulation in primate striate cortex, J. Neurosci. in press. Anatomical binding of intrinsic connections in striate cortex of tree shrews (Tupaia Glis), J. Comp. 91] B. Zenger and D. Sagi, Isolating excitatory and inhibitory non-linear spatial interactions involved in contrast detection, Vision Res. in press. A model for the neural implementation of selective visual attention based on temporal correlation among neurons, J. Comp. 65] R. L. Ogniewicz, Automatic medial axis pruning by mapping characteristics of boundaries evolving under the Eucliedean geometric heat ow onto Voronoi skeletons, Relationship between intrinsic connections and functional architecture revealed by optical imaging and in vivo targeted biocytin injections in primate striate cortex, Proc. Natl. Acad. Sci. Contextual innuences on visual discrimination in humans and on RF properties of cells in striate cortex of alert monkeys, Soc. A quantitative measure for short-term cortical plasticity in human vision, Coherent oscillations: a mechanism of feature linking in the visual cortex? multiple electrode and correlation analyses in the cat, Biol. Cybern. 60 coding in the visual cortex: new vistas on integration in the nervous system, Contour integration by the human visual system: Evidence for a local \\association eld\", Vision Res. Spatial organization and connections of iso-orientation domains in tree shrew striate cortex, Soc. Neurosci. Oscillatory responses in cat visual cortex exhibit inter-columnar synchronization which reeects global stimulus properties, Nature 338 (1989) 334{337. A transformation for extracting new descriptors of shape. in: Symposium on models for the perception of speech and visual form (ed) W. Physiological correlates of anisotropy in horizontal connections: length summation properties of neurons in layers 2 and 3 of tree shrew striate cortex, Soc. 14] J. G. Daugman, Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical lters, J. Opt. Soc. Receptive eld structure in the visual cortex: Does selective stimulation induce plasticity? Proc. Natl. 18 Acknowledgements I thank Akos Feh er for providing his inventive method to compute the medial axis transformation, Harry Schick for bringing into my view long forgotten facts of cell biology, Karl Zipser and Tai Sing Lee for facilitating discussions on the neurophysiology of the primary visual cortex, Thomas V. Papathomas, Eileen Kowler, Bela Julesz, and two anonymous reviewers for their helpful comments on the manuscript. 17 strategy is to provide the right conditions for the formation of`shape' and observe our capability to generate (or ruin) coherent patterns in any domain. 16 \u2026", "authors": ["Karl Zipser", "Victor A. F. Lamme", "Tai Sing Lee", "Edward C. Rosenow"], "id": "d26faa6f33270cbc4c6e7d20d142b92fc675b793", "title": "Gestalten of Today: Early Processing of Visual Contours and Surfaces", "references": ["dc85c68891c561e28034bfbb7ddaf4d577c86ed9", "ec54be02d6e0561992f4572ed24d42f345ba14c5", "c501afc481d50a7610ab021ec703a6a196b7e520", "c9b194830dc3286dab5dbd3b0e3d5013ec20efb6", "8b4184b6113d4c74dc3dc5f8e75b2860a55fa52b", "247e8d3bc80409fb85439a716633d1d95444291d", "28d35947b7885c87f565f07ab397200bf8a6f25e", "7540897771b4ee6848abe5def91d036059cbdac3", "95bcfe5a35df789554e1d9d35fc9fbf35d46fff5", "c8f62c808df4597ee2e8c7dfadb9aca9e121ae73"]}, {"date": "1973", "abstract": "Semantic Scholar extracted view of \"Pattern classification and scene analysis\" by Richard O. Duda et al.", "authors": ["Richard O. Duda", "Peter E. Hart"], "id": "b07ce649d6f6eb636872527104b0209d3edc8188", "title": "Pattern classification and scene analysis", "references": []}, {"date": "1962", "abstract": "Statistical classification (pattern recognition) in n -dimensional space consists in partitioning the space into category regions with decision boundaries and assigning an unknown to the category in whose region it falls. This paper demonstrates the wide utility of a particular form of decision boundary\u2014the hypersphere\u2014which, while especially easy to implement, is fully optimum for large classes of distributions which may arise in real problems. Of the broad spectrum of distributions described for which the hypersphere is optimum, particular interest centers on the normal, and the Pearson Type II and Type VII distributions; and methods for obtaining the boundary parameters are prescribed. Ordering of the coordinate directions according to their relative significance in contributing to the decision is examined, thereby indicating the most efficient reduction of dimensionality where this may be desired in order to allow further computational simplicity. A partial listing of error probabilities is also included.", "authors": ["Paul W. Cooper"], "id": "fb9f07dc246ec7b3fa2d8491a1a8c4da7cc77985", "title": "The Hypersphere in Pattern Recognition", "references": []}, {"date": "", "abstract": "Ersten Bande: Ueber die Minimalflache, deren Begrenzung als ein von vier Kanten eines regularen Tetraeders gebildetes raumliches Vierseit gegeben ist Bestimmung einer speciellen Minimalflache Bestimmung einer speciellen Minimalflache. Nachtrag Bestimmung einer speciellen Minimalflache. Anhang, enthaltend Anmerkungen und Zusatze Fortgesetzte Untersuchungen uber specielle Minimalflachen Ueber ein Modell eines Minimalflachenstuckes, welches langs seiner Begrenzung vier gegebene Ebenen rechtwinklig trifft Beitrag zur Untersuchung der zweiten Variation des Flacheninhalts von Minimalflachenstucken im Allgemeinen und von Theilen der Schraubenflache im Besonderen Miscellen aus dem Gebiete der Minimalflachen Ueber diejenigen Minimalflachen, welche von einer Schaar von Kegeln zweiten Grades eingehullt werden Ueber einige nicht algebraische Minimalflachen, welche eine Schaar algebraischer Curven enthalten Sur les surfaces a courbure moyenne nulle sur lesquelles on peut limiter une portion finie de la surface par quatre droites situees sur la surface Ueber ein die Flachen kleinsten Flacheninhalts betreffendes Problem der Variationsrechnung. Festschrift zum siebzigsten Geburtstage des Herrn Karl Weierstrass Ueber specielle zweifach zusammenhangende Flachenstucke, welche kleineren Flacheninhalt besitzen, als alle benachbarten, von denselben Randlinien begrenzten Flachenstucke Anmerkungen und Zusatze zum ersten Bande Zweiten Bande: Elementarer Beweis des Pohlkeschen Fundamentalsatzes der Axonometrie De superficiebus in planum explicabilibus primorum septem ordinum. Inauguraldissertation Ueber die geradlinigen Flachen funften Grades Ueber einige Abbildungsaufgaben Conforme Abbildung der Oberflache eines Tetraeders auf die Oberflache einer Kugel Notizia sulla rappresentazione conforme di un' area ellittica sopra un' area circolare Zur Theorie der Abbildung Ueber einen Grenzubergang durch alternirendes Verfahren Ueber die Integration der partiellen Differentialgleichung $\\frac{\\partial^{2}u}{\\partial x^2}+\\frac{\\partial^{2}u}{\\partial y^2}=0$ unter vorgeschriebenen Grenz- und Unstetigkeitsbedingungen Mittheilung uber diejenigen Falle, in welchen die Gaussische hypergeometrische Reihe $F(\\alpha, \\beta, \\gamma, x)$ eine algebraische Function ihres vierten Elementes darstellt Zur Integration der partiellen Differentialgleichung $\\frac{\\partial^{2}u}{\\partial x^2}+\\frac{\\partial^{2}u}{\\partial y^2}=0$ Ueber diejenigen Falle, in welchen die Gaussische hypergeometrische Reihe eine algebraische Function ihres vierten Elementes darstellt Ueber ebene algebraische Isothermen Beispiel einer stetigen nicht differentiirbaren Function Ueber ein vollstandiges System von einander unabhangiger Voraussetzungen zum Beweise des Satzes $\\frac{\\partial}{\\partial y}(\\frac{\\partial f(x, y)}{\\partial x})=\\frac{\\partial}{\\partial x}(\\frac{\\partial f(x, y)}{\\partial y})$ Ueber diejenigen algebraischen Gleichungen zwischen zwei veranderlichen Grossen, welche eine Schaar rationaler, eindeutig umkehrbarer Transformationen in sich selbst zulassen Essai d'une demonstration d'un theoreme de Geometrie, redige sur l'invitation de M. Charles Hermite Verallgemeinerung eines analytischen Fundamentalsatzes Auszug aus einem Briefe an Herrn F. Klein Demonstration elementaire d'une propriete fondamentale des fonctions interpolaires Sur une definition erronee de l'aire d'une surface courbe Bestimmung der scheinbaren Grosse eines Ellipsoids fur einen beliebigen Punkt des Raumes Zur conformen Abbildung der Flache eines Rechtecks auf die Flache einer Halbkugel Beweis des Satzes, dass die Kugel kleinere Oberflache besitzt, als jeder andere Korper gleichen Volumens Beweis eines fur die Theorie der trigonometrischen Reihen in Betracht kommenden Hulfssatzes Beweis des Satzes, dass unter allen einem spitzwinkligen Dreiecke eingeschriebenen Dreiecken das Dreieck der Hohenfusspunkte den kleinsten Umfang hat Bemerkung zu der Mittheilung des Herrn Weierstrass: Zur Theorie der aus $n$ Haupteinheiten gebildeten complexen Grossen Anmerkungen und Zusatze zum zweiten Bande.", "authors": ["Hermann Amandus Schwarz"], "id": "e8871e2641df536074612f73e26815bf324c0ba9", "title": "Gesammelte mathematische Abhandlungen", "references": []}, {"date": "1989", "abstract": "Abstract A fundamental challenge in speech recognition is the discrimination of the acoustic vectors characterizing each speech time-slot into classes of sub-word units. The most popular technique relies on hidden Markov models (HMM) but alternative models could make use of linear or non-linear discriminant functions and also Multilayer Perceptrons (MLP). A common property of these techniques is their learning ability. However, the advantage of discriminant functions and multilayer perceptrons is their possibility to force simultaneously the acceptance of a vector into its own class and its rejection by the rival classes. Ideally, this classification relies on a logical decision with a score invariant for any vector in a given class. Generally, classes are not linearly separable. As linear discriminant functions are not flexible enough to attain both objectives, suitable non-linear discriminant functions could approximate logical decision and achieve non-linear separation. For that purpose, multilayer perceptrons are particularly powerful tools because of their possibility to approximate a very wide range of non-linear functions with a rather restricted set of parameters. The main properties of MLP are pointed out and illustrated by simple examples. The possibility of including discriminant principles in a Dynamic Time Warping process and in a Viterbi-like training is shown. The discriminant properties of HMM, linear discriminant functions and MLP are compared from the point of view of local labeling of the acoustic vectors and of the global recognition. Finally, a phoneme based real task application (50 phonemes, 1000 vocabulary words) is described. It makes use of a particular MLP, based on the NETtalk architecture, and shows how the non-linear discriminant functions and the consideration of the temporal context dependence of the acoustic vectors are useful for the phonetic speech labeling.", "authors": ["Herv\u00e9 Bourlard", "Christian Wellekens"], "id": "a55d31784aca11871985096644a025f036633569", "title": "Speech pattern discrimination and multilayer perceptrons", "references": []}, {"date": "1999", "abstract": "In this paper we report on an unsupervised approach to learning Categorial Grammar (CG) lexicons. The learner is provided with a set of possible lexical CG categories, the forward and backward application rules of CG and unmarked positive only corpora. Using the categories and rules, the sentences from the corpus are probabilistically parsed. The parses and the history of previously parsed sentences are used to build a lexicon and annotate the corpus. We report the results from experiments on a number of small generated corpora, that contain examples from subsets of the English language. These show that the system is able to generate reasonable lexicons and provide accurately parsed corpora in the process. We also discuss ways in which the approach can be scaled up to deal with larger and more diverse corpora. 1 I n t r o d u c t i o n In this paper we discuss a potential solution to two problems in Natural Language Processing (NLP), using a combination of statistical and symbolic machine learning techniques. The first problem is learning the syntactic roles, or categories, of words o f a language i.e. learning a lexicon. Secondly, we discuss a method of annotating a corpus with parses. The aim is to learn Categorial Grammar (CG) lexicons, starting from a set of lexical categories, the functional application rules of CG and an unannotated corpus of positive examples. The CG formalism (discussed in Section 2) is chosen because it assigns distinct categories to words of different types, and the categories describe the exact syntactic role each word can play in a sentence. This problem is similar to the unsupervised part of speech tagging work of, for example, Brill (Brill, 1997) and Kupiec (Kupiec, 1992). In Brill's work a lexicon containing the parts of speech available to each word is provided and a simple tagger attaches a complex tag to each word in the corpus, which represents all the possible tags that word can have. Transformation rules are then learned which use the context of a word to determine which simple tag it should be assigned. The results are good, generally achieving around 95% accuracy on large corpora such as the Penn Treebank. Kupiec (Kupiec, 1992) uses an unsupervised version of the Baum-Welch algorithm, which is a way of using examples to iteratively estimate the probabilities of a Hidden Markov Model for part of speech tagging. Instead of supplying a lexicon, he places the words in equivalence classes. Words in the same equivalence class must take one of a specific set of parts of speech. This improves the accuracy of this algorithm to about the same level as Brill's approach. In both cases, the learner is provided with a large amount of background knowledge either a complete lexicon or set of equivalence classes. In the approach presented here, the most that is provided is a small partial lexicon. In fact the system learns the lexicon. The second problem annotating the corpus is solved because of the approach we use to learn the lexicon. The system uses parsing to determine which are the correct lexical entries for a word, thus annotating the corpus with the parse derivations (also providing less probable parses if desired). An example of another approach to doing this is the Fidditch parser of Hindle (Hindle, 1983) (based on the deterministic parser of Marcus (Marcus, 1980)), which was used to annotate the Penn Treebank (Marcus et al., 1993). However, instead of learning the lexicon, a complete grammar and lexicon", "authors": ["Stephen Watkinson", "Suresh Manandhar"], "id": "1df68a034ccc23d297e898730854ef2466377190", "title": "Unsupervised Lexical Learning with Categorial Grammars", "references": ["9d8f8caf065f3d5f329c52f7b92532cd4ee628ae", "2843e831b299440bf2c2590b8052ca1aca2a145a", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "dc961cfc7fb7a5003126d68e49f93d569adac7f0", "4f00d407aa2a700abe5427aae71038c525eaa338", "21a9505edcaee3f085c695eefbbf4e07332295b2", "383303197268a87c93f74901c602084663c2cccf", "352dbd26580856ba4b9877d43aeba304343af66d", "aee623be6841f22355fb24f955368d7a98293666", "d886320cc431c57f12027584fdce3434bf4caf9e"]}, {"date": "1996", "abstract": "We present a natural language interface system which is based entirely on trained statistical models. The system consists of three stages of processing: parsing, semantic interpretation, and discourse. Each of these stages is modeled as a statistical process. The models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames.", "authors": ["Scott Miller", "David Stallard", "Robert J. Bobrow", "Richard M. Schwartz"], "id": "e9c1f510bcf5933d3cf8ec8108a04a9ba601a843", "title": "A Fully Statistical Approach to Natural Language Interfaces", "references": ["4b04d2f1f7e0d4eb51937e8980b01b5ed8876bd0", "8df509919b31397e225280962c59384fbe83144e", "55a1bf99b29436197fd4d872a78155a196a77a88", "71a286f8e6eeb611bd216f07bb749d3cc04ed2b7", "70d2b25dfe4fbadfa3c816e01666f811dae9a4d1", "79fbfc1dc8846379074aaf4deb7fb0a96722eeed", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "ac8f1fd58be8a8c9f9599fc4da981ea3040945f6", "a8fcc058607129590fa6ab692a38807efc7d7f1f"]}, {"date": "1976", "abstract": "HIS PAPER DESCRIBES statistical methods of automatic recognition (transcription) of continuous speech that have been used successfully by the Speech Processing Group at the IBM Thomas J. Watson Research Center. The sources of these procedures will be referenced where practicable, but the working style of the Group has been deliberately cooperative (as the Acknowledgment Section indicates), so a certain amount of inadequate or udust crediting is inevitable. The author tried his best to keep it at a minimum. The exposition, appearing as it does in an IEEE publication, is aimed mostly at engineers who are less familiar with speech and language than with information transmission, statistics, or signal processing. At the same time, the author would like to enable speech specialists to read the more mathematical parts of the paper. Inevitably, a compromise between these two audiences has been attempted that resulted in a somewhat lengthened presentation. The author would like to invite his readers to skip rather boldly over material familiar to them. The fust six sections contain the essence of the formulation that is centered on the design of an actual speech recognition system. Readers of Section VI that contains experimental results may feel somewhat dissatisfied with the fact that no comparisons are attempted with performance achieved by alternate design philosophies. Unfortunately, such judgments are made difficult by the great variety in utterance corpora to be recognized, in experimental conditions, and in recognizer function goals.\u2019 However, the accompanying survey paper by Reddy [231 does assess the merits of the various speech recognition projects, and can serve as an excellent introduction to the field for the nonspecialist. In the speech recognition community, our project is somewhat controversial, since it attempts to model utterance production statistically, rather than through a grammar that would describe syntactically and semantically the allowable (mini-) universe of discourse. It is much too early to tell which emphasis is sounder. There is little doubt that before automatic recognition of speech is accomplished, the statistical", "authors": ["Frederick Jelinek"], "id": "e66aade2a97005b9f0638357421b1f72f770f75f", "title": "Speech Recognition by Statistical Methods", "references": []}, {"date": "2008", "abstract": "The combined efforts of human volunteers have recently extracted numerous facts from Wikipedia, storing them as machine-harvestable object-attribute-value triples in Wikipedia infoboxes. Machine learning systems, such as Kylin, use these infoboxes as training data, accurately extracting even more semantic knowledge from natural language text. But in order to realize the full power of this information, it must be situated in a cleanly-structured ontology. This paper introduces KOG, an autonomous system for refining Wikipedia's infobox-class ontology towards this end. We cast the problem of ontology refinement as a machine learning problem and solve it using both SVMs and a more powerful joint-inference approach expressed in Markov Logic Networks. We present experiments demonstrating the superiority of the joint-inference approach and evaluating other aspects of our system. Using these techniques, we build a rich ontology, integrating Wikipedia's infobox-class schemata with WordNet. We demonstrate how the resulting ontology may be used to enhance Wikipedia with improved query processing and other features.", "authors": ["Fei Wu", "Daniel S. Weld"], "id": "0c236e611a90018e84d9de23d1cff241354079be", "title": "Automatically refining the wikipedia infobox ontology", "references": ["00a3f6924f90fcd77e6e7e6534b957a75d0ced07", "2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2", "950a3c89dacbc3e7ddcd43d7ff6f985697e41cdb", "3b76b68c44e4d9f875e2aaa95eae689bbc67396c"]}, {"date": "1978", "abstract": "Preliminary results have been obtained with a system for recognizing continuously read sentences from a naturally-occurring corpus (Laser Patents), restricted to a 1000-word vocabulary. Our model of the task language has an entropy of about 4.8 bits/word and a perplexity of 21.11 words. Many new problems arise in recognition of a substantial natural corpus (compared to recognition of an artificially constrained language). Some techniques are described for treating these problems. On a test set consisting of 20 sentences having a total of 486 words, there was a word error rate of 33.1%.", "authors": ["Lalit R. Bahl", "James K. Baker", "Paul S. Cohen", "Frederick Jelinek", "Burn L. Lewis", "Robert L. Mercer"], "id": "609d9316f6d933bdef464e222699662ca9b2eb29", "title": "Recognition of continuously read natural corpus", "references": []}, {"date": "1993", "abstract": "A deterministic annealing approach to clustering is derived on the basis of the principle of maximum entropy. This approach is independent of the initial state and produces natural hierarchical clustering solutions by going through a sequence of phase transitions. It is modified for a larger class of optimization problems by adding constraints to the free energy. The concept of constrained clustering is explained, and three examples are are given in which it is used to introduce deterministic annealing. The previous clustering method is improved by adding cluster mass variables and a total mass constraint. The traveling salesman problem is reformulated as constrained clustering, yielding the elastic net (EN) approach to the problem. More insight is gained by identifying a second Lagrange multiplier that is related to the tour length and can also be used to control the annealing process. The open path constraint formulation is shown to relate to dimensionality reduction by self-organization in unsupervised learning. A similar annealing procedure is applicable in this case as well. >", "authors": ["Kenneth Rose", "Eitan Gurewitz", "Geoffrey Charles Fox"], "id": "bf584e89bac9daa74e77a35ee65e63deaf8aadf6", "title": "Constrained Clustering as an Optimization Method", "references": ["128e4eb6f85a8936c61a61c69ef58ed27c18c67b", "59fbb0b127de1ea9be27d5f8c1e30d0cf394a503", "e4c5184e949a7702e2c89ba04a867cfa14711609", "459b30a9a960080f3b313e41886b1aa0e51e882c", "dd5061631a4d11fa394f4421700ebf7e78dcbc59", "08b67692bc037eada8d3d7ce76cc70994e7c8116", "8c3e33fb1d7ffae33d4c97d4cc82e002d772e0a5", "477db9d84d8875da57664d39af140d884d858222", "24a34c189fe193e032711aeae6075e4b76d7c548", "a94be030ccd68f3a5a3bf9245137fe114c549819"]}, {"date": "1993", "abstract": "Clustering algorithms have the annoying characteristic of finding clusters in random data. A theoretical analysis of the threshold of the mutual neighborhood clustering algorithm (MNCA) under the hypothesis of random data is presented. This yields a theoretical minimum value of this threshold below which even unclustered data are broken into separate clusters. To derive the threshold, a theorem about mutual near neighbors in a Poisson process is stated and proved. Simple experiments demonstrate the usefulness of the theoretical thresholds. >", "authors": ["Stephen P. Smith"], "id": "d10da32c39ac3703064d99735982327af6631793", "title": "Threshold Validity for Mutual Neighborhood Clustering", "references": ["194a68a6e7039c64ea09196fd56b5346f4a7226b", "026826a0cfe545df8bb860c2484524d7494e76bd", "aa4bddbd10eafd8e1b54338517eedfee408f03ae", "ac7dc9fb84958a107946175e46f0abc2d166f403"]}, {"date": "2009", "abstract": "Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACE-style algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.", "authors": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "id": "d84b57362e2010f6f65357267df7e0157af30684", "title": "Distant supervision for relation extraction without labeled data", "references": ["dbfd191afbbc8317577cbc44afe7156df546e143", "8dff21517f7ac744089a260dbc3e2f48649e3119", "e703e928bc07900527c368db2428d0d5c57148c2", "68cd1c7c0651b116a83abab8a7a46a29975d3b5f", "0165568bcc1a819c18564567f2ec15d859be2519", "cee045e890270abae65455667b292db355d53728", "501428daffd5d70d1305582ddec7a93dae1f704e", "22e26f6eee0745c9a23274dc722593acd0e2e6a2", "498bb0efad6ec15dd09d941fb309aa18d6df9f5f", "421151fa75e40dd86414215abf29d9f2c052a2e1"]}, {"date": "1989", "abstract": "An iterative descent algorithm based on a Lagrangian formulation for designing vector quantizers having minimum distortion subject to an entropy constraint is discussed. These entropy-constrained vector quantizers (ECVQs) can be used in tandem with variable-rate noiseless coding systems to provide locally optimal variable-rate block source coding with respect to a fidelity criterion. Experiments on sampled speech and on synthetic sources with memory indicate that for waveform coding at low rates (about 1 bit/sample) under the squared error distortion measure, about 1.6 dB improvement in the signal-to-noise ratio can be expected over the best scalar and lattice quantizers when block entropy-coded with block length 4. Even greater gains are made over other forms of entropy-coded vector quantizers. For pattern recognition, it is shown that the ECVQ algorithm is a generalization of the k-means and related algorithms for estimating cluster means, in that the ECVQ algorithm estimates the prior cluster probabilities as well. Experiments on multivariate Gaussian distributions show that for clustering problems involving classes with widely different priors, the ECVQ outperforms the k-means algorithm in both likelihood and probability of error. >", "authors": ["Philip A. Chou", "Tom D. Lookabaugh", "Robert M. Gray"], "id": "af77de34c7d5435f3658cf106d53ccd72e24c89d", "title": "Entropy-constrained vector quantization", "references": ["9122f73ac37af778eeef726524b82e591805921f", "01bf34c51da904f880eda19af7718b2d45d8105d", "c303d67ed77191d6eb85595d03579da3ea3541a5", "b24e81f917a310cf22d62f49d8da0ec0e572344a", "51d56fba09a9d11977ae1726eb30e07e34277582", "9241ea3d8cb85633d314ecb74b31567b8e73f6af", "bcda63c3d609688952c2acf72f893265a16d940f", "803367ea78d349d310a224503270a006c2bc0c66", "9db4e4c528c00e5e847d60891f7150f0acf98669", "08f8f2c03daabbe6ebdc3cc39ca9cd6d6b7a503f"]}, {"date": "1996", "abstract": "Abstract Multidimensional scaling addresses the problem how proximity data can be faithfully visualized as points in a low-dimensional Euclidean space. The quality of a data embedding is measured by a stress function which compares proximity values with Euclidean distances of the respective points. The corresponding minimization problem is non-convex and sensitive to local minima. We present a novel deterministic annealing algorithm for the frequently used objective SSTRESS and for Sammon mapping , derived in the framework of maximum entropy estimation. Experimental results demonstrate the superiority of our optimization technique compared to conventional gradient descent methods.", "authors": ["Hansj\u00f6rg Klock", "Joachim M. Buhmann"], "id": "ac2eeb625c78116a61fb8307fbd5a0fba26c99fa", "title": "Data visualization by multidimensional scaling: a deterministic annealing approach", "references": ["deb40e644f78e719ca510f82b889acd3e60fed96", "88360c1f3e3d5980663034863232bb0c7f7ef27e", "e04108dc293c9cd7cabf32ee1524eaab0d4641b3", "037947eec9079c7dea0db1c23e6fd8a4032e21dd", "aaa566e3ddaef092001e173d252ef0b1fe134786", "362d61dd2a41cbc4d31faec9abacf7b01380878b", "d4ecb6b1cda922eb13a18ed6ea7d1e827c2efb6b", "3a58c3eafcc642ffa2e571e069e53f20bb1d1150", "eab6b613c5dd5a49c5b16723ff9fd21ccbd26646", "003739d648c7285c60d8aa9558f8efc713cd4fb0"]}, {"date": "2008", "abstract": "Traditionally, Information Extraction (IE) has focused on satisfying precise, narrow, pre-specified requests from small homogeneous corpora (e.g., extract the location and time of seminars from a set of announcements). Shifting to a new domain requires the user to name the target relations and to manually create new extraction rules or hand-tag new training examples. This manual labor scales linearly with the number of target relations. This paper introduces Open IE (OIE), a new extraction paradigm where the system makes a single data-driven pass over its corpus and extracts a large set of relational tuples without requiring any human input. The paper also introduces TEXTRUNNER, a fully implemented, highly scalable OIE system where the tuples are assigned a probability and indexed to support efficient extraction and exploration via user queries. We report on experiments over a 9,000,000 Web page corpus that compare TEXTRUNNER with KNOWITALL, a state-of-the-art Web IE system. TEXTRUNNER achieves an error reduction of 33% on a comparable set of extractions. Furthermore, in the amount of time it takes KNOWITALL to perform extraction for a handful of pre-specified relations, TEXTRUNNER extracts a far broader set of facts reflecting orders of magnitude more relations, discovered on the fly. We report statistics on TEXTRUNNER\u2019s 11,000,000 highest probability tuples, and show that they contain over 1,000,000 concrete facts and over 6,500,000more abstract assertions.", "authors": ["Michele Banko", "Michael J. Cafarella", "Stephen Soderland", "M. Alexander Broadhead", "Oren Etzioni"], "id": "498bb0efad6ec15dd09d941fb309aa18d6df9f5f", "title": "Open Information Extraction from the Web", "references": []}, {"date": "1996", "abstract": "We propose a novel approach to unsupervised texture segmentation, which is formulated as a combinatorial optimization problem known as pairwise data clustering with a sparse neighborhood structure. Pairwise dissimilarities between texture blocks are measured in terms of distribution differences of multi-resolution features. The feature vectors are based an a Gabor wavelet image representation. To efficiently solve the data clustering problem a deterministic annealing algorithm based on a meanfield approximation is derived. An application to Brodatz-like microtexture mixtures is shown. We statistically adress the questions of adequacy of the proposed cost function and the quality of the deterministic annealing algorithm compared with its stochastic variants.", "authors": ["Hujun Yin", "Nigel M. Allinson"], "id": "cd75b16be178aa69ad24dc222c109dacac82f44c", "title": "Unsupervised segmentation of textured images by pairwise data clustering", "references": []}, {"date": "1992", "abstract": "A deterministic annealing approach is suggested to search for the optimal vector quantizer given a set of training data. The problem is reformulated within a probabilistic framework. No prior knowledge is assumed on the source density, and the principle of maximum entropy is used to obtain the association probabilities at a given average distortion. The corresponding Lagrange multiplier is inversely related to the 'temperature' and is used to control the annealing process. In this process, as the temperature is lowered, the system undergoes a sequence of phase transitions when existing clusters split naturally, without use of heuristics. The resulting codebook is independent of the codebook used to initialize the iterations. >", "authors": ["Kenneth Rose", "Eitan Gurewitz", "Geoffrey Charles Fox"], "id": "2ee14dd35886c44c87d66f8490528fa58c19fc25", "title": "Vector quantization by deterministic annealing", "references": []}, {"date": "1991", "abstract": "Semantic Scholar extracted view of \"Neural Circuits of the Cerebral Cortex\" by M. Abeles", "authors": ["M. Abeles"], "id": "c8f62c808df4597ee2e8c7dfadb9aca9e121ae73", "title": "Neural Circuits of the Cerebral Cortex", "references": []}, {"date": "1996", "abstract": "A graduated assignment algorithm for graph matching is presented which is fast and accurate even in the presence of high noise. By combining graduated nonconvexity, two-way (assignment) constraints, and sparsity, large improvements in accuracy and speed are achieved. Its low order computational complexity [O(lm), where l and m are the number of links in the two graphs] and robustness in the presence of noise offer advantages over traditional combinatorial approaches. The algorithm, not restricted to any special class of graph, is applied to subgraph isomorphism, weighted graph matching, and attributed relational graph matching. To illustrate the performance of the algorithm, attributed relational graphs derived from objects are matched. Then, results from twenty-five thousand experiments conducted on 100 mode random graphs of varying types (graphs with only zero-one links, weighted graphs, and graphs with node attributes and multiple link types) are reported. No comparable results have been reported by any other graph matching algorithm before in the research literature. Twenty-five hundred control experiments are conducted using a relaxation labeling algorithm and large improvements in accuracy are demonstrated.", "authors": ["Steven Gold", "Anand Rangarajan"], "id": "9899003369af99d02f699cbcbf48b79019666158", "title": "A Graduated Assignment Algorithm for Graph Matching", "references": ["d3886d4267d830d538074ecc4e6aa4ba22a16701", "356e3b50bd31c36f7b81526704411e7aed63bb21", "29017216d926309d2b428991ac1e9b0b2d261a2f", "4fc956c8d8b41c1e9c499cad0cf882debf45ca64", "7d9b251a729004e468680c55c99435dd8dd15b78", "932a0700186d586bd0e6fdcc342fea25e56ba152", "79ec67bbbcea16a55afeb7ca782cbd7672f3bd9f", "44b67a5991d1567db2cef4c5336f9ca812b2905a", "0426aece3d02fe68167e40f349a1996d17214861", "d541e96f3db3e836209bd4001e8710bed53e6f00"]}, {"date": "1995", "abstract": "We propose a model of the spatial visual processes underlying the identification and representation of the shape of primitive spatial regions. We propose that a region's boundaries are sensed at multiple scales by boundariness detectors that give graded responses, that stimulated boundariness detectors of similar scale, sigma, connect to one another across a distance that is proportional to their scale, and that they connect via cores, where a core encodes the middles and widths of the region and hence is a trace in (chi, gamma, sigma), i.e. 3-D scale space.", "authors": ["Christina A. Burbeck", "Stephen M. Pizer"], "id": "7540897771b4ee6848abe5def91d036059cbdac3", "title": "Object representation by cores: Identifying and representing primitive spatial regions", "references": ["150662e38fc67ca81ac93faf981cf2151fbf6a9a", "578bc7eeda773ee1f28a73b2a1f55086522514c9", "8b37258659bcdbc380b1e6c4e22cce9ea06397a1", "bdf9b8f4de001f5f37bc844efbce1210d581d599", "247e8d3bc80409fb85439a716633d1d95444291d", "75f60a199e06dd6e37a5c5b5edd44909bda89e9f", "3527d38ae640b25250001995127cb147aefd16ee", "7d4649bb4edb81bdd7c86ab192f64159df60564a", "2258afff0f49e71f11af3427a9633b7c926f94e9", "3b8e031a7e59a70cd9b7e874214093078feadb6b"]}, {"date": "1994", "abstract": "1. Introduction 2. Learnability theorem 3. A theorem of finite elasticity 4. Classical categorial grammar 5. Basic theory of rigid grammars 6. Learning from structures I: rigid, k-valued, and least-valued grammar 7. Learning from structures II: Subclasses of the optimal grammars 8. Learning from strings 9. Variations 10. Conclusions Appendix.", "authors": ["\u91d1\u6ca2 \u8aa0"], "id": "9d8f8caf065f3d5f329c52f7b92532cd4ee628ae", "title": "Learnable classes of categorial grammars", "references": []}, {"date": "1964", "abstract": "Semantic Scholar extracted view of \"A transformation for extracting new descriptors of shape\" by Harold Francis Blum", "authors": ["Harold Francis Blum"], "id": "28d35947b7885c87f565f07ab397200bf8a6f25e", "title": "A transformation for extracting new descriptors of shape", "references": []}, {"date": "1993", "abstract": "The authors consider the estimation of powerful statistical language models using a technique that scales from very small to very large amounts of domain-dependent data. They begin with improved modeling of the grammar statistics, based on a combination of the backing-off technique and zero-frequency techniques. These are extended to be more amenable to the particular system considered here. The resulting technique is greatly simplified, more robust, and gives improved recognition performance over either of the previous techniques. The authors also consider the problem of robustness of a model based on a small training corpus by grouping words into obvious semantic classes. This significantly improves the robustness of the resulting statistical grammar. A technique that allows the estimation of a high-order model on modest computation resources is also presented. This makes it possible to run a 4-gram statistical model of a 50-million word corpus on a workstation of only modest capability and cost. Finally, the authors discuss results from applying a 2-gram statistical language model integrated in the HMM (hidden Markov model) search, obtaining a list of the N-best recognition results, and rescoring this list with a higher-order statistical model.<<ETX>>", "authors": ["Paul Placeway", "Richard M. Schwartz", "Pascale Fung", "Long H. Nguyen"], "id": "a8fcc058607129590fa6ab692a38807efc7d7f1f", "title": "The estimation of powerful language models from small and large corpora", "references": ["4ee315a9781d37f6dd9efd452eb5ebc796f6e6d7", "fcd8599fa26d7742516cc88baf36a0ca5a96216a", "b0130277677e5b915d5cd86b3afafd77fd08eb2e", "c80bcd31f077f9a632ce959278d1c2fc095131a8", "1f5d21625f8264f455591b3c7cbdac18b983b3c0", "ac8b3c53acbbd6f92e4af991c11c81905f1e64bd"]}, {"date": "2003", "abstract": "Schema matching is a critical problem for integrating heterogeneous information sources. Traditionally, the problem of matching multiple schemas has essentially relied on finding pairwise-attribute correspondence. This paper proposes a different approach, motivated by integrating large numbers of data sources on the Internet. On this \"deep Web,\" we observe two distinguishing characteristics that offer a new view for considering schema matching: First, as the Web scales, there are ample sources that provide structured information in the same domains (e.g., books and automobiles). Second, while sources proliferate, their aggregate schema vocabulary tends to converge at a relatively small size. Motivated by these observations, we propose a new paradigm, statistical schema matching: Unlike traditional approaches using pairwise-attribute correspondence, we take a holistic approach to match all input schemas by finding an underlying generative schema model. We propose a general statistical framework MGS for such hidden model discovery, which consists of hypothesis modeling, generation, and selection. Further, we specialize the general framework to develop Algorithm MGSsd, targeting at synonym discovery, a canonical problem of schema matching, by designing and discovering a model that specifically captures synonym attributes. We demonstrate our approach over hundreds of real Web sources in four domains and the results show good accuracy.", "authors": ["Bin He", "Kevin Chen-Chuan Chang"], "id": "3b76b68c44e4d9f875e2aaa95eae689bbc67396c", "title": "Statistical schema matching across web query interfaces", "references": ["580221d63ae75bdc7d68829916cf608e44a56b27", "92d92fd3de1af6a46e8cd9ca841d5433e659179f"]}, {"date": "1984", "abstract": "Testing for uniformity in multidimensional data is important in exploratory pattern analysis, statistical pattern recognition, and image processing. The goal of this paper is to determine whether the data follow the uniform distribution over some compact convex set in K-dimensional space, called the sampling window. We first provide a simple, computationally efficient method for generating a uniformly distributed sample over a set which approximates the convex hul of the data. We then test for uniformity by comparing this generated sample to the data by using Friedman-Rafsky's minimal spanning tree (MST) based test. Experiments with both simulated and real data indicate that this MST-based test is useful in deciding if data are uniform.", "authors": ["Stephen P. Smith", "Anil K. Jain"], "id": "ac7dc9fb84958a107946175e46f0abc2d166f403", "title": "Testing for Uniformity in Multidimensional Data", "references": ["474515cd408f3303c1b7ee5f4016a302d9c2a264", "2a7978c9f7851c378666e98f64aacb4945cf8753", "538de63ffad8c8f1ca18584d6f0a5000f5591130", "50d9cc44a8d3cca1f07fc130ded954230ccb58d8", "c22c66439853251672536815974c4a0d65dd4831", "2bf41285a0e8df866cfa9709de0e428edd48ce9a"]}, {"date": "2007", "abstract": "We present YAGO, a light-weight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as HASONEPRIZE). The facts have been automatically extracted from Wikipedia and unified with WordNet, using a carefully designed combination of rule-based and heuristic methods described in this paper. The resulting knowledge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organizations, products, etc. with their semantic relationships - and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact correctness shows an accuracy of about 95%. YAGO is based on a logically clean model, which is decidable, extensible, and compatible with RDFS. Finally, we show how YAGO can be further extended by state-of-the-art information extraction techniques.", "authors": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum"], "id": "00a3f6924f90fcd77e6e7e6534b957a75d0ced07", "title": "Yago: a core of semantic knowledge", "references": ["9ef07373873cc0f0b940512dcdde4e7b54b0cfb0", "49bd7635eb8661b0b3f68713748c7008a6c0a7f6", "231c603525830d26368fc9d626a0fbbfa6979417", "543d58744f0402fe0cac4b5d52a943b110afd229"]}, {"date": "1973", "abstract": "A nonparametric clustering technique incorporating the concept of similarity based on the sharing of near neighbors is presented. In addition to being an essentially paraliel approach, the computational elegance of the method is such that the scheme is applicable to a wide class of practical problems involving large sample size and high dimensionality. No attempt is made to show how a priori problem knowledge can be introduced into the procedure.", "authors": ["Ray A. Jarvis", "Edward A. Patrick"], "id": "026826a0cfe545df8bb860c2484524d7494e76bd", "title": "Clustering Using a Similarity Measure Based on Shared Near Neighbors", "references": ["0086db40d10ea10b3967103389a2259d76076ce3", "b2e5aa7c5f54ffc39bedf4c23aa37b8993d1a8fc", "0ef296d4a607612cadffc05f9b81a2df070580f9", "1167ed73114f6553bfad608bbbdcad55c86a124c", "1b6409dd74be9810816ed2d6bca00b24a4a3b6e7", "1c141df452f2f7bf212a5bf5cd0afc3fba14604c", "be096caf67c64a5cd5223c25a4679db93a120899", "fcb044efd39ce5d1e09555dfe505b764e96b7773", "7821dc2746d56b67cf6502c7ab04d38af6bdca78", "4ae384fb530f986e4f80d03ada0abd3acf89dbc0"]}, {"date": "1978", "abstract": "A nonparametric, hierarchical, disaggregative clustering algorithm is developed using a novel similarity measure, called the mutual neighborhood value (MNV), which takes into account the conventional nearest neighbor ranks of two samples with respect to each other. The algorithm is simple, noniterative, requires low storage, and needs no specification of the expected number of clusters. The algorithm appears very versatile as it is capable of discerning spherical and nonspherical clusters, linearly nonseparable clusters, clusters with unequal populations, and clusters with lowdensity bridges. Changing of the neighborhood size enables discernment of strong or weak patterns.", "authors": ["K. Kushalappa Gowda", "G. Krishna"], "id": "194a68a6e7039c64ea09196fd56b5346f4a7226b", "title": "Disaggregative Clustering Using the Concept of Mutual Nearest Neighborhood", "references": []}, {"date": "2006", "abstract": "We propose a simple approach to combining first-order logic and probabilistic graphical models in a single representation. A Markov logic network (MLN) is a first-order knowledge base with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it specifies a ground Markov network containing one feature for each possible grounding of a first-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are efficiently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Optionally, additional clauses are learned using inductive logic programming techniques. Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach.", "authors": ["Matthew Richardson", "Pedro M. Domingos"], "id": "950a3c89dacbc3e7ddcd43d7ff6f985697e41cdb", "title": "Markov logic networks", "references": ["7d171d8b815f5b758d7b0a4db7090e352d15f2b4", "e9ef7893ae7ee6826a43b5f58365092194c0e213", "6f34268b463e0219a57d049aac6fd165f4afc4d9", "454742f723d5bdeb023d4a65ede020e881d68ae4", "a0f181370d9baa090048ea31c7763fcfbfae883a", "7907f8aff65a78143b58fb3922ece5f276aac418", "8a59ce04b6a0d923cd60dc1c16b6b7d99a60bffb", "555bbd43bc82725a96e029f79db8d4d003721dbb", "cbcc245b22a31f3a344e2240cab641d34ccb80ae", "306dfea047e91f5b2952fbd1aa0b4aeb422cf398"]}, {"date": "1989", "abstract": "Simulated annealing is a stochastic relaxation algorithm which has been used successfully to optimize functions of many variables. This paper analyzes the simulated annealing algorithm when applied to the minimization of functions from two common problems encountered in exploratory pattern analysis, projection and clustering. The projection is a nonlinear mapping of patterns in high dimension to two dimensions. The simulated annealing mapping is compared to gradient descent minimization of the same objective function as well as eigenvector projection. The simulated annealing clustering is compared to a k-means algorithm. \n \nEmpirical results show that simulated annealing can produce results as good as those obtained by conventional methods, but are impractical for small data sets because of the high computational cost. Simulated annealing does, in the case of the mapping problem, yield a better optimization and better retained structure for large data sets containing tight gaussian clusters.", "authors": ["Raymond W. Klein", "Richard C. Dubes"], "id": "eab6b613c5dd5a49c5b16723ff9fd21ccbd26646", "title": "Experiments in projection and clustering by simulated annealing", "references": ["474515cd408f3303c1b7ee5f4016a302d9c2a264", "8709ecd28e22981cbd8f5f7c600464846d1ab0f9", "bb51791e53ca6c070c8ae14869ef0b12cd336958", "dd5061631a4d11fa394f4421700ebf7e78dcbc59", "f6a13f116e270dde9d67848495f801cdb8efa25d", "a281d45dcffe4eb6c0ffd7f731bfa25d7887e0e0", "0d4769363bfa44e9127334a2cd1342b1fc17026c", "e19f625a7c84d10e95811912414ea2e4c743c130", "154f8a9906bcc99fca9b17aa521649b1c3734093", "d2cfb8b5047336ec10e793447a25bf0f6e392bf9"]}, {"date": "1992", "abstract": "We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested.", "authors": ["Marti A. Hearst"], "id": "dbfd191afbbc8317577cbc44afe7156df546e143", "title": "Automatic Acquisition of Hyponyms from Large Text Corpora", "references": ["ca40dc1300ab085406455894dd42fd02f9cc36f8", "e6b46497e5ac41e1cfeb7593f5d407d6445416e6", "43865ad56b3364b39ae3badf1fc212547292b335", "0c3264d6f5c501922c28aa122c49e90982267acd", "a24b123b4adf115a0826f0dd3d7b0ef6a182a3c5", "1bcb0d197554534702640626e57b085051747fce", "094c0495ebb34c7eb61bad86a96eeebab06dab08", "c6a24aa2b33a2875e4410eb17b44e280cddf4402", "4d58fbb388884caf78a0652a02ee279883d86f75", "2510562fc1f7ff1eba53731961d3b4c4bc5a5b09"]}, {"date": "1991", "abstract": "The usual convergence proof of the SMACOF algorithm model for least squares multidimensional scaling critically depends on the assumption of nonnegativity of the quantities to be fitted, called the pseudodistances. When this assumption is violated, erratic convergence behavior is known to occur. Three types of circumstances in which some of the pseudodistances may become negative are outlined: nonmetric multidimensional scaling with normalization on the variance, metric multidimensional scaling including an additive constant, and multidimensional scaling under the city-block distance model. A generalization of the SMACOF method is proposed to resolve the difficulty that is based on the same rationale frequently involved in robust fitting with least absolute residuals.", "authors": ["Willem J. Heiser"], "id": "d4ecb6b1cda922eb13a18ed6ea7d1e827c2efb6b", "title": "A generalized majorization method for least souares multidimensional scaling of pseudodistances that may be negative", "references": []}, {"date": "1978", "abstract": "Abstract We present here an application of symmetric axis geometry to shape classification and description. Shapes are segmented into simplified segments, in which a sequential string of features is derived. These are based on both width and axis properties, and two linear combinations of them, as well as their derivatives. These are felt to have intuitively and geometrically simple meanings. A weighting measure is developed which evaluates the importance of these shape descriptors. Consequently, small perturbations are recognizable despite their generation of sizable symmetric axis arcs. A variety of other segment measures for shape is introduced.", "authors": ["Harry Blum", "Roger N. Nagel"], "id": "95bcfe5a35df789554e1d9d35fc9fbf35d46fff5", "title": "Shape description using weighted symmetric axis features", "references": ["95e3f6ea58df4298f0e2e598bf075ea2a6bed719", "9c0b5cadcb78eb88cdb973d3425975450d63fb8e", "4b561391b0825ae0bd08c1c03425e1265f4f8c37", "28d35947b7885c87f565f07ab397200bf8a6f25e", "42b08f926db582c71eaca8f5f6b3d902364c177a", "7569758696f3d6f3e063d3ed77a986e9a70ca9f7", "33e17ea65b13c5a9d9bc6ee5a6a9f07f5d13a97f", "8b4184b6113d4c74dc3dc5f8e75b2860a55fa52b", "e47c491a8717592e4151e3777cc6f43a48575d13"]}, {"date": "1995", "abstract": "Abstract This paper considers the use of radial basis functions for modelling the non-linear transformation of a data set obtained by a multidimensional scaling analysis. This approach has two advantages over conventional nonmetric multidimensional scaling. It reduces the number of parameters to estimate and it provides a transformation that may be used on an unseen test set. A scheme based on iterative majorization is proposed for obtaining the parameters of the network.", "authors": ["Andrew R. Webb"], "id": "003739d648c7285c60d8aa9558f8efc713cd4fb0", "title": "Multidimensional scaling by iterative majorization using radial basis functions", "references": ["fa33e14211d22ffa4159c20a6e7a7a5f9217a859", "87ac8f9c35011a978897119b9224d35400dede36", "e04108dc293c9cd7cabf32ee1524eaab0d4641b3", "e5e6f7c3cab417ad4efe88b026ecb06ca5d11c7f", "a473eb418b4f01ba7576332f244d6316817e60e2", "0ada5f8d0c8f71a5fe7da79e400b28fc90a31ab3", "2eeae7ab0e71c75fc65370b6cb375ffafe9fe292", "d4ecb6b1cda922eb13a18ed6ea7d1e827c2efb6b", "154f8a9906bcc99fca9b17aa521649b1c3734093", "72b98c1ef62ce89d97d981fa5779858a2c347d67"]}, {"date": "1977", "abstract": "A new procedure is discussed which fits either the weighted or simple Euclidian model to data that may (a) be defined at either the nominal, ordinal, interval or ratio levels of measurement; (b) have missing observations; (c) be symmetric or asymmetric; (d) be conditional or unconditional; (e) be replicated or unreplicated; and (f) be continuous or discrete. Various special cases of the procedure include the most commonly used individual differences multidimensional scaling models, the familiar nonmetric multidimensional scaling model, and several other previously undiscussed variants.The procedure optimizes the fit of the model directly to the data (not to scalar products determined from the data) by an alternating least squares procedure which is convergent, very quick, and relatively free from local minimum problems.The procedure is evaluated via both Monte Carlo and empirical data. It is found to be robust in the face of measurement error, capable of recovering the true underlying configuration in the Monte Carlo situation, and capable of obtaining structures equivalent to those obtained by other less general procedures in the empirical situation.", "authors": ["Yoshio Takane", "Forrest W. Young", "Jan de Leeuw"], "id": "362d61dd2a41cbc4d31faec9abacf7b01380878b", "title": "Nonmetric individual differences multidimensional scaling: An alternating least squares method with optimal scaling features", "references": ["51c8ee575f6173fa28735c1701a08c885e517e87", "deb40e644f78e719ca510f82b889acd3e60fed96", "80a966fd907a88278e0cd4308044e33e7ed2c1df", "826f38640e622f102b6a6704694f1b2579428ebc", "3c7aa0e5212c0064142ea3f074ce6bbea8709fec", "1525978ca2c861952420711871d46dbf6bac1d84", "16e0ed03f6fd71559965accf035ed8ca0c05fbed", "e81e9d66af959af54285243b5f968782edb7f278", "eba9e3c13edc63f02c3b1d005059ee4dd854f838", "462f1b58d17af6530a84efcce617a49facac18a4"]}, {"date": "1997", "abstract": "Multidimensional scaling addresses the problem how proximity data can be faithfully visualized as points in a low-dimensional Euclidian space. The quality of a data embedding is measured by a cost function called stress which compares proximity values with Euclidian distances of the respective points. We present a novel deterministic annealing algorithm to efficiently determine embedding coordinates for this continuous optimization problem. Experimental results demonstrate the superiority of the optimization technique compared to conventional gradient descent methods. Furthermore, we propose a transformation of dissimilarities to reduce the mismatch between a high-dimensional data space and a low-dimensional embedding space.", "authors": ["Hansj\u00f6rg Klock", "Joachim M. Buhmann"], "id": "aaa566e3ddaef092001e173d252ef0b1fe134786", "title": "Multidimensional Scaling by Deterministic Annealing", "references": ["deb40e644f78e719ca510f82b889acd3e60fed96", "9899003369af99d02f699cbcbf48b79019666158", "e04108dc293c9cd7cabf32ee1524eaab0d4641b3", "3db87ebd106452cd14e811258dabe82a999f87cc", "dd5061631a4d11fa394f4421700ebf7e78dcbc59", "037947eec9079c7dea0db1c23e6fd8a4032e21dd", "ac2eeb625c78116a61fb8307fbd5a0fba26c99fa", "362d61dd2a41cbc4d31faec9abacf7b01380878b", "154f8a9906bcc99fca9b17aa521649b1c3734093", "c829b01d6aac1825acae7ea90a00fee4fbc2689e"]}, {"date": "1990", "abstract": "We describe the methods and hardware that we are using to produce a real-time demonstration of an integrated Spoken Language System. We describe algorithms that greatly reduce the computation needed to compute the N-Best sentence hypotheses. To avoid grammar coverage problems we use a fully-connected first-order statistical class grammar. The speech-search algorithm is implemented on a board with a single Intel i860 chip, which provides a factor of 5 speedup over a SUN 4 for straight C code. The board plugs directly into the VME bus of the SUN4, which controls the system and contains the natural language system and application back end.", "authors": ["Steve Austin", "Pat Peterson", "Paul Placeway", "Richard M. Schwartz", "Jeff Vandergrift"], "id": "ac8b3c53acbbd6f92e4af991c11c81905f1e64bd", "title": "Toward a Real-Time Spoken Language System Using Commercial Hardware", "references": ["df53e0dc66eb13bb51c6e4803ceae56d3ebe6f23", "bdb3f20fe41bb95f6bc9d162e827de8db3f952d7", "00e347a56ebb0cf9de041beb7b3fc62922cef48b", "265e506c9bfde064424b36d84424d2333ec1ff13", "b31c0ce927fc2060066dd417532b8836e63118ce", "4edbe024f7a7b83766791e1cbcc10936bf373f7c"]}, {"date": "1993", "abstract": "Data clustering amounts to a combinatorial optimization problem to reduce the complexity of a data representation and to increase its precision. Central and pairwise data clustering are studied in the maximum entropy framework. For central clustering we derive a set of reestimation equations and a minimization procedure which yields an optimal number of clusters, their centers and their cluster probabilities. A meanfield approximation for pairwise clustering is used to estimate assignment probabilities. A selfconsistent solution to multidimensional scaling and pairwise clustering is derived which yields an optimal embedding and clustering of data points in a d-dimensional Euclidian space.", "authors": ["Joachim M. Buhmann", "Thomas Hofmann"], "id": "037947eec9079c7dea0db1c23e6fd8a4032e21dd", "title": "Central and Pairwise Data Clustering by Competitive Neural Networks", "references": ["af77de34c7d5435f3658cf106d53ccd72e24c89d", "b829f51389c328f5afa13ceaa85f385aec0a0876", "81a5952532cdd48eec5e3dc326907c36a70e0a24", "a9f7cfa972a502f2a9d724057ec94321f7a0fc21", "bfedbfa174502a12ad9227b361b9906887fb4709", "59d3b4027480c326cf541c7da69aeaf701040304", "7c46799502bebfe6a9ae0f457b7b8b92248ec260", "115462633cadff2e37e6eec8e953543d38e1db63", "f087259663c7f770cb201bbb07ce86cd6de2067f", "c829b01d6aac1825acae7ea90a00fee4fbc2689e"]}, {"date": "1991", "abstract": "It has generally been assumed that rapid visual search is based on simple features and that spatial relations between features are irrelevant for this task. Seven experiments involving search for line drawings contradict this assumption; a major determinant of search is the presence of line junctions. Arrow- and Y-junctions were detected rapidly in isolation and when they were embedded in drawings of rectangular polyhedra. Search for T-junctions was considerably slower. Drawings containing T-junctions often gave rise to very slow search even when distinguishing arrow- or Y-junctions were present. This sensitivity to line relations suggests that preattentive processes can extract 3-dimensional orientation from line drawings. A computational model is outlined for how this may be accomplished in early human vision.", "authors": ["James T. Enns", "Ronald A. Rensink"], "id": "3b8e031a7e59a70cd9b7e874214093078feadb6b", "title": "Preattentive recovery of three-dimensional orientation from line drawings.", "references": ["45e8a8e1c66e56a996e85f713fc2d75fea9bdc68", "8735690a9e8f8884bf27717877ddf7f9071472e5", "9ee1e8a9cc139282b1786d8c45c94399f150dd5d", "2dcb058ad60cad413378a26d723012001bcdd1dc", "5ea2d4236be6101826922b16b70bb90ad74f0007", "887d3a5edbf6f05f4e6d0839911efe9353be2c24", "e8f147a94d2b58c099847b963137f9779fe31325", "d42bf29ce07bd9f1ffe3de3f10f69730810ecb8f", "a4f8599c040dfd4876d9a5d99545b35b6a4e7a57", "ee71cd6e2d60b932b4020e4ab4c1093bb20ebc52"]}, {"date": "1964", "abstract": "Multidimensional scaling is the problem of representingn objects geometrically byn points, so that the interpoint distances correspond in some sense to experimental dissimilarities between objects. In just what sense distances and dissimilarities should correspond has been left rather vague in most approaches, thus leaving these approaches logically incomplete. Our fundamental hypothesis is that dissimilarities and distances are monotonically related. We define a quantitative, intuitively satisfying measure of goodness of fit to this hypothesis. Our technique of multidimensional scaling is to compute that configuration of points which optimizes the goodness of fit. A practical computer program for doing the calculations is described in a companion paper.", "authors": ["Joseph B. Kruskal"], "id": "e04108dc293c9cd7cabf32ee1524eaab0d4641b3", "title": "Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis", "references": ["b7180207a9441750f4f7e72b04a36a8378590a91", "deb40e644f78e719ca510f82b889acd3e60fed96", "6c38bbfba94d187bad14314086ba2ca0ab75ec9f", "ba3718730b7009202af2816e3dbe8f5e7851d977", "5abd46dc29463d0e2da7905194e79123be7eaa82", "f6857a8323044403cc24974f97d4586a6c445f44", "188e350a1323dc88aa79e74beb9470a102610486", "a090d68a1c052868089fd91ad5a8ad2f798b439a"]}, {"date": "1977", "abstract": "(NOTE: Each chapter concludes with Problems and Complements, Notes, and References.) 1. Statistical Models, Goals, and Performance Criteria. Data, Models, Parameters, and Statistics. Bayesian Models. The Decision Theoretic Framework. Prediction. Sufficiency. Exponential Families. 2. Methods of Estimation. Basic Heuristics of Estimation. Minimum Contrast Estimates and Estimating Equations. Maximum Likelihood in Multiparameter Exponential Families. Algorithmic Issues. 3. Measures of Performance. Introduction. Bayes Procedures. Minimax Procedures. Unbiased Estimation and Risk Inequalities. Nondecision Theoretic Criteria. 4. Testing and Confidence Regions. Introduction. Choosing a Test Statistic: The Neyman-Pearson Lemma. Uniformly Most Powerful Tests and Monotone Likelihood Ratio Models. Confidence Bounds, Intervals and Regions. The Duality between Confidence Regions and Tests. Uniformly Most Accurate Confidence Bounds. Frequentist and Bayesian Formulations. Prediction Intervals. Likelihood Ratio Procedures. 5. Asymptotic Approximations. Introduction: The Meaning and Uses of Asymptotics. Consistency. First- and Higher-Order Asymptotics: The Delta Method with Applications. Asymptotic Theory in One Dimension. Asymptotic Behavior and Optimality of the Posterior Distribution. 6. Inference in the Multiparameter Case. Inference for Gaussian Linear Models. Asymptotic Estimation Theory in p Dimensions. Large Sample Tests and Confidence Regions. Large Sample Methods for Discrete Data. Generalized Linear Models. Robustness Properties and Semiparametric Models. Appendix A: A Review of Basic Probability Theory. The Basic Model. Elementary Properties of Probability Models. Discrete Probability Models. Conditional Probability and Independence. Compound Experiments. Bernoulli and Multinomial Trials, Sampling with and without Replacement. Probabilities on Euclidean Space. Random Variables and Vectors: Transformations. Independence of Random Variables and Vectors. The Expectation of a Random Variable. Moments. Moment and Cumulant Generating Functions. Some Classical Discrete and Continuous Distributions. Modes of Convergence of Random Variables and Limit Theorems. Further Limit Theorems and Inequalities. Poisson Process. Appendix B: Additional Topics in Probability and Analysis. Conditioning by a Random Variable or Vector. Distribution Theory for Transformations of Random Vectors. Distribution Theory for Samples from a Normal Population. The Bivariate Normal Distribution. Moments of Random Vectors and Matrices. The Multivariate Normal Distribution. Convergence for Random Vectors: Op and Op Notation. Multivariate Calculus. Convexity and Inequalities. Topics in Matrix Theory and Elementary Hilbert Space Theory. Appendix C: Tables. The Standard Normal Distribution. Auxiliary Table of the Standard Normal Distribution. t Distribution Critical Values. X 2 Distribution Critical Values. F Distribution Critical Values. Index.", "authors": ["Peter J. Bickel", "Kjell A. Doksum"], "id": "92d92fd3de1af6a46e8cd9ca841d5433e659179f", "title": "Mathematical Statistics: Basic Ideas and Selected Topics", "references": []}, {"date": "2006", "abstract": "Spring Symposium on Formalizing and Compiling Background Knowledge and Its Applications to Knowledge Representation and Question Answering, Stanford, CA, March 2006.", "authors": ["Cynthia Matuszek", "John Cabral", "Michael J. Witbrock", "John DeOliveira"], "id": "543d58744f0402fe0cac4b5d52a943b110afd229", "title": "An Introduction to the Syntax and Content of Cyc", "references": ["676beb5e2182eef6c98cac65529da7bd4c81530d", "372dbeb37535ae591067425ee3c41927bca02bc9", "16b0058e31f8f0e7cc9e83acf2ff9654d7cf66c5", "b1e7bf85c7caf1306fa27802218a8e2cdc8f4268", "1d9e9a15401f037a7bff7c314ec7f8b003fcdf74", "4a5f9ac78a56c5f10fd925e6782e54a45ecd72eb", "0256614d1dc55962708db9b791f70a29370f1d76", "d97e516d0ec910d37f99618d030a95882174f762", "05e280bdaed6488a45726d6240012d865ddac892"]}, {"date": "2001", "abstract": "Abstract. Schema matching is a basic problem in many database application domains, such as data integration, E-business, data warehousing, and semantic query processing. In current implementations, schema matching is typically performed manually, which has significant limitations. On the other hand, previous research papers have proposed many techniques to achieve a partial automation of the match operation for specific application domains. We present a taxonomy that covers many of these existing approaches, and we describe the approaches in some detail. In particular, we distinguish between schema-level and instance-level, element-level and structure-level, and language-based and constraint-based matchers. Based on our classification we review some previous match implementations thereby indicating which part of the solution space they cover. We intend our taxonomy and review of past work to be useful when comparing different approaches to schema matching, when developing a new match algorithm, and when implementing a schema matching component.", "authors": ["Erhard Rahm", "Philip A. Bernstein"], "id": "580221d63ae75bdc7d68829916cf608e44a56b27", "title": "A survey of approaches to automatic schema matching", "references": ["1de22847881b12e23c52eb5e1f09c3ab5a73f4d9", "d5fc8a64a05ae1f8f148a2fee15c75d7bee2ea2f", "d459636cbbc6995f52698c6831aef602145b7bfe", "7ff9bf4d58358fc008b059028a3e33919d12b335", "7f8941fad9371a9867d239c58e4bfce38d1e4825", "22f895abe0e60563d328380adb9438874fdd33df", "2932bbe63ff3acdda4a0afe7804a3f832f85a03b", "9e8947318d6fc371614e243410d1033bb61b3690", "9ee0e31732db0bd401d32d5bb027c9c01a2e3ee9", "e1a647721a518c6f4f3ff8bc9122294432745361"]}, {"date": "1971", "abstract": "An approach to clustering and decision making is presented where a prior problem knowledge is inserted interactively. The problem knowledge inserted is in the form of subcategory mean vectors and covariance matrices and in the expert's confidence that these means and covariances accurately characterize the category. Then observations of patterns from the category are used to update these a priori supplied means and covariances. The extent to which new observations update the a priori values depends upon the expert's a priori confidence.", "authors": ["Edward A. Patrick", "Leon Y. L. Shen"], "id": "fcb044efd39ce5d1e09555dfe505b764e96b7773", "title": "Interactive Use of Problem Knowledge for Clustering and Decision Making", "references": ["ad44c1c2465ec1c5e95567b7bcb46a28feb9b626", "a25e236cded443ce7914cb9a85ceef28bd4e09c6", "7821dc2746d56b67cf6502c7ab04d38af6bdca78", "32b4c28fafc556040135a998fb31a129b2c2a2ea"]}, {"date": "1968", "abstract": "The unsupervised estimation problem has received considerable attention during the last three years. The problem usually considered, however, is only one of a class of unsupervised estimation problems. In this paper, a \"mixture approach\" defined previously is used to define this class of unsupervised estimation problems, and state precisely the a priori knowledge used to define each problem. After using available a priori knowledge to construct precisely the mixture appropriate to the unsupervised problem, the parameters characterizing this particular unsupervised problem can be estimated, or a Bayes minimum conditional risk receiver can be constructed. The class of unsupervised estimation problems includes the following cases: unknown number of pattern classes, dependent observation vectors, nonstationary class probabilities, more than one vector observation taken with a single class active, lack of synchronization, and unsupervised learning control and communications.", "authors": ["Edward A. Patrick"], "id": "be096caf67c64a5cd5223c25a4679db93a120899", "title": "On a class of unsupervised estimation problems", "references": []}, {"date": "1969", "abstract": "The unsupervised estimation problem has been conveniently formulated in terms of a mixture density. It has been shown that a criterion naturally arises whose maximum defines the Bayes minimum risk solution. This criterion is the expected value of the natural log of the mixture density. By making the assumptions that the component densities in the mixture are truncated Gaussian, the criterion has a greatly simplified form. This criterion can be used to resolve mixtures when the number of classes as well as the class covariances are unknown. In this paper a technique is presented where an assumed test covariance is supplied by an experimenter who uses a test function as a \"portable magnifying glass\" to examine data. Because the experimenter supplies the covariance and thus the test function, the technique is especially suited for interactive data analysis.", "authors": ["Edward A. Patrick", "Frederic P. Fischer"], "id": "7821dc2746d56b67cf6502c7ab04d38af6bdca78", "title": "Cluster Mapping with Experimental Computer Graphics", "references": ["9bca53122f2fc1806032603adda66a35242548db", "0086db40d10ea10b3967103389a2259d76076ce3", "5567deb3f3f1fbe5671ad830970325d336870850", "0ecb9e7861ceec80956f2fa7efbbb00e12d8dda9", "f62f90975cb8caed8efbf9dc3c4384505d9ef969", "1b6409dd74be9810816ed2d6bca00b24a4a3b6e7", "ad44c1c2465ec1c5e95567b7bcb46a28feb9b626", "154f8a9906bcc99fca9b17aa521649b1c3734093", "1413ba7961d2fef8aadadc5c07f55bf7708a787c", "145d334f7d60bb5a4ed904365eeb035f068fa826"]}, {"date": "2001", "abstract": "A new, general approach is described for approximate inference in first-order probabilistic languages, using Markov chain Monte Carlo (MCMC) techniques in the space of concrete possible worlds underlying any given knowledge base. The simplicity of the approach and its lazy construction of possible worlds make it possible to consider quite expressive languages. In particular, we consider two extensions to the basic relational probability models (RPMs) defined by Koller and Pfeffer, both of which have caused difficulties for exact algorithms. The first extension deals with uncertainty about relations among objects, where MCMC samples over relational structures. The second extension deals with uncertainty about the identity of individuals, where MCMC samples over sets of equivalence classes of objects. In both cases, we identify types of probability distributions that allow local decomposition of inference while encoding possible domains in a plausible way. We apply our algorithms to simple examples and show that the MCMC approach scales well.", "authors": ["Hanna M. Pasula", "Stuart J. Russell"], "id": "7d171d8b815f5b758d7b0a4db7090e352d15f2b4", "title": "Approximate inference for first-order probabilistic languages", "references": ["71f4bdd0b59d68076419d81cca12dda49edd3b81", "07a8b11ffd6146b21192cdf17f35f9a11f5176a1", "c10ad6e809ae7d73beba19efbd74acd1e54ed5c3", "9965aedc8690e3fbeaa3ddab27905983cbe7395b", "ee566259db1fa8de24c47da0f5bc24600015dd51"]}, {"date": "1985", "abstract": "It is shown that simulated annealing, a statistical mechanics method recently proposed as a tool in solving complex optimization problems, can be used in problems arising in image processing. The problems examined are the estimation of the parameters necessary to describe a geometrical pattern corrupted by noise, the smoothing of bi-level images, and the process of halftoning a continuous-level image. The analogy between the system to be optimized and an equivalent physical system, whose ground state is sought, is put forward by showing that some of these problems are formally equivalent to ground state problems for two-dimensional Ising spin systems. In the case of low signal-to-noise ratios (particularly in image smoothing), the methods proposed here give better results than those obtained with standard techniques.", "authors": ["Paolo Carnevali", "Lattanzio Coletti", "Stefano Patarnello"], "id": "d2cfb8b5047336ec10e793447a25bf0f6e392bf9", "title": "Image Processing by Simulated Annealing", "references": []}, {"date": "1971", "abstract": "A family of graph-theoretical algorithms based on the minimal spanning tree are capable of detecting several kinds of cluster structure in arbitrary point sets; description of the detected clusters is possible in some cases by extensions of the method. Development of these clustering algorithms was based on examples from two-dimensional space because we wanted to copy the human perception of gestalts or point groupings. On the other hand, all the methods considered apply to higher dimensional spaces and even to general metric spaces. Advantages of these methods include determinacy, easy interpretation of the resulting clusters, conformity to gestalt principles of perceptual organization, and invariance of results under monotone transformations of interpoint distance. Brief discussion is made of the application of cluster detection to taxonomy and the selection of good feature spaces for pattern recognition. Detailed analyses of several planar cluster detection problems are illustrated by text and figures. The well-known Fisher iris data, in four-dimensional space, have been analyzed by these methods also. PL/1 programs to implement the minimal spanning tree methods have been fully debugged.", "authors": ["Charles T. Zahn"], "id": "4ae384fb530f986e4f80d03ada0abd3acf89dbc0", "title": "Graph-Theoretical Methods for Detecting and Describing Gestalt Clusters", "references": ["e0945f426f71b5ca8ef0f283f2111bb81245a579", "0efb841403aa6252b39ae6975c1cc5410554ef7b", "0094175f4e0e341af2e045734664c9bd5404471d", "e609ab97186834bd97d3fbc0c97a19744d752c23", "cac7561a417ac6c2d3f1b42fa4ca55d2a7466687", "c70d1a2656ecc1335fd0c345e5014756541cae47", "a6547a35e3a87a65347e2cf077ef6f1aac278a39", "459347a1e38789cbaa98d1609ab00baa26dcc925", "686a83bc3f66cbde6d13e0dc3e045a2666ad72d2", "2f66d008f37665df60aea2e8fa1c3d7265bd9bce"]}, {"date": "2003", "abstract": "First-order probabilistic models are recognized as efficient frameworks to represent several realworld problems: they combine the expressive power of first-order logic, which serves as a knowledge representation language, and the capability to model uncertainty with probabilities. Among existing models, it is usual to distinguish the domain-frequency approach from the possible-worlds approach. Bayesian logic programs (BLPs, which conveniently encode possible-worlds semantics) and stochastic logic programs (SLPs, often referred to as a domain-frequency approach) are promising probabilistic logic models in their categories. This paper is aimed at comparing the respective expressive power of these frameworks. We demonstrate relations between SLPs\u2019 and BLPs\u2019 semantics, and argue that SLPs can encode the same knowledge as a subclass of BLPs. We introduce extended SLPs which lift the latter result to any BLP. Converse properties are reviewed, and we show how BLPs can define the same semantics as complete, range-restricted, non-recursive SLPs. Algorithms that translate BLPs into SLPs (and vice versa) are provided, as well as worked examples of the intertranslations of SLPs and BLPs.", "authors": ["Aymeric Puech"], "id": "306dfea047e91f5b2952fbd1aa0b4aeb422cf398", "title": "A Comparison of Stochastic Logic Programs and Bayesian Logic Programs", "references": []}, {"date": "1993", "abstract": "Vector quantization is a data compression method by which a set of data points is encoded by a reduced set of reference vectors: the codebook. A vector quantization strategy is discussed that jointly optimizes distortion errors and the codebook complexity, thereby determining the size of the codebook. A maximum entropy estimation of the cost function yields an optimal number of reference vectors, their positions, and their assignment probabilities. The dependence of the codebook density on the data density for different complexity functions is investigated in the limit of asymptotic quantization levels. How different complexity measures influence the efficiency of vector quantizers is studied for the task of image compression. The wavelet coefficients of gray-level images are quantized, and the reconstruction error is measured. The approach establishes a unifying framework for different quantization methods like K-means clustering and its fuzzy version, entropy constrained vector quantization or topological feature maps, and competitive neural networks. >", "authors": ["Joachim M. Buhmann", "Hans K\u00fchnel"], "id": "c829b01d6aac1825acae7ea90a00fee4fbc2689e", "title": "Vector quantization with complexity costs", "references": ["af77de34c7d5435f3658cf106d53ccd72e24c89d", "81a5952532cdd48eec5e3dc326907c36a70e0a24", "7685ec3689ec5bbe067d96d23f6c783cedf02847", "78f6560492dca53e60e601787a071f75744ad80d", "568f602166e429666cf09669c018d28582ef8208", "b24e81f917a310cf22d62f49d8da0ec0e572344a", "9241ea3d8cb85633d314ecb74b31567b8e73f6af", "803367ea78d349d310a224503270a006c2bc0c66", "8bec18c2dc662e3b2212c1863146cfcd1686b272", "c3da7bdcbf7cba94ce64abc3aff0f9b947a5cab9"]}, {"date": "1967", "abstract": "Abstract Suppose given a set of similarities (or dissimilarities) between pairs of of objects from some set of objects, such as animal species, books, colours. We wish to construct from this similarity matrix a tree, or nested set of clusterings of the objects; graphs of trees provide a striking visual display of similarity groupings of the objects. The construction requires (1) a definition specifying when a similarity matrix has exact tree structure, (2) a measure of distance between any two similarity matrices, which yields (when combined with (1)) a measure of distance between any similarity matrix and any tree, (3) a family of local operations on a tree, which can be used to search out trees which best fit a given similarity matrix. The construction technique is applied to voting behaviour of the 50 United States in the last 13 presidential elections, giving a tree clustering of the states.", "authors": ["J. A. Hartigan"], "id": "3db87ebd106452cd14e811258dabe82a999f87cc", "title": "Representation of Similarity Matrices by Trees", "references": []}, {"date": "1993", "abstract": "Semantic Scholar extracted view of \"A Handbook of Small Data Sets\" by David J. Hand et al.", "authors": ["David J. Hand", "Fergus Daly", "Kevin McConway", "David Lunn", "E. Ostrowski"], "id": "72b98c1ef62ce89d97d981fa5779858a2c347d67", "title": "A Handbook of Small Data Sets", "references": []}, {"date": "1970", "abstract": "SUMMARY It is assumed that observations on a set of variables have a multivariate normal distribution with a general parametric form of the mean vector and the variance-covariance matrix. Any parameter of the model may be fixed, free or constrained to be equal to other parameters. The free and constrained parameters are estimated by maximum likelihood. A wide range of models is obtained from the general model by imposing various specifications on the parametric structure of the general model. Examples are given of areas and problems, especially in the behavioural sciences, where the method may be useful. 1. GENERAL METHODOLOGY 11. The general model We consider a data matrix X = {xOq} of N observations on p response variables and the following model. Rows of X are independently distributed, each having a multivariate normal distribution with the same variance-covariance matrix E of the form", "authors": ["Karl G. J\u00f6reskog"], "id": "462f1b58d17af6530a84efcce617a49facac18a4", "title": "A general method for analysis of covariance structures", "references": []}, {"date": "1970", "abstract": "The degree of metric determinancy afforded by nonmetric multidimensional scaling was investigated as a function of the number of points being scaled, the true dimensionality of the data being scaled, and the amount of error contained in the data being scaled. It was found 1) that if the ratio of the degrees of freedom of the data to that of the coordinates is sufficiently large then metric information is recovered even when random error is present; and 2) when the number of points being scaled increases the stress of the solution increases even though the degree of metric determinacy increases.", "authors": ["Forrest W. Young"], "id": "eba9e3c13edc63f02c3b1d005059ee4dd854f838", "title": "Nonmetric multidimensional scaling: Recovery of metric information", "references": []}, {"date": "1976", "abstract": "A method is discussed which extends canonical regression analysis to the situation where the variables may be measured at a variety of levels (nominal, ordinal, or interval), and where they may be either continuous or discrete. There is no restriction on the mix of measurement characteristics (i.e., some variables may be discrete-ordinal, others continuous-nominal, and yet others discrete-interval). The method, which is purely descriptive, scales the observations on each variable, within the restriction imposed by the variable's measurement characteristics, so that the canonical correlation is maximal. The alternating least squares algorithm is discussed. Several examples are presented. It is concluded that the method is very robust. Inferential aspects of the method are not discussed.", "authors": ["Forrest W. Young", "Jan de Leeuw", "Yoshio Takane"], "id": "e81e9d66af959af54285243b5f968782edb7f278", "title": "Regression with qualitative and quantitative variables: An alternating least squares method with optimal scaling features", "references": []}, {"date": "1987", "abstract": "Prinzmetal (1981) has argued that the visual system does not integrate features on the basis of their proximity to one another; rather, the visual system is likely to integrate features from the same perceptual group regardless of their proximity. The present paper examines this hypothesis by studying the effect, on judgments of 90\u00b0 corners, of several geometric characteristics that can affect perceptual organization and can be spatially remote (parallel lines, enclosure, and regularity of line length). Results suggest that both parallel lines and enclosure can affect 90\u00b0 corner judgments even when the parallel lines and enclosure lines are spatially remote from the corner judged. Regularity of line length was found to affect judgments only of small stimuli. The results partially support Prinzmetal\u2019s hypothesis. Emergent features provide a better explanation of obtained results.", "authors": ["Deborah Butler", "Ann M. Kring"], "id": "ee71cd6e2d60b932b4020e4ab4c1093bb20ebc52", "title": "Integration of features in depictions as a function of size", "references": []}, {"date": "1957", "abstract": "Semantic Scholar extracted view of \"A measure of stimulus similarity and errors in some paired-associate learning tasks.\" by Ernst Z. Rothkopf", "authors": ["Ernst Z. Rothkopf"], "id": "a090d68a1c052868089fd91ad5a8ad2f798b439a", "title": "A measure of stimulus similarity and errors in some paired-associate learning tasks.", "references": []}, {"date": "2003", "abstract": "This paper presents an automatic approach for learning semantic criteria for the mass versus count noun distinction by induction over the lexical mappings contained in the Cyc knowledge base. This produces accurate results (89.5%) using a decision tree that only incorporates semantic features (i.e., Cyc ontological types). Comparable results (86.9%) are obtained using OpenCyc, the publicly available version of Cyc. For broader applicability, the mass noun criteria using Cyc are converted into criteria using WordNet, preserving the general accuracy (86.3%).", "authors": ["Tom O'Hara", "Nancy Salay", "Michael Witbrock", "Dave Schneider", "Bjrn Aldag", "Stefano Bertolo", "Kathy Panton", "Fritz Lehmann", "Jon Curtis", "Mervyn Smith", "David. Baxter", "Peter Wagner"], "id": "676beb5e2182eef6c98cac65529da7bd4c81530d", "title": "Inducing criteria for mass noun lexical mappings using the Cyc KB, and its extension to WordNet", "references": ["2df71bac56ceb1e65921821d6659ab803b93c2d4", "2bb404704664b78510f06f2c40dd9de7c554f0b6", "0e93345171c29d7cf746182c264a56c15a88e5c6", "b1e7bf85c7caf1306fa27802218a8e2cdc8f4268", "0450f071ff01e62a4cc10cd9a7fba4806402434c", "26ae952599aa9ba5815a80356024258247fc2b10", "f4645e9fcf35616a1a43ebc2c4634eecc99dfca3", "917da625c3c510e4221aeaf88065a4e1ad982438", "cffe284e3e6f27107b6156bb7d691e7c5025d2ea"]}, {"date": "2005", "abstract": "Cyc is the largest existing common-sense knowledge base. Its ontology makes heavy use of higher-order logic constructs such as a context system, first class predicates, etc. Many of these higher-order constructs are believed to be key to Cyc\u2019s ability to represent common-sense knowledge and reason with it efficiently. In this paper, we present a translati on of a large part (around 90%) of the Cyc ontology into FirstOrder Logic. We discuss our methodology, and the tradeoffs between expressivity and efficiency in representation and reasoning. We also present the results of experiments using VAMPIRE, SPASS, and the E Theorem Prover on the firstorderized Cyc KB. Our results indicate that, while the use of higher-order logic is not essential to the representabilit y of common-sense knowledge, it greatly improves the efficiency of reasoning.", "authors": ["Deepak Ramachandran", "Pace Reagan", "Keith Goolsbey"], "id": "05e280bdaed6488a45726d6240012d865ddac892", "title": "First-Orderized ResearchCyc : Expressivity and Efficiency in a Common-Sense Ontology", "references": ["9421825e01e312fa42b8205975be41ffb9e752f7", "aa081df88b5fa1e407bad7ebbf45f6c69d368b92", "f1dde5cf696510b4b0eeffc40b770779b318a5da", "7cabc21f57b5df93534151df4cc5d09e64db2ad6", "c457fc4ae15d2949cc6837dc25892daab82aa384", "49bd7635eb8661b0b3f68713748c7008a6c0a7f6", "0d20328a0dfd65b3d62315185190d3df7629fa0f", "3d63e465a826ea1cc6bf6e82d1e99f29323f0c19", "2418ee831c4fa49aa7b327bfa944f49747195e6c"]}, {"date": "2004", "abstract": "The Halo Pilot, a six-month effort to evaluate the state-of-the-art in applied Knowledge Representation and Reasoning (KRR) systems, collaboratively developed a taxonomy of failures with the goal of creating a common framework of metrics against which we could measure inter- and intra- system failure characteristics of each of the three Halo knowledge applications. This platform independent taxonomy was designed with the intent of maximizing its coverage of potential failure types; providing the necessary granularity and precision to enable clear categorization of failure types; and providing a productive framework for short and longer term corrective action. \n \nExamining the failure analysis and initial empirical use of the taxonomy provides quantitative insights into the strengths and weaknesses of individual systems and raises some issues shared by all three. These results are particularly interesting when considered against the long history of assumed reasons for knowledge system failure. Our study has also uncovered some shortcomings in the taxonomy itself, implying the need to improve both its granularity and precision. It is the hope of Project Halo to eventually produce a failure taxonomy and associated methodology that will be of general use in the fine-grained analysis of knowledge systems.", "authors": ["Noah S. Friedland", "Paul G. Allen", "Michael J. Witbrock", "Gavin Matthews", "Nancy Salay", "Pierluigi Miraglia", "J\u00fcrgen Angele", "Steffen Staab", "David J. Israel", "Vinay K. Chaudhri", "Bruce W. Porter", "Ken Barker", "Peter Clark"], "id": "d97e516d0ec910d37f99618d030a95882174f762", "title": "Towards a Quantitative, Platform-Independent Analysis of Knowledge Systems", "references": ["332e56f0e45ed10dd6b989015e0456f2b7220cb6", "a80c47321f23f1fb0589739414658ac1a98918cb", "a7a4b007728c0736936c9186d84492cc7cdba7b8", "d86b0feed5166806c5868532986ef9e203ca7e71", "80e82c0ff8130705dd0e8bbb50380bbe813d0526", "c848fea059184c3f0edc2e1f3534a34465f9737e", "4bdda26a387760c7187f2311386108f7dc12b3de", "d0b6a65e75125b5ad82944c44cc00737c4b846d0", "1d21fe9d68fac2a05e5f50e3ff8fcb8037042523", "e10ee18c8bfc096ec5b5b14260d2e7ab7afdb0d9"]}, {"date": "1999", "abstract": "In previous work, we pointed out the limitations of standard Bayesian networks as a modeling framework for large, complex domains. We proposed a new, richly structured modeling language, Object-oriented Bayesian Networks, that we argued would be able to deal with such domains. However, it turns out that OOBNs are not expressive enough to model many interesting aspects of complex domains: the existence of specific named objects, arbitrary relations between objects, and uncertainty over domain structure. These aspects are crucial in real-world domains such as battlefield awareness. In this paper, we present SPOOK, an implemented system that addresses these limitations. SPOOK implements a more expressive language that allows it to represent the battlespace domain naturally and compactly. We present a new inference algorithm that utilizes the model structure in a fundamental way, and show empirically that it achieves orders of magnitude speedup over existing approaches.", "authors": ["Avi Pfeffer", "Daphne Koller", "Brian Milch", "Ken T. Takusagawa"], "id": "ee566259db1fa8de24c47da0f5bc24600015dd51", "title": "SPOOK: A system for probabilistic object-oriented knowledge representation", "references": ["916ae7864cee447d7a3835e23367d2f3dbb6c8a4", "7c1f5a78fdb24266467ee95dbae4f2447a0add17", "8089729d8711b3ef0de37eb6016ca3a311b491b5", "b97ec7b4f8b3cd921bd44b962be00dbb199499be", "419438bc4f6652784f42cf3e62c975a5c89b817e", "7c1178f4bc8089a19ab3f52215bd14d2356f378f", "5bdd9a3317b43966f97b5c70c55c46fd19335049"]}, {"date": "1962", "abstract": "A pattern recognition technique is described in which a parametric representation of input signals or stimuli is employed. An input is considered as a vector, while the stimulus class is a multivariate process in the vector space. An adaptive sample set construction technique is described through which the conditional joint probability density of a class is approximated by the sum of Gaussian densities. The mean of each such density is an adaptively chosen \"typical\" sample of the class, and the set of samples so chosen are contained in the region of the space in which samples of the class are most populous. The decision process using the typical samples partitions the space into regions that envelop the chosen samples of a class. Arbitrary shaped and multiply connected regions can be constructed in this way, and multimodal probability densities can be approximated with a computationally simple procedure. Decision making on an incomplete set of parameters and on multiple observations of the input stimulus are discussed. This technique was successfully applied to the automatic recognition of speaker identity regardless of the spoken test. Experimental results are given.", "authors": ["George S. Sebestyen"], "id": "9bca53122f2fc1806032603adda66a35242548db", "title": "Pattern recognition by an adaptive process of sample set construction", "references": []}, {"date": "1955", "abstract": "Linear graphs have been utilized by several authors to provide a geometric depiction of the structure of a group. Graphs will be defined in the next section where it will also be shown that they are in a one-to-one correspondence with certain square matrices whose only elements are 0 and 1. These corresponding matrices have been used as a mathematical model by others. Both viewpoints are combined here to provide a systematic procedure for identifying the liaison persons in an organization.", "authors": ["Ian Clunies Ross", "Frank Harary"], "id": "459347a1e38789cbaa98d1609ab00baa26dcc925", "title": "Identification of the Liaison Persons of an Organization Using the Structure Matrix", "references": []}, {"date": "1999", "abstract": "The paper is a brief summary of an invited talk given at the Discovery Science conference. The principal points are as follows: first, that probability theory forms the basis for connecting hypotheses and data; second, that the expressive power of the probability models used in scientific theory formation has expanded significantly; and finally, that still further expansion is required to tackle many problems of interest. This further expansion should combine probability theory with the expressive power of first-order logical languages. The paper sketches an approximate inference method for representation systems of this kind.", "authors": ["Stuart J. Russell"], "id": "9965aedc8690e3fbeaa3ddab27905983cbe7395b", "title": "Expressive Probability Models in Science", "references": ["e49a71534f067cedf828c895287afa901da8dc63", "675c0745399bfe0548670bfeb5d3e9c7043896f4", "b97ec7b4f8b3cd921bd44b962be00dbb199499be", "7efc512b46f512bd8169646a0dd9e06332c38526", "454742f723d5bdeb023d4a65ede020e881d68ae4", "21339e43ba099c5729f936031c24cfaf38a95ccd", "27d5a9ee45d29c01a65110ec8f183f736f0b3cb0", "2a98ce3d8ff7b2b1794cde6661a4f9baeef5d92c", "685d93afe865ca7600a0e1c8456069b50ded0931", "71f4bdd0b59d68076419d81cca12dda49edd3b81"]}, {"date": "1969", "abstract": "Minimum spanning trees (MST) and single linkage cluster analysis (SLCA) are explained and it is shown that all the information required for the SLCA of a set of points is contained in their MST. Known algorithms for finding the MST are discussed. They are efficient even when there are very many points; this makes a SLCA practicable when other methods of cluster analysis are not. The relevant computing procedures are published in the Algorithm section of the same issue of Applied Statistics. The use of the MST in the interpretation of vector diagrams arising in multivariate analysis is illustrated by an example.", "authors": ["John C. Gower", "G. J. S. Ross"], "id": "2f66d008f37665df60aea2e8fa1c3d7265bd9bce", "title": "Minimum spanning trees and single linkage cluster analysis", "references": ["92b1f2d1098a35606342d5895b9685e20539dbf2", "f50dca0b7d54452a45913efbc38ab8d7f26e1b10", "c09510b6b0037b47310ac14ace7459da72c5e48e", "ae792794e03edd325bcd68174969af93268e776c", "a6547a35e3a87a65347e2cf077ef6f1aac278a39", "f7ddd1699359001649cbd9fb44f342ddef08d46a", "90afeef851b1fe6274bf43e2fef7323ebf234875"]}, {"date": "1998", "abstract": "A broad spectrum of data is available on the Web in distinct heterogeneous sources, and stored under different formats. As the number of systems that utilize this heterogeneous data grows, the importance of data translation and conversion mechanisms increases greatly. In this paper we present a new translation system, based on schema-matching, aimed at simplifying the intricate task of data conversion. We observe that in many cases the schema of the data in the source system is very similar to that of the target system. In such cases, much of the translation work can be done automatically, based on the schemas similarity. This saves a lot of effort for the user, limiting the amount of programming needed. We define common schema and data models, in which schemas and data (resp.) from many common models can be represented. Using a rule-based method, the source schema is compared with the target one, and each component in the source schema is matched with a corresponding component in the target schema. Then, based on the matching achieved, data instances of the source schema can be translated to instances of the target schema. We show that our schema-based translation system allows a convenient specification and customization of data conversions, and can be easily combined with the traditional data-based translation languages.", "authors": ["Tova Milo", "Sagit Zohar"], "id": "e1a647721a518c6f4f3ff8bc9122294432745361", "title": "Using Schema Matching to Simplify Heterogeneous Data Translation", "references": ["6e5fd61fe7e565816098f2d0e6f66883ed600b8f", "5bf6f4069dfa85264b90b02bf3ebda2056381812", "e84f2026cb1f77ef0177122b41df0e8cc10fd410", "b6c60161f420481226453733f5c9293c3a8ebf18", "c3039d729d4ce6b1e387f5d5a940c0a3e7cb9dab", "64e35ec9fb846ec6e0de0d06354e199bf4e344a8", "6c32c122593066efc653258afa801b42a62e58ef", "9ee0e31732db0bd401d32d5bb027c9c01a2e3ee9", "deaf24afffa5cf1c8fb009cbb1f4080fde005c27", "2d21640f965fac15e9745c6ae8c2896c9a2ebb12"]}, {"date": "1999", "abstract": "Keeping track of multiple objects over time is a problem that arises in many real-world domains. The problem is often complicated by noisy sensors and unpredictable dynamics. Previous work by Huang and Russell, drawing on the data association literature, provided a probabilistic analysis and a threshold-based approximation algorithm for the case of multiple objects detected by two spatially separated sensors. This paper analyses the case in which large numbers of sensors are involved. We show that the approach taken by Huang and Russell, who used pairwise sensor-based appearance probabilities as the elementary probabilistic model, does not scale. When more than two observations are made, the objects' intrinsic properties must be estimated. These provide the necessary conditional independencies to allow a spatial decomposition of the global probability model. We also replace Huang and Russell's threshold algorithm for object identification with a polynomial-time approximation scheme based on Markov chain Monte Carlo simulation. Using sensor data from a freeway traffic simulation, we show that this allows accurate estimation of long-range origin/destination information even when the individual links in the sensor chain are highly unreliable.", "authors": ["Hanna M. Pasula", "Stuart J. Russell", "Michael Ostland", "Yaacov Ritov"], "id": "c10ad6e809ae7d73beba19efbd74acd1e54ed5c3", "title": "Tracking Many Objects with Many Sensors", "references": ["59fa47fc237a0781b4bf1c84fedb728d20db26a1", "f6a13f116e270dde9d67848495f801cdb8efa25d", "266a32615cdc2028eb743dc19d28242c8e67e357", "2979c03105c162f15e7c3685bb3131aa5c63a589", "16f4cf679a7e0095b52804893baf847e557e3d81", "bdfb57141b2141095ed942b28be24808aeba8d54"]}, {"date": "1967", "abstract": "Techniques for partitioning objects into optimally homogeneous groups on the basis of empirical measures of similarity among those objects have received increasing attention in several different fields. This paper develops a useful correspondence between any hierarchical system of such clusters, and a particular type of distance measure. The correspondence gives rise to two methods of clustering that are computationally rapid and invariant under monotonic transformations of the data. In an explicitly defined sense, one method forms clusters that are optimally \u201cconnected,\u201d while the other forms clusters that are optimally \u201ccompact.\u201d", "authors": ["Sally C. Johnson"], "id": "686a83bc3f66cbde6d13e0dc3e045a2666ad72d2", "title": "Hierarchical clustering schemes", "references": ["deb40e644f78e719ca510f82b889acd3e60fed96", "0430b241bdd0b67d37e1143370f8d24fc46d83e9", "e04108dc293c9cd7cabf32ee1524eaab0d4641b3", "c0554983cdb15bdac75897103075f514f592a111", "1ba80088d536d79336169da20e1efcde80cf756b", "10b9c3b2923e952df0b7bd68e52d1bb56081a27b", "d8d3e6d95b60ec6ac8f91f42a6914a87b13a6bc1"]}, {"date": "1990", "abstract": "Abstract We compare a number of training algorithms for competitive learning networks applied to the problem of vector quantization for data compression. A new competitive-learning algorithm based on the \u201cconscience\u201d learning method is introduced. The performance of competitive learning neural networks and traditional non-neural algorithms for vector quantization is compared. The basic properties of the algorithms are discussed and we present a number of examples that illustrate their use. The new algorithm is shown to be efficient and yields near-optimal results. This algorithm is used to design a vector quantizer for a speech database. We conclude with a discussion of continuing work.", "authors": ["Stanley C. Ahalt", "Ashok K. Krishnamurthy", "Prakoon Chen", "Douglas E. Melton"], "id": "c3da7bdcbf7cba94ce64abc3aff0f9b947a5cab9", "title": "Competitive learning algorithms for vector quantization", "references": ["9e835891db0957a3e618f197be703f76495f73fd"]}, {"date": "1956", "abstract": "7. A. Kurosh, Ringtheoretische Probleme die mit dem Burnsideschen Problem uber periodische Gruppen in Zussammenhang stehen, Bull. Acad. Sei. URSS, Ser. Math. vol. 5 (1941) pp. 233-240. 8. J. Levitzki, On the radical of a general ring, Bull. Amer. Math. Soc. vol. 49 (1943) pp. 462^66. 9. -, On three problems concerning nil rings, Bull. Amer. Math. Soc. vol. 49 (1943) pp. 913-919. 10. -, On the structure of algebraic algebras and related rings, Trans. Amer. Math. Soc. vol. 74 (1953) pp. 384-409.", "authors": ["Joseph B. Kruskal"], "id": "a6547a35e3a87a65347e2cf077ef6f1aac278a39", "title": "On the shortest spanning subtree of a graph and the traveling salesman problem", "references": []}, {"date": "1988", "abstract": "A novel two-dimensional subband coding technique is presented that can be applied to images as well as speech. A frequency-band decomposition of the image is carried out by means of 2D separable quadrature mirror filters, which split the image spectrum into 16 equal-rate subbands. These 16 parallel subband signals are regarded as a 16-dimensional vector source and coded as such using vector quantization. In the asymptotic case of high bit rates, a theoretical analysis yields that a lower bound to the gain is attainable by choosing this approach over scalar quantization of each subband with an optimal bit allocation. It is shown that vector quantization in this scheme has several advantages over coding the subbands separately. Experimental results are given, and it is shown the scheme has a performance that is comparable to that of more complex coding techniques. >", "authors": ["Peter H. Westerink", "Dick E. Boekee", "Jan Biemond", "John W. Woods"], "id": "8bec18c2dc662e3b2212c1863146cfcd1686b272", "title": "Subband coding of images using vector quantization", "references": ["2cc0fa22e0512a032594cdaface0fe164c331fc9", "55655eaf0984f13b1bd55763150c581c2c358ea7", "6144f153b0a5caed769a2e142109492028c64a4b", "5e346e31c4ddce096fa31ded7639801a7d0870f9", "0569afddd6c4316471d922673cc156076cc87032", "2f843c68a6b5698cd44c95538f5f013fefadad2f", "212172c098dcba52d4bd5e1b1cfad3a78922abe2", "7c46799502bebfe6a9ae0f457b7b8b92248ec260", "f8d76a28e7cec25253db25692b53a97f96cf9046", "7a711436044cd6c3ab7e3f514950d3475be5fd8d"]}, {"date": "1992", "abstract": "Vector quantization for entropy coding of image subbands is investigated. Rate distortion curves are computed with mean square error as a distortion criterion. The authors show that full-search entropy-constrained vector quantization of image subbands results in the best performance, but is computationally expensive. Lattice quantizers yield a coding efficiency almost indistinguishable from optimum full-search entropy-constrained vector quantization. Orthogonal lattice quantizers were found to perform almost as well as lattice quantizers derived from dense sphere packings. An optimum bit allocation rule based on a Lagrange multiplier formulation is applied to subband coding. Coding results are shown for a still image.", "authors": ["Takanori Senoo", "Bernd Girod"], "id": "7685ec3689ec5bbe067d96d23f6c783cedf02847", "title": "Vector quantization for entropy coding of image subbands", "references": ["a3f073ac7513183a9bf3a76154dcd245748d2ab6", "af77de34c7d5435f3658cf106d53ccd72e24c89d", "e460bf649138107a09e68e0f9a33537310f7e773", "81a5952532cdd48eec5e3dc326907c36a70e0a24", "d3fd587082d48fa767942248f5258486d322de9e", "f2cfa8616f6cee9ada6fd4839ec23f1b5a5250d1", "83074157d165b6245915508d891b2d0cd066f3ad", "c2d4e1e0b44aa683637d71252b89bbc2367b4c77", "c708b754f1680d058b57d720b75f096bbb890cf7", "8bec18c2dc662e3b2212c1863146cfcd1686b272"]}, {"date": "1985", "abstract": "'Mass terms', words like water, rice and traffic, have proved very difficult to accommodate in any theory of meaning since, unlike count nouns such as house or dog, they cannot be viewed as part of a logical set and differ in their grammatical properties. In this study, motivated by the need to design a computer program for understanding natural language utterances incorporating mass terms, Harry Bunt provides a thorough analysis of the problem and offers an original and detailed solution. An extension of classical set theory, Ensemble Theory, is defined, and this provides the conceptual basis of a framework for the analysis of natural language meaning which Dr Bunt calls Two-level model-theoretic semantics. The validity of the framework is convincingly demonstrated by the formal analysis of a fragment of English including sentences with quantified and modified mass terms. Separate chapters of the book are devoted to an axiomatic definition of Ensemble Theory and a detailed discussion of its status as a mathematical formalism.", "authors": ["Phillip Bricker", "Harry Bunt"], "id": "917da625c3c510e4221aeaf88065a4e1ad982438", "title": "Mass Terms and Model-Theoretic Semantics", "references": []}, {"date": "1990", "abstract": "A pseudo-Gray code is an assignment of n-bit binary indexes to 2\" points in a Euclidean space so that the Hamming distance between two points corresponds closely to the Euclidean distance. Pseudo-Gray coding provides a redundancy-free error protection scheme for vector quantization (VQ) of analog signals when the binary indexes are used as channel symbols on a discrete memoryless channel and the points are signal codevectors. Binary indexes are assigned to codevectors in a way that reduces the average quantization distortion introduced in the reproduced source vectors when a transmitted index is corrupted by channel noise. A globally optimal solution to this problem is generally intractable due to an inherently large computational complexity. A locally optimal solution, the binary switching algorithm, is introduced, based on the objective of minimizing a useful upper bound on the average system distortion. The algorithm yields a significant reduction in average distortion, and converges in reasonable running times. The sue of pseudo-Gray coding is motivated by the increasing need for low-bit-rate VQ-based encoding systems that operate on noisy channels, such as in mobile radio speech communications. >", "authors": ["Kenneth Zeger", "Allen Gersho"], "id": "568f602166e429666cf09669c018d28582ef8208", "title": "Pseudo-Gray coding", "references": ["44bddbc5dfdfbb09ff3f3c14b9e1668334ebcf4c", "0ae04ce3d0cabfde7942bfdd9b8e0d8808399865", "11b2cf35412b9059769c8caefe5c8c30c2572837", "78f6560492dca53e60e601787a071f75744ad80d", "d7499839dc171e546251359442d1dc4f2662ba0b", "e49a30fa9751eab20a55512feba2577cfebf367f", "ea6f9973b9c1f154cd9f2f470635256a41b6df9d", "9d382d782f5bfd02fb5542eb37f0ad2f70240eff", "77a877b31643797c168f4657e481e58a12ac0383", "96021c3cc33be6781e3f51b83516043ce11a0313"]}, {"date": "1998", "abstract": "This paper investigates interactions between collocational properties and methods for organizing them into features for machine learning. In experiments performing an event categorization task, Wiebe et al. (1997a) found that different organizations are best for different properties. This paper presents a statistical analysis of the results across different machine learning algorithms. In the experiments, the relationship between property and organization was strikingly consistent across algorithms. This prompted further analysis of this relationship, and an investigation of criteria for recognizing beneficial ways to include collocational properties in machine learning experiments. While many types of collocational properties and methods of organizing them into features have been used in NLP, systematic investigations of their interaction are rare. 1 I n t r o d u c t i o n Properties can be mapped to features in a machine learning algorithm in different ways, potentially yielding different results (see, e.g., Hu and Kibler 1996 and Pagallo and Haussler 1990). This paper investigates interactions between collocational properties and methods for organizing them into features. Collocations, conceived broadly as words meeting certain constraints that are correlated with the targeted classification, are used in a wide range of NLP applications, from word-sense disambiguation to discourse processing. They must be selected and represented in s o m e way. Thus, this work is widely applicable to experimental design in NLP. In experiments performing an event categorization task, Wiebe et al. (1997a) co-varied four types of organization and three types of collocational property. They found that different organizations are best for different properties, and that the best results are obtained with the most constrained properties and an organization that is not common in NLP (but see Goldberg 1995 and Cohen 1996). However, they experimented with only one machine learning algorithm, and did not offer any insight into the results. This paper presents a statistical analysis of the results across different machine learning algorithms. In the experiments, the relationship between property and organization is strikingly consistent across algorithms. This prompted further analysis of this relationship, and a study of criteria for recognizing beneficial ways to include collocations in machine learning experiments. While many types of collocational properties and methods for representing them as features have been used in NLP, systematic investigations of their interaction are rare. The paper is organized as follows. The event categorization task is described in second 2. The collocational properties, methods for selecting collocations, and methods for organizing them into features are presented in sections 3, 4.1, and 4.2, respectively. The machine learning algorithms are identified in section 5, and the results and statistical analysis of them are presented in section 6. The study of interaction between property and organization is presented in section 7.", "authors": ["Janyce Wiebe", "Kenneth J. McKeever", "Rebecca F. Bruce"], "id": "cffe284e3e6f27107b6156bb7d691e7c5025d2ea", "title": "Mapping Collocational Properties into Machine Learning Features", "references": ["dbfd191afbbc8317577cbc44afe7156df546e143", "807c1f19047f96083e13614f7ce20f2ac98c239a", "fd303a438cef5eb647c63cd1f25bad12a5babba3", "ed3d65b4ce8edcfee5d132e1943006f1007aa0b8", "43896ea7d488100d135645fbb4be6e7eb2e7f4e2", "44d8152faffd1d71380d090270c3a8d6599981eb", "ec179f9b30d3a4e0f8a50c86d951557dfcbb20d1", "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214", "98c5a252475fce36934e6c3d4710af1aa08a382b", "ca83e953bfef858e3fd2f26a9ce956952e94f008"]}, {"date": "1998", "abstract": "Now completing its first year, the High-Performance Knowledge Bases Project promotes technology for developing very large, flexible, and reusable knowledge bases. The project is supported by the Defense Advanced Research Projects Agency and includes more than 15 contractors in universities, research laboratories, and companies. The evaluation of the constituent technologies centers on two challenge problems, in crisis management and battlespace reasoning, each demanding powerful problem solving with very large knowledge bases. This article discusses the challenge problems, the constituent technologies, and their integration and evaluation.", "authors": ["Paul R. Cohen", "Robert Schrag", "Eric K. Jones", "Adam Pease", "Albert Lin", "Barbara Starr", "David Gunning", "Murray Burke"], "id": "e10ee18c8bfc096ec5b5b14260d2e7ab7afdb0d9", "title": "The DARPA High-Performance Knowledge Bases Project", "references": ["4683a6bd8f9b2f526a18cdd42426e0b483f93e3a", "df8568c6e19d427aae989887a47c3a88f8124dda", "4cda8418bcedaff2685df011b38a910067b8be4c", "d1e04a5ab31e59100e161c854e6660bf8b191029", "797cbdf444a2360932c188cdfc8c56d8802c693b", "c362690da56fb12795281f7a84cd45e45a34630f", "20c82e0409b03151960c84958e67af8acd69c37c", "7539d7e65790fdd180f37b1f87f945c65ea9d8b6", "bdc4771b217252c2460281d19fc01d3e63888f4d", "a16e384a33791c400730a40f4252e5d8c2225f5a"]}, {"date": "2001", "abstract": "Despite some successes, the lack of tools to allow subject matter experts to directly enter, query, and debug formal domain knowledge in a knowledge-base still remains a major obstacle to their deployment. Our goal is to create such tools, so that a trained knowledge engineer is no longer required to mediate the interaction. This paper presents our work on the knowledge entry part of this overall knowledge capture task, which is based on several claims: that users can construct representations by connecting pre-fabricated, representational components, rather than writing low-level axioms; that these components can be presented to users as graphs; and the user can then perform composition through graph manipulation operations. To operationalize this, we have developed a novel technique of graphical dialog using examples of the component concepts, followed by an automated process for generalizing the user's graphically-entered assertions into axioms. We present these claims, our approach, the system (called SHAKEN) that we are developing, and an evaluation of our progress based on having users encode knowledge using the system.", "authors": ["Peter Clark", "John A. Thompson", "Ken Barker", "Bruce W. Porter", "Vinay K. Chaudhri", "Andres C. Rodriguez", "J\u00e9r\u00f4me Thom\u00e9r\u00e9", "Sunil Mishra", "Yolanda Gil", "Patrick J. Hayes", "Thomas Reichherzer"], "id": "1d21fe9d68fac2a05e5f50e3ff8fcb8037042523", "title": "Knowledge entry as the graphical assembly of components", "references": []}, {"date": "1996", "abstract": "Developing a large belief network, like any large system, requires systems engineering to manage the design and construction process. We propose that network engineering follow a rapid prototyping approach to network construction. We describe criteria for identifying network modules and the use of 'stubs' within a belief network. We propose an object oriented representation for belief networks which captures the semantic as well as representational knowledge embedded in the vaziables, their values and their parameters. Methods for evaluating complex networks are described. Throughout the discussion, tools which support the engineering of large belief networks are identified.", "authors": ["Suzanne M. Mahoney", "Kathryn B. Laskey"], "id": "7c1178f4bc8089a19ab3f52215bd14d2356f378f", "title": "Network Engineering for Complex Belief Networks", "references": ["590fa138a8e4528badd46c1a94af52a62c6d59c3", "afaaf62f791424c69cef312b067a801d9b9a35be"]}, {"date": "1996", "abstract": "Bayesian networks provide a language for qualitatively representing the conditional independence properties of a distribution, This allows a natural and compact representation of the distribution, eases knowledge acquisition, and supports effective inference algorithms. It is well-known, however, that there are certain independencies that we cannot capture qualitatively within the Bayesian network structure: independencies that hold only in certain contexts, i.e., given a specific assignment of values to certain variables, In this paper, we propose a formal notion of context-specific independence (CSI), based on regularities in the conditional probability tables (CPTs) at a node. We present a technique, analogous to (and based on) d-separation, for determining when such independence holds in a given network. We then focus on a particular qualitative representation scheme--tree-structured CPTs-- for capturing CSI. We suggest ways in which this representation can be used to support effective inference algorithms, in particular, we present a structural decomposition of the resulting network which can improve the performance of clustering algorithms, and an alternative algorithm based on outset conditioning.", "authors": ["Craig Boutilier", "Nir Friedman", "Mois\u00e9s Goldszmidt", "Daphne Koller"], "id": "5bdd9a3317b43966f97b5c70c55c46fd19335049", "title": "Context-Specific Independence in Bayesian Networks", "references": ["4caab8957a369f4e021dc78bc54e45b1f9b7309e", "0767a86afd814c7d13b93d9c749d97d3370134b5", "964a39b5ca27d2635cd5bebc817791f0077807f2", "e9ef7893ae7ee6826a43b5f58365092194c0e213", "7fcb895b597302b1f6a15c8e3ea8371960ceb014", "7c9e02656982419870ccc0b60d4c8b1a6e4b449d", "37d42035b991b91a9c7118894068dbcb6990bdd2", "b7e0086124e5cb301861d5e424394f438e46d5b2", "1e7c5ad8e357e687e96e77bece4c9ee8f471553b", "51b6d09c69ebb3e786d14c5497470311ba1dc123"]}, {"date": "1992", "abstract": "In recent years there has been a growing interest among AI researchers in probabilistic and decision modelling, spurred by significant advances in representation and computation with network modelling formalisms. In applying these techniques to decision support tasks, fixed network models have proven to be inadequately expressive when a broad range of situations must be handled. Hence many researchers have sought to combine the strengths of flexible knowledge representation languages with the normative status and well-understood computational properties of decision-modelling formalisms and algorithms. One approach is to encode general knowledge in an expressive language, then dynamically construct a decision model for each particular situation or problem instance. We have developed several systems adopting this approach, which illustrate a variety of interesting techniques and design issues.", "authors": ["Michael P. Wellman", "John S. Breese", "Robert P. Goldman"], "id": "8089729d8711b3ef0de37eb6016ca3a311b491b5", "title": "From knowledge bases to decision models", "references": ["17b76657a834964843495d96699fefa219e959f5", "6710b1fffd6e850e04f972441732ecf66cf372a2", "d294138f826a37d3113e4e198e62ba845270c4d8", "b07a5e09daaf90296c1cead402d9fbf73e688433", "9e1c26d71c62120ecfa0784bdf0b417ba6c6a982", "b33cc034ed30ebae6262e646c916d063a2cd7ec8", "70ef29e6f0ce082bb8a47fd85b9bfb7cc0f20c93", "ed31558a9fef2bb3fb09914b642b9252d4cd9a77", "59735d8798db201970434caecedf12ed4a9dc7dd", "18b5b221cf5779dd58a938a941e3142469f8bdba"]}, {"date": "1998", "abstract": "Two of the most important threads of work in knowledge representation today are frame-based representation systems (FRS's) and Bayesian networks (BNs). FRS's provide an excellent representation for the organizational structure of large complex domains, but their applicability is limited because of their inability to deal with uncertainty and noise. BNs provide an intuitive and coherent probabilistic representation of our uncertainty, but are very limited in their ability to handle complex structured domains. In this paper, we provide a language that cleanly integrates these approaches, preserving the advantages of both. Our approach allows us to provide natural and compact definitions of probability models for a class, in a way that is local to the class frame. These models can be instantiated for any set of interconnected instances, resulting in a coherent probability distribution over the instance properties. Our language also allows us to represent important types of uncertainty that cannot be accomodated within the framework of traditional BNs: uncertainty over the set of entities present in our model, and uncertainty about the relationships between these entities. We provide an inference algorithm for our language via a reduction to inference in standard Bayesian networks. We describe an implemented system that allows most of the main frame systems in existence today to annotate their knowledge bases with probabilistic information, and to use that information in answering probabilistic queries.", "authors": ["Daphne Koller", "Avi Pfeffer"], "id": "b97ec7b4f8b3cd921bd44b962be00dbb199499be", "title": "Probabilistic Frame-Based Systems", "references": ["e9ef7893ae7ee6826a43b5f58365092194c0e213", "8089729d8711b3ef0de37eb6016ca3a311b491b5", "3b54cb0af7601b84bf5db370c5379e164cdbb7eb", "d1e04a5ab31e59100e161c854e6660bf8b191029", "5bf6f01402e1648b7d1e6c9200ede6cb1af30123", "497cc2e9e10543ad46af6d217c7eb4d9c63c684a", "419438bc4f6652784f42cf3e62c975a5c89b817e", "8a59ce04b6a0d923cd60dc1c16b6b7d99a60bffb", "cd4ba23c89b5b4eecd9b542e79dbdc195de65f6d", "5bdd9a3317b43966f97b5c70c55c46fd19335049"]}, {"date": "1989", "abstract": "Abstract In this paper we study the parallel implementation of optimum classifiers. Specifically, we present a parallel implementation of the optimum (or maximum likelihood Gaussian) classifier that uses a cellular automaton to very rapidly find the output vector with minimum Euclidean distance from the input vector. This implementation also has the feature of easily cascadable chips allowing the number of output vectors to easily grow to arbitrary size.", "authors": ["Jack H. Winters", "Christopher Rose"], "id": "9e835891db0957a3e618f197be703f76495f73fd", "title": "Minimum distance automata in parallel networks for optimum classification", "references": []}, {"date": "1997", "abstract": "Bayesian networks provide a modeling language and associated inference algorithm for stochastic domains. They have been successfully applied in a variety of medium-scale applications. However, when faced with a large complex domain, the task of modeling using Bayesian networks begins to resemble the task of programming using logical circuits. In this paper, we describe an object-oriented Bayesian network (OOBN) language, which allows complex domains to be described in terms of inter-related objects. We use a Bayesian network fragment to describe the probabilistic relations between the attributes of an object. These attributes can themselves be objects, providing a natural framework for encoding part-of hierarchies, Classes are used to provide a reusable probabilistic model which can be applied to multiple similar objects. Classes also support inheritance of model fragments from a class to a subclass, allowing the common aspects of related classes to be defined only once. Our language has clear declarative semantics: an OOBN can be interpreted as a stochastic functional program, so that it uniquely specifies a probabilistic model. We provide an inference algorithm for OOBNs, and show that much of the structural information encoded by an OOBN--particularly the encapsulation of variables within an object and the reuse of model fragments in different contexts---can also be used to speed up the inference process.", "authors": ["Daphne Koller", "Avi Pfeffer"], "id": "419438bc4f6652784f42cf3e62c975a5c89b817e", "title": "Object-Oriented Bayesian Networks", "references": ["3b54cb0af7601b84bf5db370c5379e164cdbb7eb", "235602075d482fb8b9586436177dd49b8c454bef", "8edfe224ef331ea0b872f232cb627c5be56225d5", "5bf6f01402e1648b7d1e6c9200ede6cb1af30123"]}, {"date": "1948", "abstract": "Semantic Scholar extracted view of \"A method of establishing group of equal amplitude in plant sociobiology based on similarity of species content and its application to analyses of the vegetation on Danish commons\" by Tia S\u00f8rensen et al.", "authors": ["Tia S\u00f8rensen", "T. S\u00f8rensen", "T Biering-S\u00f8rensen", "Tia S\u00f8rensen", "J. T. S\u00f8rensen"], "id": "d8d3e6d95b60ec6ac8f91f42a6914a87b13a6bc1", "title": "A method of establishing group of equal amplitude in plant sociobiology based on similarity of species content and its application to analyses of the vegetation on Danish commons", "references": []}, {"date": "1990", "abstract": "Several issues related to vector quantization for noisy channels are discussed. An algorithm based on simulated annealing is developed for assigning binary codewords to the vector quantizer code-vectors. It is shown that this algorithm could result in dramatic performance improvements as compared to randomly selected codewords. A modification of the simulated annealing algorithm for binary codeword assignment is developed for the case where the bits in the codeword are subjected to unequal error probabilities (resulting from unequal levels of error protection). An algorithm for the design of an optimal vector quantizer for a noisy channel is briefly discussed, and its robustness under channel mismatch conditions is studied. Numerical results for a stationary first-order Gauss-Markov source and a binary symmetric channel are provided. It is concluded that the channel-optimized vector quantizer design algorithm, if used carefully, can result in a fairly robust system with no additional delay. The case in which the communication channel is nonstationary (as in mobile radio channels) is studied, and some preliminary ideas for quantizer design are presented. >", "authors": ["Nariman Farvardin"], "id": "78f6560492dca53e60e601787a071f75744ad80d", "title": "A study of vector quantization for noisy channels", "references": []}, {"date": "1960", "abstract": "Semantic Scholar extracted view of \"Hierarchical Linkage Analysis for the Isolation of Types\" by Louis L. McQuitty", "authors": ["Louis L. McQuitty"], "id": "1ba80088d536d79336169da20e1efcde80cf756b", "title": "Hierarchical Linkage Analysis for the Isolation of Types", "references": []}, {"date": "1990", "abstract": "The authors consider 2-D predictive vector quantization (PVQ) of images subject to an entropy constraint and demonstrate the substantial performance improvements over existing unconstrained approaches. They describe a simple adaptive buffer-instrumented implementation of this 2-D entropy-coded PVQ scheme which can accommodate the associated variable-length entropy coding while completely eliminating buffer overflow/underflow problems at the expense of only a slight degradation in performance. This scheme, called 2-D PVQ/AECQ (adaptive entropy-coded quantization), is shown to result in excellent rate-distortion performance and impressive quality reconstructions of real-world images. Indeed, the real-world coding results shown demonstrate little distortion at rates as low as 0.5 b/pixel. >", "authors": ["James W. Modestino", "Yong Han Kim"], "id": "c708b754f1680d058b57d720b75f096bbb890cf7", "title": "Adaptive entropy-coded predictive vector quantization of images", "references": []}, {"date": "1955", "abstract": "Sixteen English consonants were spoken over voice communication systems with frequency distortion and with random masking noise. The listeners were forced to guess at every sound and a count was made of all the different errors that resulted when one sound was confused with another. With noise or low\u2010pass filtering the confusions fall into consistent patterns, but with high\u2010pass filtering the errors are scattered quite randomly. An articulatory analysis of these 16 consonants provides a system of five articulatory features or \u201cdimensions\u201d that serve to characterize and distinguish the different phonemes: voicing, nasality, affrication, duration, and place of articulation. The data indicate that voicing and nasality are little affected and that place is severely affected by low\u2010pass and noisy systems. The indications are that the perception of any one of these five features is relatively independent of the perception of the others, so that it is as if five separate, simple channels were involved rather tha...", "authors": ["George A. Miller", "Patricia E. Nicely"], "id": "10b9c3b2923e952df0b7bd68e52d1bb56081a27b", "title": "An analysis of perceptual confusions among some English consonants", "references": []}, {"date": "1986", "abstract": "Subband coding has become quite popular for the source encoding of speech. This paper presents a simple yet efficient extension of this concept to the source coding of images. We specify the constraints for a set of two-dimensional quadrature mirror filters (QMF's) for a particular frequency-domain partition, and show that these constraints are satisfied by a separable combination of one-dimensional QMF's. Bits are then optimally allocated among the subbands to minimize the mean-squared error for DPCM coding of the subbands. Also, an adaptive technique is developed to allocate the bits within each subband by means of a local variance mask. Optimum quantization is employed with quantizers matched to the Laplacian distribution. Subband coded images are presented along with their signal-to-noise ratios (SNR's). The SNR performance of the subband coder is compared to that of the adaptive discrete cosine transform (DCT), vector quantization, and differential vector quantization for bit rates of 0.67, 1.0, and 2.0 bits per pixel for 256 \u00d7 256 monochrome images. The adaptive subband coder has the best SNR performance.", "authors": ["John W. Woods", "Sean D. O'Neil"], "id": "c2d4e1e0b44aa683637d71252b89bbc2367b4c77", "title": "Subband coding of images", "references": []}, {"date": "1997", "abstract": "The Core Plan Representation (CPR) [Pease and Carrico, 1996] is an effort to develop a plan ontology which supports the representation needs of many different planning systems. It is being developed for the Joint Task Force Advanced Technology Demonstration (JTF-ATD). The goal of this effort is to leverage common functionality and facilit ate the reuse and sharing of information between a variety of planning and control systems. The CPR attempts to embody a standard which is general enough to cover a spectrum of domains from planning and process management to workflow and activity models. In addition, the proposed representation will be powerful enough to support complex, hierarchical plan structures. The prime motivation for the CPR effort is to address plan interchange requirements of several milit ary planning systems, but this proposed ontology attempts to go beyond milit ary planning and present a more general plan representation. The design of the CPR is an attempt to unify the major concepts and advancements in plan and process representation into one comprehensive model. The problem of planning, and plan representation has been an ongoing area of active research. This paper presents the current state of the Core Plan Representation and shares some of the experiences of trying to draw many successful domain efforts into a single, unified, domain independent representation.", "authors": ["Robert A. Pease", "Todd M. Carrico"], "id": "a16e384a33791c400730a40f4252e5d8c2225f5a", "title": "JTF-ATD Core Plan Representation: a Progress Report", "references": ["3214d11153388188bde2fcd432c632f1bd7fdf18"]}, {"date": "1987", "abstract": "We present an analysis of the zero-memory quantization of memoryless sources when the quantizer output is to be encoded and transmitted across a noisy channel. Necessary conditions for the joint optimization of the quantizer and the encoder/decoder pair are presented, and an iterative algorithm for obtaining a locally optimum system is developed. The performance of this locally optimal system, obtained for the class of generalized Gaussian distributions and the binary symmetric channel, is compared against the optimum performance theoretically attainable (using rate-distortion theoretic arguments), as well as against the performance of Lloyd-Max quantizers encoded using the natural binary code and the folded binary code. It is shown that this optimal design could result in substantial performance improvements. The performance improvements are more noticeable at high bit rates and for broad-tailed densities.", "authors": ["Nariman Farvardin", "Vinay A. Vaishampayan"], "id": "44bddbc5dfdfbb09ff3f3c14b9e1668334ebcf4c", "title": "Optimal quantizer design for noisy channels: An approach to combined source - channel coding", "references": ["44bddbc5dfdfbb09ff3f3c14b9e1668334ebcf4c", "0ae04ce3d0cabfde7942bfdd9b8e0d8808399865", "5846adf064731f3aecccaa177221169a86cc0c76", "ba79e57952f035a2b05a5a48eb24b0d9b705bf27", "21cca641bd030c76444740a22a3e0d260d3a7973", "72c027d4024816eb0bf4b4dd75343b1bd7fc41b6", "623e25cd9dcbb08a5053f55693af571e39946b9b", "ea6f9973b9c1f154cd9f2f470635256a41b6df9d"]}, {"date": "1985", "abstract": "Quantization, the process of approximating continuous-amplitude signals by digital (discrete-amplitude) signals, is an important aspect of data compression or coding, the field concerned with the reduction of the number of bits necessary to transmit or store analog data, subject to a distortion or fidelity criterion. The independent quantization of each signal value or parameter is termed scalar quantization, while the joint quantization of a block of parameters is termed block or vector quantization. This tutorial review presents the basic concepts employed in vector quantization and gives a realistic assessment of its benefits and costs when compared to scalar quantization. Vector quantization is presented as a process of redundancy removal that makes effective use of four interrelated properties of vector parameters: linear dependency (correlation), nonlinear dependency, shape of the probability density function (pdf), and vector dimensionality itself. In contrast, scalar quantization can utilize effectively only linear dependency and pdf shape. The basic concepts are illustrated by means of simple examples and the theoretical limits of vector quantizer performance are reviewed, based on results from rate-distortion theory. Practical issues relating to quantizer design, implementation, and performance in actual applications are explored. While many of the methods presented are quite general and can be used for the coding of arbitrary signals, this paper focuses primarily on the coding of speech signals and parameters.", "authors": ["John Makhoul", "Salim Roucos", "Herbert Gish"], "id": "a3f073ac7513183a9bf3a76154dcd245748d2ab6", "title": "Vector quantization in speech coding", "references": ["e88163aab24aa8866d27ebf091ab4c256a30de53", "9a9fdbcbe033053b3a41bb2d07f1ff7efc99c9ab", "d4de6f0516c24abf35941f88f522fa6221c6ccf8", "e1769c81e58ab772ce405314cae48ee048a71843", "5cc0a228e57369fb8097851778fb4f8b6fc33b4e", "7ef0d444274c20449918171517b999e62ba7f32d", "52db3d916ece4e7802da52bf934d55867328e8c1", "b440d02a26368cc54fc4423c37f3771298c67bb5", "2e7a0e9796787e1640901cb1afcf66c6bf9dffe4", "c66a056f09c9f15442f12d732f3892a728d83734"]}, {"date": "1989", "abstract": "In this age of modern era, the use of internet must be maximized. Yeah, internet will help us very much not only for important thing but also for daily activities. Many people now, from any level can use internet. The sources of internet connection can also be enjoyed in many places. As one of the benefits is to get the on-line building large knowledge based systems book, as the world window, as many people suggest.", "authors": ["Douglas B. Lenat", "Ramanathan V. Guha"], "id": "bdc4771b217252c2460281d19fc01d3e63888f4d", "title": "Building large knowledge-based systems", "references": []}, {"date": "1996", "abstract": "In the area of statistical physics, Monte Carlo algorithms based on Markov chain simulation have been in use for many years. The validity of these algorithms depends crucially on the rate of convergence to equilibrium of the Markov chain being simulated. Unfortunately, the classical theory of stochastic processes hardly touches on the sort of non-asymptotic analysis required in this application. As a consequence, it had previously not been possible to make useful, mathematically rigorous statements about the quality of the estimates obtained. Within the last ten years, analytical tools have been devised with the aim of correcting this deficiency. As well as permitting the analysis of Monte Carlo algorithms for classical problems in statistical physics, the introduction of these tools has spurred the development of new approximation algorithms for a wider class of problems in combinatorial enumeration and optimization. The \u201cMarkov chain Monte Carlo\u201d method has been applied to a variety of such problems, and often provides the only known efficient (i.e., polynomial time) solution technique.", "authors": ["Mark Jerrum", "Alistair Sinclair"], "id": "16f4cf679a7e0095b52804893baf847e557e3d81", "title": "The Markov chain Monte Carlo method: an approach to approximate counting and integration", "references": ["dc6a60b284a8f57d361eb485f595c8bd1f2d96c5", "110078eb3f05c7376aa4127719aedd7755fccfaf", "181eba118d1be4a6c90069f9f008712b5e82074a", "6ff88178eed68b2d86d44a8210e5a504eeddebf9", "0fd2399d0d75307685d3af9fca24b32f5aaf1f4f", "b3295bc1b2df23d0e01ab4c395a27a5a9a867625", "eae4a397e716b677e79187ce51ab5dc1f6c3a34d", "226aead62ce797aedc2e4a3ed90083b206a576e2", "1bc474099e193ecdc7ca24b1d7faf8d8b3d081e4", "9f8161bcac9614b15e7a97dbaf89b41ad0d0651f"]}, {"date": "1993", "abstract": "When eliciting a probability model from experts, knowledge engineers may compare the results of the model with expert judgment on test scenarios, then adjust model parameters to bring the behavior of the model more in line with the experts intuition. This paper presents a methodology for analytic computation of sensitivity values in Bayesian network models. Sensitivity values are partial derivatives of output probabilities with respect to parameters being varied in the sensitivity analysis. They measure the impact of small changes in a network parameter on a target probability value or distribution. Sensitivity values can be used to focus knowledge elicitation effort on those parameters having the most impact on outputs of concern. Analytic sensitivity values are computed for an example and compared to sensitivity analysis by direct variation of parameters. >", "authors": ["Kathryn B. Laskey"], "id": "afaaf62f791424c69cef312b067a801d9b9a35be", "title": "Sensitivity analysis for probability assessments in Bayesian networks", "references": ["781346bad276191229e4496ce1f455bd34f5c526", "37d42035b991b91a9c7118894068dbcb6990bdd2", "be5100800b1b77cbf79e1c149cb1bfddf1ce2747", "f6f2f95405922022acfee38a58aadb3cd853140f", "5bf6f01402e1648b7d1e6c9200ede6cb1af30123", "011fc271a69a3aa4cf2683099a5abcdc03317e26", "85c924cc43e653469d511046864a367f39c880a4", "dcce2a3564685657c23d1afa00155c03560e76ac"]}, {"date": "1994", "abstract": "Heckerman (1993) defined causal independence in terms of a set of temporal conditional independence statements. These statements formalized certain types of causal interaction where (1) the effect is independent of the order that causes are introduced and (2) the impact of a single cause on the effect does not depend on what other causes have previously been applied. In this paper, we introduce art equivalent a temporal characterization of causal independence based on a functional representation of the relationship between causes and the effect. In this representation, the interaction between causes and effect can be written as a nested decomposition of functions. Causal independence can be exploited by representing this decomposition in the belief network, resulting in representations that are more efficient for inference than general causal models. We present empirical results showing the benefits of a causal-independence representation for belief-network inference.", "authors": ["David Heckerman", "John S. Breese"], "id": "51b6d09c69ebb3e786d14c5497470311ba1dc123", "title": "A New Look at Causal Independence", "references": ["0767a86afd814c7d13b93d9c749d97d3370134b5", "135160dd42685e671f003ddc0f8f3fcf007f800e", "07ec1d17477f48dcda53e68927c3644e47ca95d1", "d0c60157cc268b47e842065c13efc2d7ee454e26", "a4b363f52335e0f021ebf4534d56babccb6da8f1", "2ea11691f817bcf0457d16c525166dfe52ee345c"]}, {"date": "1993", "abstract": "An influence diagram is a graphical representation of a decision problem that is at once a formal description of a decision problem that can be treated by computers and a representation that is easily understood by decision makers who may be unskilled in the art of complex probabilistic modeling. The power of an influence diagram, both as an analysis tool and a communication tool, lies in its ability to concisely summarize the structure of a decision problem. However, when confronted with highly asymmetric problems in which particular acts or events lead to very different possibilities, many analysts prefer decision trees to influence diagrams. In this paper, we extend the definition of an influence diagram by introducing a new representation for its conditional probability distributions. This extended influence diagram representation, combining elements of the decision tree and influence diagram representations, allows one to clearly and efficiently represent asymmetric decision problems and provides an attractive alternative to both the decision tree and conventional influence diagram representations.", "authors": ["James E. Smith", "Samuel Holtzman", "James E. Matheson"], "id": "1e7c5ad8e357e687e96e77bece4c9ee8f471553b", "title": "Structuring Conditional Relationships in Influence Diagrams", "references": []}, {"date": "1990", "abstract": "Probabilistic information has many uses in an intelligent system. This book explores logical formalisms for representing and reasoning with probabilistic information that will be of particular value to researchers in nonmonotonic reasoning, applications of probabilities, and knowledge representation. It demonstrates that probabilities are not limited to particular applications, like expert systems; they have an important role to play in the formal design and specification of intelligent systems in general.Fahiem Bacchus focuses on two distinct notions of probabilities: one propositional, involving degrees of belief, the other proportional, involving statistics. He constructs distinct logics with different semantics for each type of probability that are a significant advance in the formal tools available for representing and reasoning with probabilities. These logics can represent an extensive variety of qualitative assertions, eliminating requirements for exact point-valued probabilities, and they can represent first-order logical information. The logics also have proof theories which give a formal specification for a class of reasoning that subsumes and integrates most of the probabilistic reasoning schemes so far developed in AI.Using the new logical tools to connect statistical with propositional probability, Bacchus also proposes a system of direct inference in which degrees of belief can be inferred from statistical knowledge and demonstrates how this mechanism can be applied to yield a powerful and intuitively satisfying system of defeasible or default reasoning.Contents: Introduction. Propositional Probabilities. Statistical Probabilities. Combining Statistical and Propositional Probabilities Default Inferences from Statistical Knowledge.", "authors": ["Fahiem Bacchus"], "id": "59735d8798db201970434caecedf12ed4a9dc7dd", "title": "Representing and reasoning with probabilistic knowledge - a logical approach to probabilities", "references": []}, {"date": "1991", "abstract": "Goals, as typically conceived in AI planning, provide an insufficient basis for choice of action, and hence are deficient as the sole expression of an agent's objectives. Decision-theoretic utilities offer a more adequate basis, yet lack many of the computational advantages of goals. We provide a preferential semantics for goals that grounds them in decision theory and preserves the validity of some, but not all, common goal operations performed in planning. This semantic account provides a criterion for verifying the design of goal-based planning strategies, thus providing a new framework for knowledge-level analysis of planning systems.", "authors": ["Michael P. Wellman", "Jon Doyle"], "id": "18b5b221cf5779dd58a938a941e3142469f8bdba", "title": "Preferential Semantics for Goals", "references": ["2b23cfe24a6742192eec8d4bff5f3bc245a9db4d", "81e12497024daf8fe4de9517780fff21f4ebde75", "1b115815f55d48ff59dc1208d7d410edeeefddff", "fb3f2aa591cf3df136f48aff0eb999e125f15ae4", "69e615272b21d74dde59052aad5485170c3fd136", "da9fb97bbf4d6f629a46d99504fa33c473e5cc0b", "372a0d798b623408624643ff72bb1f7107f082bf", "66205bb61255357ecc1dd7aa2aceb1a6bae64b55", "24638aa6dd8c8f03d2b5785a13ebc1ff2c5e3356", "6a222f2787607ecdb75b08e5f495fbed3c15d418"]}, {"date": "1988", "abstract": "We describe the use of decision-theory to optimize the value of computation under uncertain and varying resource limitations. The research is motivated by the pursuit of formal models of rational decision making for computational agents, centering on the explicit consideration of preferences and resource availability. We focus here on the importance of identifying the multiattribute structure of partial results generated by approximation methods for making control decisions. Work on simple algorithms and on the control of decision-theoretic inference itself is described.", "authors": ["Eric Horvitz"], "id": "ed31558a9fef2bb3fb09914b642b9252d4cd9a77", "title": "Reasoning under Varying and Uncertain Resource Constraints", "references": ["461c0d703fa19ea521f86bddc46c9c66e3c4b56b", "055e0d54725f964b61362d087d66fd2cbd9d2047", "55395c610edc5e271b8991118ca3638973e53b77", "89da92a02d878b898ea2b051d34874c8df9d89d5", "bb0419bccc2244ed33c9c42341f342511262daa3", "da6231ac3da628e748d407c3842bc06b32433ab6", "5d9dd70be51c2bb12a1e9a6addd0af5dd42aed13"]}, {"date": "1994", "abstract": "Model-based diagnosis reasons backwards from a functional schematic of a system to isolate faults given observations of anomalous behavior. We develop a fully probabilistic approach to model based diagnosis and extend it to support hierarchical models. Our scheme translates the functional schematic into a Bayesian network and diagnostic inference takes place in the Bayesian network. A Bayesian network diagnostic inference algorithm is modified to take advantage of the hierarchy to give computational gains.", "authors": ["Sampath Srinivas"], "id": "8edfe224ef331ea0b872f232cb627c5be56225d5", "title": "A Probabilistic Approach to Hierarchical Model-based Diagnosis", "references": ["5abb6b0a468e98605489d62f7afe9b71c8073ebf", "d2af69880204f8368e3aaf7a6f38e0fe243d2a50", "1ffc977d82798cfab971e4abdb46ae7b707c57c0", "bb75e5a3b46ec37a72922c706acd87ebab35b666", "9fdeb8801349708a820a579fca4ab5a799d3d439", "9e0daca0acc6ee3baf7573fe2e2b3cc94276e7f4", "0a3767909649cf31d32e087693d93171af28ebe0", "34bc0d893569a6414ce4d8b98abd261f04e99579"]}, {"date": "1987", "abstract": "Presented in this paper is the data model for ORION, a prototype database system that adds persistence and sharability to objects created and manipulated in object-oriented applications. The ORION data model consolidates and modifies a number of major concepts found in many object-oriented systems, such as objects, classes, class lattice, methods, and inheritance. These concepts are reviewed and three major enhancements to the conventional object-oriented data model, namely, schema evolution, composite objects, and versions, are elaborated upon. Schema evolution is the ability to dynamically make changes to the class definitions and the structure of the class lattice. Composite objects are recursive collections of exclusive components that are treated as units of storage, retrieval, and integrity enforcement. Versions are variations of the same object that are related by the history of their derivation. These enhancements are strongly motivated by the data management requirements of the ORION applications from the domains of artificial intelligence, computer-aided design and manufacturing, and office information systems with multimedia documents.", "authors": ["Jay Banerjee", "Hong-Tai Chou", "Jorge F. Garza", "Won Kim", "Darrell Woelk", "Nat Ballou", "Hyoung-Joo Kim"], "id": "235602075d482fb8b9586436177dd49b8c454bef", "title": "Data model issues for object-oriented applications", "references": ["068339f4f29c09a48c59c95dfd5d96b3f1eaff45", "d62613f19b90b172bfbc362969a212505519795d"]}, {"date": "1997", "abstract": "Knowledge representation languages invariably reflect a trade-off between expressivity and tractability. Evidence suggests that the compromise chosen by description logics is a particularly successful one. However, description logiC (as for all vanants of first-order logic) is severely limited in its ability to express uncertainty. In this paper, we present P-CLASSIC, a probabilistic version of the description logiC CLASSIC. In addition to teoninological knowledge, the language utilizes Bayesian networks to express uncertainty about the basic properties of an individual, the number of fillers for its roles, and the properties of these fillers. We provide a semantics for P-CLASSIC and an effective inference procedure for probabilistic subsumption: computing the probability that a random individual in class C is also in class D. The effectiveness of the algorithm relies on independence assumptions and on our ability to execute lifted inference: reasoning about similar individuals as a group rather than as separate ground teons. We show that the complexity of the inference algorithm is the best that can be hoped for in a language that combines description logic with Bayesian networks. In particular, if we restrict to Bayesian networks that support polynomial time inference, the complexity of our inference procedure is also polynomial time.", "authors": ["Daphne Koller", "Alon Y. Halevy", "Avi Pfeffer"], "id": "3b54cb0af7601b84bf5db370c5379e164cdbb7eb", "title": "P-CLASSIC: A Tractable Probablistic Description Logic", "references": ["e9ef7893ae7ee6826a43b5f58365092194c0e213", "71f4bdd0b59d68076419d81cca12dda49edd3b81", "59d21ecca2bced50d66dd566a4957e0c05b85f6e", "4aae026dbb50d27b5c9f668ed032573fbff58208", "e63bd12e86c5ea9f864b140eaa25f383061b8d96", "e6ae1759b0ded0ee45800b7dfbd73631f7f064a8", "74290b88bcbbff3a118e910e3bb713d5d238f926", "b4d885cf904f5d061e04c1bdf585bda3de1c956f", "0cb1267b586f4180445a374e1a6aa58782732bcf", "ac114959aa46b0912c60ff7291a76acb97cc66c2"]}, {"date": "1997", "abstract": "Reusable ontologies are becoming increasingly important for tasks such as information integration, knowledge-level interoperation and knowledge-base development. We have developed a set of tools and services to support the process of achieving consensus on commonly shared ontologies by geographically distributed groups. These tools make use of the World Wide Web to enable wide access and provide users with the ability to publish, browse, create and edit ontologies stored on anontology server. Users can quickly assemble a new ontology from a library of modules. We discuss how our system was constructed, how it exploits existing protocols and browsing tools, and our experience supporting hundreds of users. We describe applications using our tools to achieve consensus on ontologies and to integrate information.The Ontolingua Server may be accessed through the URLhttp://ontolingua.stanford.edu", "authors": ["Adam Farquhar", "Richard Fikes", "James Rice"], "id": "497cc2e9e10543ad46af6d217c7eb4d9c63c684a", "title": "The Ontolingua Server: a tool for collaborative ontology construction", "references": ["2237eb141ee510bfa39f74ff791367e4af61d4b0", "ad0008dd411522a65b5364788fbc27f0d8314852", "6c289fb5b177064518c2007f197c77088b99c74f", "8c9c76ad95041350f9415e7ac7f0094ffe121d02", "5120f65919f77859a974fcc1ad08f72b2918b8ec", "a6d418b6bc036d15577e610cc06b956584671169", "2271e898a9f07e88556f7a46cfb94e52377466a9", "265e1bc600fdeb14d0157a73a57a2cbe6c3dc739", "44d80660cbb9254450e8fcb794192a6eed1b4e6e"]}, {"date": "1989", "abstract": "We describe a mechanism for performing probabilistic reasoning in influence diagrams using interval rather than point valued probabilities. We derive the procedures for node removal (corresponding to conditional expectation) and arc reversal (corresponding to Bayesian conditioning) in influence diagrams where lower bounds on probabilities are stored at each node. The resulting bounds for the transformed diagram are shown to be optimal within the class of constraints on probability distributions that can be expressed exclusively as lower bounds on the component probabilities of the diagram. Sequences of these operations can be performed to answer probabilistic queries with indeterminacies in the input and for performing sensitivity analysis on an influence diagram. The storage requirements and computational complexity of this approach are comparable to those for point-valued probabilistic inference mechanisms, making the approach attractive for performing sensitivity analysis and where probability information is not available. Limited empirical data on an implementation of the methodology are provided.", "authors": ["Kenneth W. Fertig", "John S. Breese"], "id": "b33cc034ed30ebae6262e646c916d063a2cd7ec8", "title": "Interval Influence Diagrams", "references": ["586cecb7ea651894a20999fa104f9073abb1451b", "34e055ffa9d2b17304d8d574b7ae1c36d518c9c8", "78a896bc8eec2381813831de014167b552c278f6", "e2a302a52eb55018fb05412d79f3b45b5d909a1e", "5bf6f01402e1648b7d1e6c9200ede6cb1af30123", "5989b9579462bb1cbc03f7719b73d7aef9b8193e", "9e0daca0acc6ee3baf7573fe2e2b3cc94276e7f4", "80d6c1198b7c935f7f48252fae0ff0dc8bd3ffbd", "760f13df512e1f1a5f82122646ec6a341a778b05", "373b1817afebdced6119cb6564a6be187b4823a9"]}, {"date": "1988", "abstract": "Description: Probabilistic Reasoning in Intelligent Systems is a complete and accessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic.", "authors": ["Judea Pearl"], "id": "5bf6f01402e1648b7d1e6c9200ede6cb1af30123", "title": "Probabilistic reasoning in intelligent systems", "references": []}, {"date": "1996", "abstract": "This paper presents an approach to representing and manipulating plans based on a model of plans as a set of constraints. The (Issues - Nodes - Orderings/Variables/Auxiliary) model is used to characterise the plan representation used within O-Plan and to relate this work to emerging formal analyses of plans and planning. This synergy of practical and formal approaches can stretch the formal methods to cover realistic plan representations, as needed for real problem solving, and can improve the analysis that is possible for production planning systems. \n \n is intended to act as a bridge to improve dialogue between a number of communities working on formed planning theories, practical planning systems and systems engineering process management methodologies. It is intended to support new work on automatic manipulation of plans, human communication about plans, principled and reliable acquisition of plan information, and formal reasoning about plans.", "authors": ["Austin Tate"], "id": "3214d11153388188bde2fcd432c632f1bd7fdf18", "title": "Representing Plans as a Set of Constraints - the  Model", "references": []}, {"date": "1984", "abstract": "Locally optimum vector quantizer (VQ) designs are presented for memoryless Gaussian, gamma, and Laplacian sources. For Gaussian sources, low (2-6) dimensional vector quantization provides relatively little improvement in mean-squared error (MSE) compared to the minimum mean-squared error (MMSE) scalar quantizer. For Laplacian or gamma sources, however, significant improvement in MSE is available with vector quantization. The Laplacian and gamma 6 bit, sixdimensional vector quantizers achieve, respectively, improvements of 2 and 4.5 dB over the corresponding scalar MMSE quantizer distortions.", "authors": ["Thomas R. Fischer", "R. M. Dicharry"], "id": "e88163aab24aa8866d27ebf091ab4c256a30de53", "title": "Vector Quantizer Design for Memoryless Gaussian, Gamma, and Laplacian Sources", "references": ["15525b54f23553c6163d9be812a886357f82cd21", "42704ee7edbd19c1ac59891782d7cbbfe255aec3", "51d56fba09a9d11977ae1726eb30e07e34277582", "556b439b658d78783dd1951b9f453191cb6928c7", "7c46799502bebfe6a9ae0f457b7b8b92248ec260", "05ebdd75b4680471f0951fc84ed475f1170e06b6", "2db6419ff9f893dfac7820f350a2986aecd948b9"]}, {"date": "1989", "abstract": "From the Publisher: \nProbabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty\u0097and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition\u0097in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. \nProbabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability.", "authors": ["Judea Pearl"], "id": "70ef29e6f0ce082bb8a47fd85b9bfb7cc0f20c93", "title": "Probabilistic reasoning in intelligent systems - networks of plausible inference", "references": []}, {"date": "1985", "abstract": "We introduce a new Monte Carlo algorithm for the self-avoiding walk (SAW), and show that it is particularly efficient in the critical region (long chains). We also introduce new and more efficient statistical techniques. We employ these methods to extract numerical estimates for the critical parameters of the SAW on the square lattice. We find\u03bc=2.63820 \u00b1 0.00004 \u00b1 0.00030\u03b3=1.352 \u00b1 0.006 \u00b1 0.025\u03bdv=0.7590 \u00b1 0.0062 \u00b1 0.0042 where the first error bar represents systematic error due to corrections to scaling (subjective 95% confidence limits) and the second bar represents statistical error (classical 95% confidence limits). These results are based on SAWs of average length \u2248 166, using 340 hours CPU time on a CDC Cyber 170\u2013730. We compare our results to previous work and indicate some directions for future research.", "authors": ["Alberto Berretti", "Alan D. Sokal"], "id": "9f8161bcac9614b15e7a97dbaf89b41ad0d0651f", "title": "New Monte Carlo method for the self-avoiding walk", "references": []}, {"date": "1993", "abstract": "The Noisy-Or model is convenient for describing a class of uncertain relationships in Bayesian networks [Pearl 1988]. Pearl describes the Noisy-Or model for Boolean variables. Here we generalize the model to nary input and output variables and to arbitrary functions other than the Boolean OR function. This generalization is a useful modeling aid for construction of Bayesian networks. We illustrate with some examples including digital circuit diagnosis and network reliability analysis.", "authors": ["Sampath Srinivas"], "id": "2ea11691f817bcf0457d16c525166dfe52ee345c", "title": "A Generalization of the Noisy-Or Model", "references": ["6355b7e80cd22d22feb3f105ba53fd0375d276bd", "36651ce9be871ce50f194c901122293d62d7c1b2", "7565eb729b5c5104cd16233b27fb80c680e885e4", "d2af69880204f8368e3aaf7a6f38e0fe243d2a50", "7e6147724ae2bd0874b089e63975b9ddaaf9e8f2"]}, {"date": "1994", "abstract": "Most traditional models of uncertainty have focused on the associational relationship among variables as captured by conditional dependence. In order to successfully manage intelligent systems for decision making, however, we must be able to predict the effects of actions. In this paper, we attempt to unite two branches of research that address such predictions: causal modeling and decision analysis. First, we provide a definition of causal dependence in decision-analytic terms, which we derive from consequences of causal dependence cited in the literature. Using this definition, we show how causal dependence can be represented within an influence diagram. In particular, we identify two inadequacies of an ordinary influence diagram as a representation for cause. We introduce a special class of influence diagrams, called causal influence diagrams, which corrects one of these problems, and identify situations where the other inadequacy can be eliminated. In addition, we describe the relationships between Howard Canonical Form and existing graphical representations of cause.", "authors": ["David Heckerman", "Ross D. Shachter"], "id": "a4b363f52335e0f021ebf4534d56babccb6da8f1", "title": "A Decision-based View of Causality", "references": []}, {"date": "1987", "abstract": "Semantic Scholar extracted view of \"Some Practical Issues in Constructing Belief Networks\" by Max Henrion", "authors": ["Max Henrion"], "id": "d0c60157cc268b47e842065c13efc2d7ee454e26", "title": "Some Practical Issues in Constructing Belief Networks", "references": []}, {"date": "1993", "abstract": "Numerous methods for probabilistic reasoning in large, complex belief or decision networks are currently being developed. There has been little research on automating the dynamic, incremental construction of decision models. A uniform value-driven method of decision model construction is proposed for the hierarchical complete diagnosis. Hierarchical complete diagnostic reasoning is formulated as a stochastic process and modeled using influence diagrams. Given observations, this method creates decision models in order to obtain the best actions sequentially for locating and repairing a fault at minimum cost. This method construct decision models incrementally, interleaving probe actions with model construction and evaluation. The method treats meta-level and base-level tasks uniformly. That is, the method takes a decision-theoretic look at the control of search in causal pathways and structural hierarchies.", "authors": ["Soe-Tsyr Yuan"], "id": "34bc0d893569a6414ce4d8b98abd261f04e99579", "title": "Knowledge-Based Decision Model Construction for the Hierarchical Diagnosis: A Preliminary Report", "references": ["b886f2c097b635ee9550ca29fff7dcbbb7727ff7", "8eb37d516db3f63f3514ac605dc5a714da52a121", "dc8bc65dc57ad3489a88bbcf96094c4ff0a642ab", "fd96d4116fa6c748b13befc398a4940648c178a2", "66205bb61255357ecc1dd7aa2aceb1a6bae64b55", "b420ce31f56a34df13caa73730056183a3108976", "d5bf5a705ffb87c95ceb6ef652bbd1c251d7f0d9"]}, {"date": "1989", "abstract": "A randomised approximation scheme for the permanent of a 0\u20131s presented. The task of estimating a permanent is reduced to that of almost uniformly generating perfect matchings in a graph; the latter is accomplished by simulating a Markov chain whose states are the matchings in the graph. For a wide class of 0\u20131 matrices the approximation scheme is fully-polynomial, i.e., runs in time polynomial in the size of the matrix and a parameter that controls the accuracy of the output. This class includes all dense matrices (those that contain sufficiently many 1\u2019s) and almost all sparse matrices in some reasonable probabilistic model for 0\u20131 matrices of given density.For the approach sketched above to be computationally efficient, the Markov chain must be rapidly mixing: informally, it must converge in a short time to its stationary distribution. A major portion of the paper is devoted to demonstrating that the matchings chain is rapidly mixing, apparently the first such result for a Markov chain with genuinely c...", "authors": ["Mark Jerrum", "Alistair Sinclair"], "id": "1bc474099e193ecdc7ca24b1d7faf8d8b3d081e4", "title": "Approximating the Permanent", "references": ["dc6a60b284a8f57d361eb485f595c8bd1f2d96c5", "e5addb9547a806643664b216de0a5199e249e80a", "ec65b052f5e7dfef84bb2c15062544877d8eec68", "4690b473de66c744b408a1b1bf655b82c33c3362", "a8edf5efa51d3cb84e2f4be76f422db3675716ff", "5b9ed72cf239572a7e1477971250e34f9630891c", "a317a1d2ec646d6e39cdba90e28e2d8ecf417e6a", "3c01b0ae05bbda8a61753acb030fe533b3545a69", "02dd503ed03fb2abe1bc03598116927c5def3669", "cdb4df5fdb0ebd93af0482d6858d550983fa29da"]}, {"date": "1989", "abstract": "Semantic Scholar extracted view of \"Bayesian updating in recursive graphical models by local computations\" by Finn Verner Jensen et al.", "authors": ["Finn Verner Jensen", "Steffen L. Lauritzen", "Kristian G. Olesen"], "id": "135160dd42685e671f003ddc0f8f3fcf007f800e", "title": "Bayesian updating in recursive graphical models by local computations", "references": []}, {"date": "1993", "abstract": "We address the problem of causal interpretation of the graphical structure of Bayesian belief networks (BBNs). We review the concept of causality explicated in the domain of structural equations models and show that it is applicable to BBNs. In this view, which we call mechanism-based, causality is defined within models and causal asymmetries arise when mechanisms are placed in the context of a system. We lay the link between structural equations models and BBNs models and formulate the conditions under which the latter can be given causal interpretation.", "authors": ["Marek J Druzdzel", "Herbert A. Simon"], "id": "07ec1d17477f48dcda53e68927c3644e47ca95d1", "title": "Causality in Bayesian Belief Networks", "references": ["c3068f75e8927c51a89c3b99488f1cd0702a02dc", "95a0c7fcb2516044f88cc2a14942953fa71786eb", "daee83c0fa212ba616bba8bbe6895dd2b6670f63", "eed84509d87fc40109845e3a2d8882583e6ed599", "3c404080453c1aa648899542deb141900fe39011", "12646fb60d3b56356ee267aa5a80f5c720c108da", "a3f3d190f7bc629a2db660da657d6ba3757b0892", "179ef2ab12e58c0fcf3355874ca1bb5ff5f9b211", "0a3767909649cf31d32e087693d93171af28ebe0", "a14dbe357bac0aed3aef8861b4782bf6c8dee2ee"]}, {"date": "1986", "abstract": "Many of the ideas behind object-oriented programming have roots going back to SIMULA. The first substantial interactive, display-based implementation was the SMALLTALK language. The object-oriented style has often been advocated for simulation programs, systems programming, graphics, and AI programming. The history of ideas has some additional threads including work on message passing as in ACTORS, and multiple inheritance as in FLAVORS. It is also related to a line of work in AI on the theory of frames and their implementation in knowledge representation languages such as KRL, KEE, FRL, and UNITS.", "authors": ["Mark Stefik", "Daniel G. Bobrow"], "id": "068339f4f29c09a48c59c95dfd5d96b3f1eaff45", "title": "Object-Oriented Programming: Themes and Variations", "references": ["19314a8212c1a79bd5f0e8c6d70843757cdf8142", "6d801505d744dff6bb787b284ded9c2ef901ebbc", "e99ffc7cc0ecfc13cd44cd1ab2f15f7d0d89d80a", "6bda929b6782d50aac6242be0d79d576f37c4ebf", "131265a02f063212345ccd5d3b4365c06bc56623", "033ecd544c4e71b14a7d3ee0611a300a20b91232", "8854e412a9367a76deb2168407bb3aa065009abd", "95075b1b971e970be5aad3a15011f9042c72f873", "54ca8185df979b5888d1d59c10391710a624573c"]}, {"date": "1989", "abstract": "The Cyc project, started by Doug Lenat at MCC in 1984, is the most ambitious knowledge representation project ever undertaken. It embodies Lenat's current ideas for a system intended to encode all of commonsense knowledge. By the year 1999, he hopes that \"no one would even think of buying a computer that doesn't have Cyc running on it\". The book by Lenat and Guha is a report on the project as it was in 1989. A review of that book must distinguish four different things: the book itself, the Cyc project as it was when the book was written, the Cyc project today, and the developments that the designers are planning for the future. Of these four, the last two are probably the most interesting. This review has been difficult for me to write, because my thoughts about Cyc have changed a great deal since I first read the book in the spring of 1990. Doug Skuce showed me a copy of his review (which also appears in this issue), and it is similar to what I had originally intended to write. I agree with his complaints about the confusing organization of the book and the lack of precise definitions. Despite our reservations, we both used", "authors": ["Douglas B. Lenat", "Ramanathan V. Guha"], "id": "cd4ba23c89b5b4eecd9b542e79dbdc195de65f6d", "title": "Building Large Knowledge-Based Systems; Representation and Inference in the Cyc Project", "references": []}, {"date": "1986", "abstract": "It is conventional to apply Bayes' formula only to point estimates of the prior probabilities. This convention is unnecessarily restrictive. The analyst may prefer to estimate that the priors belong to some set of probability vectors. Set estimates allow the non-paradoxical expression of ignorance and support rigorous inference on such everyday assertions as \"one event is more likely than another\" or that an event \"usually\" occurs. Bayes' formula can revise set estimates, often at little computational cost beyond that needed for point priors. Set estimates can also inform statistical decisions, although disagreement exists about what decision methods are best.", "authors": ["Paul Snow"], "id": "760f13df512e1f1a5f82122646ec6a341a778b05", "title": "Bayesian Inference without Point Estimates", "references": ["62ffc927cf92a93ba747373c1a85f5052523aff2", "a47176e0ec512a450c2aed106700fdb3b5f653ca", "99485775cce4c1afa7361a7fcbd3c9d362309554"]}, {"date": "1988", "abstract": "The invention comprises pigments of the general Formula I in which each X is hydrogen, chlorine or bromine, A is (a) an aliphatic hydrocarbon radical, (b) the phenylene group or (c) the naphthylene group and R is identical with the 1: 2-phthaloylcarbazole-aminocarbonyl radical shown in the above formula when A has the meaning defined under (a), and R is alkoxy, alkylamino, alkoylamino, trifluoroalkyl, alkylsulphonyl or fluoro when A has the meaning defined under (b) or (c), and in which general formula the group R-A-CO-NH- is in the 3- to 9-position. The invention further comprises a process for making pigments of the above Formula I in which each X is hydrogen, chloride, bromine or alkyl and (i) A is an aliphatic hydrocarbon radical and R is the 1: 2-phthaloylcarbazole-aminocarbonyl radical shown in the formula or (ii) A is an aromatic hydrocarbon radical and R is hydrogen, alkyl, alkoxy, alkylamino, alkoylamino, trifluoralkyl, alkylsulphonyl, halogen or the 1: 2-phthaloylcarbazoleaminocarbonyl radical, R-A-CO-NH- being linked in the 3- or 9-position, wherein an amino-1: 2-phthaloylcarbazole of the above Formula I in which the group R-A-CO- is replaced by hydrogen, is acylated with an appropriate aliphatic or aromatic dicarboxylic acid or an appropriate substituted or unsubstituted aromatic mono-carboxylic acid. The products are pigments for lacquers, printing pastes and plastics such as polyvinyl chloride.", "authors": ["Steffen L. Lauritzen", "David J. Spiegelhalter"], "id": "0a3767909649cf31d32e087693d93171af28ebe0", "title": "Local computations with probabilities on graphical structures and their application to expert systems", "references": []}, {"date": "1983", "abstract": "From the Preface (See Front Matter for full Preface) \n \nAdvances in the design and production of computer hardware have brought many more people into direct contact with computers. Similar advances in the design and production of computer software are required in order that this increased contact be as rewarding as possible. The Smalltalk-80 system is a result of a decade of research into creating computer software that is appropriate for producing highly functional and interactive contact with personal computer systems. This book is the first detailed account of the Smalltalk-80 system. It is divided into four major parts: \n \nPart One -- an overview of the concepts and syntax of the programming language. \n \nPart Two -- an annotated and illustrated specification of the system's functionality. \n \nPart Three -- an example of the design and implementation of a moderate-size application. \n \nPart Four -- a specification of the Smalltalk-80 virtual machine.", "authors": ["Adele Goldberg", "David Robson"], "id": "d62613f19b90b172bfbc362969a212505519795d", "title": "Smalltalk-80: The Language and Its Implementation", "references": []}, {"date": "1984", "abstract": "Abstract This paper describes a device-independent diagnostic program called dart. dart differs from previous approaches to diagnosis taken in the Artificial Intelligence community in that it works directly from design descriptions rather than mycin -like symptom-fault rules. dart differs from previous approaches to diagnosis taken in the design-automation community in that it is more general and in many cases more efficient. dart uses a device-independent language for describing devices and a device-independent inference procedure for diagnosis. The resulting generality allows it to be applied to a wide class of devices ranging from digital logic to nuclear reactors. Although this generality engenders some computational overhead on small problems, it facilitates the use of multiple design descriptions and thereby makes possible combinatoric savings that more than offsets this overhead on problems of realistic size.", "authors": ["Michael R. Genesereth"], "id": "9fdeb8801349708a820a579fca4ab5a799d3d439", "title": "The Use of Design Descriptions in Automated Diagnosis", "references": ["5236b4b8febea74174977614009cc9ec305f9856", "f5a4b527dc644d5f1f524a5576f92ef2191cec15", "03a7ac102394262280fb42fd8fd65b5d7fd9f195", "d14a60b4058c036197276d24a57239915d345a52", "958b26814086f6e1529eccf4f05e59565b06110c", "d0877e158883081c1e071b6a712c51c1e4327865", "d60058cfffb3bf1082d624b3884d5259e03112e2", "d2109eba4f160755f0b9a7497b6b691c2fa2d5d8", "750601d2b390cbd851773999b200a93cf80fa05d", "1b4fb6c2f60ec90ef629b25f496525920fbf2a4d"]}, {"date": "1986", "abstract": "An influence diagram is a graphical structure for modeling uncertain variables and decisions and explicitly revealing probabilistic dependence and the flow of information. It is an intuitive framework in which to formulate problems as perceived by decision makers and to incorporate the knowledge of experts. At the same time, it is a precise description of information that can be stored and manipulated by a computer. We develop an algorithm that can evaluate any well-formed influence diagram and determine the optimal policy for its decisions. Since the diagram can be analyzed directly, there is no need to construct other representations such as a decision tree. As a result, the analysis can be performed using the decision maker's perspective on the problem. Questions of sensitivity and the value of information are natural and easily posed. Modifications to the model suggested by such analyses can be made directly to the problem formulation, and then evaluated directly.", "authors": ["Ross D. Shachter"], "id": "9e0daca0acc6ee3baf7573fe2e2b3cc94276e7f4", "title": "Evaluating Influence Diagrams", "references": []}, {"date": "1978", "abstract": "The space segment of a satellite system is proposed wherein a fixed number of identical transponders are shared among a larger number of spot beam regions which completely span a large total service area. Time-division multiple-access techniques are employed, and each transponder is rapidly scanned over appropriately defined group pairs of spot beam regions, thereby establishing full coverage and full interconnectivity. The service is matched to the nonuniform traffic requirements exhibited among the various spot beam regions, reliability can be optimized since all transponders are identical, and each transponder is utilized with an efficiency of 100 percent. A mathematical proof is presented which shows that the traffic can always be assigned on a nonconflicting basis, and an efficient assignment technique is described.", "authors": ["Anthony S. Acampora", "Bruce R. Davis"], "id": "2db6419ff9f893dfac7820f350a2986aecd948b9", "title": "Efficient utilization of satellite transponders via time-division multibeam scanning", "references": ["02a93f35d5c0ae449bc2648b9283a445928009e5", "d55371356fb918ab98ae180a05cd22132b267349", "37a2306636a410d102a557d509b017929855f08a"]}, {"date": "1980", "abstract": "The performance of two-dimensional polar quantization of independent Gaussian variates is evaluated as a possible improvement over one-dimensional quantizers. The distortion measure is assumed to be squared error. The quantizers are a generalization of more restrictive polar quantizers analyzed previously, admitting a differing number of phase positions in each magnitude sector. Results are provided for several bit rates up to 2.5 bits/sample. The optimal design always performs as well or better than the restricted polar quantizer, and, in the cases analyzed, slightly outperforms the Max (single-sample) quantizers for a fixed number of levels.", "authors": ["Stephen G. Wilson"], "id": "556b439b658d78783dd1951b9f453191cb6928c7", "title": "Magnitude/Phase Quantization of Independent Gaussian Variates", "references": []}, {"date": "1984", "abstract": "Abstract : In order to address some existing problems in computer-aided medical decision making, a computer program called NESTOR has been developed to aid physicians in determining the most likely diagnostic hypothesis to account for a set of patient findings. The domain of hypercalcemic disorders is used to test solution methods that should be applicable to other medical areas. A key design philosophy underlying NESTOR is that the physicians should have control of the computer interaction to determine what is done and when. In order to provide such controllable, interactive aid, specific technical tasks to be addressed. The unifying philosophy in addressing them is the use of knowledge-based methods within a formal probability theory framework. A user interface module gives the physician control over when and how these tasks are used to aid in diagnosing the cause of a patient's condition. This dissertation presents the problems that are addressed by each of the three tasks, and the details of the methods used to address them. In addition, the results of an evaluation of the hypothesis scoring and search techniques are presented and discussed. Additional keywords: artificial intelligence; expert systems; medical applications; computer aided diagnosis; medical computer applications.", "authors": ["Gregory F. Cooper"], "id": "373b1817afebdced6119cb6564a6be187b4823a9", "title": "NESTOR: A Computer-Based Medical Diagnostic Aid That Integrates Causal and Probabilistic Knowledge.", "references": ["39d132c2e7873d2f24a67a951e53e39c1008bfbd", "a5bd983187e41de85fb00ce281e59e5e8f1bf761", "8edd09452e744ad6a292b78ad27c4886a2d4824b", "776f6d45b38cf048ff30cea41f0cd3ceeefed622", "4bb50d6c49fc2105b1c0b6389d32f738291fafdf", "4c2b670bff2e611a2c820c1c0201aad2b7fccf11", "cc2b719c7ff4c7105f30015c3b129480e16c6fa4"]}, {"date": "1994", "abstract": "Truth maintenance systems (TMS) provide a method of improving the e ciency of search during problem solving. The problem solver uses the TMS to record the reasons that facts are derivable so that facts need not be rederived during the course of the search. De Kleer's Assumption Based Truth Maintenance system (ATMS) [deKleer 86] overcomes the limitations of many earlier systems, such as not being able to switch states swiftly and not being able to consider multiple solutions to a problem at once. We describe a probabilistic extension to the ATMS { An ATMS structure is augmented with a probability distribution over the set of assumptions. A probabilistic model is then constructed in the form of a Bayesian network from the ATMS structure. The probabilistic ATMS provides signi cant new functionality such as the derivation of the probability of a fact being derivable, the posterior probability over the assumptions given that a fact is derivable and the most probable context in which a fact is derivable. Our technique does not require the probability distribution of an assumption to be independent of the distributions of other assumptions. As an example of the use of the probabilistic ATMS, we show that it can be applied to construct probabilistic models to do multiple fault diagnosis. This generalizes some aspects of de Kleer and Williams' work on model based diagnosis [deKleer et. al. 87, deKleer et. al. 89]. The probabilistic ATMS has been implemented in IDEAL [Srinivas et. al. 90], a Bayesian network solver. 1", "authors": ["Sampath Srinivas"], "id": "36651ce9be871ce50f194c901122293d62d7c1b2", "title": "A Probabilistic Atms", "references": ["38d13f5fa13afb460dfce86e7091464b48fbcad7", "5abb6b0a468e98605489d62f7afe9b71c8073ebf", "bdf379596eac144908c1ae83bf1f1753c1f1c7fe", "4042a3202256c9c01eeeaf81b395797dcac8fe89", "7e6147724ae2bd0874b089e63975b9ddaaf9e8f2", "135160dd42685e671f003ddc0f8f3fcf007f800e", "7565eb729b5c5104cd16233b27fb80c680e885e4", "d8e122618cbe4449ea860e96e6a41bb97147ce09", "0a3767909649cf31d32e087693d93171af28ebe0", "2ea11691f817bcf0457d16c525166dfe52ee345c"]}, {"date": "1990", "abstract": "IDEAL (Influence Diagram Evaluation and Analysis in Lisp) is a software environment for creation and evaluation of belief networks and influence diagrams. IDEAL is primarily a research tool and provides an implementation of many of the latest developments in belief network and influence diagram evaluation in a unified framework. This paper describes IDEAL and some lessons learned during its development.", "authors": ["Sampath Srinivas", "John S. Breese"], "id": "7565eb729b5c5104cd16233b27fb80c680e885e4", "title": "IDEAL: A Software Package for Analysis of Influence Diagrams", "references": ["84a7eac0cea32b80f66a19f5224e5c69bb45cb62", "964a39b5ca27d2635cd5bebc817791f0077807f2", "17d87b9ac0bedad64489022ef415df05829843ad", "6049c0a410322aa74f362c0b749e812b7b556a78", "55185964a5c1a8e2f5018d78882198f36c8d21da", "b07a5e09daaf90296c1cead402d9fbf73e688433", "bbdf4ffe14535688f7c488bd90a81cebb12f0604", "9e0daca0acc6ee3baf7573fe2e2b3cc94276e7f4", "b33cc034ed30ebae6262e646c916d063a2cd7ec8", "9b6c027f89c8e83c680a0d3e9a78dc8e96447057"]}, {"date": "1989", "abstract": "Diagnostic tasks involve identifying faulty components from observations of symptomatic device behavior. This paper presents a general diagnostic theory that uses the perspective of diagnosis as ideniifying consisieni modes of behavior, correct or faulty. Our theory draws on the intuitions behind recent diagnostic theories to identify faulty components without necessarily knowing how they fail. To derive additional diagnostic discrimination we use the models for behavioral modes together with probabilistic information about the likelihood of each mode of behavior.", "authors": ["Johan de Kleer", "Brian C. Williams"], "id": "7e6147724ae2bd0874b089e63975b9ddaaf9e8f2", "title": "Diagnosis with Behavioral Modes", "references": ["21f31610db3b357c9e318f60e5d75ce78c0721ca", "3f2bb00de675c19666097b8576bd5f9e19a79ff0", "e585e431b00f096bf6f75563808414347460bf72", "6355b7e80cd22d22feb3f105ba53fd0375d276bd", "fd96d4116fa6c748b13befc398a4940648c178a2", "46ff54d752fa8ef746150f06ac206cfdf67a4049", "546a899f17065ac5091e68432c206007080d773f", "9fdeb8801349708a820a579fca4ab5a799d3d439", "5a7e51ee61bad2acd4b5b82a484fc7722a62d8f5"]}, {"date": "1982", "abstract": "An algorithm for the design of vector quantizers that are locally optimum in the sense of minimizing an average quantitative distortion measure is used to design 1 and 2 bit/sample vector quantizers for both real sampled speech and a simulated speech-like auto-regressive random process. Both weighted and unweighted squared-error distortion measures are considered. Several comparisons are made and discussed based on the average distortions of the vector quantization schemes. The results for the simulated speech are compared to mathematical performance bounds from information theory to provide an indication of how nearly globally optimal vector quantization is for such highly correlated sources. A comparison of the results for the real speech and the simulated speech provides a quantitative measure of the accuracy of such models and, hence, of the applicability of information theory bounds and code designs based on probabilistic models. The signal-to-quantization-noise ratios of vector quantizers designed to minimize squared-error distortion are compared to those of several popular speech waveform coding systems of similar rates.", "authors": ["H\u00fcseyin Abut", "Robert M. Gray", "Guillermo Rebolledo"], "id": "42704ee7edbd19c1ac59891782d7cbbfe255aec3", "title": "Vector quantization of speech and speech-like waveforms", "references": []}, {"date": "1982", "abstract": "A simple (combinatorial) special case of the generalized Lloyd-Max (or quantization) problem is shown to be nondeterministic polynomial (NP)-complete. {\\em A fortiori}, the general problem of communication theory, in its combinatorial forms, has at least that complexity.", "authors": ["M. R. Garey", "David S. Johnson", "H. Witsenhausen"], "id": "05ebdd75b4680471f0951fc84ed475f1170e06b6", "title": "The complexity of the generalized Lloyd - Max problem", "references": ["5e29000d24d5ded11e7a32216a91bdadaa9877f1"]}, {"date": "1954", "abstract": "Abstract * I am indebted to Richard M. Cyert, Paul F. Lazarsfeld, Roy Radner, and T. C Koopmans for valuable comments on earlier drafts of this paper. To test whether a correlation between two variables is genuine or spurious, additional variables and equations must be introduced, and sufficient assumptions must be made to identify the parameters of this wider system. If the two original variables are causally related in the wider system, the correlation is \u201cgenuine.\u201d", "authors": ["Herbert A. Simon"], "id": "179ef2ab12e58c0fcf3355874ca1bb5ff5f9b211", "title": "Spurious Correlation: A Causal Interpretation*", "references": []}, {"date": "1987", "abstract": "Diagnostic tasks require determining the difierences between a model of an artifact and the artifact itself. The difierences between the manifested behavior of the artifact and the predicted behavior of the model guide the search for the difierences between the artifact and its model. The diagnostic procedure presented in this paper is model-based, inferring the behavior of the composite device from knowledge of the structure and function of the individual components comprising the device. The system (GDE | General Diagnostic Engine) has been implemented and tested on many examples in the domain of troubleshooting digital circuits. This research makes several novel contributions: First, the system diagnoses failures due to multiple faults. Second, failure candidates are represented and manipulated in terms of minimal sets of violated assumptions, resulting in an e\u2010cient diagnostic procedure. Third, the diagnostic procedure is incremental, exploiting the iterative nature of diagnosis. Fourth, a clear separation is drawn between diagnosis and behavior prediction, resulting in a domain (and inference procedure) independent diagnostic procedure. Fifth, GDE combines modelbased prediction with sequential diagnosis to propose measurements to localize the faults. The normally required conditional probabilities are computed from the structure of the device and models of its components. This capability results from a novel way of incorporating probabilities and information theory into the context mechanism provided by AssumptionBased Truth Maintenance.", "authors": ["Johan de Kleer", "Brian C. Williams"], "id": "6355b7e80cd22d22feb3f105ba53fd0375d276bd", "title": "Diagnosing Multiple Faults", "references": ["5236b4b8febea74174977614009cc9ec305f9856", "5e3e78fec9f42c2927858df682f0bf2349add900", "9711a14bb53bfbac6f88bfec0f6b32ccc05f9254", "e585e431b00f096bf6f75563808414347460bf72", "bb8ba5d61f3610955f00c1254fd32e9036f9d4c5", "7f7174340c6780a5f1357ad8ad290005c0ccdb2e", "f08f699374a27cdbc2c1ecf050ae285b01bda723", "4bb50d6c49fc2105b1c0b6389d32f738291fafdf", "9fdeb8801349708a820a579fca4ab5a799d3d439", "2f4813c9c251623a71c65bb683f970c52ccd0b02"]}, {"date": "1977", "abstract": "In careful discussions of scientific methodology, particularly those carried on within a positivist or operationalist framework, it is now customary to avoid any use of the notion of causation and to speak instead of \u2018functional relations\u2019 and \u2018interdependence\u2019 among variables. This avoidance is derived, no doubt, from the role that the concept of causality has played in the history of philosophy since Aristotle, and particularly from the objectionable ontological and epistemological overtones that have attached themselves to the causal concept over the course of that history.", "authors": ["Herbert A. Simon"], "id": "a14dbe357bac0aed3aef8861b4782bf6c8dee2ee", "title": "Causal Ordering and Identifiability", "references": []}, {"date": "1986", "abstract": "Trillium is a computer-based environment for simulating and experimenting with interfaces for simple machines. For the past four years it has been use by Xerox designers for fast prototyping and testing of interfaces for copiers and printers. This paper defines the class of \u201cfunctioning frame\u201d interfaces which Trillium is used to design, discusses the major concerns that have driven the design of Trillium, and describes the Trillium mechanisms chosen to satisfy them.", "authors": ["Douglas Alton Henderson"], "id": "6bda929b6782d50aac6242be0d79d576f37c4ebf", "title": "The Trillium user interface design environment", "references": []}, {"date": "1976", "abstract": "The general problem of drawing inferences from uncertain or incomplete evidence has invited a variety of technical approaches, some mathematically rigorous and some largely informal and intuitive. Most current inference systems in artificial intelligence have emphasized intuitive methods, because the absence of adequate statistical samples forces a reliance on the subjective judgment of human experts. We describe in this paper a subjective Bayesian inference method that realizes some of the advantages of both formal and informal approaches. Of particular interest are the modifications needed to deal with the inconsistencies usually found in collections of subjective statements.", "authors": ["Richard 0. Duda", "Peter E. Hart", "Nils J. Nilsson"], "id": "a47176e0ec512a450c2aed106700fdb3b5f653ca", "title": "Subjective bayesian methods for rule-based inference systems", "references": []}, {"date": "1977", "abstract": "Semantic Scholar extracted view of \"An efficient digital satellite technique for serving users of differing capacities\" by Hamilton W. Arnold", "authors": ["Hamilton W. Arnold"], "id": "02a93f35d5c0ae449bc2648b9283a445928009e5", "title": "An efficient digital satellite technique for serving users of differing capacities", "references": []}, {"date": "1983", "abstract": "Palladio is a circuit design environment for experimenting with methodologies and knowledge-based, expert-system design aids. Its framework is based on several premises about circuit design: (1) circuit design is a process of incremental refinement; (2) it is an exploratory process in which design specifications and design goals coevolve; and (3) most important, circuit designers need an integrated design environment that provides compatible design tools ranging from simulators to layout generators, that permits specification of digital systems in compatible languages ranging anywhere from architectural to layout, and includes the means for explicitly representing, constructing, and testing such design tools and languages. The Palladio environment is part of a growing trend toward creating integrated design environments and away from isolated design aids. Recently several commercial computer-aided engineering (CAE) workstations have emerged, providing multiple-level, circuit-specification entry systems and integrated analysis aids. Integrated circuit designers have a special need for such workstations because of the complexity of large integrated circuits and the high costs of prototyping them.", "authors": ["Harold Brown", "Christopher Tong", "Gordon Foyster"], "id": "1b4fb6c2f60ec90ef629b25f496525920fbf2a4d", "title": "Palladio: An exploratory environment for circuit design", "references": []}, {"date": "1961", "abstract": "Semantic Scholar extracted view of \"Consistency in Statistical Inference and Decision\" by Cedric A. B. Smith", "authors": ["Cedric A. B. Smith"], "id": "62ffc927cf92a93ba747373c1a85f5052523aff2", "title": "Consistency in Statistical Inference and Decision", "references": []}, {"date": "1982", "abstract": "This paper presents a new algorithm for the diagnosis of computer hardware faults. The algorithm uses a general inference procedure to compute suspect components and generate discriminatory tests from information about the design of the device being diagnosed. In the current implementation this procedure is linear-input resolution, guided by explicit meta-level control rules. The algorithm exploits the hierarchy inherent in most computer system designs to diagnose systems a level at a time. In this way the number of parts under consideration at any one time is kept small, and the cost of test generation remains manageable.", "authors": ["Michael R. Genesereth"], "id": "750601d2b390cbd851773999b200a93cf80fa05d", "title": "Diagnosis Using Hierarchical Design Models", "references": []}, {"date": "1982", "abstract": "This paper presents generalizations of Bayes likelihood-ratio updating rule which facilitate an asynchronous propagation of the impacts of new beliefs and/or new evidence in hierarchically organized inference structures with multi-hypotheses variables. The computational scheme proposed specifies a set of belief parameters, communication messages and updating rules which guarantee that the diffusion of updated beliefs is accomplished in a single pass and complies with the tenets of Bayes calculus.", "authors": ["Judea Pearl"], "id": "99485775cce4c1afa7361a7fcbd3c9d362309554", "title": "Reverend Bayes on Inference Engines: A Distributed Hierarchical Approach", "references": ["7a4a434543c67975a6d6dcba5457fe3e8bf87391", "7896a59e995ca7b1aa7a6b799e725b58772f0c19", "33c287027925af674f2ca3c27a834ebf0aa04876", "ad9ac8938d230ef41cf2aa6a795743c8b1520200", "c8462629a67c76b5512c3a3c2d6f1cff9fa35fa2"]}, {"date": "1983", "abstract": "A mold for use in casting concrete blocks wherein the rear and end walls are stationary, a floor or bottom wall is vertically movable, and the front wall is vertically and horizontally movable under the control of links at its ends which define a parallelogram-type mounting, the fixed side being the vertical line between two link pivot points on each end of the mold; the movable front wall may be shaped to emboss grooves or other configurations on the front surface of the block.", "authors": ["Adele Goldberg", "D. J. Robson"], "id": "54ca8185df979b5888d1d59c10391710a624573c", "title": "The language and its implementation", "references": []}, {"date": "1977", "abstract": "E x p e r i e n c e w i t h t h e MYCIN [ 1 ] t h e r a p y s e l e c t o r shows t h a t a c l e a n l y s t r u c t u r e d a l g o r i t h m makes p o s s i b l e a s i . -np le , b u t u s e f u l e x p l a n a t i o n c a p a b i l i t y . The a l g o r i t h m uses t h e g e n e r a t e and t e s t method t o s e l e c t a s m a l l s e t o f d r u g s f o r a d m i n i s t r a t i o n t o a p a t i e n t h a v i n g a n i n f e c t i o u s d i s e a s e . T r a c e s o f t h e a p p l i c a t i o n o f m e d i c a l s t r a t e g i e s a re l e f t b e h i n d f o r l a t e r s e l e c t i v e r e t r i e v a l and p r i n t i n g b y t h e e x p l a n a t i o n s y s t e m . These s t r a t e g i e s a r e r e a d i l y c o m p r e h e n s i b l e t o t h e u s e r o f t h e program because t h e y a r e based on e x i s t i n g c l i n i c a l r a t i o n a l e f o r s e l e c t i n g a n t i b i o t i c s ( a s opposed t o u s i n g a n e v a l u a t i o n p o l y n o m i a l , f o r e x a m p l e . ) The g e n e r a t e and t e s t a l g o r i t h m i s d e s c r i b e d as a s e r i e s o f s t e p s t h r o u g h w h i c h each recommended d r u g must p roceed s u c c e s s f u l l y :", "authors": ["William J. Clancey"], "id": "4c2b670bff2e611a2c820c1c0201aad2b7fccf11", "title": "An Antibiotic Therapy Selector which Provides for Explanations", "references": []}, {"date": "1977", "abstract": "The importance that an understanding of time plays in many problem-solving situations requires that intelligent programs be equipped with extensive temporal knowledge. This paper discusses one route to that goal, namely the construction of a time specialist, a program knowledgable about time in general which can be used by a higher level program to deal with the temporal aspects of its problem-solving. Some examples are given of such a use of a time specialist. The principal issues addressed in this paper are how the time specialist organizes statements involving temporal references, checks them for consistency, and uses them in answering questions.", "authors": ["Kenneth M. Kahn", "G. Anthony Gorry"], "id": "cc2b719c7ff4c7105f30015c3b129480e16c6fa4", "title": "Mechanizing Temporal Knowledge", "references": []}, {"date": "1989", "abstract": "Abstract Plan recognition does not work the same way in stories and in \u201creal life\u201d (people tend to jump to conclusions more in stories). We present a theory of this, for the particular case of how objects in stories (or in life) influence plan recognition decisions. We provide a Bayesian network formalization of a simple first-order theory of plans, and show how a particular network parameter seems to govern the difference between \u201clife-like\u201d and \u201cstory-like\u201d response. We then show why this parameter would be influenced (in the desired way) by a model of speaker (or author) topic selection which assumes that facts in stories are typically \u201crelevant\u201d.", "authors": ["Eugene Charniak", "Robert P. Goldman"], "id": "9b6c027f89c8e83c680a0d3e9a78dc8e96447057", "title": "Plan Recognition in Stories and in Life", "references": []}, {"date": "1982", "abstract": "Semantic Scholar extracted view of \"Pedagogical, natural language, and knowledge engineering techniques in SOPHIE-I, II and III\" by John Seely Brown", "authors": ["John Seely Brown"], "id": "546a899f17065ac5091e68432c206007080d773f", "title": "Pedagogical, natural language, and knowledge engineering techniques in SOPHIE-I, II and III", "references": []}, {"date": "1978", "abstract": "Medical decision making can be viewed along a spectrum, with categorical (or deterministic) reasoning at one extreme and probabilistic (or evidential) reasoning at the other. In this paper we examine the flowchart as the prototype of categorical reasoning and decision analysis as the prototype of probabilistic reasoning. Within this context we compare PIP, INTERNIST, CASNET, and MYCIN\u2014four of the present programs which apply the techniques of artificial intelligence to medicine. Although these systems can exhibit impressive expert-like behavior, we believe that none of them is yet capable of truly expert reasoning. We suggest that a program which can demonstrate expertise in the area of medical consultation will have to use a judicious combination of categorical and probabilistic reasoning\u2014the former to establish a sufficiently narrow context and the latter to make comparisons among hypotheses and eventually to recommend therapy.", "authors": ["Peter Szolovits", "Stephen G. Pauker"], "id": "4bb50d6c49fc2105b1c0b6389d32f738291fafdf", "title": "Categorical and Probabilistic Reasoning in Medical Diagnosis", "references": ["ad9ac8938d230ef41cf2aa6a795743c8b1520200", "f1511e427fe46bd8065847ae3b0233cbafc3b469", "4bec6eb7d9b0290aef5b85d5206ede2b788f8731", "309685bfeac5094a313427cac26d8d6c9a33ab02", "6664bf61c019a4564bb1d988218b70c35db83ccd", "dbdb10f066b857baa6e129f49b8d832c3f6267ae", "37701059b94920c0c93d1e46c0b133eef5eebf1e", "00e4987bd8ed6de94c1c9462f299e55e228f599b", "349eaba62b2651873ac01978c8ca84c244f44e2a", "cd57879a4295e9372d503c575322679ac6f51f00"]}, {"date": "1988", "abstract": "A prototype, called the generic model-based diagnostic system (GMODS), has been implemented to detect and diagnose failures in launch system hardware at the Kennedy Space Center. GMODS diagnoses both single and multiple failures from a model of the structure and function of a physical system. The diagnostic algorithm extends the strategy proposed by J. de Kleer and B. Williams (1987) for efficiently diagnosing multiple failures. GMODS departs from previous work by using knowledge of component states to diagnose failures in components that operate in one of several states. The resulting approach also prunes suspects by using knowledge of the failed-state behavior of components.<<ETX>>", "authors": ["Lester J. Holtzblatt"], "id": "46ff54d752fa8ef746150f06ac206cfdf67a4049", "title": "Diagnosing multiple failures using knowledge of component states", "references": []}, {"date": "1987", "abstract": "The issue of how to effectively integrate and use symbolic causal knowledge with numeric estimates of probabilities in abductive diagnostic expert systems is examined. In particular, a formal probabilistic causal model that integrates Bayesian classification with a domain-independent artificial intelligence model of diagnostic problem solving (parsimonious covering theory) is developed. Through a careful analysis, it is shown that the causal relationships in a general diagnostic domain can be used to remove the barriers to applying Bayesian classification effectively (large number of probabilities required as part of the knowledge base, certain unrealistic independence assumptions, the explosion of diagnostic hypotheses that occurs when multiple disorders can occur simultaneously, etc.). Further, this analysis provides insight into which notions of \"parsimony\" may be relevant in a given application area. In a companion paper, Part Two, a computationally efficient diagnostic strategy based on the probabilistic causal model discussed in this paper is developed.", "authors": ["Yun Peng", "James A. Reggia"], "id": "2f4813c9c251623a71c65bb683f970c52ccd0b02", "title": "A Probabilistic Causal Model for Diagnostic Problem Solving Part I: Integrating Symbolic Causal Inference with Numeric Probabilistic Inference", "references": ["bb8ba5d61f3610955f00c1254fd32e9036f9d4c5", "37f4fe1978432bb71d9073a2322813dd53636938", "ad3bc116d25c635ed66ecc72281b1b694209da91", "7f7174340c6780a5f1357ad8ad290005c0ccdb2e", "e674fa71ac319ceaa5da640e415a1f24be425b3e", "309685bfeac5094a313427cac26d8d6c9a33ab02", "406da6ea2c34427e8f8067c01b9a8debdb0d9a79", "a6dd51bc649918e7ecc236f7b905ec02264b9c09", "c3dc50296f24afb1e141b72898ea45fe76ea1dff", "373b1817afebdced6119cb6564a6be187b4823a9"]}, {"date": "1978", "abstract": "MEMBER, IEEE, AND HENK C. A. V~ TILBORG The fact that the general decoding problem for linear codes and the general problem of finding the weights of a linear code are both NP-complete is shown. This strongly suggests, but does not rigorously imply, that no algorithm for either of these problems which runs in polynomial time exists.", "authors": ["Elwyn R. Berlekamp", "Robert J. McEliece", "Henk C. A. van Tilborg"], "id": "5e29000d24d5ded11e7a32216a91bdadaa9877f1", "title": "On the inherent intractability of certain coding problems (Corresp.)", "references": []}, {"date": "1987", "abstract": "An important issue in diagnostic problem solving is how to generate and rank plausible hypotheses for a given set of manifestations. Since the space of possible hypotheses can be astronomically large if multiple disorders can be present simultaneously, some means is required to focus an expert system's attention on those hypotheses most likely to be valid. A domain-independent algorithm is presented that uses symbolic causal knowledge and numeric probabilistic knowledge to generate and evaluate plausible hypotheses during diagnostic problem solving. Given a set of manifestations known to be present, the algorithm uses a merit function for partially completed competing hypotheses to guide itself to the provably most probable hypothesis or hypotheses.", "authors": ["Yun Peng", "James A. Reggia"], "id": "7f7174340c6780a5f1357ad8ad290005c0ccdb2e", "title": "A Probabilistic Causal Model for Diagnostic Problem Solving Part II: Diagnostic Strategy", "references": ["bb8ba5d61f3610955f00c1254fd32e9036f9d4c5", "23094078f1ae8a76ca6dc0627009c50d9ed02e57", "37f4fe1978432bb71d9073a2322813dd53636938", "ad3bc116d25c635ed66ecc72281b1b694209da91", "b25ec86e382b937e8a0cccdf86b0eb3c0315e3ba", "7f0b26ee6e708a08d29536d26e532501626c42fd", "c3dc50296f24afb1e141b72898ea45fe76ea1dff", "f884c9cd761732a2b41849c1ec0e0a33585ae854", "0848408ed195beb70701e0d9d70b1d57010da08f", "373b1817afebdced6119cb6564a6be187b4823a9"]}, {"date": "1979", "abstract": "Abstract To choose their actions, reasoning programs must be able to make assumptions and subsequently revise their beliefs when discoveries contradict these assumptions. The Truth Maintenance System (TMS) is a problem solver subsystem for performing these functions by recording and maintaining the reasons for program beliefs. Such recorded reasons are useful in constructing explanations of program actions and in guiding the course of action of a problem solver. This paper describes (1) the representations and structure of the tms , (2) the mechanisms used to revise the current set of beliefs, (3) how dependency-directed backtracking changes the current set of assumptions, (4) techniques for summarizing explanations of beliefs, (5) how to organize problem solvers into \u201cdialectically arguing\u201d modules, (6) how to revise models of the belief systems of others, and (7) methods for embedding control structures in patterns of assumptions. We stress the need of problem solvers to choose between alternative systems of beliefs, and outline a mechanism by which a problem solver can employ rules guiding choices of what to believe, what to want, and what to do.", "authors": ["Jon Doyle"], "id": "f08f699374a27cdbc2c1ecf050ae285b01bda723", "title": "A Truth Maintenance System", "references": ["f8ddb251bf94e4b055c6f520f21816e403c30e2a", "884c316ffab19b1ae8cfad412b0306512b8f5c0b", "d7d927cda381864aeaf4f4d2aa15d5ec05ffcdee", "8c813f13be97f2c0d2114a5a6a05afdff744676c", "0cd96a1be99dd3c2c515a622d63378ed90b52253", "0f6c7ea83494f4c921758115a925ecf55ea5ec70", "da9fc17631b3b3a05ac98f3af39dfcff5e895823", "2255db1a8ada12287fb175f52805f4c5bac26873", "d9c80173ba244764128f49e62476a9065f5a5404", "68ff263250279e00572fc43a050fa348838f34a1"]}, {"date": "1980", "abstract": "A knowledge-based interactive sequential diagnostic system is introduced which provides for diagnosis of multiple disorders in several body systems. The knowledge base consists of disorder patterns in a hierarchical structure that constitute the background medical information required for diagnosis in the domain under consideration (emergency and critical care medicine, in our case). Utilizing this knowledge base, the diagnostic process is driven by a multimembership classification algorithm for diagnostic assessment as well as for information acquisition [1]. A key characteristic of the system is congenial man-machine interface which comes to expression in, for instance, the flexibility it offers to the user in controlling its operation. At any stage of the diagnostic process the user may decide on an operation strategy that varies from full user control, through mixed initiative to full system control. Likewise, the system is capable of explaining to the user the reasoning process for its decisions. The model is independent of the knowledge base, thereby permitting continuous update of the knowledge base, as well as expansions to include disorders from other disciplines. The information structure lends itself to compact storage and provides for efflcient computation. Presently, the system contains 53 high-level disorders which are diagnosed by means of 587 medical findings.", "authors": ["Moshe Ben-Bassat", "Richard W. Carlson", "Venod K. Puri", "Mark D. Davenport", "John A. Schriver", "Mohamed Latif", "Ronald Smith", "Larry D. Portigal", "Edward H. Lipnick", "Max Harry Weil"], "id": "bb8ba5d61f3610955f00c1254fd32e9036f9d4c5", "title": "Pattern-Based Interactive Diagnosis of Multiple Disorders: The MEDAS System", "references": ["cf7068be939ac5f2a04dc392e7d08604bfd3aa45", "b5b2ee1af1c0a867f564e44f3f79b8d2bdeca749", "b5538b083a7366f5f87b78b8a58e3c96d79021b5", "8150a611a5732736c22cbfbe4b76c138822b8bfd", "309685bfeac5094a313427cac26d8d6c9a33ab02", "ce2e220f50dd6c911b387a3d069d9d0927daeb44", "4043708f0b25cd25a5245459edc19ff59d3d690b", "1c62bce34da5568429c2cf05415ca139eaa3e1f2", "34d3cc43929c49bd4bb5f8fff9ea987f11ffe993"]}, {"date": "1984", "abstract": "\u201cHard problems\u201d can be hard because they are computationally intractable. or because they are underconstrained. Here we describe candidate generation for digital devrces with state, a fault localization problem that is intractable when the devices are described at low levels of abstraction, and is underconstrained when described at higher levels of abstraction. Previous v;ork [l] has shown that a fault in a combinatorial digital circuit can be localized using a constraint-based representation of structure and behavior. ln this paper we (1) extend this represerltation to model a circuit with state by choosrng a time granularity and vocabulary of signals appropriate to that circuit; (2) demonstrate that the same candidate generation procedure that works for combinatorial circuits becomes indiscriminate when applied to a state circuit modeled in that extended representationL(3) show how the common technique of singlestepping can be viewed as a divide-and-conquer approach to overcoming that lack of constraint; and (4) illustrate how using structural de?ail can help to make the candidate generator discriminating once again, but only at great cost.", "authors": ["Walter Hamscher", "Randall Davis"], "id": "5e3e78fec9f42c2927858df682f0bf2349add900", "title": "Diagnosing Circuits With State: An Inherently Underconstrained Problem", "references": ["4acd4974722f3b6dc13939d2a509a58ffd32be3b", "e45c66b49d10f38cea30a3812f7f21fd656deec6"]}, {"date": "1986", "abstract": "Recent work in qualitative reasoning has focused on predicting the dynamic behavior of continuous physical systems. Significant headway has been made in identifying the principles necessary to predict this class of behavior. However, the predictive inference engines based on these principles are limited in their ability to reason about time. \n \nThis paper presents a general approach to behavioral prediction which overcomes many of these limitations. Generality results from a clean separation between principles relating to time, continuity, and qualitative representations. The resulting inference mechanism, based on propagation of constraints, is applicable to a wide class of physical systems exhibiting discrete or continuous behavior, and can be used with a variety of representations (e.g., digital, quantitative, qualitative or symbolic abstractions). In addition, it provides a framework in which to explore a broad range of tasks including prediction, explanation, diagnosis, and design.", "authors": ["Brian C. Williams"], "id": "9711a14bb53bfbac6f88bfec0f6b32ccc05f9254", "title": "Doing Time: Putting Qualitative Reasoning on Firmer Ground", "references": ["2d43f81f37dc795fa65ef2f2109ab887d8328d7c", "eacfa0d609a23d6fb2067929394326b8a3c1076e", "37f4fe1978432bb71d9073a2322813dd53636938", "f0c415e1c97b88ade402fa85ebb72e2e4d7680d2", "9b1a687d836ffc318ff1e125acd5a0dd7a21f3b5", "f4d642d674aa63aafc11562c2557cb4772946147", "d14a60b4058c036197276d24a57239915d345a52", "7e63b661797906f69791a4454bbf9444a743bfd8", "237421ea502d1c462fa828c957e5555d7a19e2f5"]}, {"date": "1981", "abstract": "Abstract Current rule-based, expert systems must cope with uncertain, subjective information. This is normally done by some form of probabilistic reasoning. Duda, Hart, and Nilsson have proposed one such scheme that is based on Bayes' rule. In this note we provide further mathematical analysis related to this rule. In particular, we prove the following proposition: if the assumptions made in deriving Duda et al.'s scheme are satisfied, together with the additional assumptions that the space of hypotheses is mutually exclusive and exhaustive, then no updating can take place. However, since this latter assumption is rarely satisfied in realistic systems, we then indicate how our analysis changes as exclusivity and exhaustivity are relaxed.", "authors": ["Edwin P. D. Pednault", "Steven W. Zucker", "L. V. Muresan"], "id": "7896a59e995ca7b1aa7a6b799e725b58772f0c19", "title": "On the Independence Assumption Underlying Subjective Bayesian Updating", "references": []}, {"date": "1988", "abstract": "We survey the current state of the art in model-based reasoning, particularly its application to diagnosis and troubleshooting, reviewing areas that are well understood and exploring areas that present challenging research topics. We conclude that diagnostic reasoning from a model is reasonably well understood, but that there is a rich supply of research issues in the modeling process itself. In a sense we know how to do model- based reasoning; we don''t know how to model the behavior of complex devices, how to create models, and how to select the ``right'''' model for the task at hand.", "authors": ["Randall Davis", "Walter Hamscher"], "id": "21f31610db3b357c9e318f60e5d75ce78c0721ca", "title": "Model-based reasoning: troubleshooting", "references": []}, {"date": "1976", "abstract": "This disclosure describes reversible complexes of antibiotic trans-BM123 gamma with an alkali metal alkyl sulfate and a process for preparing same. The complexes are useful as animal feed supplements which significantly enhance the growth rate of animals and poultry.", "authors": ["J. Kleer"], "id": "3f2bb00de675c19666097b8576bd5f9e19a79ff0", "title": "Local Methods for Localizing Faults in Electronic Circuits", "references": ["03120f2eb1b2ab5ce083f6def371c798be505182"]}, {"date": "1987", "abstract": "Suppose one is given a description of a system, together with an observation of the system's behaviour which conflicts with the way the system is meant to behave. The diagnostic problem is to determine those components of the system which, when assumed to be functioning abnormally, will explain the discrepancy between the observed and correct system behaviour. We propose a general theory for this problem. The theory requires only that the system be described in a suitable logic. Moreover, there are many such suitable logics, e.g. first-order, temporal, dynamic, etc. As a result, the theory accommodates diagnostic reasoning in a wide variety of practical settings, including digital and analogue circuits, medicine, and database updates. The theory leads to an algorithm for computing all diagnoses, and to various results concerning principles of measurement for discriminating among competing diagnoses. Finally, the theory reveals close connections between diagnostic reasoning and nonmonotonic reasoning.", "authors": ["Raymond Reiter"], "id": "e585e431b00f096bf6f75563808414347460bf72", "title": "A Theory of Diagnosis from First Principles", "references": ["ad3bc116d25c635ed66ecc72281b1b694209da91", "e156a328a3c53aece927dbe600bb4820d35be420", "6355b7e80cd22d22feb3f105ba53fd0375d276bd", "fd96d4116fa6c748b13befc398a4940648c178a2", "697817d6ebacf03edd547d97bfa6d66582e4b40e", "4ae93f8cae228353e07bf549c45b47ed622397eb", "93bdca51c9c0477121ba9708ffe2747855b93aef", "9fdeb8801349708a820a579fca4ab5a799d3d439", "7270d2dedc35ed03ac322778d52ccbef663dc471", "f0a8a68c0a40ce4aa0905045471a118f6e3460f7"]}, {"date": "1978", "abstract": "Semantic Scholar extracted view of \"Development of the Prospector Consultation System for Mineral Exploration: Final Report\" by Richard O. Duda et al.", "authors": ["Richard O. Duda", "Peter E. Hart", "Paul Barrett", "John Gaschnig", "Kurt Konolige", "Ren\u00e9 Reboh", "I D J. Slocum"], "id": "33c287027925af674f2ca3c27a834ebf0aa04876", "title": "Development of the Prospector Consultation System for Mineral Exploration: Final Report", "references": []}, {"date": "1981", "abstract": "Semantic Scholar extracted view of \"Subjective Bayesian methods for rule-based inference systems\" by Richard O. Duda et al.", "authors": ["Richard O. Duda", "Peter E. Hart", "Nils J. Nilsson"], "id": "c8462629a67c76b5512c3a3c2d6f1cff9fa35fa2", "title": "Subjective Bayesian methods for rule-based inference systems", "references": []}, {"date": "1977", "abstract": "The Hearsay model has heen presented as a paradigm for attacking errorful knowledge-intensive problems requiring multiple, cooperating knowledge sources. The Hearsay-II architecture is the latest attempt to explore the model. This paper describes experiences gained while successfully applying this architecture to the problem of speech understanding. The major conclusions are: 1. The paradigm of viewing problem solving in terms of hypothesize-and-test actions distributed among distinct representations of the problem has been shown to be computationally feasible. 2. A global working memory (the \"blackboard\"), in which the distinct representations are integrated in a uniform manner, has made it convenient to construct and integrate the individual sources of knowledge needed for the problem solution. 3. The use of a uniform data-directed structure for controlling knowledge-source activity has made the system easy to understand and modify. 4. A solution has been demonstrated to the problem of focus-of-attention in this type of control environment. This solution does not need to be modified when the sources of knowledge in the system are changed.", "authors": ["Victor R. Lesser", "Lee D. Erman"], "id": "7a4a434543c67975a6d6dcba5457fe3e8bf87391", "title": "A Retrospective View of the Hearsay-II Architecture", "references": ["bc587390dcf5198671fb0fa9304f52dea5afdacb", "2874b1dd8e06161fd615b0e500c2101608645d7f", "e262c54e0eca34a3146b911134b5a0fcb4e20133", "13b55dfb568050d86191b755c4d9d6e5bd09ac78", "4597f9a93809a01462ee895d524df485583aeff4", "87613876fde8ae60c0c3024a5f1fa9772fd33481", "b11b568ff9aa55d408b7cad572cf2032cc5baa36", "43f347e70f9c39bf636e867dc0c0b89173f7eb35", "d625945e6c7dc923d98feeeab91de19107130c83", "e97795382386ecd24300f3a6449ed5732b200bfa"]}, {"date": "1975", "abstract": "Abstract Medical science often suffers from having so few data and so much imperfect knowledge that a rigorous probabilistic analysis, the ideal standard by which to judge the rationality of a physician's decision, is seldom possible. Physicians nevertheless seem to have developed an ill-defined mechanism for reaching decisions despite a lack of formal knowledge regarding the interrelationships of all the variables that they are considering. This report proposes a quantification scheme which attempts to model the inexact reasoning processes of medical experts. The numerical conventions provide what is essentially an approximation to conditional probability, but offer advantages over Bayesian analysis when they are utilized in a rule-based computer diagnostic system. One such system, a clinical consultation program named mycin , is described in the context of the proposed model of inexact reasoning.", "authors": ["Edward H. Shortliffe", "Bruce G. Buchanan"], "id": "ad9ac8938d230ef41cf2aa6a795743c8b1520200", "title": "A model of inexact reasoning in medicine", "references": []}, {"date": "1984", "abstract": "Going from data describing a situation to an explanatory hypothesis that best accounts for the data is a commonly occurring knowledge-based reasoning problem. Its presence can be detected in such diverse tasks as diagnosis, perception, and theory formation. This form of reasoning has been called \"abductive inference\"1, 2 and we adopt the term here. \n \nSometimes in order to accomplish an abduction the need is to assemble hypothesis parts into a unified explanatory hypothesis. In this paper we describe a general mechanism for accomplishing the unification of sub-hypotheses with possibly overlapping domains of explanation. This mechanism makes use of plausibility information concerning the sub-hypotheses, along with information about what a sub-hypothesis can explain in the particular situation, to build up a best explanation. The novel capability arises of \"abductive confirmation\" of a sub-hypothesis based on its ability to explain some feature for which no other plausible explanation can be found. \n \nA version of this mechanism has been used successfully as the basis for a knowledge-based system, RED, which solves real-world problems of red-cell antibody identification3. These are problems which arise in the hospital blood bank, and are currently solved by specially trained human experts.", "authors": ["John R. Josephson", "B. Chandrasekaran", "Jack W. Smith"], "id": "c3dc50296f24afb1e141b72898ea45fe76ea1dff", "title": "Assembling the best explanation", "references": []}, {"date": "1978", "abstract": "Abstract : Truth maintenace systems have been used in recently developed problem solving systems. A truth maintenance system (TMS) is designed to be used by deductive systems to maintain the logical relations among the beliefs which those systems manipulate. These relations are used to incrementally modify the belief structure when premises are changed, giving a more flexible context mechanism than has been present in earlier artificial intelligence systems. The relations among beliefs can also be used to directly trace the source of contradictions or failures, resulting in far more efficient backtracking. In this paper a new approach is taken to truth maintenance algorithms. Each belief, or proposition, can be in any one of three truth states, true, false, or unknown. The relations among propositions are represented in disjunctive classes. By representing an implication in a clause the same algorithm that is used to deduce its consequent can be used to deduce the negation of antecedents that would lead to contradictions. A simple approach is also taken to the handling of assumptions and back tracking which does not involve the non-monotonic dependency structures present in other truth maintenance systems. (Author)", "authors": ["David A. McAllester"], "id": "68ff263250279e00572fc43a050fa348838f34a1", "title": "A Three Valued Truth Maintenance System.", "references": []}, {"date": "1984", "abstract": "A display device employing a planar array of elements disposed on a dielectric substrate. The dielectric substrate is affixed to a metallic plate which functions as an anode element in an electrical discharge while a perimetrical metallic strip serves as a cathode element. These components are disposed between two non-conductive plates in which a partial vacuum is created and backfilled with an inert gas. The non-conductive plate proximate the display surface of the array of elements is transparent enabling an undistorted view of the segments. A relatively large potential applied between the anode and the cathode elements sustains the electrical discharge thereby providing a source of free electrons in the area between the elements and the transparent plate. The application of a small bias voltage between selected segments of the array of elements and the anode element produces a thin luminous sheath over the energized segments which provides a sharply defined display.", "authors": ["Dana S. Nau", "James A. Reggia"], "id": "a6dd51bc649918e7ecc236f7b905ec02264b9c09", "title": "Relationships Between Deductive and Abductive Inference in Knowledge-Based Diagnostic Problem Solving", "references": []}, {"date": "1991", "abstract": "Abstract The Polytree Algorithm of Kim and Pearl [16, 19] is a fundamental method for evidential reasoning in belief networks. Not only does it provide exact solutions to singly connected networks using efficient, local computations, but variations of it can be applied to more general networks as well. When a belief network is singly connected, the Polytree Algorithm can compute a posterior marginal distribution for each variable by visiting each node at most once for each piece of evidence. By contrast, the related algorithms based on undirected graphs [1, 14\u201316, 28] only need to visit each node at most twice no matter how much evidence is observed. In this paper, a Revised Polytree Algorithm is developed with the same complexity as the undirected methods, but within the directed framework of the Polytree Algorithm. When this new algorithm is applied via \u201ccutset conditioning\u201d to general networks it obtains not just the corresponding significant improvement in speed, but also a much simpler form for combination. Furthermore, the revised algorithm requires only minor modifications to existing implementations of the Polytree Algorithm.", "authors": ["Mark A. Peot", "Ross D. Shachter"], "id": "bbdf4ffe14535688f7c488bd90a81cebb12f0604", "title": "Fusion and Propagation with Multiple Observations in Belief Networks", "references": ["4b15bd9477e5a7dd59a1b6cc414d6c530405e19f", "6049c0a410322aa74f362c0b749e812b7b556a78", "262f353855b0e6cbeda26ba3b19fff5df1d7c1a2", "9e475eff11c29f0d532f18a0710bfd87010ef44d", "55185964a5c1a8e2f5018d78882198f36c8d21da", "af115c60376909328560fa9312ef0e274ea1e7fa", "bb0419bccc2244ed33c9c42341f342511262daa3", "0e7c8ac116f3d3b3052d23fa4b269b0000286105", "1eb1583c2d2f7f075075cc54b0ef1640d2b7da4b", "f8d16924d37ac2ad2fe23e641673f9f2b5434733"]}, {"date": "1986", "abstract": "This paper extends the applications of belief-networks models to include the revision of belief commitments, i.e., the categorical instantiation of a subset of hypotheses which constitute the most satisfactory explanation of the evidence at hand. We show that, in singly-connected networks, the most satisfactory explanation can be found in linear time by a message-passing algorithm similar to the one used in belief updating. In multiply-connected networks, the problem may be exponentially hard but, if the network is sparse, topological considerations can be used to render the interpretation task tractable. In general, finding the most probable combination of hypotheses is no more complex than computing the degree of belief for any individual hypothesis.", "authors": ["Judea Pearl"], "id": "406da6ea2c34427e8f8067c01b9a8debdb0d9a79", "title": "Distributed Revision of Belief Commitment in Multi-Hypothesis Interpretations", "references": ["4e9c168f6d744174efad3764e03522fe55be5ada", "bb8ba5d61f3610955f00c1254fd32e9036f9d4c5", "e585e431b00f096bf6f75563808414347460bf72", "ad3bc116d25c635ed66ecc72281b1b694209da91", "edb4f2b7751aa8b4d479d8333c63101305bc8b9b", "bb0419bccc2244ed33c9c42341f342511262daa3", "738b875c6e8237235b038960785bb32521a908c6", "373b1817afebdced6119cb6564a6be187b4823a9"]}, {"date": "1980", "abstract": "The sensitivity of Bayesian pattern recognition models to multiplicative deviations in the prior and conditional probabilities is investigated for the two-class case. Explicit formulas are obtained for the factor K by which the computed posterior probabilities should be divided in order to eliminate the deviation effect. Numerical results for the case of binary features indicate that the Bayesian model tolerates large deviations in the prior and conditional probabilities. In fact, the a priori ratio and the likelihood ratio may deviate within a range of 65-135 percent and still produce posterior probabilities in accurate proximity of at most \u00b10.10. The main implication is that Bayesian systems which are based on limited data or subjective probabilities are expected to have a high percentage of correct classification despite the fact that the prior and conditional probabilities they use may deviate rather significantly from the true values.", "authors": ["Moshe Ben-Bassat", "Karin L. Klove", "Max Harry Weil"], "id": "34d3cc43929c49bd4bb5f8fff9ea987f11ffe993", "title": "Sensitivity Analysis in Bayesian Classification Models: Multiplicative Deviations", "references": ["e4737916b31c60be6752e61ab3dd7b24d5e4cc76", "bb8ba5d61f3610955f00c1254fd32e9036f9d4c5", "cef0f13b9f90ba862356785c01ab2d9943994882", "c39f136dc6c5d54d16ca2275919541facd4f3070", "fe30eeed7e01866e90a12ea64940de1c7f4cab17"]}, {"date": "1986", "abstract": "Semantic Scholar extracted view of \"An assumption-based truth maintenance system\" by Johan de Kleer", "authors": ["Johan de Kleer"], "id": "5a7e51ee61bad2acd4b5b82a484fc7722a62d8f5", "title": "An assumption-based truth maintenance system", "references": []}, {"date": "1976", "abstract": "Abstract : This report investigates some techniques appropriate to representing the knowledge necessary for understanding a class of electronic machines -- radio receivers. A computational performance model 'WATSON' is presented. WATSON's task is to isolate failures in radio receivers whose principles of operation have been appropriately described in the knowledge base. (Author)", "authors": ["Allen L. Brown"], "id": "03120f2eb1b2ab5ce083f6def371c798be505182", "title": "Qualitative Knowledge, Causal Reasoning, and the Localization of Failures", "references": []}, {"date": "1984", "abstract": "The ability to reason about a series of complex events over time is essential in analyzing physical systems. This paper discusses the role of continuity in qualitative physics and its application in a system for analyzing the behavior of Digital MOS circuits that exhibit analog behavior. The discussion begins with a brief overview of the reasoning steps necessary to perform a qualitative simulation using Temporal Qualitative (TQ) Analysis. The discussion then focuses in on the use of continuity and the relationship between quantities and their higher order derivatives in describing how physical quantities change over time.", "authors": ["Brian C. Williams"], "id": "237421ea502d1c462fa828c957e5555d7a19e2f5", "title": "The Use of Continuity in a Qualitative Physics", "references": ["f1f24bb771786c7ae6df4bd087a481c7414dbcde"]}, {"date": "1975", "abstract": "Abstract This report describes progress in the development of an interactive computer program, termed MYCIN, that uses the clinical decision criteria of experts to advise physicans who request advice regarding selection of appropriate antimicrobial therapy for hospital patients with bacterial infections. Since patients with infectious diseases often require therapy before complete information about the organism becomes available, infectious disease experts have identified clinical and historical criteria that aid in the early selection of antimicrobial therapy. MYCIN gives advice in this area by means of three subprograms: (1) A Consultation System that uses information provided by the physician, together with its own knowledge base, to choose an appropriate drug or combination of drugs; (2) An Explanation System that understands simple English questions and answers them in order to justify its decisions or instruct the user; and (3) A Rule Acquisition System that acquires decision criteria during interactions with an expert and codes them for use during future consultation sessions. A variety of human engineering capabilities have been included to heighten the program's acceptability to the physicians who will use it. Early experience indicates that a sample knowledge base of 200 decision criteria can be used by MYCIN to give appropriate advice for many patients with bacteremia. The system will be made available for evaluation in the clinical setting after its reliability has been shown to approach that of infectious disease experts.", "authors": ["Edward H. Shortliffe", "Rich Davis", "Stanton G. Axline", "Bruce G. Buchanan", "Clarence C. Green", "Sanford N. Cohen"], "id": "1c62bce34da5568429c2cf05415ca139eaa3e1f2", "title": "Computer-based consultations in clinical therapeutics: explanation and rule acquisition capabilities of the MYCIN system.", "references": []}, {"date": "1984", "abstract": "Abstract This paper presents a qualitative-reasoning method for predicting the behavior of mechanisms characterized by continuous, time-varying parameters. The structure of a mechanism is described in terms of a set of parameters and the constraints that hold among them: essentially a \u2018qualitative differential equation\u2019. The qualitative-behavior description consists of a discrete set of time-points, at which the values of the parameters are described in terms of ordinal relations and directions of change. The behavioral description, or envisionment, is derived by two sets of rules: propagation rules which elaborate the description of the current time-point, and prediction rules which determine what is known about the next qualitatively distinct state of the mechanism. A detailed example shows how the envisionment method can detect a previously unsuspected landmark point at which the system is in stable equilibrium.", "authors": ["Benjamin Kuipers"], "id": "7e63b661797906f69791a4454bbf9444a743bfd8", "title": "Commonsense Reasoning about Causality: Deriving Behavior from Structure", "references": ["6813e5400681a1704c4c4aef2cb7a805fa99c30b", "f5a4b527dc644d5f1f524a5576f92ef2191cec15", "18ade4f5a83e3b1915a3d8d15f092af6ee63c9f9", "5fc326b249b73b8050ef5cda3243c466b733ff2b", "b3aebbb0e7bc39c263df37d25482916065b2768f", "2c3be6c350b54a9722b2c7fd8dda80341912b5d6", "453f149546048b895fd39bac304d654c0e24a35d", "e5b9efabfef885ec48528c4d1c9e1e506a193d57", "b36643e9400f75ca50618d240b1e898e44310556", "1b212f5757e1fcfff386223bc361e471d5a3f4df"]}, {"date": "1984", "abstract": "Abstract Objects move, collide, flow, bend, heat up, cool down, stretch, compress, and boil. These and other things that cause changes in objects over time are intuitively characterized as processes . To understand commonsense physical reasoning and make programs that interact with the physical world as well as people do we must understand qualitative reasoning about processes, when they will occur, their effects, and when they will stop. Qualitative process theory defines a simple notion of physical process that appears useful as a language in which to write dynamical theories. Reasoning about processes also motivates a new qualitative representation for quantity in terms of inequalities, called the quantity space . This paper describes the basic concepts of qualitative process theory, several different kinds of reasoning that can be performed with them, and discusses its implications for causal reasoning. Several extended examples illustrate the utility of the theory, including figuring out that a boiler can blow up, that an oscillator with friction will eventually stop, and how to say that you can pull with a string, but not push with it.", "authors": ["Kenneth D. Forbus"], "id": "9b1a687d836ffc318ff1e125acd5a0dd7a21f3b5", "title": "Qualitative Process Theory", "references": ["2fcf66998c4b67b389627fa2aaa31a103cbde102", "6813e5400681a1704c4c4aef2cb7a805fa99c30b", "18ade4f5a83e3b1915a3d8d15f092af6ee63c9f9", "157e750a1259d0a5f839bb5cb8779ccb9d7702d6", "3087b4b62d15d89a69b5764c0591a1c438091a94", "a130daf60ae16581744d0051d8197c9fc7ce653c", "f1f24bb771786c7ae6df4bd087a481c7414dbcde", "c2dc03a92f03dbbf145a6f8b6568740abb325e19", "e78fea4c45a983c3d2083799e7d54119be58a217", "5fc326b249b73b8050ef5cda3243c466b733ff2b"]}, {"date": "1984", "abstract": "Abstract With the push towards submicron technology, transistor models have become increasingly complex. The number of components in integrated circuits has forced designers' efforts and skills towards higher levels of design. This has created a gap between design expertise and the performance demands increasingly imposed by the technology. To alleviate this problem, software tools must be developed that provide the designer with expert advice on circuit performance and design. This requires a theory that links the intuitions of an expert circuit analyst with the corresponding principles of formal theory (i.e., algebra, calculus, feedback analysis, network theory, and electrodynamics), and that makes each underlying assumption explicit. Temporal qualitative analysis is a technique for analyzing the qualitative large signal behavior of MOS circuits that straddle the line between the digital and analog domains. Temporal qualitative analysis is based on the following four components: First, a qualitative representation is composed of a set of open regions separated by boundaries. These boundaries are chosen at the appropriate level of detail for the analysis. This concept is used in modeling time, space, circuit state variables, and device operating regions. Second, constraints between circuit state variables are established by circuit theory. At a finer time scale, the designer's intuition of electrodynamics is used to impose a causal relationship among these constraints. Third, large signal behavior is modeled by transition analysis, using continuity and theorems of calculus to determine how quantities pass between regions over time. Finally, feedback analysis uses knowledge about the structure of equations and the properties of structure classes to resolve ambiguities.", "authors": ["Brian C. Williams"], "id": "f0c415e1c97b88ade402fa85ebb72e2e4d7680d2", "title": "Qualitative Analysis of MOS Circuits", "references": ["4acd4974722f3b6dc13939d2a509a58ffd32be3b", "94645dd1765c274d1e61849580a2677ea0063cbe", "b3aebbb0e7bc39c263df37d25482916065b2768f", "54f35b4edba6ddee8ce2eac489bde78308e3e708", "6886763fd4fe5e9bef58b55b68ca8ac19018151a", "f1f24bb771786c7ae6df4bd087a481c7414dbcde", "7e63b661797906f69791a4454bbf9444a743bfd8", "e42ea2775d667707632fb8f8950e391851de75aa", "5abea8ca4265106a1ec9f37eaeff01c37e0c57fa", "237421ea502d1c462fa828c957e5555d7a19e2f5"]}, {"date": "1983", "abstract": "An interval-based temporal logic is introduced, together with a computationally effective reasoning algorithm based on constraint propagation. This system is notable in offering a delicate balance between", "authors": ["James F. Allen"], "id": "f4d642d674aa63aafc11562c2557cb4772946147", "title": "Maintaining knowledge about temporal intervals", "references": ["3bcc8272a88796ae5d86a3dded89f66034c1cef7", "5fc326b249b73b8050ef5cda3243c466b733ff2b", "cc2b719c7ff4c7105f30015c3b129480e16c6fa4", "46f41bcaf5d69e3586571c6b8c91f525096726f5"]}, {"date": "1987", "abstract": "We provide an introduction to Theorist, a logic programming system that uses a uniform deductive reasoning mechanism to construct explanations of observations in terms of facts and hypotheses. Observations, facts, and possible hypotheses are each sets of logical formulas that represent, respectively, a set of observations on a partial domain, a set of facts for which the domain is a model, and a set of tentative hypotheses which may be required to provide a consistent explanation of the observations.", "authors": ["David Poole", "Randy Goebel", "Romas Aleliunas"], "id": "f0a8a68c0a40ce4aa0905045471a118f6e3460f7", "title": "Theorist: A Logical Reasoning System for Defaults and Diagnosis", "references": []}, {"date": "1976", "abstract": "The Hearsay II speech understanding system being developed at Carnegie-Mellon University has an independent knowledge source module for each type of speech knowledge. Modules communicate by reading, writing, and modifying hypotheses about various constituents of the spoken utterance in a global data structure. The syntax and semantics module uses rules (productions) of four types: (1) recognition rules for generating a phrase hypothesis when its needed constituents have already been hypothesized; (2) prediction rules for inferring the likely presence of a word or phrase from previously recognized portions of the utterance; (3) respelling rules for hypothesizing the constituents of a predicted phrase; and (4) postdiction rules for supporting an existing hypothesis on the basis of additional confirming evidence. The rules are automatically generated from a declarative (Le., non-procedural) description of the grammar and semantics, and are embedded in a parallel recognition network for efficient retrieval of applicable rules. The current grammar uses a 450-word vocabulary and accepts simple English queries for an information retrieval system.", "authors": ["Frederick Hayes-Roth", "Jack Mostow"], "id": "2874b1dd8e06161fd615b0e500c2101608645d7f", "title": "Syntax and semantics in a distributed speech understanding system", "references": ["f027ce53a12f36f93897a2b5733549ca323c18d0", "9da40f8b672a74ee64cf2cef9f3e2804fe810eb4", "21ef510c0f44f796abced7b17169037043030a0e", "8ef1568b4377fce96f9d350d6d46a619d71a1462", "73ed12a7b3019f266adca7beef47163b8b737402"]}, {"date": "1985", "abstract": "Abstract This paper, which is Part I of a two-part series, introduces a new model of diagnostic problem solving based on a generalization of the set-covering problem. The model formalizes the concepts of 1. (1) whether or not a set of one or more disorders is sufficient to explain a set of occurring manifestations, 2. (2) what a solution is for a diagnostic problem, and 3. (3) how to generate all of the alternative explanations in a problem's solution. In addition, conditions for decomposing a diagnostic problem into independent subproblems are stated and proven. This model is of interest because it captures several intuitively plausible features of human diagnostic inference, it directly addresses the issue of multiple simultaneous causative disorders, it can serve as a theoretical basis for expert systems for diagnostic problem solving, and it provides a conceptual framework within which to view some recent AI work on diagnostic problem solving in general. In Part II, the concepts developed in this paper will be used to present algorithms for diagnostic problem solving.", "authors": ["James A. Reggia", "Dana S. Nau", "Pearl Y. Wang"], "id": "7270d2dedc35ed03ac322778d52ccbef663dc471", "title": "A formal model of diagnostic inference. I. Problem formulation and decomposition", "references": []}, {"date": "1975", "abstract": "Abstract : MSYS is a system for reasoning with uncertain information and inexact rules of inference. Its major application, to date, has been to the interpretation of visual features (such as regions) in scene analysis. In this application, features are assigned sets of possible interpretations with associated likelihoods based on local attributes (e.g., color, size, and shape). Interpretations are related by rules of inference that adjust the likelihoods up or down in accordance with the interpretation likelihoods of related features. An asynchronous relaxation process repeatedly applies the rules until a consistent set of likelihood values is attained. At this point, several alternative interpretations still exist for each feature. One feature is chosen and the most likely of its alternatives is assumed. The rules are then used in this more precise context to determine likelihoods for the interpretations of remaining features by a further round of relaxation. The selection and relaxation steps are repeated until all features have been interpreted.", "authors": ["Jay M. Tenenbaum", "Harry G. Barrow"], "id": "d625945e6c7dc923d98feeeab91de19107130c83", "title": "MSYS: A System for Reasoning About Scenes.", "references": []}, {"date": "1977", "abstract": "The Locus model of search is a non-backtracking, deterministic search technique in which a beam of near-miss alternatives around the best path are extended in parallel for graph searching problems. In this paper we formulate image interpretation as a giaph searching problem and show bow the Locus model provides a near-optimal minimal effort solution. The structure of the model is illustrated using a detailed example. The relationship of the present approach to earlier attempts at image interpretation are discussed.", "authors": ["Steven M. Rubin", "Raj Reddy"], "id": "43f347e70f9c39bf636e867dc0c0b89173f7eb35", "title": "The LOCUS Model of Search and its Use in Image Interpretation", "references": ["f027ce53a12f36f93897a2b5733549ca323c18d0"]}, {"date": "1977", "abstract": "A key problem for speech understanding systems is the verification of word hypotheses generated by various knowledge sources in the system. In this paper we will discuss the general problem of word verification in speech understanding systems. A description of our matching algorithm for word verification which is based on that used in the HARPY system, a general connected speech recognition system (Lowerre, 1976), is given. An example of the verification of a word hypothesis using this algorithm is presented. Problems which arose in applying this technique to verification of individual words in a connected speech understanding system and their solutions are discussed. A performance analysis of the verifier in terms of accuracy and speed is given and directions for future work are indicated.", "authors": ["David Mckeown"], "id": "b11b568ff9aa55d408b7cad572cf2032cc5baa36", "title": "Word verification in the Hearsay II speech understanding system", "references": []}, {"date": "1971", "abstract": "Abstract : PLANNER is a formalism for proving theorems and manipulating models in a robot. The formalism is built out of a number of problem-solving primitives together with a hierarchical multiprocess backtrack control structure. Statements can be asserted and perhaps later withdrawn as the state of the world changes. Under BACKTRACK control structure, the hierarchy of activations of functions previously executed is maintained so that it is possible to revert to any previous state. Thus programs can easily manipulate elaborate hypothetical tentative states. In addition PLANNER uses multiprocessing so that there can be multiple loci of control over the problem-solving.", "authors": ["Carl Hewitt"], "id": "e97795382386ecd24300f3a6449ed5732b200bfa", "title": "Description and Theoretical Analysis (Using Schemata) of Planner: A Language for Proving Theorems and Manipulating Models in a Robot", "references": []}, {"date": "1985", "abstract": "Techniques in developing a coherent probabilistic reasoning system are illustrated with reference to a simplified example. Recent work relating statistical models to graphical representation of causal and associative relationships allows a straightforward means of propagating evidence whilst retaining a probabilistic interpretation for predictive statements. This interpretation allows continual criticism of a system's performance, while imprecise quantitative assessments permit learning from experience. Possible limitations of a formal probabilistic approach are discussed.", "authors": ["David J. Spiegelhalter"], "id": "4e9c168f6d744174efad3764e03522fe55be5ada", "title": "Probabilistic Reasoning in Predictive Expert Systems", "references": []}, {"date": "1982", "abstract": "The problem of estimating the error probability of a given classification system is considered. Statistical properties of the empirical error count (C) and the average conditional error (R) estimators are studied. It is shown that in the large sample case the R estimator is unbiased and its variance is less than that of the C estimator. In contrast to conventional methods of Bayes error estimation the unbiasedness of the R estimator for a given classifier can be obtained only at the price of an additional set of classified samples. On small test sets the R estimator may be subject to a pessimistic bias caused by the averaging phenomenon characterizing the functioning of conditional error estimators.", "authors": ["Josef Kittler", "Pierre A. Devijver"], "id": "c39f136dc6c5d54d16ca2275919541facd4f3070", "title": "Statistical Properties of Error Estimators in Performance Assessment of Recognition Systems", "references": ["d0508318a2363a7a2c7fd4dd8fed20891ec879c4", "5b83d0c4b3806ca2f57243e2e89c46eada9580b5", "7788f412782947c94b92f5f47620ce979c0c11b8", "6a9b3cd5462530aa920f4d3893712cae1e632b76", "d21bf792a1cfe670d9baff0381f69e67fa1a275d", "e47e4602cd78d9676a6787ef7bf9594863631bcf", "bb02e96226166db03b9aad203ac80114eac60bb6", "210899af38b9e74b669a23ccbafc4967d5c42794", "09884c74169d606b7dd501da91e195d15a50bcbe", "a7611cb05bc0fd9513a7ab72f7183edfb2cf2c4b"]}, {"date": "1977", "abstract": "Abstract : This report provides a critical overview to work, its applicability and directions for further research. As might be expected with a burgeoning, multi-disciplinary field, it is difficult to keep track of all that is done and to draw the implications of research done in one discipline for research done in another. This report surveys the entire field asking, (1) what is known. (2) what good is it. and (3) what else must we learn. Particular attention is given to work integrating research describing how people do make decisions with normative work that prescribes how people should make decisions.", "authors": ["Paul Slovic", "Baruch Fischhoff", "Sarah Lichtenstein"], "id": "fe30eeed7e01866e90a12ea64940de1c7f4cab17", "title": "Behavioral Decision Theory", "references": []}, {"date": "1983", "abstract": "This paper introduces a representation of evidential relationships which permits updating of belief in two simultaneous modes: causal (i. e. top-down) and diagnostic (i.e. bottom-up). It extends the hierarchical tree representation by allowing multiple causes to a given manifestation. We develop an updating scheme that obeys the axioms of probability, is computationally efficient, and is compatible with experts reasoning. The belief parameters of each variable are defined and updated by those of its neighbors in such a way that the impact of each new evidence propagates and settles through the network in a single pass.", "authors": ["Jin H. Kim", "Judea Pearl"], "id": "f8d16924d37ac2ad2fe23e641673f9f2b5434733", "title": "A Computational Model for Causal and Diagnostic Reasoning in Inference Systems", "references": ["95a0c7fcb2516044f88cc2a14942953fa71786eb", "7ab7ae534033e443205f61d8184c166a36e52838", "683fe3bbf2b2e628cf40d90e35fb39effc63b7e9", "99485775cce4c1afa7361a7fcbd3c9d362309554", "4cd91c51098783ec972f6a0ab430cacdd634a5b2", "bbd37d26a2bd074a5c10d5f909dc5e9343e7c6d2", "fc79391e2d08c6265fa30669a21b8b3cc08ac147"]}, {"date": "1977", "abstract": "From the subjectivist point of view (de Finetti, 1937) a probability is a degree of belief in a proposition whose truth has not been ascertained. A probability expresses a purely internal state; there is no \u201cright\u201d or \u201ccorrect\u201d probability that resides somewhere \u201cin reality\u201d against which it can be compared. However, in many circumstances, it may become possible to verify the truth o\u00a3 falsity of the proposition to which a probability was attached. Today, we assess the probability of the proposition\u201cit will rain tomorrow\u201d. Tomorrow, we go outside and look at the rain gauge to see whether or not it has rained. When verification is possible, we can use it to gauge the adequacy of our probability assessments.", "authors": ["Sarah Lichtenstein", "Baruch Fischhoff", "Lawrence D. Phillips"], "id": "cef0f13b9f90ba862356785c01ab2d9943994882", "title": "CALIBRATION OF PROBABILITIES: THE STATE OF THE ART+", "references": []}, {"date": "1975", "abstract": "Abstract This article considers the implications of recent research on judgmental processes for the assessment of subjective probability distributions. It is argued that since man is a selective, sequential information processing system with limited capacity, he is ill-suited for assessing probability distributions. Various studies attesting to man's difficulties in acting as an \u201cintuitive statistician\u201d are summarized in support of this contention. The importance of task characteristics on judgmental performance is also emphasized. A critical survey of the probability assessment literature is provided and organized around five topics: (1) the \u201cmeaningfulness\u201d of probability assessments; (2) methods of eliciting distributions; (3) feedback and evaluation of assessors; (4) differential ability of groups of assessors and (5) the problems of eliciting a single distribution from a group of assessors. Conclusions from the analysis with respect to future work include the need to capitalize on cognitive simplific...", "authors": ["Robin M. Hogarth"], "id": "e4737916b31c60be6752e61ab3dd7b24d5e4cc76", "title": "Cognitive Processes and the Assessment of Subjective Probability Distributions", "references": []}, {"date": "1979", "abstract": "The causal problem has become topical once again. While we are no longer causalists or believers in the universal truth of the causal principle we continue to think of causes and effects, as well as of causal and noncausal relations among them. Instead of becoming indeterminists we have enlarged determinism to include noncausal categories. And we are still in the process of characterizing our basic concepts and principles concerning causes and effects with the help of exact tools. This is because we want to explain, not just describe, the ways of things. The causal principle is not the only means of understanding the world but it is one of them.The demand for a fourth edition of this distinguished book on the subject of causality is clear evidence that this principle continues to be an important and popular area of philosophic enquiry. Non-technical and clearly written, this book focuses on the ontological problem of causality, with specific emphasis on the place of the causal principle in modern science. Mario Bunge first defines the terminology employed and describes various formulations of the causal principle. He then examines the two primary critiques of causality, the empiricist and the romantic, as a prelude to the detailed explanation of the actual assertions of causal determinism.Bunge analyzes the function of the causal principle in science, touching on such subjects as scientific law, scientific explanation, and scientific prediction. In so doing, he offers an education to layman and specialist alike on the history of a concept and its opponents. Professor William A. Wallace, author of \"Causality and Scientific Explanation\" said of an earlier edition of this work: \"I regard it as a truly seminal work in this field.\"", "authors": ["Mario Bunge"], "id": "e78fea4c45a983c3d2083799e7d54119be58a217", "title": "Causality and modern science", "references": []}, {"date": "1983", "abstract": "Interest has grown recently in developing expert systems that reason \"from first principles\", i.e., capable of the kind of problem solving exhibited by an engineer who can diagnose a malfunctioning device by reference to its schematics, even though he may never have seen that device before. In developing such a system for troubleshooting digital electronics, we have argued for the importance of pathways of causal interaction as a key concept. We have also suggested using a layered set of interaction paths as a way of constraining and guiding the diagnostic process. \n \nWe report here on the implementation and use of these ideas. We show how they make it possible for our system to generate a few sharply constrained hypotheses in diagnosing a bridge fault. \n \nAbstracting from this example, we find a number of interesting general principles at work. We suggest that diagnosis can be viewed as the interaction of simulation and inference and we find that the concept of locality proves to be extremely useful in understanding why bridge faults are difficult to diagnose and why multiple representations are useful.", "authors": ["Randall Davis"], "id": "1b212f5757e1fcfff386223bc361e471d5a3f4df", "title": "Diagnosis Via Causal Reasoning: Paths of Interaction and the Locality Principle", "references": ["fc79391e2d08c6265fa30669a21b8b3cc08ac147", "f5a4b527dc644d5f1f524a5576f92ef2191cec15"]}, {"date": "1981", "abstract": "Many MPC projects, such as video frame buffers, need a large memory subsystem. A one transistor per bit dynamic memory using Mead-Conway design rules is being designed with this purpose in mind. The memory cell size is 16.5 \u03bb by 8\u03bb (about the same size as a 1975 4K RAM cell with \u03bb = 2.5 microns). \n \nWhile a complete high density memory subsystem has not been designed, two chips have been designed to test its major components. One chip is a 1K memory array that tests the sense amplifier, column decoder/driver, and read/write logic. This chip lacks a timing generator and clock drivers. The second chip tests some low power bootstrapped clock drivers. These test chips are currently being fabricated.", "authors": ["James J. Cherry", "Gerald Roylance"], "id": "5abea8ca4265106a1ec9f37eaeff01c37e0c57fa", "title": "A One Transistor RAM for MPC Projects", "references": []}, {"date": "1980", "abstract": "This paper describes a generative theory of bugs. It claims that all bugs of a procedural skill can be derived by a highly constrained form of problem solving acting on incomplete procedures. These procedures are characterized by formal deletion operations that model incomplete learning and forgetting. The problem solver and the deletion operator have been constrained to make it impossible to derive \u201cstar-bugs\u201d\u2014algorithms that are so absurd that expert diagnosticians agree that the alogorithm will never be observed as a bug. Hence, the theory not only generates the observed bugs, it fails to generate star-bugs. \n \nThe theory has been tested on an extensive data base of bugs for multidigit subtraction that was collected with the aid of the diagnostic systems buggy and debuggy. In addition to predicting bug occurrence, by adoption of additional hypotheses, the theory also makes predictions about the frequency and stability of bugs, as well as the occurrence of certain latencies in processing time during testing. Arguments are given that the theory can be applied to domains other than subtraction and that it can be extended to provide a theory of procedural learning that accounts for bug acquisition. Lastly, particular care has been taken to make the theory principled so that it can not be tailored to fit any possible data.", "authors": ["John Seely Brown", "Kurt VanLehn"], "id": "b36643e9400f75ca50618d240b1e898e44310556", "title": "Repair Theory: A Generative Theory of Bugs in Procedural Skills", "references": ["d55fc396338da162550aab6b0397d69063abffa8", "377e0d546e6c8931bef131e3c5b0beec328c9f47", "a5ceacbbbf78b1b5ff55be25cb031fac63581359", "a39e6f35c30f95fdae465237ab1941b1e99c18ac", "01051f2670097848c0941124dbad19ab970771bf"]}, {"date": "1990", "abstract": "A steam iron soleplate, generator, and distributor subassembly of a thin soleplate with a coverplate spaced from and supported on the soleplate by spaced peripheral rib means to define a steam distributing passage therebetween. The coverplate is integrally attached to the soleplate by a continuous weld between the ribs and soleplate and steam generating means are provided in the upper surface of the cover-plate separate and spaced from the soleplate and ducted below to the steam passage means. A heat generating element forms an integral part of the coverplate for heat transfer to the soleplate through the ribs primarily by conduction. Both the method of assembly and the subassembly itself are disclosed.", "authors": ["Patrick J. Hayes"], "id": "46f41bcaf5d69e3586571c6b8c91f525096726f5", "title": "The Naive Physics Manifesto", "references": []}, {"date": "1974", "abstract": "This talk describes the present state of performance of the HEARSAY system. [For more complete descriptions of the system see D. R. Reddy, L. D. Erman, and R. D. Neely, \u201cA Model and a System for Machine Recognition of Speech,\u201d IEEE Trans. Audio Electroacoust. AU\u201021, 229\u2013238 (1973) and D. R. Reddy, L. D. Erman, R. D. Fennell, and R. B. Neely, \u201cThe HEARSAY Speech Understanding System : An Example of the Recognition Process,\u201d Proc. 3rd Int. Joint Conf. on Artificial Intelligence (Aug. 1973)]. The system uses task and context\u2010dependent information to help in the recognition of the utterance; this system consists of a set of cooperating parallel processes, each representing a different source of knowledge (e.g., acoustic\u2010phonetic, syntactic, semantic). The knowledge is used either to predict what may appear in a given context or to verify an hypothesis resulting from a previous prediction. Performance data of the system on several tasks (e.g., medical diagnosis, news retrieval, chess, and programming) will be ...", "authors": ["D. Raj Reddy", "Lee D. Erman", "Richard D. Fennell", "Bruce T. Lowerre", "Richard B. Neely"], "id": "73ed12a7b3019f266adca7beef47163b8b737402", "title": "The HEARSAY Speech Understanding System", "references": []}, {"date": "1968", "abstract": "A new equivalent circuit for the insulated-gate field-effect transistor (IGFET) is described. This device model is particularly useful for computer-aided analysis of monolithic integrated IGFET switching circuits. The results of computer simulations using the new equivalent circuit are in close agreement with experimental observations. As an example of a practical application, simulation results are shown for an integrated circuit IGFET memory cell.", "authors": ["Harold Shichman", "David A. Hodges"], "id": "e42ea2775d667707632fb8f8950e391851de75aa", "title": "Modeling and simulation of insulated-gate field-effect transistor switching circuits", "references": ["e01b8499b09223e57ac439c10942d6920c79b72e", "2ad33a6ec576ee7c05c087986f545f95f0387e67", "0ba1c1324163787f7be7dd875944caf7475adb29"]}, {"date": "1973", "abstract": "This paper presents a model for machine recognition of connected speech and the details of a specific implementation of the model, the HEARSAY system. The model consists of a small set of cooperating independent parallel processes that are capable of helping in the decoding of a spoken utterance either individually or collectively. The processes use the \"hypothesize-and-test\" paradigm. The structure of HEARSAY is illustrated by considering its operation in a particular task situation: voice-chess. The task is to recognize a spoken move in a given board position. Procedures for determination of parameters, segmentation, and phonetic descriptions are outlined. The use of semantic, syntactic, lexical, and phonological sources of knowledge in the generation and verification of hypotheses is described. Preliminary results of recognition of some utterances are given.", "authors": ["D. Raj Reddy", "Lee D. Erman", "Richard B. Neely"], "id": "8ef1568b4377fce96f9d350d6d46a619d71a1462", "title": "A model and a system for machine recognition of speech", "references": []}, {"date": "1982", "abstract": "A constraint satisfaction problem revolves finding values for a set of variables subject to a set of constraints (relations) on those variables Backtrack search is often used to solve such problems. A relationship involving the structure of the constraints is described which characterizes to some degree the extreme case of mimmum backtracking (none) The relationship involves a concept called \"width,\" which may provide some guidance in the representation of constraint satisfaction problems and the order m which they are searched The width concept is studied and applied, in particular, to constraints which form tree structures.", "authors": ["Eugene C. Freuder"], "id": "3bcc8272a88796ae5d86a3dded89f66034c1cef7", "title": "A Sufficient Condition for Backtrack-Free Search", "references": []}, {"date": "1976", "abstract": "The Hearsay II speech understanding system under development at Carnegie-Mellon University is a complex, distributed-logic processing system: Processing in the system is affected by independent, data-directed knowledge sources processes which examine and alter values in a global data base representing hypothesized phones, phonemes, syllables, words, and phrases, as well as the hypothetical temporal and logical relationships among them. The question of how to schedule the numerous potential activities of the knowledge sources so as to understand the utterance in minimal time is called the \"focus of attention problem\". Near optimal focusing is especially important in a speech understanding system because of the very large solution space that potentially needs to be searched. Using the concepts of stimulus and response frames of scheduled knowledge source instantiations, competition among alternative responses, goals, and the desirability of a knowledge source instantiation, a general attentional control mechanism is developed. This general focusing mechanism facilitates the experimental evaluation of a variety of specific attentional control policies (such as best-first, bottom-up, and top-down search heuristics) and allows the modular addition of specialized heuristics for the speech understanding task.", "authors": ["Frederick Hayes-Roth", "Victor R. Lesser"], "id": "21ef510c0f44f796abced7b17169037043030a0e", "title": "Focus of attention in a distributed-logic speech understanding system", "references": ["f027ce53a12f36f93897a2b5733549ca323c18d0", "13b55dfb568050d86191b755c4d9d6e5bd09ac78", "3034a2fc90ec0baad8cb71c3a0037a9d6b1fd38a", "8ef1568b4377fce96f9d350d6d46a619d71a1462"]}, {"date": "1975", "abstract": "A new method for efficient recognition of general relational structures is described and compared with existing methods. Patterns to be recognized are defined by templates consisting of a set of predicate calculus relations. Productions are representable by associating actions with templates. A network for recognizing occurrences of any of the template patterns in data may be automatically compiled. The compiled network is economical in the sense that conjunctive products (subsets) of relations common to several templates are represented in and computed by the network only once. The recognition network operates in a bottom-up fashion, in which all possibilities for pattern matches are evaluated simultaneously. The distribution of the recognition process throughout the network means that it can readily be decomposed into parallel processes for use on a multiprocessor machine. The method is expected to be especially useful in errorful domains (e.g., vision, speech) where parallel treatment of alternative hypotheses is desired. The network is illustrated with an example from the current syntax and semantics module in the Hearsay II speech understanding system.", "authors": ["Frederick Hayes-Roth", "D. Moslow"], "id": "9da40f8b672a74ee64cf2cef9f3e2804fe810eb4", "title": "An Automatically Compilable Recognition Network For Structured Patterns", "references": ["6f780f04a9b9822212f0c44c943aa69e3afd96b0", "f027ce53a12f36f93897a2b5733549ca323c18d0", "338b4a29b872fcd7b5d12f3de0652da84dd38478", "aa74dbd824050dd9a5dca5aaf76f295dde623746", "54d6dcbe41370e06eb0a7f66f1a422c45fd9af0a", "e97795382386ecd24300f3a6449ed5732b200bfa", "89729006f2b9358b46b8748b1958c7e8f0d6ffd8", "af52d8878f388ad5818fd6da1770e2ab9ef2335a"]}, {"date": "1975", "abstract": "Hearsay II (HSII) is a system currently under development at Carnegie-Mellon University to study the connected speech understanding problem. It is similar to Hearsay I (HSI) in that it is based on the hypothesize-and-test paradigm, using cooperating independent knowledge sources communicating with each other through a global data structure (blackboard). It differs in the sense that many of the limitations and shortcomings of HSI are resolved in HSII. The main new features of the Hearsay II system structure are: 1) the representation of knowledge as self-activating, asynchronous, parallel processes, 2) the representation of the partial analysis in a generalized three-dimensional network (the dimensions being level of representation (e.g., acoustic, phonetic, phonemic, lexical, syntactic), time, and alternatives) with contextual and structural support connections explicitly specified, 3) a convenient modular structure for incorporating new knowledge into the system at any level, and 4) a system structure suitable for execution on a parallel processing system. The main task domain under study is the retrieval of daily wire-service news stories upon voice request by the user. The main parametric representations used for this study are 1/3-octave filter-bank and linear-predictive coding (LPC)-derived vocal tract parameters [10], [11]. The acoustic segmentation and labeling procedures are parameter-independent [7]. The acoustic, phonetic, and phonological components [23] are feature-based rewriting rules which transform the segmental units into higher level phonetic units. The vocabulary size for the task is approximately 1200 words. This vocabulary information is used to generate word-level hypotheses from phonetic and surface-phonemic levels based on prosodic (stress) information. The syntax for the task permits simple English-like sentences and is used to generate hypotheses based on the probability of occurrence of that grammatical construct [19]. The semantic model is based on the news items of the day, analysis of the conversation, and the presence of certain content words in the partial analysis. This knowledge is to be represented as a production system. The system is expected to be operational on a 16-processor minicomputer system [3] being built at Carnegie-Mellon University. This paper deals primarily with the issues of the system organization of the HSII system.", "authors": ["Victor R. Lesser", "Richard D. Fennell", "Lee D. Erman", "Dabbala Rajagopal Reddy"], "id": "f027ce53a12f36f93897a2b5733549ca323c18d0", "title": "Organization of the Hearsay II speech understanding system", "references": []}, {"date": "1977", "abstract": "Since the Bayes classifier is the optimum classifier in the sense of having minimum probability of misclassification among all the classifiers using the same set of pattern features, the error rate of the Bayes classifier using the set of features provided by a feature extractor, called the Bayes error of the feature extractor, is the smallest possible for the feature extractor. Consequently, the Bayes error can be used to evaluate the effectiveness of the feature extractors in a pattern recognition system. In this paper, a nonparametric technique for estimating the Bayes error for any two-category feature extractor is presented. This technique uses the nearest neighbor sample sets and is based on an infinite series expansion of the general form of the Bayes error. It is shown that this technique is better than the existing methods, and the estimates obtained by this technique are more meaningful in evaluating the quality of feature extractors. Computer simulation as well as application to electrocardiogram analysis are used to demonstrate this technique.", "authors": ["James M. Garnett", "Stephen S. Yau"], "id": "a7611cb05bc0fd9513a7ab72f7183edfb2cf2c4b", "title": "Nonparametric Estimation of the Bayes Error of Feature Extractors Using Ordered Nearest Neighbor Sets", "references": ["32d8a10c47096fefe17a68f3166059f172ad094e", "241604a20e9727d235c2600908dbd6fb4bf588af", "d21bf792a1cfe670d9baff0381f69e67fa1a275d"]}, {"date": "1975", "abstract": "Nonparametric estimation of the Bayes risk R^\\ast using a k -nearest-neighbor ( k -NN) approach is investigated. Estimates of the conditional Bayes error r(X) for use in an unclassified test sample approach to estimate R^\\ast are derived using maximum-likelihood estimation techniques. By using the volume information as well as the class representations of the k -NN's to X , the mean-squared error of the conditional Bayes error estimate is reduced significantly. Simulations are presented to indicate the performance of the estimates using unclassified testing samples.", "authors": ["Keinosuke Fukunaga", "Larry D. Hostetler"], "id": "09884c74169d606b7dd501da91e195d15a50bcbe", "title": "k-nearest-neighbor Bayes-risk estimation", "references": []}, {"date": "1984", "abstract": "The goals of qualitative physics are to identify the distinctions and laws which govern qualitative behavior of devices such that it is possible to predict and explain the behavior of physical devices without recourse to quantitative methods. Although qualitative analysis lacks quantitative information, it predicts significant characteristics of device functioning such as feedback, ringing, oscillation, etc. This paper defines higher-order qualitative derivatives and uses them to formulate six fundamental laws which govern the gross-time behavior of physical devices. These qualitative laws are based on the Mean Value Theorem and Taylor's Expansion of the quantitative calculus. They substitute for what often requires sophisticated problem-solving. We claim they are the best that can be achieved relying on qualitative information.", "authors": ["Johan de Kleer", "Daniel G. Bobrow"], "id": "6886763fd4fe5e9bef58b55b68ca8ac19018151a", "title": "Qualitative Reasoning With Higher-Order Derivatives", "references": ["46f41bcaf5d69e3586571c6b8c91f525096726f5", "1cf6628cabc2f899222db78249a3a25d75c9e872"]}, {"date": "1971", "abstract": "Abstract The basic question of how to optimally make use of a finite number of available samples in designing pattern recognition systems is considered. This has several components: optimal use of the samples for design and testing; and the relationship between the number of measurements and the number of samples for various probability structural constraints. A spectrum of possibilities has been demonstrated, placing several apparently conflicting recent results in perspective.", "authors": ["Laveen N. Kanal", "B. Chandrasekaran"], "id": "210899af38b9e74b669a23ccbafc4967d5c42794", "title": "On dimensionality and sample size in statistical pattern classification", "references": ["3e3ec72e932d7205a541e67e0f9a1fde5235eefd", "62262a28fec4d8ea99bd9e851ff159249725eb32", "a0eabe75839e1b7e58f1e2f9b746615d92926677", "3baebcdba313a1a52533fb5c43987169c0c3b5a4", "cf7b8aca0f2b57a8f59c440b5a7d4b96755d860b"]}, {"date": "1977", "abstract": "Abstract : This handbook is intended to provide decision makers and their staffs (current or potential) with an introduction to the basic concepts and operations of decision analysis. Decision analysis is a quantitative method which permits the systematic evaluation of the costs or benefits accruing to courses of action that might be taken in a decision problem. It entails identification of the alternative choices involved, the assignment of values (costs/benefits) for possible outcomes, and the expression of the probability of those outcomes being realized. With this information at hand, one can then systematically combine the values and probabilities to show the probable gain or loss that is associated with alternative choices.", "authors": ["Scott Barclay", "Rex V. Brown", "Clinton W. Kelly", "Cameron R. Peterson", "Lawrence D. Phillips"], "id": "bbd37d26a2bd074a5c10d5f909dc5e9343e7c6d2", "title": "Handbook for Decision Analysis", "references": []}, {"date": "1981", "abstract": "First generation AI in Medicine programs have clearly demonstrated the usefulness of AI techniques However, il has also been recognized that the use. of notions such as causal relationships, temporal patterns, and aggregate disease categories in these programs has been too weak From our study of clinician's behavior we realized that a diagnostic or therapeutic program must consider a case at various levels of detail to integrate overall understanding with detailed knowledge, To explore these issues, we have undertaken a study of the problem of providing expert consultation for electrolyte and acid-base disturbances We have partly completed an implementation of ABEL, the diagnostic component of the overall effort. In this paper we concentrate on ABLL.s mechanism for describing a patient. Called the patient-specific model, this description includes data about the patient as well as the program's hypothetical interpretations of these data in a multilevel causal network. The lowest level of this description consists of pathophysiological knowledge about the patient, which is successively aggregated into higher level concepts and relations, gradually shifting the content from pathophysiological to syndromic knowledge The aggregate level of this description summarizes the patient data providing a global perspective for efficient exploration of the diagnostic possibilities The pathophysiological level description provides the ability to handle complex clinical situations arising in illnesses with multiple etiologies, to evaluate the physiological validity of diagnostic possibilities being explored, and to organize large amounts of seemingly unrelated facts into coherent causal descriptions.", "authors": ["Ramesh S. Patil", "Peter Szolovits", "William B. Schwartz"], "id": "fc79391e2d08c6265fa30669a21b8b3cc08ac147", "title": "Causal Understanding of Patient Illness in Medical Diagnosis", "references": []}, {"date": "1979", "abstract": "Abstract : SLIPS OR UNINTENDED ACTIONS HAVE BOTH THEORETICAL AND APPLIED IMPORTANCE, BY PROVIDING INSIGHT INTO POSSIBLE UNDERLYING MECHANISMS OF BEHAVIOR AND BECAUSE OF THEIR REAL-WORLD CONSEQUENCES. Several collections of slips are examined and an outline for a theory of action is proposed, based on selection of appropriate schemas, their activation in memory, and triggering by appropriate conditions, with errors possible at each stage. A categorization of slips is presented based on this outline. Finally, the role of feedback and cybernetic theories of human performance are discussed. (Author)", "authors": ["Donald A. Norman"], "id": "01051f2670097848c0941124dbad19ab970771bf", "title": "Slips of the Mind and an Outline for a Theory of Action", "references": []}, {"date": "1959", "abstract": "Semantic Scholar extracted view of \"Evaluation of Learning in Arithmetic\" by Laura K. Eads", "authors": ["Laura K. Eads"], "id": "a39e6f35c30f95fdae465237ab1941b1e99c18ac", "title": "Evaluation of Learning in Arithmetic", "references": []}, {"date": "1968", "abstract": "Semantic Scholar extracted view of \"Computer-aided design and characterization of MOS integrated circuits\" by Dov Frohman-Bentchkowsky et al.", "authors": ["Dov Frohman-Bentchkowsky", "L. L. Vadasz"], "id": "0ba1c1324163787f7be7dd875944caf7475adb29", "title": "Computer-aided design and characterization of MOS integrated circuits", "references": []}, {"date": "1980", "abstract": "The computational view of mind rests on certain intuitions regarding the fundamental similarity between computation and cognition. We examine some of these intuitions and suggest that they derive from the fact that computers and human organisms are both physical systems whose behavior is correctly described as being governed by rules acting on symbolic representations. Some of the implications of this view are discussed. It is suggested that a fundamental hypothesis of this approach (the \u201cproprietary vocabulary hypothesis\u201d) is that there is a natural domain of human functioning (roughly what we intuitively associate with perceiving, reasoning, and acting) that can be addressed exclusively in terms of a formal symbolic or algorithmic vocabulary or level of analysis. Much of the paper elaborates various conditions that need to be met if a literal view of mental activity as computation is to serve as the basis for explanatory theories. The coherence of such a view depends on there being a principled distinction between functions whose explanation requires that we posit internal representations and those that we can appropriately describe as merely instantiating causal physical or biological laws. In this paper the distinction is empirically grounded in a methodological criterion called the \u201ccognitive impenetrability condition.\u201d Functions are said to be cognitively impenetrable if they cannot be influenced by such purely cognitive factors as goals, beliefs, inferences, tacit knowledge, and so on. Such a criterion makes it possible to empirically separate the fixed capacities of mind (called its \u201cfunctional architecture\u201d) from the particular representations and algorithms used on specific occasions. In order for computational theories to avoid being ad hoc, they must deal effectively with the \u201cdegrees of freedom\u201d problem by constraining the extent to which they can be arbitrarily adjusted post hoc to fit some particular set of observations. This in turn requires that the fixed architectural function and the algorithms be independently validated. It is argued that the architectural assumptions implicit in many contemporary models run afoul of the cognitive impenetrability condition, since the required fixed functions are demonstrably sensitive to tacit knowledge and goals. The paper concludes with some tactical suggestions for the development of computational cognitive theories.", "authors": ["Zenon W. Pylyshyn"], "id": "377e0d546e6c8931bef131e3c5b0beec328c9f47", "title": "Computation and cognition: Issues in the foundations of cognitive science.", "references": []}, {"date": "1976", "abstract": "Both in science and in practical affairs we reason by combining facts only inconclusively supported by evidence. Building on an abstract understanding of this process of combination, this book constructs a new theory of epistemic probability. The theory draws on the work of A. P. Dempster but diverges from Depster's viewpoint by identifying his \"lower probabilities\" as epistemic probabilities and taking his rule for combining \"upper and lower probabilities\" as fundamental. The book opens with a critique of the well-known Bayesian theory of epistemic probability. It then proceeds to develop an alternative to the additive set functions and the rule of conditioning of the Bayesian theory: set functions that need only be what Choquet called \"monotone of order of infinity.\" and Dempster's rule for combining such set functions. This rule, together with the idea of \"weights of evidence,\" leads to both an extensive new theory and a better understanding of the Bayesian theory. The book concludes with a brief treatment of statistical inference and a discussion of the limitations of epistemic probability. Appendices contain mathematical proofs, which are relatively elementary and seldom depend on mathematics more advanced that the binomial theorem.", "authors": ["Glenn Shafer"], "id": "4cd91c51098783ec972f6a0ab430cacdd634a5b2", "title": "A Mathematical Theory Of Evidence", "references": ["42eda8d46ca9fb35d9c844baa37b578e24cf20e3", "76c266867d725398a6e9ca6a3f8785d6f212b929"]}, {"date": "1968", "abstract": "In connection with the design of transistor circuits, for example, it is frequently necessary to obtain a numerical solution of a system of nonlinear ordinary differential equations. In some cases, these equations possess a property that leads to intolerable computational requirements relative to the use of standard predictor-corrector techniques or general linear multipoint formulas of open type. Here we describe an alternative approach which has been used to solve some practical problems by permitting dramatic step-size increases (for example, a factor of 104). The approach is developed in a way which provides some detailed understanding of why it is useful.", "authors": ["Irwin W. Sandberg", "Harold Shichman"], "id": "2ad33a6ec576ee7c05c087986f545f95f0387e67", "title": "Numerical integration of systems of stiff nonlinear differential equations", "references": []}, {"date": "1966", "abstract": "This article gives a brief survey of various methods for network analysis by digital computer. Topics discussed include methods and programs for ladder networks, mesh and nodal analysis, network topology, electronic circuit analysis, state-variable analysis, n-port hybrid matrix analysis, and nonlinear circuit analysis. Also given is a brief discussion concerning algorithms for inverse Laplace transformation, and methods for obtaining magnitude, phase and delay responses in the frequency domain.", "authors": ["Franklin F. Kuo"], "id": "e01b8499b09223e57ac439c10942d6920c79b72e", "title": "Network analysis by digital computer", "references": []}, {"date": "1968", "abstract": "A method is presented to approximate optimally an n -dimensional discrete probability distribution by a product of second-order distributions, or the distribution of the first-order tree dependence. The problem is to find an optimum set of n - 1 first order dependence relationship among the n variables. It is shown that the procedure derived in this paper yields an approximation of a minimum difference in information. It is further shown that when this procedure is applied to empirical observations from an unknown distribution of tree dependence, the procedure is the maximum-likelihood estimate of the distribution.", "authors": ["C. K. Chow", "C. N. Liu"], "id": "683fe3bbf2b2e628cf40d90e35fb39effc63b7e9", "title": "Approximating discrete probability distributions with dependence trees", "references": []}, {"date": "1973", "abstract": "Publisher Summary This chapter discusses production systems and the way in which they operate. A production system is a scheme for specifying an information processing system. It consists of a set of productions, each production consisting of a condition and an action. It has also a collection of data structures: expressions that encode the information upon which the production system works\u2014on which the actions operate and on which the conditions can be determined to be true or false. The chapter discusses the possibility of having a theory of the control structure of human information processing. Gains seem possible in many forms such as completeness of the microtheories of how various miniscule experimental tasks are performed, the ability to pose meaningfully the problem of what method a subject is using, the ability to suggest new mechanisms for accomplishing a task, and the facilitation of comparing behavior on diverse tasks. The chapter presents a theory of the control structure.", "authors": ["Allen Newell"], "id": "af52d8878f388ad5818fd6da1770e2ab9ef2335a", "title": "Production systems: Models of control structures.", "references": []}, {"date": "1969", "abstract": "A high level programming language for large, complex associative structures has been designed and implemented. The underlying data structure has been implemented using a hash-coding technique. The discussion includes a comparison with other work and examples of applications of the language.", "authors": ["Jerome A. Feldman", "Paul Rovner"], "id": "89729006f2b9358b46b8748b1958c7e8f0d6ffd8", "title": "An ALGOL-based associative language", "references": ["4aac001b7741dc79056c86e9aaa4a2e20841b68e", "412f7940a8edea0b52c2fcd019a9d7cdaf7f14e9"]}, {"date": "1974", "abstract": "This paper describes LPARS, a locally-organized parsing system, designed for use in a continuous speech recognizer. LPARS processes a string of phonemes which contains ambiguity and error. The system is locally-organized in the sense that it builds local parse structures from reliable word candidates recognized anywhere in an input utterance. These local structures are used as \u201cislands of reliability\u201d to guide the search for more highly garbledwords which might complete the utterance.", "authors": ["Perry Lowell Miller"], "id": "54d6dcbe41370e06eb0a7f66f1a422c45fd9af0a", "title": "A locally-organized parser for spoken input", "references": ["338b4a29b872fcd7b5d12f3de0652da84dd38478", "10f7507b8408bf35125b8e04254ad890c8d45e1d"]}, {"date": "1972", "abstract": "Publisher Summary This chapter discusses some techniques for recognizing structures in pictures. The research aims to develop techniques whereby a machine may observe its surroundings and then use its observations to achieve goals in an effective and efficient manner. To fulfill such requirements, the machine will inevitably use knowledge gained from past experience and observation to plan its activities, and also to interpret its sensory data. The chapter discusses the idea of a finite relational structure, that is, a set of elements with given properties and relations among them, as a useful mathematical tool for describing pictures, and to describe general techniques for matching such structures against each other. The matching process for relational structures attempts to find whether one structure occurs in or is a substructure of another structure. More precisely one need a function that assigns to each element of the first structure a distinct element of the second structure in such a way as to preserve the properties and relations which subsist in the first structure.", "authors": ["Harry G. Barrow", "A. P. Ambler", "Rod M. Burstall"], "id": "aa74dbd824050dd9a5dca5aaf76f295dde623746", "title": "SOME TECHNIQUES FOR RECOGNISING STRUCTURES IN PICTURES", "references": []}, {"date": "1967", "abstract": "Upper bounds for the error probability of a Bayes decision function are derived in terms of the differences among the probability distributions of the features used in character recognition. Applications to feature selection and error reduction are discussed. It is shown that if a sufficient number of well-selected features is used, the error probability can be made arbitrarily small.", "authors": ["J. T. Chu", "J. C. Chueh"], "id": "cf7b8aca0f2b57a8f59c440b5a7d4b96755d860b", "title": "Error Probability in Decision Functions for Character Recognition", "references": ["aef8e70f18bd32d9c27467ce2a4d5f4d7117f88d"]}, {"date": "1971", "abstract": "A direct method of measurement selection is proposed to determine the best subset of d measurements out of a set of D total measurements. The measurement subset evaluation procedure directly employs a nonparametric estimate of the probability of error given a finite design sample set. A suboptimum measurement subset search procedure is employed to reduce the number of subsets to be evaluated. Teh primary advantage of the approach is the direct but nonparametric evaluation of measurement subsets, for the M class problem.", "authors": ["A. Wayne Whitney"], "id": "241604a20e9727d235c2600908dbd6fb4bf588af", "title": "A Direct Method of Nonparametric Measurement Selection", "references": []}, {"date": "1964", "abstract": "A two-level statistical classification procedure has been applied to the problem of detecting complex targets in aerial photography. At the first level, a set of classification functions designed on the basis of samples from the target class and from other images is used to make subdecisions on local-area statistically-designed features associated with the target class. At the second level these sub-decisions are combined into a single decision as to the presence or absence of the target. The nature of the data does not allow for the direct application of classical methods of multivariate discriminant analysis; rather, modifications of classical methods are used. This procedure has been simulated on a digital computer with the aid of a special input-output device which converts imagery to computer language. Excellent results were obtained on independent text samples of actual imagery.", "authors": ["Laveen N. Kanal", "Neil C Randall"], "id": "a0eabe75839e1b7e58f1e2f9b746615d92926677", "title": "Recognition system design by statistical analysis", "references": []}, {"date": "1968", "abstract": "Several methods of estimating error rates in Discriminant Analysis are evaluated by sampling methods. Multivariate normal samples are generated on a computer which have various true probabilities of misclassification for different combinations of sample sizes and different numbers of parameters. The two methods in most common use are found to be significantly poorer than some new methods that are proposed.", "authors": ["Peter A. Lachenbruch", "M. Ray Mickey"], "id": "3baebcdba313a1a52533fb5c43987169c0c3b5a4", "title": "Estimation of Error Rates in Discriminant Analysis", "references": []}, {"date": "1964", "abstract": "For the recognition of patterns as members of certain classes it is assumed that the probability distributions of certain characteristic features are known. For a decision based on the maximum likelihood criterion bounds on the statistical error are derived. In the case of normally distributed and statistically independent features these bounds are evaluated. Conditions are given under which the error tends to zero as the number of characteristic features goes to infinity.", "authors": ["R. Albrecht", "W. Werner"], "id": "62262a28fec4d8ea99bd9e851ff159249725eb32", "title": "Error analysis of a statistical decision method", "references": []}, {"date": "1968", "abstract": "The overall mean recognition probability (mean accuracy) of a pattern classifier is calculated and numerically plotted as a function of the pattern measurement complexity n and design data set size m . Utilized is the well-known probabilistic model of a two-class, discrete-measurement pattern environment (no Gaussian or statistical independence assumptions are made). The minimum-error recognition rule (Bayes) is used, with the unknown pattern environment probabilities estimated from the data relative frequencies. In calculating the mean accuracy over all such environments, only three parameters remain in the final equation: n, m , and the prior probability p_{c} of either of the pattern classes. With a fixed design pattern sample, recognition accuracy can first increase as the number of measurements made on a pattern increases, but decay with measurement complexity higher than some optimum value. Graphs of the mean accuracy exhibit both an optimal and a maximum acceptable value of n for fixed m and p_{c} . A four-place tabulation of the optimum n and maximum mean accuracy values is given for equally likely classes and m ranging from 2 to 1000 . The penalty exacted for the generality of the analysis is the use of the mean accuracy itself as a recognizer optimality criterion. Namely, one necessarily always has some particular recognition problem at hand whose Bayes accuracy will be higher or lower than the mean over all recognition problems having fixed n, m , and p_{c} .", "authors": ["G. F. Hughes"], "id": "3e3ec72e932d7205a541e67e0f9a1fde5235eefd", "title": "On the mean accuracy of statistical pattern recognizers", "references": ["ebd3034f0e468d9e6f31018ef928d402876d2db3", "bc7dc2be536d4a80d4b76c3e83396302ffa48354"]}, {"date": "1982", "abstract": "This paper explores a particular kind of qualitative reasoning, called envisioning, that is capable of producing causal explanations for device behavior. It has been implemented in a computer program, ENVISION, which can analyze a wide variety of thermal, fluid, electrical, translational and rotational devices. Rather than present the technical details of the envisioning process, this paper examines the theoretical foundations upon which it is built. Many of these considerations are ones that any builder of qualitative reasoning systems must pay attention to. Two such considerations are explanation and robustness: What notion of causality is adequate for causal explanations of device behavior? How can there be any confidence in the analysis of a novel device?", "authors": ["Johan de Kleer", "John Seely Brown"], "id": "1cf6628cabc2f899222db78249a3a25d75c9e872", "title": "Foundations of Envisioning", "references": ["bef38d9ea1040ef0b6a53a7b98a922ba9b4e13e8", "9b1a687d836ffc318ff1e125acd5a0dd7a21f3b5"]}, {"date": "1974", "abstract": "Man-machine dialogues using everyday conversational English present problems for computer processing of natural language. Grammar-based parsers which perform a word-by-word, parts-of-speech analysis are too fragile to operate satisfactorily in real time intervieus allowing unrestricted English. In constructing a simulation of paranoid thought processes, we designed an algorithm capable of handling the linguistic expressions used by interviewers in teletyped diagnostic psychiatric interviews. The algorithm uses pattern-matching rules which attempt to characterize the input expressions by progressively transforming them into patterns uhich match, completely or fuzzily, abstract stored patterns. The power of this approach lies in its ability to ignore recognized and unrecognized words and still grasp the meaning of the message. The methods utilized are general and could serve any \"host\" system uhich takes natural language input.", "authors": ["Kenneth Mark Colby", "Roger C. Parkison", "Bill Fought"], "id": "6f780f04a9b9822212f0c44c943aa69e3afd96b0", "title": "Pattern-Matching Rules For The Recognition Of Natural Language Dialogue Expressions", "references": []}, {"date": "1957", "abstract": "The character recognition problem, usually resulting from characters being corrupted by printing deterioration and/or inherent noise of the devices, is considered from the viewpoint of statistical decision theory. The optimization consists of minimizing the expected risk for a weight function which is preassigned to measure the consequences of system decisions As an alternative minimization of the error rate for a given rejection rate is used as the critenon. The optimum recogition is thus obtained. The optimum system consists of a conditional-probability densisities computer; character channels, one for each character; a rejection channel; and a comparison network. Its precise structure and and ultimate performance depend essentially upon the signals and noise structure. Explicit examples for an additive Gaussian noise and a ``cosine'' noise are presented. Finally, an error-free recognition system and a possible criterion to measure the character style and deteriortation are presented.", "authors": ["C. K. Chow"], "id": "32d8a10c47096fefe17a68f3166059f172ad094e", "title": "An optimum character recognition system using decision functions", "references": []}, {"date": "1978", "abstract": "A new diagnostic modeling system for automatically synthesizing a deep-structure model of a student's misconceptions or bugs in his basic mathematical skills provides a mechanism for explaining why a student is making a mistake as opposed to simply identifying the mistake. This report is divided into four sections: The first provides examples of the problems that must be handled by a diagnostic model. It then introduces procedural networks as a general framework for representing the knowledge underlying a skill. The challenge in designing this representation is to find one that facilitates the discovery of misconceptions or bugs existing in a particular student's encoding of this knowledge. The second section discusses some of the pedagogical issues that have emerged from the use of diagnostic models within an instructional system. This discussion is framed in the context of a computer-based tutoring/gaming system developed to teach students and student teachers how to diagnose bugs strategically as well as how to provide a better understanding of the underlying structure of arithmetic skills. The third section describes our uses of an executable network as a tool for automatically diagnosing student behavior, for automatically generating \u201cdiagnostic\u201d tests, and for judging the diagnostic quality of a given exam. Included in this section is a discussion of the success of this system in diagnosing 1300 school students from a data base of 20,000 test items. The last section discusses future research directions. Development of the general framework of Diagnostic Models which underlies this research was supported, in part, by the Advanced Research Projects Agency, Air Force Human Resources Laboratory, Army Research Institute for Behavioral and Social Sciences, and Navy Personnel Research and Development Center under Contract No. MDA903-76-C-0108.", "authors": ["John Seely Brown", "Richard R. Burton"], "id": "a5ceacbbbf78b1b5ff55be25cb031fac63581359", "title": "Diagnostic Models for Procedural Bugs in Basic Mathematical Skills", "references": ["e64db9777a991f55029bd13c9d03741fa2b17046"]}, {"date": "1952", "abstract": "Semantic Scholar extracted view of \"Logical Foundations of Probability.\" by David Lindley et al.", "authors": ["David Lindley", "Rudolf Carnap"], "id": "76c266867d725398a6e9ca6a3f8785d6f212b929", "title": "Logical Foundations of Probability.", "references": []}, {"date": "1981", "abstract": "Decision-support technologies are founded on the paradigm that direct judgments are less reliable and less valid than synthetic inferences produced from more \u201cfragmentary\u201d judgments. Moreover, certain types of fragments are normally assumed to be more valid than others. In particular, judgments about the likelihood of a certain state of affairs given a particular set of data (diagnostic inferences) are routinely fabricated from judgments about the likelihood of that data given various states of affairs (causal inferences), and not vice versa. This study was designed to test the benefits of causal synthesis schemes by comparing the validity of causal and diagnostic judgments against \u201cground-truth\u201d standards. The results demonstrate that the validity of causal and diagnostic inferences are strikingly similar; direct diagnostic estimates of conditional probabilities were found to be as accurate as their synthetic counterparts deduced from causal judgments. The reverse is equally true. Moreover, these accuracies were found to be roughly equal for each causal category tested. Thus, if the validity of judgments produced by a given mode of reasoning is a measure of whether it matches the format of human semantic memory, then neither one of the causal or diagnostic schema is a more universal or more natural format for encoding knowledge about common, everyday experiences. These findings imply that one should approach the \u201cdivide and conquer\u201d ritual with caution; not every division leads to a conquest, even when the atoms are cast in causal phrasings. Dogmatic decompositions performed at the expense of conceptual simplicity may lead to inferences of lower quality than those of direct, unaided judgments.", "authors": ["Michael J Burns", "Judea Pearl"], "id": "7ab7ae534033e443205f61d8184c166a36e52838", "title": "Causal and diagnostic inferences: A comparison of validity\u2606", "references": []}, {"date": "1975", "abstract": "SPEECHLIS is a research prototype of an intelligent speech understanding system which makes use of advanced techniques of artificial intelligence, natural language processing, and acoustical and phonological analysis in an integrated way to determine the interpretation of continuous speech utterances. This paper describes a number of the characteristics of the speech understanding task which influence the ways in which syntactic, semantic, pragmatic and lexical knowledge interact with acoustical and phonological information in the process of understanding speech utterances. The focus is on what the different knowledge sources have to contribute at different points in the analysis and the organization of a computer system to combine these different sources of information into an integrated system.", "authors": ["William A. Woods"], "id": "3034a2fc90ec0baad8cb71c3a0037a9d6b1fd38a", "title": "Motivation and overview of SPEECHLIS: An experimental prototype for speech understanding research", "references": []}, {"date": "1960", "abstract": "Semantic Scholar extracted view of \"Foundations of the Theory of Probability\" by Aleksey N. Kolmogorov", "authors": ["Aleksey N. Kolmogorov"], "id": "42eda8d46ca9fb35d9c844baa37b578e24cf20e3", "title": "Foundations of the Theory of Probability", "references": []}, {"date": "1977", "abstract": "Production systems designed to function and grow in environments that make large numbers of different, sometimes competing, and sometimes unexpected demands require support from their interpreters that is qualitatively different from the support required by systems that can be carefully hand crafted to function in constrained environments. In this paper we explore the role of conflict resolution in providing such support. Using criteria developed in the paper, we evaluate both individual conflict resolution rules and strategies that make use of several rules.", "authors": ["John R. McDermott", "Charles L. Forgy"], "id": "d55fc396338da162550aab6b0397d69063abffa8", "title": "Production system conflict resolution strategies", "references": []}, {"date": "1971", "abstract": "Abstract : The paper describes a system for the computer understanding of English. The system answers questions, executes commands, and accepts information in normal English dialog. It uses semantic information and context to understand discourse and to disambiguate sentences. It combines a complete syntactic analysis of each sentence with a 'heuristic understander' which uses different kinds of information about a sentence, other parts of the discourse, and general information about the world in deciding what the sentence means.", "authors": ["Terry Winograd"], "id": "10f7507b8408bf35125b8e04254ad890c8d45e1d", "title": "Procedures As A Representation For Data In A Computer Program For Understanding Natural Language", "references": []}, {"date": "1963", "abstract": "@article{m:lisp, author = {McCarthy, J.}, title = {Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I}, journal = {Communications of the ACM}, volume = {3}, number = {4}, pages = {184-195}, year = 1960 } Summary: McCarthy introduces LISP. LISP deviates from other contemporary languages such as ALGOL as its design revolves around the idea that partial recursive functions of symbolic expressions can be an effective programming language. Mc-Carthy gives meaning to LISP through a meta-circular \" defunctionalized \" interpreter and discusses the implementation of LISP's runtime. Evaluation: This is a seminal paper for multiple reasons. First, it introduces a new programming model that affected the design of programming languages in the decades to come. Second, LISP pioneers many ideas that have evolved nowadays into staple features of most programming languages such as anonymous first-class functions (closures), reflection and garbage collection. Third, the way McCarthy presents LISP foreshadows the formal definition of programming languages with abstract syntax and meta-circular interpreters.", "authors": ["John W. Backus", "Friedrich L. Bauer", "Julien Green", "C. Katz", "John McCarthy", "Peter Naur", "Alan J. Perlis", "Heinz Rutishauser", "Klaus Samelson", "Bernard Vauquois", "J. H. Wegstein", "Adriaan van Wijngaarden", "Michael Woodger"], "id": "412f7940a8edea0b52c2fcd019a9d7cdaf7f14e9", "title": "Revised report on the algorithmic language ALGOL 60", "references": []}, {"date": "1980", "abstract": "The probability distribution of the error incurred by a classification system on a given data set is derived. It is shown that the distribution is mixed binomial. A method for calculating the mixed binomial distribution recursively is proposed.", "authors": ["Josef Kittler", "Pierre A. Devijver"], "id": "bb02e96226166db03b9aad203ac80114eac60bb6", "title": "The Probability Distribution of Conditional Classification Error", "references": ["d0508318a2363a7a2c7fd4dd8fed20891ec879c4", "d21bf792a1cfe670d9baff0381f69e67fa1a275d", "7a225f0f083b864a480ca334fdfa6ccbc2f2cacb", "f7cebcb46b48dc56257353853236e0264616d800", "09884c74169d606b7dd501da91e195d15a50bcbe", "c39f136dc6c5d54d16ca2275919541facd4f3070"]}, {"date": "1972", "abstract": "The key measure of performance in a pattern recognition problem is the cost of making a decision. For the special case in which the relative cost of a correct decision is zero and the relative cost of an incorrect decision is unity, this cost is equal to the probability of an incorrect decision or error. A pattern recognition system may be viewed as a decision rule which transforms measurements into class assignments. The Bayes error is the minimum achievable error, where the minimization is with respect to all decision rules. The Bayes error is a function of the prior probabilities and the probability density functions of the respective classes. Unfortunately, in many applications, the probability density functions are unknown and therefore the Bayes error is unknown.", "authors": ["Keinosuke Fukunaga", "David L. Kessell"], "id": "d21bf792a1cfe670d9baff0381f69e67fa1a275d", "title": "Nonparametric Bayes error estimation using unclassified samples", "references": []}, {"date": "1977", "abstract": "The performance of a pattern classification system is often evaluated based on the risk committed by the classification procedure. The minimum attainable risk is the Bayes risk. Therefore, the Bayes risk can be used as a measure of the intrinsic complexity of the system, and it also serves as a reference of the optimality measure of a classification procedure. There are many practical situations in which the nonparametric methods may have to be called upon to estimate the Bayes risk. One of the nonparametric methods is via the probability density estimation technique. The convergence properties of this estimation technique are studied under fairly general assumptions. In the computer experiments reported, the estimate of the Bayes risk is taken as the sample mean of the density estimate by making use of the leave-one-out method. The probability density estimate used is the one proposed by Loftsgaarden and Quesenberry. This estimate is shown to be, in general, superior to the risk associated with a Bayes-like decision rule based on the error-counting scheme. This estimate is also compared experimentally with the risk estimate associated with the nearest neighbor rule.", "authors": ["Zen Chen", "King-Sun Fu"], "id": "e47e4602cd78d9676a6787ef7bf9594863631bcf", "title": "Nonparametric Bayes Risk Estimation for Pattern Classification", "references": ["3baebcdba313a1a52533fb5c43987169c0c3b5a4", "9c191a25ddbdc47a7b596b34bd756f4540f7a81f", "942f37ae066e1db5b6660241c84e689d2f6b5c20", "0efb841403aa6252b39ae6975c1cc5410554ef7b", "a4eabd1f87d62809a54de30dd480cf8de9e8d2f8", "676d2e74dd55ca0fdf631b32ba75c54bce70addd", "c71946ef140ec2dd8a61a9bd05960ad384b2941d", "a5277f4af5f79950ba03f928434152987ca8dbf1"]}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Sur l'extension du th\u00e9or\u00e9me limite du calcul des probabilit\u00e9s aux sommes de quantit\u00e9s d\u00e9pendantes\" by Serge Bernstein", "authors": ["Serge Bernstein"], "id": "aef8e70f18bd32d9c27467ce2a4d5f4d7117f88d", "title": "Sur l'extension du th\u00e9or\u00e9me limite du calcul des probabilit\u00e9s aux sommes de quantit\u00e9s d\u00e9pendantes", "references": []}, {"date": "1962", "abstract": "A popular procedure for testing a pattern recognition machine is to present the machine with a set of patterns taken from the real world. The proportion of these patterns which are misrecognized or rejected is taken as the estimate of the error probability or rejection probability for the machine. In Part I, this testing procedure is discussed for the cases of unknown and known a priori probabilities of occurrence of the pattern classes. The differences between the tests that should be made in the two cases are noted, and confidence intervals for the test results are indicated. These concepts are applied to various published pattern recognition results by determining the appropriate confidence interval for each result. In Part II, the problem of the optimum partitioning of a sample of fixed size between the design and test phases of a pattern recognition machine is discussed. One important nonparametric result is that the proportion of the total sample used for testing the machine should never be less than that proportion used for designing the machine, and in some cases should be a good deal more.", "authors": ["Wilbur H. Highleyman"], "id": "6a9b3cd5462530aa920f4d3893712cae1e632b76", "title": "The design and analysis of pattern recognition experiments", "references": []}, {"date": "1968", "abstract": "ASP is a general purpose Associative Data Structure Package in which an arbitrary number of data items and an arbitrary number of the relationships between these data items may be represented. A special picture language is described which has proved very useful for drawing ASP structures on paper. ASP structures are built and manipulated by means of a series of macro calls, which are outlined in the Appendix. Emphasis is on the philosophy of the system rather than a particular implementation, though sufficient information is included to enable the reader to produce his own implementation of ASP.", "authors": ["C. A. Lang", "J. C. Gray"], "id": "4aac001b7741dc79056c86e9aaa4a2e20841b68e", "title": "Programming Techniques: ASP\u2014a ring implemented associative structure package", "references": []}, {"date": "1981", "abstract": "An important goal of a computer aided instruction system is to provide students with understandable explanations . Generating explanations requires that the instructional system must itself have some understanding of the topic, prefereably close to the kind the student should have . There is a growing amount of evidence that human understanding of physical systems is based on qualitative models of those systems . This evidence comes from psychological studies [Larkin, McDermott, Simon 6 Simon, 1980, Stevens, Collins S Goldin, 19791 and is suppported by successes in artificial intelligence in actually constructing systems that reason about physical situations using qualitative models [deKleer, 1979a, Forbus, 1980].", "authors": ["Kenneth D. Forbus", "Albert L. Stevens"], "id": "bef38d9ea1040ef0b6a53a7b98a922ba9b4e13e8", "title": "Using qualitative simulation to generate explanations", "references": ["f1f24bb771786c7ae6df4bd087a481c7414dbcde"]}, {"date": "1962", "abstract": "Design of pattern recognition systems usually involves a number of uncertainties which can be resolved only by experiment. In statistical recognition systems, pattern information is stored in the machine in the form of pattern characteristics with statistics relating the characteristics to the patterns. If the number of characteristics a system can process is limited, and if the system designer, while able to conceive of a large number of relevant characteristics, does not know which are the most important, then an experimental selection of these characteristics is required. As tools, the designer may have at his disposal a large computer, a large sample of the patterns to be recognized, and a set of programs to measure the characteristics. Thus, he can compare certain statistics relating the patterns to the characteristics. The problem is: what statistics should he calculate in order to select the best characteristics? Assuming the characteristics to be independent in their effect on the decision, this paper examines the notion of a single number statistic for each characteristic which would have certain desirable properties related to the \"goodness\" of the characteristic. It is shown that, in general, no such statistic exists. However, a statistic is proposed which, while not having these properties in general, at least has them in a wide range of situations. An experimental study of the validity of this choice is reported together with the design of a letter recognition system. Using a sample of 15 complete 62 symbol alphabets, 13 characteristics were selected. The resulting system recognized correctly 81.9% of the letters presented to it.", "authors": ["Philip M. Lewis"], "id": "bc7dc2be536d4a80d4b76c3e83396302ffa48354", "title": "The characteristic selection problem in recognition systems", "references": []}, {"date": "1960", "abstract": "According to the model discussed in this paper, a pattern recognizer is said to consist of two parts: a receptor, which generates a set of measurements of the physical sample to be recognized, and a categorizer, which assigns each set of measurements to one of a finite number of categories. The rule of operation of the categorizer is called the ``recognition function.'' The optimization of the recognition function is discussed, and the form of the optimal function is derived. In practice, a prohibitively large sample is required to provide a basis for estimating the optimal recognition function. If, however, certain assumptions about the probability distributions of the measurements are warranted, recognition functions that are asymptotically optimal may be obtained readily. A small numerical example, involving the recognition of the hand-printed characters A, B, and C is solved by means of the techiques described. The recognition accuracy is found to be 95 per cent.", "authors": ["Thomas Marill", "David M. Green"], "id": "ebd3034f0e468d9e6f31018ef928d402876d2db3", "title": "Statistical Recognition Functions and the Design of Pattern Recognizers", "references": []}, {"date": "1969", "abstract": "This paper is specifically concerned with the problem of inferring from a finite set of patterns the classification of an unknown pattern. A discussion of the general problems inherent in the concept of \u201clearning\u201d and \u201cdata reduction\u201d are discussed from a standpoint of measurement selection for the general pattern recognition problem. A brief history of the existent work in empirical Bayes and compound sequential Bayes procedures will be presented. It is felt that these procedures are basically non-Bayesian, despite their names, and are therefore especially suited to problems arising in pattern recognition. Finally, a discussion is made of some nonparametric approaches to the problem of the classification of an unknown pattern when the only information on the underlying distributions associated with the various categories is that which can be obtained from a finite number of samples.", "authors": ["Thomas M. Cover"], "id": "7788f412782947c94b92f5f47620ce979c0c11b8", "title": "LEARNING IN PATTERN RECOGNITION", "references": []}, {"date": "1974", "abstract": "A collection of powerful ideas\u2014description, plans, linearity, insertions, global knowledge and imperative semantics--are explored which are fundamental to debugging skill. To make these concepts precise, a computer monitor called MYCROFT is described that can debug elementary programs for drawing pictures. The programs are those written for LOGO turtles.", "authors": ["Ira P. Goldstein"], "id": "e64db9777a991f55029bd13c9d03741fa2b17046", "title": "Understanding Simple Picture Programs", "references": []}, {"date": "1976", "abstract": "The L^{ \\alpha} -distance between posterior density functions (PDF's) is proposed as a separability measure to replace the probability of error as a criterion for feature extraction in pattern recognition. Upper and lower bounds on Bayes error are derived for \\alpha > 0 . If \\alpha = 1 , the lower and upper bounds coincide; an increase (or decrease) in \\alpha loosens these bounds. For \\alpha = 2 , the upper bound equals the best commonly used bound and is equal to the asymptotic probability of error of the first nearest neighbor classifier. The case when \\alpha = 1 is used for estimation of the probability of error in different problem situations, and a comparison is made with other methods. It is shown how unclassified samples may also be used to improve the variance of the estimated error. For the family of exponential probability density functions (pdf's), the relation between the distance of a sample from the decision boundary and its contribution to the error is derived. In the nonparametric case, a consistent estimator is discussed which is computationally more efficient than estimators based on Parzen's estimation. A set of computer simulation experiments are reported to demonstrate the statistical advantages of the separability measure with \\alpha = 1 when used in an error estimation scheme.", "authors": ["Tsvi Lissack", "King-Sun Fu"], "id": "5b83d0c4b3806ca2f57243e2e89c46eada9580b5", "title": "Error estimation in pattern recognition via LAlpha -distance between posterior density functions", "references": ["deb40e644f78e719ca510f82b889acd3e60fed96", "0efb841403aa6252b39ae6975c1cc5410554ef7b", "4907fd0bf5b982de6a00f008bc59077814996a01", "5ee6ff0c6d4c610be461d41f47d7c3ee3b11a74d", "ae1c34e48989399c4faa90260647934f44c9d572", "dea8658ee4750ec6bb408a2281cf922cbb300a0a"]}, {"date": "1974", "abstract": "Articles, books, and technical reports on the theoretical and experimental estimation of probability of misclassification are listed for the case of correctly labeled or preclassified training data. By way of introduction, the problem of estimating the probability of misclassification is discussed in order to characterize the contributions of the literature.", "authors": ["Godfried T. Toussaint"], "id": "d0508318a2363a7a2c7fd4dd8fed20891ec879c4", "title": "Bibliography on estimation of misclassification", "references": []}, {"date": "1975", "abstract": "Abstract : This thesis investigates what knowledge is necessary to solve mechanics problems. A program NEWTON is described which understands and solves problems in a mechanics mini-world of objects moving on surfaces. Facts and equations such as those given in a mechanics text need to be represented. However, this is far from sufficient to solve problems. Human problem solvers rely on 'common sense' and 'qualitative' knowledge which the physics text tacitly assumes to be present. A mechanics problem solver must embody such knowledge. Quantitative knowledge given by equations and more qualitative common sense knowledge are the major research points exposited in this thesis. The major issue in solving problems is planning. Planning involves tentatively outlining a possible path to the solution without actually solving the problem. Such a plan needs to be constructed and debugged in the process of solving the problem. Envisionment, or qualitative simulation of the event, plays a central role in this planning process. (Author)", "authors": ["Johan de Kleer"], "id": "c2dc03a92f03dbbf145a6f8b6568740abb325e19", "title": "Qualitative and Quantitative Knowledge in Classical Mechanics", "references": []}, {"date": "1980", "abstract": "A major component in the process of design is synthesis, the determination of the parameters of the parts of a network given desiderata for the behaviour of the network as a whole. Traditional automated synthesis techniques are either restricted to small, precisely defined classes of circuit functions for which exact mathematical methods exist or they depend upon numerical optimization methods in which it is difficult to determine the basis for any of the answers generated and their relations to the design desiderata and constraints. We are developing a symbolic computer-aided design tool, SYN, which can be of assistance to an engineer in the synthesis of a large class of circuits. The symbolic methods produce solutions which are clear and insightful. The dependence of each parameter on the individual design desiderata and circuit constraints can be easily traced.", "authors": ["Johan de Kleer", "Gerald J. Sussman"], "id": "94645dd1765c274d1e61849580a2677ea0063cbe", "title": "Propagation of Constraints Applied to Circuit Synthesis", "references": ["f823f24b2da710cc3c7c6e76f262bcfffdffdbf7", "f98a72eb0858b3ca9da57ce6180afaf702aae44a", "59e5c4c530e996e2ec75ac88a0402e87ce85370e", "17d5522a5f02426bdb06a15c42724c9e3fff3cf1", "8e97127b3f186215dd5b70176c44f5f577103f20"]}, {"date": "1982", "abstract": "The central component of commonsense reasoning about causality is the envisionment: a description of the behavior of a physical system that is derived from its structural description by qualitative simulation. Two problems with creating the envisionment are the qualitative representation of quantity and the detection of previously-unsuspected points of qualitative change. The representation presented here has the expressive power of differential equations, and the qualitative envisionment strategy needed for commonsense knowledge. A detailed example shows how it is able to detect a previously unsuspected point at which the system is in stable equilibrium.", "authors": ["Benjamin Kuipers"], "id": "a130daf60ae16581744d0051d8197c9fc7ce653c", "title": "Getting the Envisionment Right", "references": ["6813e5400681a1704c4c4aef2cb7a805fa99c30b", "e5b9efabfef885ec48528c4d1c9e1e506a193d57"]}, {"date": "1980", "abstract": "Abstract The paper is concerned with the application of pattern recognition to the problem of automatic inspection of products by lots. It is shown that in the presence of classification errors, the classical approach to the design of acceptance sampling plans cannot be used. An alternative quality control procedure is proposed for the model assuming an arbitrary distribution of patterns in the lot.", "authors": ["Josef Kittler", "Louis-Fran\u00e7ois Pau"], "id": "7a225f0f083b864a480ca334fdfa6ccbc2f2cacb", "title": "Automatic inspection by lots in the presence of classification errors", "references": []}, {"date": "1979", "abstract": "BACON.3 is a production system that discovers empirical laws. The program uses a few simple heuristics to solve a broad range of tasks. These rules detect constancies and trends in data, and lead to the formulation of hypotheses and the definition of theoretical terms. BACON.3 represents data at varying levels of description, where the lowest have been directly observed and the highest correspond to hypotheses that explain everything so far observed. The system can also run and relate multiple experiments, collapse hypotheses with identical conditions, ignore differences between similar concepts, and discover and ignore irrelevant variables. BACON.3 has shown its generality by rediscovering versions of the Ideal gas law, Kepler's third law, Coulomb's law, Ohm's law, and Galileo's laws for the pendulum and constant acceleration.", "authors": ["Pat Langley"], "id": "3087b4b62d15d89a69b5764c0591a1c438091a94", "title": "Rediscovering Physics with BACON.3", "references": []}, {"date": "1961", "abstract": "Semantic Scholar extracted view of \"The Advanced Theory of Statistics\" by M. G. Kendall", "authors": ["M. G. Kendall"], "id": "f7cebcb46b48dc56257353853236e0264616d800", "title": "The Advanced Theory of Statistics", "references": []}, {"date": "1981", "abstract": "Abstract : Reasoning about motion is an important part of our commonsense knowledge, involving fluent spatial reasoning. This work studies the qualitative and geometric knowledge required to reason in a world that consists of balls moving through space constrained by collisions with surfaces, including dissipative forces and multiple moving objects. An analog geometry representation serves the program as a diagram, allowing many spatial questions to be answered by numeric calculation. It also provides the foundation for the construction and use of a place vocabulary, the symbolic descriptions of space required to do qualitative reasoning about motion in the domain. The actual motion of a ball is described as a network consisting of descriptions of qualitatively distinct types of motion. Implementing the elements of these networks in a constraint language allows the same elements to be used for both analysis and simulation motion. A qualitative description of the actual motion is also used to check the consistency of assumptions about motion. A process of qualitative simulation is used to describe the kinds of motion possible from some state. The ambiguity inherent in such a description can be reduced by assumptions about physical properties of the ball or assumptions about its motion. Each assumption directly rules out some kinds of motion, but other knowledge is required to determine the indirect consequences of making these assumptions.", "authors": ["Kenneth D. Forbus"], "id": "157e750a1259d0a5f839bb5cb8779ccb9d7702d6", "title": "A Study of Qualitative and Geometric Knowledge in Reasoning about Motion. Revision.", "references": []}, {"date": "1966", "abstract": "Intumescent compositions of the invention comprise certain arylsulfonamidopyrazines, such as 2-sulfanilamidopyrazine, 2-sulfanilamidobenzopyrazine, and derivatives thereof. The intumescent agents may be employed in conjuction with additives conventionally used in intumescent compositions. For use in protecting substrates from heat and fire, the intumescent compositions may be applied to the substrates in any suitable manner, such as by electrodeposition, spraying onto an adhesive substrate, or application of a coating composition comprising the intumescent agent.", "authors": ["Timothy W. Anderson"], "id": "c71946ef140ec2dd8a61a9bd05960ad384b2941d", "title": "Some nonparametric multivariate procedures based on statistically equivalent blocks", "references": []}, {"date": "1970", "abstract": "This paper focuses on the problem of the relationship between the risk incurred using a nearest neighbor rule and the size of the data base. Theoretical results include demonstrations of the facts that the proximity of the nearest neighbor to a new sample in a collection of n samples becomes (in probability) arbitrarily small as n is increased; that the convergence is often (but not always) with probability 1; that as a result of these convergences, the risk associated with a decision may be closely controlled; and that these facts and their demonstrations aid one in determining the size of a sample of data to be used as a nearest neighbor decision-making base. An example serves to demonstrate that the size of the data base required to meet performance criteria other than the relatively lax expected risk criterion can be unreasonably large.", "authors": ["D. W. Peterson"], "id": "676d2e74dd55ca0fdf631b32ba75c54bce70addd", "title": "Some convergence properties of a nearest neighbor decision rule", "references": []}, {"date": "1977", "abstract": "Abstract We present a rule-based system for computer-aided circuit analysis. The set of rules, called EL, is written in a rule language called ARS. Rules are implemented by ARS as pattern-directed invocation demons monitoring an associative data base. Deductions are performed in an antecedent manner, giving EL's analysis a catch-as-catch-can flavour suggestive of the behavior of expert circuit analyzers. We call this style of circuit analysis propagation of constraints. The system threads deduced facts with justifications which mention the antecedent facts and the rule used. These justifications may be examined by the user to gain insight into the operation of the set of rules as they apply to a problem. The same justifications are used by the system to determine the currently active data-base context for reasoning in hypothetical situations. They are also used by the system in the analysis of failures to reduce the search space. This leads to effective control of combinatorial search which we call dependency-directed backtracking.", "authors": ["Richard M. Stallman", "Gerald J. Sussman"], "id": "54f35b4edba6ddee8ce2eac489bde78308e3e708", "title": "Forward Reasoning and Dependency-Directed Backtracking in a System for Computer-Aided Circuit Analysis", "references": ["694d6894a9223144e967f2511d327260632b6995", "83f054294ba2726d02aa03e471da773c3383b146", "b62608c716819965f2755759ce3a7edb8a93829f", "59e5c4c530e996e2ec75ac88a0402e87ce85370e", "da9fc17631b3b3a05ac98f3af39dfcff5e895823", "a0adea7988254f3d0740b587334c8ca6357cdd8b", "e97795382386ecd24300f3a6449ed5732b200bfa", "01da4c5f9486e850cb4f112a9c059a87ad16d3ed", "2cfe1adca4b3fd0b20eb37260d7013303f112111", "567a07403759d35092263bbc437f6cd59c4e66a1"]}, {"date": "1972", "abstract": "A nonparametric classification procedure based on distribution-free tolerance regions is presented. Without knlowledge of the class probability distributions, the procedure gives information about the expected performance of the classifier through use of only one sample of statistically independent observations from each class. With this procedure, a two-class discriminant can be designed for a given expected false alarm probability or for a given confidence that the false alarm probability is less than a given amount. Three ordering methods are presented that appear intuitively reasonable for minimizing the miss probability. Even though the methods do not, in general, meet this objective, they are easily implemented on a computer and can give good results. A procedure for obtaining a measure of the miss probability is also presented. These methods are applied to the problem of verifying the purported identity of a speaker from a sample of the speaker's voice.", "authors": ["Guy W. Beakley", "Franz B. Tuteur"], "id": "a5277f4af5f79950ba03f928434152987ca8dbf1", "title": "Distribution-Free Pattern Verification Using Statistically Equivalent Blocks", "references": ["85a55e4bf4ba97ed3e164ad7c2188c7f29a99a09", "0c4c71cd1d2930a36a5d554b1368a55b12dda47f", "676c04de8bbd02bd405e27d18a498026b3e4282a", "970c09b481cd8f07384b709642418ab0c4f82454", "19e21669e7b9e205e0a4e9176e4ea36e7ecb724d", "e3adbdf77f6a1faa09794453275ba0f4c5eda2f2", "c71946ef140ec2dd8a61a9bd05960ad384b2941d", "162a1ab643ba0cc8802263cc0d5f294e84ba0713", "2704755776a79344729aee50c8b04dea6b997788", "4e247d5630f32657774fdc18016cc6ea418ba84f"]}, {"date": "1971", "abstract": "Two nonparametric methods to estimate the Bayes risk using classified sample sets are described and compared. The first method uses the nearest neighbor error rate as an estimate to bound the Bayes risk. The second method estimates the Bayes decision regions by applying Parzen probability-density function estimates and counts errors made using these regions. This estimate is shown to be asymptotically consistent in mean square. The results of experiments with these estimators on simulated and empirical data imply that the estimators both have acceptable small-sample properties; however, small-sample convergence of both estimators depends strongly on the choice of metric and local area or window size in the measurement space.", "authors": ["Stanley C. Fralick", "Richard W. Scott"], "id": "a4eabd1f87d62809a54de30dd480cf8de9e8d2f8", "title": "Nonparametric Bayes-risk estimation", "references": ["83a5c430e65fe74d8905c7e81a8aeaf11aead8e2"]}, {"date": "1983", "abstract": "Interpreting measurements of physical systems consists in part of constructing an account of \"what's happening\" in terms of our commonsense physical theories. Since most systems involve change, qualitative dynamics plays a central role in such deductions. This paper presents a theory of measurement interpretation at an instant, based on Qualitative Process Theory. Appropriate notions of measurement and interpretation are defined and the computational issues involved in constructing interpretations are examined. After describing an algorithm and illustrating its use by example, possible extensions to interpreting measurements over time will be discussed.", "authors": ["Kenneth D. Forbus"], "id": "2fcf66998c4b67b389627fa2aaa31a103cbde102", "title": "Measurement Interpretation in Qualitative Process Theory", "references": ["46f41bcaf5d69e3586571c6b8c91f525096726f5"]}, {"date": "1979", "abstract": "The causal arguments that people typically use to explain the behavior of physical systems contain ambiguities and hidden assumptions which result from imposing a particular point of view on the behavior of the system. The causality of such an argument is an artifact of imposing this point of view. Usually there exist other equally \"valid\" but conflicting arguments based on the same evidence. The inherent local nature of causal arguments makes it impossible for them to capture the more global effects that arc needed to resolve these ambiguities. However, their local nature makes causal arguments computationally simple to construct. This paper discusses these ideas in the context of electronics after first presenting a general theory of causal arguments. The causal rules that electrical engineers appear to use to reason about circuits are presented, and their use in constructing causal arguments for circuit behavior is discussed.", "authors": ["Johan de Kleer"], "id": "e5b9efabfef885ec48528c4d1c9e1e506a193d57", "title": "The Origin and Resolution of Ambiguities in Causal Arguments", "references": []}, {"date": "1967", "abstract": "Semantic Scholar extracted view of \"Mathematical Statistics: A Decision Theoretic Approach\" by Gerald S. Rogers", "authors": ["Gerald S. Rogers"], "id": "9c191a25ddbdc47a7b596b34bd756f4540f7a81f", "title": "Mathematical Statistics: A Decision Theoretic Approach", "references": []}, {"date": "1984", "abstract": "The ability to identify and represent the knowledge that a human expert has about a particular domain is a key method in the creation of an expert computer system. The first part of this paper demonstrates a methodology for collecting and analyzing observations of experts at work, in order to find the conceptual framework used for the particular domain. The second part develops a representation for qualitative knowledge of the structure and behavior of a mechanism. The qualitative simulation, or envisionment, process is given a qualitative structural description of a mechanism and some initialization information, and produces a detailed description of the mechanism's behavior. The simulation process has been fully implemented, and its results are shown for a particular disease mechanisms in nephrology. This vertical slice of the construction of a cognitive model demonstrates an effective knowledge acquisition method for the purpose of determining the structure of the representation itself, not simply the content of the knowledge to be encoded in that representation. Most importantly, it demonstrates the interaction among constraints derived from the textbook knowledge of the domain, from observations of the human expert, and from the computational requirements of successful performance.", "authors": ["Benjamin Kuipers", "Jerome P. Kassirer"], "id": "453f149546048b895fd39bac304d654c0e24a35d", "title": "Causal reasoning in medicine: Analysis of a protocol", "references": []}, {"date": "1972", "abstract": "What do you do to start reading computer oriented approaches to pattern recognition? Searching the book that you love to read first or find an interesting book that will make you want to read? Everybody has difference with their reason of reading a book. Actuary, reading habit must be from earlier. Many people may be love to read, but not a book. It's not fault. Someone will be bored to open the thick book with small words to read. In more, this is the real condition. So do happen probably with this computer oriented approaches to pattern recognition.", "authors": ["William S. Meisel"], "id": "5ee6ff0c6d4c610be461d41f47d7c3ee3b11a74d", "title": "Computer-oriented approaches to pattern recognition", "references": []}, {"date": "1973", "abstract": "A new method is presented for obtaining network functions in which some, none, or all of the network elements are represented by symbolic parameters (i.e., symbolic network functions). Unlike the topological tree enumeration or signal flow graph methods generally used to derive symbolic network functions, this new process uses fast, efficient, numerical-type algorithms to determine the contribution of those network branches not represented by symbolic parameters. A computer program [Network Analysis Program Using Parameter Extractions (NAPPE)] that incorporates all of the concepts discussed in this paper has been written. Several examples illustrating the usefulness and efficiency of NAPPE are included.", "authors": ["Gary E. Alderson", "P. M. Lin"], "id": "17d5522a5f02426bdb06a15c42724c9e3fff3cf1", "title": "Computer generation of symbolic network functions-A new theory and implementation", "references": []}, {"date": "1976", "abstract": "Abstract : This report is concerned with the problem of achieving flexibility (additivity, modularity) and efficiency (performance, expertise) simultaneously in one AI program. It deals with the domain of elementary electronic circuit design. The proposed solution is to provide a deduction-driven problem solver with built-in control-structure concepts. This problem solver and its knowledge base in the application areas of design and electronics are described. The program embodying it is being used to explore the solution of some modest problems in circuit design. It is concluded that shallow reasoning about problem-solver plans is necessary for flexibility, and can be implemented with reasonable efficiency. (Author)", "authors": ["D. McDermott"], "id": "59e5c4c530e996e2ec75ac88a0402e87ce85370e", "title": "Flexibility and Efficiency in a Computer Program for Designing Circuits", "references": []}, {"date": "1971", "abstract": "The tableau approach to automated network design optimization via implicit, variable order, variable time-step integration, and adjoint sensitivity computation is described. In this approach, the only matrix operation required is that of repeatedly solving linear algebraic equations of fixed sparsity structure. Required partial derivatives and numerical integration is done at the branch level leading to a simple input language, complete generality and maximum sparsity of the characteristic coefficient matrix. The bulk of computation and program complexity is thus located in the sparse matrix routines; described herein are the routines OPTORD and 1-2-3 GNSO. These routines account for variability type of the matrix elements in producing a machine code for solution of Ax=b in nested iterations for which a weighted sum of total operations count and round-off error incurred in the optimization is minimized.", "authors": ["Gary D. Hachtel", "Robert K. Brayton", "Fred G. Gustavson"], "id": "8e97127b3f186215dd5b70176c44f5f577103f20", "title": "The Sparse Tableau Approach to Network Analysis and Design", "references": ["5c7c2b1a9234d3116926bd1bfcfcb2a45b7cbfca", "9ca2e2a6e8bb8bd1f72768081fdd0d6f6109ed40", "1ec0eab9cd3c8980e906d2a8e8e4b3afc05f9ae7", "111af1a955ee6cb44f504d36602c00f937541df4", "4713a6bea2d6e55c0a88e90fa0b25f60a4e6faf3", "52d44a7937d076ea82726911cb79ce76e6597e40", "88365ce55de9d60fa75ea3a4c0663e41b30341d4", "60edb775ea89a73846da0723816be83d5069b0c0", "cad2bb2ae2b7709ba61cd49888ae053bee236f81", "422917a715870329e9d2d4447fc4e4a69a3b3b7b"]}, {"date": "1970", "abstract": "Relationships between the probability of error, the equivocation, and the Chernoff bound are examined for the two-hypothesis decision problem. The effect of rejections on these bounds is derived. Finally, the results are extended to the case of any finite number of hypotheses.", "authors": ["Martin E. Hellman", "Josef Raviv"], "id": "ae1c34e48989399c4faa90260647934f44c9d572", "title": "Probability of error, equivocation, and the Chernoff bound", "references": ["b5b2ee1af1c0a867f564e44f3f79b8d2bdeca749", "77ce3697fc01e0bebba1dfe81cedb712b0b604a0", "d150b9533bf510812a364094ca0c787a9e400ed2", "bc22d1610ce680c91b4323a1899b1f22cfdf533f", "685eb09b27dbaa301c8c6fc48fd6f7d1ec66ccc9", "eff026fee8b554a445ac881e5ffa899688e799a6", "90a565559dcb438e24db4bd9fc53b38f30cad267"]}, {"date": "1970", "abstract": "Semantic Scholar extracted view of \"SNAP: A computer program for generating symbolic network functions\" by P. M. Lin et al.", "authors": ["P. M. Lin", "Gary E. Alderson"], "id": "f98a72eb0858b3ca9da57ce6180afaf702aae44a", "title": "SNAP: A computer program for generating symbolic network functions", "references": []}, {"date": "1978", "abstract": "Abstract : It is profitable to view the process of writing programs as an engineering activity. A program is a deliberately contrived mechanism constructed from parts whose behaviors are combined to produce the behavior of the whole. One proposes to develop a notion of understanding a program which is analogous to similar notions in other engineering subjects. Understanding is a rich notion in engineering domains. It includes the ability to identify the parts of a mechanism and assign a purpose to each part. Understanding also entails being able to explain to someone how a mechanism works and rationalize its behavior under unusual circumstances. Part of the methodology for investigating these ideas is to build a computer-aided design tool for computer programs. The construction of this tool will serve both as a concrete realization of the theoretical ideas and as a testbed for our practical techniques.", "authors": ["Charles Rich", "Howard E. Shrobe", "Richard C. Waters", "Gerald J. Sussman", "Carl Hewitt"], "id": "f823f24b2da710cc3c7c6e76f262bcfffdffdbf7", "title": "Programming Viewed as an Engineering Activity", "references": []}, {"date": "1972", "abstract": "The convergence properties of a nearest neighbor rule that uses an editing procedure to reduce the number of preclassified samples and to improve the performance of the rule are developed. Editing of the preclassified samples using the three-nearest neighbor rule followed by classification using the single-nearest neighbor rule with the remaining preclassified samples appears to produce a decision procedure whose risk approaches the Bayes' risk quite closely in many problems with only a few preclassified samples. The asymptotic risk of the nearest neighbor rules and the nearest neighbor rules using edited preclassified samples is calculated for several problems.", "authors": ["Dennis L. Wilson"], "id": "dea8658ee4750ec6bb408a2281cf922cbb300a0a", "title": "Asymptotic Properties of Nearest Neighbor Rules Using Edited Data", "references": ["f18307ebabf398bb7fd1b3375b2f09a7f9f6c5be", "0217065021a8166e88dbcdf60f23f2222c6338bc", "0efb841403aa6252b39ae6975c1cc5410554ef7b", "9dfb0027eb1219e714b874f1e6434136fc003f50", "0d57e30df0bd0ac93915c06ca1a46a64f26e4662", "f2ab42faef66168c86cd53c201fe6f228be3dc2b", "13a60d6190e095fee1837cd392255a9e42af97ed", "8e30371163dada311d2cef48e73aafbdad864d42", "145d334f7d60bb5a4ed904365eeb035f068fa826", "a4090f2a8a6ef750c195273c92ff74f43cd4daed"]}, {"date": "1974", "abstract": "An important measure concerning the use of statistical decision schemes is the error probability associated with the decision rule. Several methods giving bounds on the error probability are presently available, but, most often, the bounds are loose. Those methods generally make use of so-cailed distances between statistical distributions. In this paper a new distance is proposed which permits tighter bounds to be set on the error probability of the Bayesian decision rule and which is shown to be closely related to several certainty or separability measures. Among these are the nearest neighbor error rate and the average conditional quadratic entropy of Vajda. Moreover, our distance bears much resemblance to the information theoretic concept of equivocation. This relationship is discussed. Comparison is made between the bounds on the Bayes risk obtained with the Bhattacharyya coefficient, the equivocation, and the new measure which we have named the Bayesian distance.", "authors": ["Pierre A. Devijver"], "id": "4907fd0bf5b982de6a00f008bc59077814996a01", "title": "On a New Class of Bounds on Bayes Risk in Multihypothesis Pattern Recognition", "references": []}, {"date": "1983", "abstract": "Recentl~there has been a areat deal of interest aithin the .\\rtificial intelligence communits in deseloping a qualiiauseph~sies (e.g.. qualitatiseprocesstheor~. ensisioning. naiveph~sics) that predicts and explains the behasiorof mechanismsin qualitatiseterms. The goalsfor the qualitatisephysics are(1) to be far simpler thanclassical ph~sicsand ~et retainall the importantdistinctions(e.g.. state,oscillation, gain, momentum) without insokng the mathematicsof continuousl~sarsingquantitiesanddifferential equations,(2) to produce causalaccountsof phssicaimechanismsthat are easy to understand, and (3) to prosidethe foundationsfor common-sensemodels for the next generationof expertsystems.This paperconsistsof threeparts. First, se present a framework for understandingand unifying the various approachesto qualitative physics as well as posing certain criteria that sucha physics should satisfy. Second,we compareand contrastthreeapproachesto qualitativephysics. Finally, we explore theuseof proofasexplanationandtherole reduciio ad absurduni plays in qualitativeargumentsin physics.", "authors": ["John Seely Brown", "Johan de Kleer"], "id": "b3aebbb0e7bc39c263df37d25482916065b2768f", "title": "The Origin, Form, and Logic of Qualitative Physical Laws", "references": ["6813e5400681a1704c4c4aef2cb7a805fa99c30b", "2b9501887cfe85eff2d9d717d3803eb908e26ff7"]}, {"date": "1979", "abstract": "Publisher Summary This chapter provides an overview of representation of commonsense knowledge. Commonsense knowledge is knowledge about the world that everybody knows. This chapter explains how the performance of a representation for commonsense knowledge can survive the unpleasant computational environment of the real world. Commonsense knowledge is knowledge about the structure of the external world that is acquired and applied without concentrated effort by any normal human that allows him or her to meet the everyday demands of the physical, spatial, temporal, and social environment with a reasonable degree of success. Commonsense knowledge is useful exactly because it is a description of the environment that is maintained at very low cost. The fact that commonsense knowledge is reasonably useful under real-world performance constraints without concentrated effort suggests an opportunistic way of operating.", "authors": ["Benjamin Kuipers"], "id": "2c3be6c350b54a9722b2c7fd8dda80341912b5d6", "title": "ON REPRESENTING COMMONSENSE KNOWLEDGE", "references": ["4667cc16ce37f20fa48797c01bc32f0309c5b19f", "51ec18700378b92c5912c098dbb8664d31823ac7", "69ace1cc308979643ba2875a45b3286786f7c1ce", "4023ae0ba18eed43a97e8b8c9c8fcc9a671b7aa3", "494aedf82da4755badc1fe74e4d21cf5fc029e9d", "3490c242c3d6b5a735792a75058de0d108d04731", "aa4b60b5847999c2f778e3e67ca1f2201e396abb"]}, {"date": "1958", "abstract": "Semantic Scholar extracted view of \"Tables for Obtaining Non-parametric Tolerance Limits\" by Paul N. Somerville", "authors": ["Paul N. Somerville"], "id": "85a55e4bf4ba97ed3e164ad7c2188c7f29a99a09", "title": "Tables for Obtaining Non-parametric Tolerance Limits", "references": []}, {"date": "1945", "abstract": "Semantic Scholar extracted view of \"Non-Parametric Estimation. I. Validation of Order Statistics\" by Henry Scheff\u00e9 et al.", "authors": ["Henry Scheff\u00e9", "John W. Tukey"], "id": "162a1ab643ba0cc8802263cc0d5f294e84ba0713", "title": "Non-Parametric Estimation. I. Validation of Order Statistics", "references": []}, {"date": "1941", "abstract": "Semantic Scholar extracted view of \"Determination of Sample Sizes for Setting Tolerance Limits\" by S. S. Wilks", "authors": ["S. S. Wilks"], "id": "4e247d5630f32657774fdc18016cc6ea418ba84f", "title": "Determination of Sample Sizes for Setting Tolerance Limits", "references": []}, {"date": "1965", "abstract": "Semantic Scholar extracted view of \"A Nonparametric Estimate of a Multivariate Density Function\" by Don Owen Loftsgaarden et al.", "authors": ["Don Owen Loftsgaarden", "Charles P. Quesenberry"], "id": "83a5c430e65fe74d8905c7e81a8aeaf11aead8e2", "title": "A Nonparametric Estimate of a Multivariate Density Function", "references": ["0c4c71cd1d2930a36a5d554b1368a55b12dda47f", "676c04de8bbd02bd405e27d18a498026b3e4282a", "f64b5ce0663bd77e72d8e5f3349c18db9125ea6f", "a27140f31c7ea6f6afdf51280c9dc23d923198b6", "de28c165623adabcdba0fdb18b65eba685aaf31d", "37259a2ae44002790febdc7aa07fbb260f5d2eee", "9cb87e945b81a526a59e19e2e27d494d88b56e23"]}, {"date": "1948", "abstract": "Semantic Scholar extracted view of \"Nonparametric Estimation, III. Statistically Equivalent Blocks and Multivariate Tolerance Regions--The Discontinuous Case\" by John W. Tukey", "authors": ["John W. Tukey"], "id": "e3adbdf77f6a1faa09794453275ba0f4c5eda2f2", "title": "Nonparametric Estimation, III. Statistically Equivalent Blocks and Multivariate Tolerance Regions--The Discontinuous Case", "references": []}, {"date": "1951", "abstract": "Semantic Scholar extracted view of \"Sequentially Determined Statistically Equivalent Blocks\" by Donald Fraser", "authors": ["Donald Fraser"], "id": "19e21669e7b9e205e0a4e9176e4ea36e7ecb724d", "title": "Sequentially Determined Statistically Equivalent Blocks", "references": []}, {"date": "1969", "abstract": "Automatic speaker verification was accomplished in this study using cepstral measurements to characterize short segments in each of the first two vowels of the standard test phrase \u201cMy code is .\u201d The length of the word \u201cmy\u201d and the speaker's pitch were used as additional parameters. The verification decision is treated as a two\u2010class problem, the speaker being either the authorized speaker or an impostor. Reference data is used only for the authorized speaker. The decision is based on the test sample's distance to the nearest reference sample. Data is presented to show that, if reference samples are collected over a period of many days, then verification is possible more than two months later, whereas, if reference data is collected at one sitting, verification is highly inaccurate as little as 1 h later. Four authorized speakers and 30 impostors were examined, with error rates obtained from 6% to 13%. Impostors attempting to mimic the authorized speaker could not improve their ability to deceive the syst...", "authors": ["Joanne E. Luck"], "id": "2704755776a79344729aee50c8b04dea6b997788", "title": "Automatic speaker verification using cepstral measurements.", "references": []}, {"date": "1956", "abstract": "Semantic Scholar extracted view of \"Generalized Tolerance Limits\" by J. H. B. Kemperman", "authors": ["J. H. B. Kemperman"], "id": "970c09b481cd8f07384b709642418ab0c4f82454", "title": "Generalized Tolerance Limits", "references": []}, {"date": "1943", "abstract": "Semantic Scholar extracted view of \"An Extension of Wilks' Method for Setting Tolerance Limits\" by Abraham Wald", "authors": ["Abraham Wald"], "id": "676c04de8bbd02bd405e27d18a498026b3e4282a", "title": "An Extension of Wilks' Method for Setting Tolerance Limits", "references": []}, {"date": "1947", "abstract": "Semantic Scholar extracted view of \"Non-Parametric Estimation II. Statistically Equivalent Blocks and Tolerance Regions--The Continuous Case\" by John W. Tukey", "authors": ["John W. Tukey"], "id": "0c4c71cd1d2930a36a5d554b1368a55b12dda47f", "title": "Non-Parametric Estimation II. Statistically Equivalent Blocks and Tolerance Regions--The Continuous Case", "references": []}, {"date": "1967", "abstract": "The variational approach to the optimal design of high-speed switching circuits is explored. The approach implements the variational calculus to obtain an expression for the vector sensitivity of a scalar performance function (e.g., delay, or switching time) to changes in the vector of design parameters. Gradient methods are established for using the vector sensitivity to iteratively update the parameter vector and obtain an optimal design. It is shown that the variational approach retains, typically, an M-fold computational advantage over conventional step-and-repeat methods in determining the sensitivity of a scalar performance function to M design parameters. The approach is shown to be well adapted for incorporation into package analysis programs with matrix formulations, and vested with sufficient generality to be applicable to a wide range of switching circuit problems (e.g., low-power or large-scale integrated circuits). It is further shown that subsumed in the general class of nonlinear parameter-value synthesis problems is the class of delay-minimization problems, and that the switching time minimization problem is a special case of the classical \"time-optimal\" problem.", "authors": ["Gary D. Hachtel", "Ronald A. Rohrer"], "id": "422917a715870329e9d2d4447fc4e4a69a3b3b7b", "title": "Techniques for the optimal design and synthesis of switching circuits", "references": []}, {"date": "1968", "abstract": "This paper attempts to lay bare the underlying ideas used in various pattern classification algorithms reported in the literature. It is shown that these algorithms can be classified according to the type of input information required and that the techniques of estimation, decision, and optimization theory can be used effectively to derive known as well as new results.", "authors": ["Yu-Chi Ho", "Ashok K. Agrawala"], "id": "a4090f2a8a6ef750c195273c92ff74f43cd4daed", "title": "On pattern classification algorithms--Introduction and survey", "references": []}, {"date": "1968", "abstract": "By an extension of Gallager's bounding methods, exponential error bounds applicable to coding schemes involving erasures, variable-size lists, and decision feedback are obtained. The bounds are everywhere the tightest known.", "authors": ["G. David Forney"], "id": "90a565559dcb438e24db4bd9fc53b38f30cad267", "title": "Exponential error bounds for erasure, list, and decision feedback schemes", "references": ["9cceb260a10279fc20b93089c9140c601ac53dc5", "4e54c231bd7f8e5cfbe93b578bb75d6faf852357", "e39acbeffb4dc5d7a45cdb889084d5be690fc8ce", "b2b3c943eaa378b93dc22b7a4cf8f7c08844e8d3"]}, {"date": "1967", "abstract": "Man's intelligent behavior is due in part to his ability to select, classify, and abstract significant information reaching him from his environment by way of his senses. This function, pattern recognition, has become a major focus of research by scientists working in the field of artificial intelligence. At the lowest level, pattern recognition reduces to pattern classification, which consists of the separation, into desired classes, of groups of objects, sounds, odors, events, properties, and the like; the separations are based on sets of measurements made on the entities being classified. The pattern classifier is composed of a data filter and a categorizer. The data filter selects the distinguishing features and represents them as sets of real numbers; each set is termed a pattern. The categorizer assigns each pattern to one of several desired classes. Patterns can be represented geometrically as points in an n-dimensional space; the n coordinates of each point are the numerical values of the features selected to represent the pattern. A pattern classification system separates an n-dimensional space into regions, each of which ideally contains points of only one class. One method to effect this separation is by means of ldquo;trainablerdquo; categorizers\u2014major components of adaptive machines. They consist of networks whose internal parameters are varied according to a set of fixed rules during a training cycle. A statistically large sample of known patterns are presented, one at a time, to the networks; internal corrections are made each time a pattern is erroneously classified. Classifica-tion performance tends to improve as the set of known patterns is cycled repetitively through the machine. Finally, the adequacy of adaptation is tested by a separate set of similar patterns which have not been used in the training process. A number of different machine organizations and training rules have been developed and are being applied successfully to numerous classification problems. More difficult recognition problems requiring the aid of logioal tests and analysis, search and association, use the digital computer programmed to supplement the functions of the adaptive classifier.", "authors": ["Charles A. Rosen"], "id": "13a60d6190e095fee1837cd392255a9e42af97ed", "title": "Pattern Classification by Adaptive Machines", "references": []}, {"date": "1943", "abstract": "A hydrofining process wherein an olefinic naphtha hydrocarbon feed in vapor phase is contacted, in a fixed bed, with a catalyst of small particle size at low pressure. Suitably, a catalyst of average particle size no greater than, or less than about 1/20 inch is employed. Preferably, the catalyst is of average particle size ranging from about 200 microns to about 1/20 inch, and more preferably the catalyst is of average particle size ranging from about 1/40 inch to about 1/20 inch. By the use of such catalyst at total pressures ranging from about 60 to about 300 psig, preferably from about 80 to about 200 psig, it has been found that the rate of hydrodesulfurization of the naphtha feed is considerably increased, and yet there is significantly less saturation of the olefins, and other nonreactive hydrocarbons with hydrogen.", "authors": ["Henry B. Mann", "Abraham Wald"], "id": "8e30371163dada311d2cef48e73aafbdad864d42", "title": "On Stochastic Limit and Order Relationships", "references": []}, {"date": "1975", "abstract": "A new kind of circuit analysis program, EL, is presented. Whereas other circuit analysis systems rely on classical, formal, analysis techniques, EL employs heuristic \"inspection\" methods to solve rather complex dc bias circuits. These techniques also give EL the ability to explain any result in terms of its own qualitative reasoning processes. EL's reasoning is based on the concept of a \"local one-step deduction\" augmented by various \"teleological\" principles and by the concept of a \"macro-element.\" Several annotated examples of EL in operation and an explanation of how it works are presented. Also how EL can be extended in several directions, including sinusoidal steady-state analysis is shown. Finally, the possible implications of this work for engineering education and computer-aided design technology are discussed briefly. EL is significant not only as a novel approach to circuit analysis but also as an application of Artificial Intelligence techniques to a new and interesting domain.", "authors": ["Gerald J. Sussman", "Richard M. Stallman"], "id": "567a07403759d35092263bbc437f6cd59c4e66a1", "title": "Heuristic techniques in computer-aided circuit analysis", "references": []}, {"date": "1968", "abstract": "Communication Systems and Information Theory. A Measure of Information. Coding for Discrete Sources. Discrete Memoryless Channels and Capacity. The Noisy-Channel Coding Theorem. Techniques for Coding and Decoding. Memoryless Channels with Discrete Time. Waveform Channels. Source Coding with a Fidelity Criterion. Index.", "authors": ["Robert G. Gallager"], "id": "eff026fee8b554a445ac881e5ffa899688e799a6", "title": "Information Theory and Reliable Communication", "references": []}, {"date": "1975", "abstract": "Publisher Summary This chapter focuses on the structure of alternative solutions to the design issues and illustrates the design options through three specific representations. It is often convenient and sometimes necessary to use several different representations within a single system. In this way, it is sometimes possible to combine the advantages of different representational forms within one system. The use of multiple representations leads to two primary problems\u2014choice and consistency. Representations determine the ease of answering certain questions, and of performing updating operations. At times, it is best to enter information directly into one representational form and then, from there, compute the way to enter it into the other form. Thus, an object's position might first be entered by its coordinates, and then its position relative to all others is computed and inserted into the appropriate representation. In a system with multiple representations, the same information can be stored in more than one form. When one form is changed, the other forms must be checked for consistency.", "authors": ["Daniel G. Bobrow"], "id": "3490c242c3d6b5a735792a75058de0d108d04731", "title": "DIMENSIONS OF REPRESENTATION", "references": []}, {"date": "1976", "abstract": "An introduction is given to a theory of early visual information processing. The theory has been implemented, and examples are given of images at various stages of analysis. It is argued that the first step of consequence is to compute a primitive but rich description of the grey-level changes present in an image. The description is expressed in a vocabulary of kinds of intensity change (EDGE, SHADING-EDGE, EXTENDED-EDGE, LINE, BLOB etc.). Modifying parameters are bound to the elements in the description, specifying their POSITION, ORIENTATION, TERMINATION points, CONTRAST, SIZE and FUZZINESS. This description is obtained from the intensity array by fixed techniques, and it is called the primal sketch. For most images, the primal sketch is large and unwieldy. The second important step in visual information processing is to group its contents in a way that is appropriate for later recognition. From our ability to interpret drawings with little semantic content, one may infer the presence in our perceptual equipment of symbolic processes that can define \"place-tokens\" in an image in various ways, and can group them according to certain rules. Homomorphic techniques fail to account for many of these grouping phenomena, whose explanations require mechanisms of construction rather than mechanisms of detection. The necessary grouping of elements in the primal sketch may be achieved by a mechanism that has available the processes inferred from above, together with the ability to select items by first order discriminations acting on the elements' parameters. Only occasionally do these mechanisms use downward-flowing information about the contents of the particular image being processed. It is argued that \"non-attentive\" vision is in practice implemented by these grouping operations and first order discriminations acting on the primal sketch. The class of computations so obtained differs slightly from the class of second order operations on the intensity array. The extraction of a form from the primal sketch using these techniques amounts to the separation of figure from ground. It is concluded that most of the separation can be carried out by using techniques that do not depend upon the particular image in question. Therefore, figure-ground separation can normally precede the description of the shape of the extracted form. Up to this point, higher-level knowledge and purpose are brought to bear on only a few of the decisions taken during the processing. This relegates the widespread use of downward-flowing information to a later stage than is found in current machine-vision programs, and implies that such knowledge should influence the control of, rather than interfering with, the actual data-processing that is taking place lower down.", "authors": ["David Marr"], "id": "aa4b60b5847999c2f778e3e67ca1f2201e396abb", "title": "Early processing of visual information.", "references": []}, {"date": "1977", "abstract": "A person's cognitive map, or knowledge of large-scale space, is built up from observations gathered as he travels through the environment. It acts as a problem-solver to find routes and relative positions, as well as describing the current location. The TOUR model describes the multiple representations that make up the cognitive map, the problem-solving strategies it uses, and the mechanisms for assimilating new information. The representations have rich collections of states of partial knowledge, which support many of the attractive qualities of common-sense knowledge.", "authors": ["Benjamin Kuipers"], "id": "51ec18700378b92c5912c098dbb8664d31823ac7", "title": "Modelling spatial knowledge", "references": []}, {"date": "1973", "abstract": "Publisher Summary This chapter presents a collection of theoretical and empirical arguments against the separation of testing from a theory of cognition. It describes a general model of cognition and presents some of its implications for individual differences. The chapter presents a number of experiments, which relate the model to the present tests of intelligence and considers the implications of these results for both psychometrics and cognitive psychology, indicating some directions for future research. The theoretical model used is the Distributed Memory model, which is representative of a class of models acceptable to the majority of experimental psychologists interested in cognition. The theoretical approach underlying the distributed memory model is that the brain can be thought of as a computing system, and that as such it has a physical and implied logical construction which is called its system architecture. The physical structures comprising the system architecture are exercised by control processes analogous to programs in an actual computer.", "authors": ["Earl Hunt", "Nancy E. Frost", "Clifford E. Lunneborg"], "id": "69ace1cc308979643ba2875a45b3286786f7c1ce", "title": "Individual Differences in Cognition: A New Approach to Intelligence", "references": []}, {"date": "1960", "abstract": "Abstract : This paper discusses programs to manipulate in a suitable formal language (most likely a part of the predicate calculus) common instrumental statements. The basic program will draw immediate conclusions from a list of premises. These conclusions will be either declarative or imperative sentences. When an imperative sentence is deduced the program takes a corresponding action. These actions may include printing sentences, moving sentences on lists, and reinitiating the basic deduction process on these lists.", "authors": ["John W. Mccarthy"], "id": "494aedf82da4755badc1fe74e4d21cf5fc029e9d", "title": "Programs with common sense", "references": ["ff433838bb2b178e1d6b1f045b0215385e1757f5"]}, {"date": "1944", "abstract": "Semantic Scholar extracted view of \"Mathematical Statistics\" by R. A. F.", "authors": ["R. A. F."], "id": "9cb87e945b81a526a59e19e2e27d494d88b56e23", "title": "Mathematical Statistics", "references": []}, {"date": "1966", "abstract": "Semantic Scholar extracted view of \"Estimation of a multivariate density\" by Theophilos Cacoullos", "authors": ["Theophilos Cacoullos"], "id": "37259a2ae44002790febdc7aa07fbb260f5d2eee", "title": "Estimation of a multivariate density", "references": []}, {"date": "1958", "abstract": "Semantic Scholar extracted view of \"On the Smoothing of Probability Density Functions\" by Peter Whittle", "authors": ["Peter Whittle"], "id": "f64b5ce0663bd77e72d8e5f3349c18db9125ea6f", "title": "On the Smoothing of Probability Density Functions", "references": []}, {"date": "1963", "abstract": "Semantic Scholar extracted view of \"On the Estimation of the Probability Density, I\" by Geoffrey Stuart Watson et al.", "authors": ["Geoffrey Stuart Watson", "M. R. Leadbetter"], "id": "a27140f31c7ea6f6afdf51280c9dc23d923198b6", "title": "On the Estimation of the Probability Density, I", "references": ["87cedbb32d574bf2e6abe4c4b78742053e42b2e8", "f64b5ce0663bd77e72d8e5f3349c18db9125ea6f", "de28c165623adabcdba0fdb18b65eba685aaf31d"]}, {"date": "1978", "abstract": "Semantic Scholar extracted view of \"Fundamental aspects of cognitive representation\" by Stephen E. Palmer", "authors": ["Stephen E. Palmer"], "id": "4667cc16ce37f20fa48797c01bc32f0309c5b19f", "title": "Fundamental aspects of cognitive representation", "references": []}, {"date": "1982", "abstract": "Representing motion is an important part of Naive Physics. Previous qualitative models of motion were centered around the idea of Qualitative States. This paper discusses an alternative representation in terms of Qualitative Process Theory. Viewing motion as a process has several advantages, notably the ability to make more detailed inferences about dynamics and the ability to combine process descriptions to model more complex systems. After examining the relationship between Qualitative State and QP theory representations of motion, the utility of the QP representations are illustrated by analyzing an oscillator.", "authors": ["Kenneth D. Forbus"], "id": "2b9501887cfe85eff2d9d717d3803eb908e26ff7", "title": "Modeling Motion With Qualitative Process Theory", "references": ["157e750a1259d0a5f839bb5cb8779ccb9d7702d6"]}, {"date": "1962", "abstract": "Abstract : Given a sequence of independent identically distributed random variables with a common probability density function, the problem of the estimation of a probability density function and of determining the mode of a probability function are discussed. Only estimates which are consistent and asymptotically normal are constructed. (Author)", "authors": ["Emanuel Parzen"], "id": "de28c165623adabcdba0fdb18b65eba685aaf31d", "title": "ON ESTIMATION OF A PROBABILITY DENSITY FUNCTION AND MODE", "references": []}, {"date": "1959", "abstract": "Semantic Scholar extracted view of \"On a General Concept of \"In Probability\"\" by John W. Pratt", "authors": ["John W. Pratt"], "id": "f2ab42faef66168c86cd53c201fe6f228be3dc2b", "title": "On a General Concept of ", "references": []}, {"date": "1956", "abstract": "First, the span of absolute judgment and the span of immediate memory impose severe limitations on the amount of information that we are able to receive, process, and remember. By organizing the stimulus input simultaneously into several dimensions and successively into a sequence or chunks, we manage to break (or at least stretch) this informational bottleneck. Second, the process of recoding is a very important one in human psychology and deserves much more explicit attention than it has received. In particular, the kind of linguistic recoding that people do seems to me to be the very lifeblood of the thought processes. Recoding procedures are a constant concern to clinicians, social psychologists, linguists, and anthropologists and yet, probably because recoding is less accessible to experimental manipulation than nonsense syllables or T mazes, the traditional experimental psychologist has contributed little or nothing to their analysis. Nevertheless, experimental techniques can be used, methods of recoding can be specified, behavioral indicants can be found. And I anticipate that we will find a very orderly set of relations describing what now seems an uncharted wilderness of individual differences. Third, the concepts and measures provided by the theory of information provide a quantitative way of getting at some of these questions. The theory provides us with a yardstick for calibrating our stimulus materials and for measuring the performance of our subjects. In the interests of communication I have suppressed the technical details of information measurement and have tried to express the ideas in more familiar terms; I hope this paraphrase will not lead you to think they are not useful in research. Informational concepts have already proved valuable in the study of discrimination and of language; they promise a great deal in the study of learning and memory; and it has even been proposed that they can be useful in the study of concept formation. A lot of questions that seemed fruitless twenty or thirty years ago may now be worth another look. In fact, I feel that my story here must stop just as it begins to get really interesting. And finally, what about the magical number seven? What about the seven wonders of the world, the seven seas, the seven deadly sins, the seven daughters of Atlas in the Pleiades, the seven ages of man, the seven levels of hell, the seven primary colors, the seven notes of the musical scale, and the seven days of the week? What about the seven-point rating scale, the seven categories for absolute judgment, the seven objects in the span of attention, and the seven digits in the span of immediate memory? For the present I propose to withhold judgment. Perhaps there is something deep and profound behind all these sevens, something just calling out for us to discover it. But I suspect that it is only a pernicious, Pythagorean coincidence.", "authors": ["George Abram Miller"], "id": "4023ae0ba18eed43a97e8b8c9c8fcc9a671b7aa3", "title": "The magical number seven plus or minus two: some limits on our capacity for processing information.", "references": ["5322a485d71098e245e4c7c36cfdb7f5fce2858c", "2e8136581d934e18eb17d124f0224a32b01272d5", "5e73f775ea677849942c2b299dffa75f69cfca42", "28628e1a8bf31402113cef0e87fef9c23c1c4d3e", "6bda9f1be3714d2a9866d4714b7f20e15c5a6e74", "352a7df7e022c1760b496b6b3e1d39a8369ad5c4", "10b9c3b2923e952df0b7bd68e52d1bb56081a27b", "5eec4e79df13c612a02b01185e1f72143db81ae5", "a546750bd366343354b745de9476d3cc52025a93"]}, {"date": "1970", "abstract": "A family of supervised, nonparametric decision rules, based on tolerance regions, is described which includes the k -Nearest Neighbor decision rules when there are two classes. There are two practical reasons for doing so: first, a family of decision rules similar to the k -Nearest Neighbor rules can be specified which applies to a broader collection of pattern recognition problems. This is because in the general class of rules constraints are weakened between the number of training samples required in each training sample set and the respective a priori class probabilities; and, a discrete loss function weighting the importance of the finite number of ways to make a decision error can be introduced. Second, within the family of decision rules based on tolerance regions, there are decision rules which have a property allowing for preprocessing of the training set data resulting in significant data reduction. Theoretical performance for a special case is presented.", "authors": ["Edward A. Patrick", "F. P. Fischer"], "id": "0d57e30df0bd0ac93915c06ca1a46a64f26e4662", "title": "A Generalized k-Nearest Neighbor Rule", "references": ["0c4c71cd1d2930a36a5d554b1368a55b12dda47f", "4e247d5630f32657774fdc18016cc6ea418ba84f", "0efb841403aa6252b39ae6975c1cc5410554ef7b"]}, {"date": "1965", "abstract": "Upper bounds are derived on the probability of error that can be achieved by using block codes on general time-discrete memoryless channels. Both amplitude-discrete and amplitude-continuous channels are treated, both with and without input constraints. The major advantages of the present approach are the simplicity of the derivations and the relative simplicity of the results; on the other hand, the exponential behavior of the bounds with block length is the best known for all transmission rates between 0 and capacity. The results are applied to a number of special channels, including the binary symmetric channel and the additive Gaussian noise channel.", "authors": ["Robert G. Gallager"], "id": "b2b3c943eaa378b93dc22b7a4cf8f7c08844e8d3", "title": "A simple derivation of the coding theorem and some applications", "references": ["ee361838116550043733e900bf93d9a1b4c71c82", "6862577e05c8f06e666d8cfc750c84678f996ea3", "a538bc641f7af52d29a6dab9e53ce42231f99081"]}, {"date": "1966", "abstract": "044.~a (/ (1 The Research Laboratory of Electronics is an interdepartmental laboratory in which faculty members and graduate students from numerous academic departments conduct research. Abstract This report is concerned with the extension of known bounds on the achievable probability of error with block coding to several types of paralleled channel models. One such model is that of non-white additive Gaussian noise. We are able to obtain upper and lower bounds on the exponent of the probability of error with an average power constraint on the transmitted signals. The upper and lower bounds agree at zero rate and for rates between a certain Rcrit and the capacity of the channel. The surprising result is that the appropriate bandwidth used for transmission depends only on the desired rate and not on the power or exponent desired over the range wherein the upper and lower bounds agree. We also consider the problem of several channels in parallel with the option of using separate coders and decoders on the parallel channels. We find that there are some cases in which there is a saving in coding and decoding equipment by coding for the parallel channels separately. We determine the asymptotic ratio of the optimum block-length for the parallel channels and analyze one specific coding scheme to determine the effect of rate and power distribution among the parallel channels.", "authors": ["P. M. Ebert"], "id": "e39acbeffb4dc5d7a45cdb889084d5be690fc8ce", "title": "Error bounds for parallel communication channels", "references": ["500380db6d60c33f0b92a41f851fa3f30abe488f", "faeefd748786d4edcb0128d9000e163fc270447b", "976b69ee6aad8c377479a808db770c60af3f6e5c", "b2b3c943eaa378b93dc22b7a4cf8f7c08844e8d3", "6862577e05c8f06e666d8cfc750c84678f996ea3", "87f90ec62d630426ed3ecc33d287031c9ca42b53"]}, {"date": "1962", "abstract": "Semantic Scholar extracted view of \"Decision-Making Processes In Pattern Recognition\" by George S. Sebestyen", "authors": ["George S. Sebestyen"], "id": "0217065021a8166e88dbcdf60f23f2222c6338bc", "title": "Decision-Making Processes In Pattern Recognition", "references": []}, {"date": "1966", "abstract": "We introduce a new distance measure which permits likelihood information to be used in algebraic minimum distance decoding techniques. We give an efficient decoding algorithm, and develop exponential bounds on the probability of not decoding correctly. In one application, this technique yields the same probability of error as maximum likelihood decoding.", "authors": ["G. David Forney"], "id": "4e54c231bd7f8e5cfbe93b578bb75d6faf852357", "title": "Generalized minimum distance decoding", "references": ["4bdf42b07d253fee3ec9359475302a60685b6476", "bc22d1610ce680c91b4323a1899b1f22cfdf533f", "83b3fac83062673c4a5529adfc66a43775df85e7", "98f2c2fdaf7502e8bcebcccf937f9e9e8a878ccb", "3ba43a0adc105ba81acbfd188ef48df40d3a8331", "7f10c78b3a5842c46eb17ae2a2eea4a88af42d33", "6b7a1c8caad357971434cd5dfd1b5b73d18c70a3"]}, {"date": "1966", "abstract": "Sequential decoding is a technique for encoding and decoding at moderate cost with a decoding reliability which approximates that of the optimum, and expensive, maximum-likelihood decoder. The several known sequential decoding algorithms enjoy a cost advantage over the maximum-likelihood decoder because they allow the level of the channel noise to regulate the level of the decoding computation. Since the average level of the required decoding computation for sequential decoders is small for source rates below a rate R comp , such a decoder can be realized for these rates with a relatively small logic unit and a buffer. The logic unit is normally designed to handle computation rates which are less than two or three times the average computation rate; the buffer serves to store data during those noisy periods when the required computation rate exceeds the computation rate of the logic unit. If the periods of high computation, which are caused by noise, are too frequent or too long, the buffer, which is necessarily finite in capacity, will fill and overflow. Since data are lost during an overflow, continuity in the decoding process cannot be maintained. The decoder, then, cannot continue to decode without error. For this reason, buffer overflow is an important event. In addition, since errors in the absence of overflow are much less frequent than are overflows themselves, the overflow event is of primary concern in the design of a sequential decoder. This paper presents some recent analytical results concerning the probability of a buffer overflow. In particular, it is shown that this probability is relatively insensitive to both the buffer capacity and the maximum speed of the logic unit for moderate capacities and speeds. By contrast, it is shown that the overflow probability decreases rapidly with a decrease in the source", "authors": ["John E. Savage"], "id": "9cceb260a10279fc20b93089c9140c601ac53dc5", "title": "Sequential decoding \u2014 the computation problem", "references": ["021319dd390c4fe9c36f0fc1ee4f3e15ed438c46", "67e6fb1d5d47585ca87cd7a6f01e96d71886ab67"]}, {"date": "1958", "abstract": "Semantic Scholar extracted view of \"An Optimum Character Recognition System Using Decision Functions\" by Ivan Flores", "authors": ["Ivan Flores"], "id": "685eb09b27dbaa301c8c6fc48fd6f7d1ec66ccc9", "title": "An Optimum Character Recognition System Using Decision Functions", "references": []}, {"date": "1956", "abstract": "Semantic Scholar extracted view of \"Large-Sample Theory: Parametric Case\" by Herman Chernoff", "authors": ["Herman Chernoff"], "id": "9dfb0027eb1219e714b874f1e6434136fc003f50", "title": "Large-Sample Theory: Parametric Case", "references": []}, {"date": "1956", "abstract": "Consider the problem of designing a machine to solve well-defined intellectual problems. We call a problem well-defined if there is a test which can be applied to a proposed solution. In case the proposed solution is a solution, the test must confirm this in a finite number of steps. If the proposed solution is not correct, we may either require that the test indicate this in a finite number of steps or else allow it to go on indefinitely. Since any test may be regarded as being performed by a Turing machine, this means that welldefined intellectual problems may be regarded as those of inverting functions and partial functions defined by Turing machines. Let fm(n) be the partial function computed by the m th Turing machine. It is not defined for a given value of n if the computation does not come to an end. This paper deals with the problem of designing a Turing machine which, when confronted by the number pair (m, r), computes as efficiently as possible a function g(m, r) such that fm(g(m, r)) = r. Again, for particular values of m and r no g(m, r) need exist. In fact, it has been shown that the existence of g(m, r) is an undecidable question in that there does not exist a Turing machine which will eventually come to a stop and print a 1 if g(m, r) does not exist. In spite of this, it is easy to show that a Turing machine exists which will", "authors": ["John McCarthy"], "id": "ff433838bb2b178e1d6b1f045b0215385e1757f5", "title": "The Inversion of Functions Defined by Turing Machines", "references": []}, {"date": "1963", "abstract": "Semantic Scholar extracted view of \"Probability Theory I\" by Michel Lo\u00e8ve", "authors": ["Michel Lo\u00e8ve"], "id": "f18307ebabf398bb7fd1b3375b2f09a7f9f6c5be", "title": "Probability Theory I", "references": []}, {"date": "1952", "abstract": "Semantic Scholar extracted view of \"A Measure of Asymptotic Efficiency for Tests of a Hypothesis Based on the sum of Observations\" by Herman Chernoff", "authors": ["Herman Chernoff"], "id": "bc22d1610ce680c91b4323a1899b1f22cfdf533f", "title": "A Measure of Asymptotic Efficiency for Tests of a Hypothesis Based on the sum of Observations", "references": []}, {"date": "1958", "abstract": "Semantic Scholar extracted view of \"On Asymptotically Efficient Consistent Estimates of the Spectral Density Function of a Stationary Time Series\" by Emanuel Parzen", "authors": ["Emanuel Parzen"], "id": "87cedbb32d574bf2e6abe4c4b78742053e42b2e8", "title": "On Asymptotically Efficient Consistent Estimates of the Spectral Density Function of a Stationary Time Series", "references": []}, {"date": "1967", "abstract": "Semantic Scholar extracted view of \"Distance measures and related criteria\" by Hideharu Kobayashi et al.", "authors": ["Hideharu Kobayashi", "John F. Thomas"], "id": "d150b9533bf510812a364094ca0c787a9e400ed2", "title": "Distance measures and related criteria", "references": []}, {"date": "1941", "abstract": "Semantic Scholar extracted view of \"A Short Method for Evaluating Determinants and Solving Systems of Linear Equations With Real or Complex Coefficients\" by Prescott D. Crout", "authors": ["Prescott D. Crout"], "id": "cad2bb2ae2b7709ba61cd49888ae053bee236f81", "title": "A Short Method for Evaluating Determinants and Solving Systems of Linear Equations With Real or Complex Coefficients", "references": []}, {"date": "1955", "abstract": "As contrasted with the conventional discussions of network synthesis which concern themselves with the analytic character of the network functions, this paper presents the algebraic topological considerations in the realisation of the driving point functions. Abstractly a network may be considered as a linear graph with certain \"weights\" attached to the elements of the graph. These weights may be either impedance or admittance functions of the elements. If the network contains no mutual inductances, the network functions are expressible simply in terms of the topology of this weighted graph. On the other hand if the function is specified in terms of these weights it should be possible to deduce the topology. The paper begins with the assumption that the given driving point admittance function Y_{d} (s) is expressed as a ratio of homogeneous polynomials V(y_i)/W(y_i) in the weights or the elementary admittance functions of resistors, inductors and capacitors. Just how the function Y_{d} (s) is to be expressed as V(y_{i})/W(y_{i}) is left as an unsolved problem. The properties of the polynomials V(y_{i} and W( y_{i} ) with regard to their realisability are the main results of the paper. A procedure for exhibiting the network from the realisable functions V(y_{i}) and W(y_{i }) is also given. Relationships between the vertex and circuit matrices of the network and the polynomials V(y_{i}) and W(y_{i}) are derived for this purpose. A generalization of Brune's theorem I and a sufficient criterion for a matrix to be a circuit matrix are two auxiliary results obtained. The value of the paper is primarily academic in providing results which are useful in a discussion of basic ideas and in suggesting problems in the rich and unexplored field of topological methods of network synthesis.", "authors": ["Sundaram Seshu"], "id": "88365ce55de9d60fa75ea3a4c0663e41b30341d4", "title": "Topological Considerations in the Design of Driving-Point Functions", "references": []}, {"date": "1952", "abstract": "Whereas the ear's sensitivity for detecting a difference in frequency between two tones is remarkably acute, the ability of listeners to identify (and name) tones presented in isolation is relatively poor. When the frequency of a single tone (in the frequency range 100\u20138000 cps) is varied in equal logarithmic steps (and when the sound level is arbitrarily varied to reduce loudness cues), the amount of information transferred is about 2.3 bits. (The equivalent proficiency of response for an informational transfer of 2.3 bits is perfect identification among only 5 tones.) The amount of information that can be transferred is, within rather wide limits, independent of the number of tones and the range of frequencies employed.", "authors": ["Irwin Pollack"], "id": "a546750bd366343354b745de9476d3cc52025a93", "title": "The Information of Elementary Auditory Displays. II", "references": []}, {"date": "1964", "abstract": "Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering, 1964.", "authors": ["Elwyn R. Berlekamp"], "id": "87f90ec62d630426ed3ecc33d287031c9ca42b53", "title": "Block coding with noiseless feedback", "references": []}, {"date": "1970", "abstract": "An efficient implementation of the Crout elimination method in solving large sparse systems of linear algebraic equations of arbitrary structure is described. A computer program, GNSO, by symbolic processing, generates another program, SOLVE, which represents the optimal reduced Crout algorithm in the sense that only nonzero elements are stored and operated on. The method presented is particularly powerful when a system of fixed sparseness structure must be solved repeatedly with different numerical values. In practical examples, the execution of SOLVE was observed to be typically N times as fast as that of the full Crout algorithm, where N is the order of the system.", "authors": ["Fred G. Gustavson", "Werner Liniger", "R. Willoughby"], "id": "60edb775ea89a73846da0723816be83d5069b0c0", "title": "Symbolic Generation of an Optimal Crout Algorithm for Sparse Systems of Linear Equations", "references": []}, {"date": "1955", "abstract": "Abstract : The study is closely related to one previously reported in that both deal with the influence of reinforcement on the incidence of clustering in the recall of randomly arranged words. Both employ the same hypotheses. They differ, however, in the techniques used for controlling reinforcement. The present study deals with differences in reinforcement during a pre-experimental phase whereas the earlier study undertook to manipulate reinforcement during the experimental phase. (Author)", "authors": ["Weston A. Bousfield", "Burton H. Cohen"], "id": "5eec4e79df13c612a02b01185e1f72143db81ae5", "title": "The Occurrence of Clustering in the Recall of Randomly Arranged Words of Different Frequencies-Of-Usage", "references": []}, {"date": "1950", "abstract": "Recent work by C. E. Shannon and others has led to an expression for the maximum rate at which information can be transmitted in the presence of random noise. Here two encoding schemes are described in which the ideal rate is approached when the signal length is increased. Both schemes are based upon drawing random numbers from a normal universe, an idea suggested by Shannon's observation that in an efficient encoding system the typical signal will resemble random noise. In choosing these schemes two requirements were kept in mind: (1) the ideal rate must be approached, and (2) the problem of computing the probability of error must be tractable. Although both schemes meet both requirements, considerable work has been required to put the expression for the probability of error into manageable form.", "authors": ["Stephen O. Rice"], "id": "976b69ee6aad8c377479a808db770c60af3f6e5c", "title": "Communication in the presence of noise \u2014 Probability of error for two encoding schemes", "references": []}, {"date": "1966", "abstract": "Let P1 and P2 be two probability measures on the same space and let 0 be the generalized Radon-Nikodym derivative of P2 with respect to P1. If C is a continuous convex function of a real variable such that the Pl-expectation (generalized as in Section 3) of C(+) provides a reasonable coefficient of the Pl-dispersion of 0, then this expectation has basic properties which it is natural to demand of a coefficient of divergence of P2 from P1. A general class of coefficients of divergence is generated in this way and it is shown that various available measures of divergence, distance, discriminatory information, etc., are members of this class.", "authors": ["Sami M. Ali", "S. D. Silvey"], "id": "77ce3697fc01e0bebba1dfe81cedb712b0b604a0", "title": "A General Class of Coefficients of Divergence of One Distribution from Another", "references": []}, {"date": "1957", "abstract": "Shannon's fundamental coding theorem for noisy channels states that such a channel has a capacity C, and that for any transmission rate R less than C it is possible for the receiver to use a received sequence of n symbols to select one of the 2 n R possible transmitted sequences, with an error probability Pe which can be made arbitrarily small by increasing n, keeping R and C fixed. Recently upper and lower bounds have been found for the best obtainable Pe as a function of C,R and n. This paper investigates this relationship for a modified decoding procedure , in which the receiver lists L messages, rather than one, after reception. In this case for given C and R, it is possible to choose L large enough so that the ratio of upper and lower bounds to the error probability is arbitrarily near to 1 for all large n. This implies that for large L, the average of all codes is almost as good as the best code, and in fact that almost all codes are almost as good as the best code.", "authors": ["Peter Elias"], "id": "500380db6d60c33f0b92a41f851fa3f30abe488f", "title": "List decoding for noisy channels", "references": ["bc22d1610ce680c91b4323a1899b1f22cfdf533f"]}, {"date": "1962", "abstract": "This is another in a series of invited tutorial, status and survey papers that are being regularly solicited by the PTGIT Committee on Special Papers. We invited Profess01 Fano to commit to paprr his elegant but, unelaborate explanation of the principles of sequential decoding, a scheme which is currently contending for a position as the most practical implementation to dale of Shannon\u2019s theory of noisy communication channels. -&e&l Pcqwrs Committw.", "authors": ["Robert M. PANOt"], "id": "67e6fb1d5d47585ca87cd7a6f01e96d71886ab67", "title": "A Heuristic Discussion of Probabilistic Decoding", "references": []}, {"date": "1962", "abstract": "A class of decoding algorithms using encoding-and-comparison is considered for error-correcting code spaces. Code words, each of which agrees on some information set for the code with the word r to be decoded, are constructed and compared with r . An operationally simple algorithm of this type is studied for cyclic code spaces A . Let A have length n , dimension k over some finite field, and minimal Hamming distance m . The construction of fewer than n^2/2 code words is required in decoding a word r . The procedure seems to be most efficient for small minimal distance m , but somewhat paradoxically it is suggested on operational grounds that it may prove most useful in those cases where m is relatively large with respect to the code length n .", "authors": ["Eugene Prange"], "id": "6b7a1c8caad357971434cd5dfd1b5b73d18c70a3", "title": "The use of information sets in decoding cyclic codes", "references": ["ff14c5d1dee9e55050e4de232e0562d607687158", "cc69ba18137b42eaa09a2924112697447f0031e6"]}, {"date": "1964", "abstract": "Semantic Scholar extracted view of \"Implementation of decoders for cyclic codes (Corresp.)\" by Luther D. Rudolph et al.", "authors": ["Luther D. Rudolph", "M. E. Mitchell"], "id": "3ba43a0adc105ba81acbfd188ef48df40d3a8331", "title": "Implementation of decoders for cyclic codes (Corresp.)", "references": []}, {"date": "1960", "abstract": "Semantic Scholar extracted view of \"THE PARETO-LEVY LAW AND THE DISTRIBUTION OF INCOME*\" by Benoit B. Mandelbrot", "authors": ["Benoit B. Mandelbrot"], "id": "021319dd390c4fe9c36f0fc1ee4f3e15ed438c46", "title": "THE PARETO-LEVY LAW AND THE DISTRIBUTION OF INCOME*", "references": ["68e0d7448473c8086e5780f5381a63cbe2cad2d0", "37ffcabb2b88599478a5d5125915efeef3ea1e70"]}, {"date": "1965", "abstract": "The Gorenstein-Zierler decoding algorithm for BCH codes is extended, modified, and analyzed; in particular, we show how to correct erasures as well as errors, exhibit improved procedures for finding error and erasure values, and consider in some detail the implementation of these procedures in a special-purpose computer.", "authors": ["G. David Forney"], "id": "98f2c2fdaf7502e8bcebcccf937f9e9e8a878ccb", "title": "On decoding BCH codes", "references": []}, {"date": "1957", "abstract": "In this paper we will develop certain extensions and refinements of coding theory for noisy communication channels. First, a refinement of the argument based on \u201crandom\u201d coding will be used to obtain an upper bound on the probability of error for an optimal code in the memoryless finite discrete channel. Next, an equation is obtained for the capacity of a finite state channel when the state can be calculated at both transmitting and receiving terminals. An analysis is also made of the more complex case where the state is calculable at the transmitting point but not necessarily at the receiving point.", "authors": ["Claude E. Shannon"], "id": "faeefd748786d4edcb0128d9000e163fc270447b", "title": "Certain Results in Coding Theory for Noisy Channels", "references": ["bc22d1610ce680c91b4323a1899b1f22cfdf533f", "6fc87d91d54e35fd106eb2f2935924d31b3cdf62", "3e57b0c954031c31bd8ff459b006af264e16c21d"]}, {"date": "1960", "abstract": "The present paper is a sequel to the paper \u201cOn a class of error-correcting binary group codes\u201d, by R. C. Base and D. K. Ray-Chaudhuri, appearing in Information and Control in which an explicit method of constructing a t -error correcting binary group code with n = 2 m \u2212 1 places and k = 2 m \u2212 1 \u2212 R(m,t) \u2267 2 m \u2212 1 \u2212 mt information places is given. The present paper generalizes the methods of the earlier paper and gives a method of constructing a t -error correcting code with n places for any arbitrary n and k = n \u2212 R(m,t) \u2267 [(2 m \u2212 1)/c] \u2212 mt information places where m is the least integer such that cn = 2 m \u2212 1 for some integer c . A second method of constructing t -error correcting codes for n places when n is not of the form 2 m \u2212 1 is also given.", "authors": ["R. C. Bose", "Dwijendra K. Ray-Chaudhuri"], "id": "4bdf42b07d253fee3ec9359475302a60685b6476", "title": "Further Results on Error Correcting Binary Group Codes", "references": ["09e511b46320d59d3013b6ed261f45c8ffe04db9", "00204d2e0af4abadc3c5e4a8ed423d9271e776f4"]}, {"date": "1964", "abstract": "A symmetry of a systematic code is a permutation of bit positions in each code word (the same permutation is applied to all code words) which preserves the code as a whole. Permutation decoding makes use of these symmetries to build up a decoding algorithm for the code. It is difficult to find an appropriate set of symmetries for a code picked at random. For cyclic codes the problem is somewhat easier, and for some special cyclic codes it is solved completely in this paper. For these codes, at least, it is evident that permutation decoding is easy to implement and inexpensive compared with other decoding schemes. Permutation decoding as a n eans of error control is evaluated for the binary symmetric channel and for the switched telephone network as represented by experimental data. It is found to be extremely effective on the binary symmetric channel and of very doubtful value on the present telephone network.", "authors": ["Jessie Macwilliams"], "id": "83b3fac83062673c4a5529adfc66a43775df85e7", "title": "Permutation decoding of systematic codes", "references": []}, {"date": "1963", "abstract": "Semantic Scholar extracted view of \"Low Density Parity Check Codes\" by R. G. Galager", "authors": ["R. G. Galager"], "id": "a538bc641f7af52d29a6dab9e53ce42231f99081", "title": "Low Density Parity Check Codes", "references": []}, {"date": "1955", "abstract": "Semantic Scholar extracted view of \"Limit Distributions for Sums of Independent Random Variables.\" by Dennis V. Lindley et al.", "authors": ["Dennis V. Lindley", "Boris Gnedenko", "Aleksey N. Kolmogorov", "Kai Lai Chung"], "id": "ee361838116550043733e900bf93d9a1b4c71c82", "title": "Limit Distributions for Sums of Independent Random Variables.", "references": []}, {"date": "1954", "abstract": "This experiment measured confusion contours for 58 standard colors distributed throughout the CIE constant-luminance diagram. Matches were made to these standards from an assortment of 342 heterogeneous colors arranged in a display which presented 171 of them simultaneously. The viewing conditions approximate those found in certain complex display situations. Twenty subjects were used in the experiment. Each subject was required to indicate those colors on the display board which provided satisfactory matches for each of the standard colors. Contours on the CIE diagram are drawn to show the percentage of times various chromaticities were confused. From these contours we have selected colors which can be discriminated with high levels of accuracy and so are suitable for coding qualitative and quantitative information. In general, the contours follow trends suggested by extrapolation from precise threshold data.", "authors": ["Rita M. Halsey", "Alphonse Chapanis"], "id": "352a7df7e022c1760b496b6b3e1d39a8369ad5c4", "title": "Chromaticity-Confusion Contours in a Complex Viewing Situation*", "references": []}, {"date": "1951", "abstract": "Abstract : An experiment was conducted to determine the accuracy with which a subject can determine the value of a continuous variable when only discrete values representing ranges of the continuous variable are presented.", "authors": ["Harold W. Hake", "William R. Garner"], "id": "6bda9f1be3714d2a9866d4714b7f20e15c5a6e74", "title": "The effect of presenting various numbers of discrete steps on scale reading accuracy.", "references": []}, {"date": "1959", "abstract": "A study is made of coding and decoding systems for a continuous channel with an additive gaussian noise and subject to an average power limitation at the transmitter. Upper and lower bounds are found for the error probability in decoding with optimal codes and decoding systems. These bounds are close together for signaling rates near channel capacity and also for signaling rates near zero, but diverge between. Curves exhibiting these bounds are given.", "authors": ["Claude E. Shannon"], "id": "6862577e05c8f06e666d8cfc750c84678f996ea3", "title": "Probability of error for optimal codes in a Gaussian channel", "references": []}, {"date": "1954", "abstract": "A given amount of information may be incorporated within elementary auditory displays by utilizing a small number of finely subdivided stimulus dimensions or by utilizing a relatively larger number of crudely subdivided stimulus dimensions. Experimental interest to date has been confined largely to the former type of display; the present study considers the latter type. Specifically, the informational transmission with elementary auditory displays of a large number (6\u20138) stimulus dimensions was investigated. In general, nearly perfect identification was obtained with skilled listeners when each dimension was crudely subdivided into two alternative states. Finer subdivision of each dimension does not produce a proportional gain in information transmission with the display.", "authors": ["Irwin Pollack", "Lawrence Ficks"], "id": "28628e1a8bf31402113cef0e87fef9c23c1c4d3e", "title": "The Information of Elementary Multidimensional Auditory Displays", "references": []}, {"date": "1955", "abstract": "Semantic Scholar extracted view of \"Absolute judgments as a function of stimulus range and number of stimulus and response categories.\" by Charles W. Eriksen et al.", "authors": ["Charles W. Eriksen", "Harold W. Hake"], "id": "5e73f775ea677849942c2b299dffa75f69cfca42", "title": "Absolute judgments as a function of stimulus range and number of stimulus and response categories.", "references": []}, {"date": "1955", "abstract": "Determined discrimination accuracy using the method of absolute judgment for a series of stimuli varying along the single dimensions of size, hue and brightness. These measures were compared with measures obtained when the stimuli varied on several dimensions simultaneously. It was found that discriminability for a multidimensional series of stimuli was considerably greater than that obtained for any of the compounding dimensions used alone. Also showed that the discrimination accuracy for a compounded series of stimuli could be predicted with reasonable accuracy if the discrimination accuracy of the compounding dimensions is known. Language: en", "authors": ["Charles W. Eriksen", "Harold W. Hake"], "id": "5322a485d71098e245e4c7c36cfdb7f5fce2858c", "title": "Multidimensional stimulus differences and accuracy of discrimination.", "references": []}, {"date": "1956", "abstract": "A class of binary signaling alphabets called \u201cgroup alphabets\u201d is described. The alphabets are generalizations of Hamming's error correcting codes and possess the following special features: (1) all letters are treated alike in transmission; (2) the encoding is simple to instrument; (3) maximum likelihood detection is relatively simple to instrument; and (4) in certain practical cases there exist no better alphabets. A compilation is given of group alphabets of length equal to or less than 10 binary digits.", "authors": ["David S. Slepian"], "id": "ff14c5d1dee9e55050e4de232e0562d607687158", "title": "A class of binary signaling alphabets", "references": []}, {"date": "1954", "abstract": "Semantic Scholar extracted view of \"Noise and stochastic processes\" by Nelson Wax", "authors": ["Nelson Wax"], "id": "37ffcabb2b88599478a5d5125915efeef3ea1e70", "title": "Noise and stochastic processes", "references": []}, {"date": "1949", "abstract": "Suppose that there are two collections or groups of objects-coins, trees, beans, or aircraft-and we do not know how many objects there are. Suppose further that for some reason we cannot count the number of objects in either group. Still, some property of each group makes it possible for a person to say that one of these groups is greater-than, lessthan, or equal-to the other group. It is this property of a collection of objects that we define as numerousness.l We might say that numerousness is that property of a group of objects which we can discriminate, without counting, under instruction to judge how many objects the group contains. We shall wish to modify this definition later as a result of the experiments reported in this paper, but it is adequate for the present discussion of the problem. The judgment of 'numerousness' may be made in several different ways: (a) it may be comparative-more numerous or less numerous, larger or smaller, etc.; (b) or it may be 'absolute.' There is one special form that the absolute judgment of numerousness can take. It is called the direct reporting of number. In this method of reporting, a numeral is assigned to represent how many things there are in any given collection of objects. After a brief look-so brief that counting is impossible-we say 10, 23, or 250 to indicate that we estimate that the group contained 10, 23, or 250 members.", "authors": ["Eugenia Kaufman", "My Lord"], "id": "2e8136581d934e18eb17d124f0224a32b01272d5", "title": "The discrimination of visual number.", "references": []}, {"date": "1956", "abstract": "Semantic Scholar extracted view of \"Income of the American People.\" by Peter Otto Steiner et al.", "authors": ["Peter Otto Steiner", "Herman Phillip Miller"], "id": "68e0d7448473c8086e5780f5381a63cbe2cad2d0", "title": "Income of the American People.", "references": []}, {"date": "1963", "abstract": "General expressions for the probability of word error for several encoding/detection schemes using group codes are derived. Correlation detection, digital decoding, straight Wagner codes, \"Wagnerized\" codes, and direct transmission are covered. Specific results are given for each in the case where the additive channel noise is white and Gaussian. A good indication of the relative amounts of power required by two schemes operating at the same error rate is available from an asymptotic relationship as the signal-to-noise ratio approaches infinity. Numerical results are presented for three easily implemented codes.", "authors": ["C. M. Hackett"], "id": "7f10c78b3a5842c46eb17ae2a2eea4a88af42d33", "title": "Word error rate for group codes detected by correlation and other means", "references": []}, {"date": "1955", "abstract": "It is shown that for any noisy channel without memory having only a finite number of received signals, the error in transmitting information at a rate H and using uniformly good codes of length n is bounded by an expression Fe^{-Bn(C-H)^2} where F and B are constants depending upon the channel parameters but not upon H or n .", "authors": ["Amiel Feinstein"], "id": "6fc87d91d54e35fd106eb2f2935924d31b3cdf62", "title": "Error bounds in noisy channels without memory", "references": []}, {"date": "1956", "abstract": "Semantic Scholar extracted view of \"Fundamental Concepts of Higher Algebra\" by Anthony A. Albert", "authors": ["Anthony A. Albert"], "id": "cc69ba18137b42eaa09a2924112697447f0031e6", "title": "Fundamental Concepts of Higher Algebra", "references": []}, {"date": "1963", "abstract": "\u00a9 The British Computer Society Issue Section: Articles Download all figures A powerful iterative descent method for finding a local minimum of a function of several variables is described. A number of theorems are proved to show that it always converges and that it converges rapidly. Numerical tests on a variety of functions confirm these theorems. The method has been used to solve a system of one hundred non-linear simultaneous equations. Related articles in Web of Science", "authors": ["R. Fletcher", "M. J. D. Powell"], "id": "52d44a7937d076ea82726911cb79ce76e6597e40", "title": "A Rapidly Convergent Descent Method for Minimization", "references": ["efa32848ae69aba45d37e02b872272b87e36d020", "8fd6a3faa76b6a7dbcb63b599dbd07eb5a98aba9", "b1afd5740cdb03295f14e6c993b8d36844956dce", "017ddb7e815236defd0566bc46f6ed8401cc6ba6", "5fbca39db49ed2158cea2ae79d99b7634ab06524", "466daddfb6340c28cb8da548007028c8cc5df687", "c1d66a6d27efabbaaa1005a860d220824c1eaaec", "294ef41979de001d4eadb5ec3578e60a43010f93", "b2f499747eb3c6da7bc400950d12833f8965c468"]}, {"date": "1956", "abstract": "The zero error capacity C_o of a noisy channel is defined as the least upper bound of rates at which it is possible to transmit information with zero probability of error. Various properties of C_o are studied; upper and lower bounds and methods of evaluation of C_o are given. Inequalities are obtained for the C_o relating to the \"sum\" and \"product\" of two given channels. The analogous problem of zero error capacity C_oF for a channel with a feedback link is considered. It is shown that while the ordinary capacity of a memoryless channel with feedback is equal to that of the same channel without feedback, the zero error capacity may be greater. A solution is given to the problem of evaluating C_oF .", "authors": ["Claude E. Shannon"], "id": "3e57b0c954031c31bd8ff459b006af264e16c21d", "title": "The zero error capacity of a noisy channel", "references": []}, {"date": "1969", "abstract": "Semantic Scholar extracted view of \"Application of sparse matrix methods in electric power system analysis\" by Alexander T. Chang", "authors": ["Alexander T. Chang"], "id": "1ec0eab9cd3c8980e906d2a8e8e4b3afc05f9ae7", "title": "Application of sparse matrix methods in electric power system analysis", "references": []}, {"date": "1969", "abstract": "Abstract : The authors empirically compared ten pivot selection rules for representing the inverse of a sparse basis in triangularized product form. On examples drawn from actual applications, one of the rules yield inverses that were only slightly less sparse than the original basis. The rule was used in the M5 mathematical programming system and has resulted in substantial reduction in running time.", "authors": ["George B. Dantzig", "R. P. Harvey", "R. D. McKnight", "Stanley S. Smith"], "id": "111af1a955ee6cb44f504d36602c00f937541df4", "title": "SPARSE MATRIX TECHNIQUES IN TWO MATHEMATICAL PROGRAMMING CODES", "references": []}, {"date": "1960", "abstract": "Bose and Ray-Chaudhuri have recently described a class of binary codes which for arbitrary m and t are t -error correcting and have length 2^m - 1 of which no more than mt digits are redundancy. This paper describes a simple error-correction procedure for these codes. Their cyclic structure is demonstrated and methods of exploiting it to implement the coding and correction procedure using shift registers are outlined. Closer bounds on the number of redundancy digits are derived.", "authors": ["W. Wesley Peterson"], "id": "09e511b46320d59d3013b6ed261f45c8ffe04db9", "title": "Encoding and error-correction procedures for the Bose-Chaudhuri codes", "references": []}, {"date": "1960", "abstract": "A general method of constructing error correcting binary group codes is obtained. A binary group code with n places, k of which are information places is called an ( n , k ) code. An explicit method of constructing t -error correcting ( n , k ) codes is given for n = 2 m \u22121 and k = 2 m \u22121\u2212R( m , t ) \u2267 2 m \u22121\u2212 mt where R ( m , t ) is a function of m and t which cannot exceed mt . An example is worked out to illustrate the method of construction.", "authors": ["R. C. Bose", "Dwijendra K. Ray-Chaudhuri"], "id": "00204d2e0af4abadc3c5e4a8ed423d9271e776f4", "title": "On A Class of Error Correcting Binary Group Codes", "references": ["ff4324c1bb61c3903ea0d94701549a45900fdbd9", "502ef871471691a5ac631258f3483221d8d7f1c0", "8336c536d4e4166ea978b5d849f85dfc415ae6f7"]}, {"date": "1967", "abstract": "This is a tutorial paper which highlights the influence of the computer not only on the modus operandi of circuit design but also on network theory itself. It reviews the topological properties of linear graphs and describes a matrix-topological formulation of the network problem. In addition to the classical mesh, node, and outset methods, a mixed method of analysis is described which is applicable to d-c, a-c, and transient problems.\n Numerical methods of solving linear and non-linear d-c network problems are discussed and a new approach to a-c analysis, using the mixed method and a numerical solution of the matrix eigenvalue problem, is described. The extension of this method to the transient analysis of linear networks is also explained. Finally, the problem of instability in the numerical integration of differential equations is discussed and several means of solving the problem are outlined.", "authors": ["Franklin H. Branin"], "id": "9ca2e2a6e8bb8bd1f72768081fdd0d6f6109ed40", "title": "Computer methods of network analysis", "references": []}, {"date": "1965", "abstract": "Theoretical background Perturbation theory Error analysis Solution of linear algebraic equations Hermitian matrices Reduction of a general matrix to condensed form Eigenvalues of matrices of condensed forms The LR and QR algorithms Iterative methods Bibliography Index.", "authors": ["James Hardy Wilkinson"], "id": "4713a6bea2d6e55c0a88e90fa0b25f60a4e6faf3", "title": "The algebraic eigenvalue problem", "references": []}, {"date": "1957", "abstract": "It is common for matrices in industrial applications of linear programming to have a large proportion of zero coefficients. While every item raw material, intermediate material, end item, equipment item in, say, a petroleum refinery may be indirectly related to every other, any particular process uses few of these. Thus the matrix describing petroleum technology has a small percentage of non-zeros. If spacial or temporal distinctions are introduced into the model the percentage of non-zeros generally falls further. \n \nThe present paper discusses a form of inverse which is especially convenient to obtain and use for matrices with a high percentage of zeros. The application of this form of inverse in linear programming is also discussed.", "authors": ["Harry M. Markowitz"], "id": "5c7c2b1a9234d3116926bd1bfcfcb2a45b7cbfca", "title": "The Elimination form of the Inverse and its Application to Linear Programming", "references": []}, {"date": "1976", "abstract": "Computer technology has had a limited success in producing useful business applications. Management systems seldom meet users'' requirements, are often inappropriate to an application, and are frequently abandoned. But why? Business lacks expertise in the application of computers. Managers who are expert in solving business problems find it difficult to specify formal procedures for the solution of these problems. It is not surprising that programmers who work from poorly defined specifications produce poorly written software. The computer industry has provided only limited support in business applications. Application packages are seldom appropriate to a problem and are often misapplied. Misapplication is the principle reason for their failure in practice situations. Improvements are certainly possible. The manager could be supplied with a system that assists in the design of an application. This system could help the manager specify his requirements by providing him with a framework for thinking about issues relevant to the design of a particular application. The manager might then be better equipped to select a commercial package or to guide in the design of his implementation. This thesis describes a prototype version of such a system. PROCTOR is a program that assists in the design of a hierarchical planning and control system for a procurement firm. PROCTOR is implemented as an \"unstructured\" questionnaire. It guides the user in investigating various aspects of a problem while giving him complete freedom in deciding how and when to supply answers to questions. It allows him to change and skip answers whenever he desires. PROCTOR is implemented in OWL, a system for representing and processing conceptual knowledge. It uses the OWL data base to represent procedures for the questionnaire and to store data accumulated during the iteration. This representation makes possible the presentation of an English-like problem description, various evaluations and the reasons for the evaluations.", "authors": ["Michael Bosyj"], "id": "694d6894a9223144e967f2511d327260632b6995", "title": "A PROGRAM FOR THE DESIGN OF PROCUREMENT SYSTEMS", "references": []}, {"date": "1974", "abstract": "This paper describes BUILD, a computer program which generates plans for building specified structures out of simple objects such as toy blocks. A powerful heuristic control structure enables BUILD to use a number of sophisticated construction techniques in its plans. Among these are the incorporation of pre-existing structure into the final design, pre-assembly of movable sub-structures on the table, and the use of extra blocks as temporary supports and counterweights in the course of the construction. \n \nBUILD does its planning in a modeled 3-space in which blocks of various shapes and sizes can be represented in any orientation and location. The modeling system can maintain several world models at once, and contains modules for displaying states, testing them for inter-object contact and collision, and for checking the stability of complex structures involving frictional forces. \n \nSuggestions are included for the extension of BUILD-like systems to other domains. Also discussed are the merits of BUILD's implementation language, conniver, for this type of problem solving.", "authors": ["Scott E. Fahlman"], "id": "2cfe1adca4b3fd0b20eb37260d7013303f112111", "title": "A Planning System for Robot Construction Tasks", "references": []}, {"date": "1975", "abstract": "The notion of a commonsense algorithm is presented as a basic data structure for modeling human cognition. This data structure unifies many current ideas about human memory and information processing. The structure is defined by specifying a set of proposed cognitive primitive links which, when used to build up large structures of actions, states, statechanges and tendencies, provide an adequate formalism for expressing human plans and activities, as well as general mechanisms and computer algorithms. The commonsense algorithm is a type of framework (as Minsky has defined the term) for representing algorithmic processes, hopefully the way humans do.", "authors": ["Chuck Rieger"], "id": "a0adea7988254f3d0740b587334c8ca6357cdd8b", "title": "The commonsense algorithm as a basis for computer models of human memory, inference, belief and contextual language comprehension", "references": []}, {"date": "1978", "abstract": "This paper reports on the initial design and partial implementation of an interactive programming environment to be used by expert programmers. The system is based on three forms of program description: 1) definition of structured data objects, their parts, properties, and relations between them, 2) input\u2013output specification of the behavior of program segments, and 3) a hierarchical representation of the internal structure of programs (plans). The plan representation is of major theoretical interest because it includes not only data flow and control flow relationships between subsegments of a program, but also goal-subgoal, prerequisite, and other logical dependencies between the specifications of the subsegments. Plans are utilized both for describing particular programs and in the compilation of a knowledge base of more abstract knowledge about programming, such as the concept of a loop and various specializations, such as enumeration loops and search loops. We also describe a deductive system which can verify the correctness of plans involving side effects on complex data with structure sharing.", "authors": ["Charles Rich", "Howard E. Shrobe"], "id": "01da4c5f9486e850cb4f112a9c059a87ad16d3ed", "title": "Initial Report on a Lisp Programmer's Apprentice", "references": ["006ae42f9e23ae43afff97082a484de2443c616f", "6321686427c86b87e1071497ffd633b71aad6fb6", "23174bbbc77874d0ebee7b8c2719037d86993170", "5a061b1cab0f241d6f7226f6c0b12e931cabd90f", "47330daa19a95f093b7628e325bee238439abc40", "7d06bf84338e89456f609896de4e41f61086d98e", "bf15ce3d1575d124527496cb249dc1249eee0acb", "17fe58e6115711ce4d5ceef941c60eb6d6898dcf", "d22000e27a064baa027fa685353abb216b0d06da", "92dfbf02fc68df56b7ae3b27188b0314797d072e"]}, {"date": "1975", "abstract": "A computer program is described which operates on a subset of plane geometry. Its performance not only compares favorably with previous computer programs, but within its limited problem domain (e.g., no curved lines nor introduction of new points), it also invites comparison with the best human theorem provers. The program employs a combination of forward and backward chaining with the forward component playing the more important role. This, together with a deeper use of diagrammatic information, allows the program to dispense with the diagram filter in contrast with its central role in previous programs. An important aspect of human problem solving may be the ability to structure a problem space so that forward chaining techniques can be used effectively.", "authors": ["Arthur J. Nevins"], "id": "b62608c716819965f2755759ce3a7edb8a93829f", "title": "Plane Geometry Theorem Proving Using Forward Chaining", "references": []}, {"date": "1981", "abstract": "Common sense reasoning about the physical world must include an understanding of physical processes and the changes they cause. For example, heating a liquid causes its temperature to rise and if continued long enough may cause it to boil. A style of analysis is presented that combines deKleer's Incremental Qualitative analysis wjth the Quantity Space idea from Naive Physics to reason about the effects of physical processes and their limits. The analysis is demonstrated on an example with practical importance, and further possibilities for applications are discussed.", "authors": ["Kenneth D. Forbus"], "id": "6813e5400681a1704c4c4aef2cb7a805fa99c30b", "title": "Qualitative Reasoning about Physical Processes", "references": ["bef38d9ea1040ef0b6a53a7b98a922ba9b4e13e8"]}, {"date": "1962", "abstract": "Eighteen months ago Rosenbrock (1960) published a paper in this journal on finding the greatest or least value of a function of several variables. A number of methods were listed and they all have first-order convergence. Six months ago Martin and Tee (1961) published a paper in which they mentioned gradient methods which have second-order convergence for finding the minimum of a quadratic positive definite function. In this paper will be described an iterative method which is not unlike the conjugate gradient method of Hestenes and Stiefel (1952), and which finds stationary values of a general function. It has second-order convergence, so near a stationary value it converges more quickly than Rosenbrock's variation of the steepest descents method and, although each iteration is rather longer because the method is applicable to a general function, the rate of convergence is comparable to that of the more powerful of the gradient methods described by Martin and Tee.", "authors": ["M. J. D. Powell"], "id": "b2f499747eb3c6da7bc400950d12833f8965c468", "title": "An Iterative Method for Finding Stationary Values of a Function of Several Variables", "references": []}, {"date": "1982", "abstract": "A common disclaimer by an AI author is that he has neglected temporal considerations to avoid complication. The implication is nearly made that adding a temporal dimension to the research (on engineering, medical diagnosis, etc.) would be a familiar but tedious exercise that would obscure the new material presented by the author. Actually, of course, no one has ever dealt with time correctly in an AI program, and there is reason to believe that doing it would change everything. Because time has been neglected, medical diagnosis programs cannot talk about the course of a disease. Story understanding programs have trouble with past events. Problem solvers have had only the crudest models of the future, in spite of the obvious importance of future events. Many researchers have compensated by modeling the course of external time with the program\u2019s own internal time, changing the world model to reflect changing reality. This leads to a confusion between correcting a mistaken belief and updating an outdated belief. Most AI data bases have some sort of operator for removing formulas. (e.g., ERASE in PLANNER, Hewitt, 1972) This operator has tended to be used for two quite different purposes: getting rid of tentative or hypothetical assertions that turned out not to be true, and noting that an assertion is no longer true. The confusion is natural, since some of the same consequences must follow in either case. For example, if \u2018\u2018The car is drivable\u2019\u2019 follows from \u2018\u2018There is gas in the car,\u2019\u2019 then the former statement must be deleted when the latter is, whether you have discovered there to be no gas after all, or the gas has been used up. But in many cases, the two behave quite differently, and efforts to make them the same have resulted in awkward, inextensible programs. For example, from \u2018\u2018x is beating his wife,\u2019\u2019 you are entitled to infer, \u2018\u2018x is a bad man.\u2019\u2019 But if x pauses to catch his breath, only the former statement must be deleted from the data base. Clearly, the proper inference is from \u2018\u2018If x has beat his wife recently, he is a bad man,\u2019\u2019 and \u2018\u2018x is beating his wife,\u2019\u2019 to \u2018\u2018For the next year or so, x will have beaten his wife recently,\u2019\u2019 and hence to", "authors": ["Drew McDermott"], "id": "5fc326b249b73b8050ef5cda3243c466b733ff2b", "title": "A Temporal Logic for Reasoning About Processes and Plans", "references": ["9b19b116dfb01a59c5bcc0fe7f792162a2cb3614", "6aa3a9605c073b6d5f925c08136e0ef352293abd", "6a6bfc2d27fe3c3a88d8fd5c21d9c3e2cc93ccaf", "d7d927cda381864aeaf4f4d2aa15d5ec05ffcdee", "3cb699fddc1018222e58b9bddc4bb98ca94a74b2", "372a0d798b623408624643ff72bb1f7107f082bf", "93bdca51c9c0477121ba9708ffe2747855b93aef", "0dc8b5d3452ddfd26b998cd923f4c46cdd80faca", "a0adea7988254f3d0740b587334c8ca6357cdd8b", "e97795382386ecd24300f3a6449ed5732b200bfa"]}, {"date": "1962", "abstract": "The LISP language is designed primarily for symbolic data processing used for symbolic calculations in differential and integral calculus, electrical circuit theory, mathematical logic, game playing, and other fields of artificial intelligence.The manual describes LISP, a formal mathematical language. LISP differs from most programming languages in three important ways. The first way is in the nature of the data. In the LISP language, all data are in the form of symbolic expressions usually referred to as S-expressions, of indefinite length, and which have a branching tree-type of structure, so that significant subexpressions can be readily isolated. In the LISP system, the bulk of the available memory is used for storing S-expressions in the form of list structures. The second distinction is that the LISP language is the source language itself which specifies in what way the S-expressions are to be processed. Third, LISP can interpret and execute programs written in the form of S-expressions. Thus, like machine language, and unlike most other high level languages, it can be used to generate programs for further executions.", "authors": ["John McCarthy"], "id": "83f054294ba2726d02aa03e471da773c3383b146", "title": "LISP 1.5 Programmer's Manual", "references": []}, {"date": "1977", "abstract": "A theory of cause-effect representation is used to describe man-made mechanisms and natural laws. The representation, consisting of 10 link types that interconnect events into large declarative patterns, is illustrated on a relatively sophisticated device, the home gas forced air furnace, Next, a procedure and framework for translating the declarative description of a mechanism into a population of associatively triggerable computation units is described. The associative, of procedural, form can then be used to perform a discrete cause-effect simulation of the device. The declarative to procedural translation, including a simulation trace, is shown for the furnace. Topics of mechanism abstraction and mechanism invention are discussed, and the entire \"Mechanisms Laboratory\" is placed in the larger perspective of our research into human problem solving.", "authors": ["Chuck Rieger", "Maurice Grinberg"], "id": "18ade4f5a83e3b1915a3d8d15f092af6ee63c9f9", "title": "The Declarative Representation and Procedural Simulation of Causality in Physical Mechanisms", "references": ["e494e85bda74b856f20eee75bd274ea74e4861aa", "0dc8b5d3452ddfd26b998cd923f4c46cdd80faca", "7cfee4257cd03c70151af5ba71915b1d0f608484", "b5aea3ab2364c257835ae2c9d961aa8956fa8eda"]}, {"date": "1957", "abstract": "Numerical MethodsBy Dr. R. A. Buckingham. Pp. xii + 597. (London: Sir Isaac Pitman and Sons, Ltd., 1957.) 70s. net.", "authors": ["A. D. Booth"], "id": "294ef41979de001d4eadb5ec3578e60a43010f93", "title": "Numerical Methods", "references": []}, {"date": "1947", "abstract": "IT is now twenty years since the theory of quantum mechanics was founded, and not much less since the first edition of Dirac's book was published. Ever since, it has been a classic of scientific literature.The Principles of Quantum MechanicsBy Prof. P. A. M. Dirac. (International Series of Monographs on Physics.) Third edition. Pp. xii + 312. (Oxford: Clarendon Press; London: Oxford University Press, 1947.) 25s. net.", "authors": ["Walter Heitler"], "id": "5fbca39db49ed2158cea2ae79d99b7634ab06524", "title": "The Principles of Quantum Mechanics", "references": []}, {"date": "", "abstract": "IN many ways this book will prove a useful companion to treatises already available; especially, perhaps, on account of the large number of examples which it contains, and the hints for their solution. It may be confidently asserted that no example in group-theory is too elementary to be useful; the subject is on one side so very abstract, while on the other the individual properties of groups are numerous, and the protean disguises of the same group are amazingly varied.An Introduction to the Theory of Groups of Finite Order.By H. Hilton. Pp. xii + 236. (Oxford: Clarendon Press, 1908.) Price 14s. net.", "authors": ["G. B. M."], "id": "8336c536d4e4166ea978b5d849f85dfc415ae6f7", "title": "An Introduction to the Theory of Groups of Finite Order", "references": []}, {"date": "1937", "abstract": "Semantic Scholar extracted view of \"Introduction to the theory of groups of finite order\" by R. D. Carmichael", "authors": ["R. D. Carmichael"], "id": "502ef871471691a5ac631258f3483221d8d7f1c0", "title": "Introduction to the theory of groups of finite order", "references": []}, {"date": "1961", "abstract": "In dealing with numerical problems for which classical methods of solution are unfeasible, many people have tried various procedures of searching for an answer on a computer. Our efforts in this direction have produced procedures which seem to have had (for us and for others who have used them) more success than has been achieved elsewhere, so that we have been encouraged to publish this report of our studies. We use the phrase \"direct search\" to describe sequential examination of trial solutions involving comparison of each trial solution with the \"best\" obtained up to that time together with a strategy for determining (as a function of earlier results) what the next trial solution will be. The phrase implies our preference, based on experience, for straightforward search strategies which employ no techniques of classical analysis except where there is a demonstrable advantage in doing so. We have found it worthwhile to study direct search methods for the following reasons: (a) They have provided solutions to some problems, of importance to us, which had been unsuccessfully attacked by classical methods. (Examples are given below.) (b) They promise to provide faster solutions for some problems that are solvable by classical methods. (For example, a method for solving systems of linear equations, proposed in Section 5, seems to take an amount of time that is proportional only to the first power of the number of equations.) (c) They are well adapted to use on electronic computers, since they tend to use repeated identical arithmetic operations with a simple logic. Classical methods, developed for human use, often stress minimization of arithmetic by increased sophistication of logic, a goal which may not be desirable when a computer is to be used. (d) They provide an approximate solution, improving all the while, at all stages of the calculation. This feature can be important when a tentative solution is needed before the calculations are completed. (e) They require (or permit) different kinds of assumptions about the functions involved in various problems, and thus suggest new classifications of functions which may repay study. Direct search is described roughly in Section 2, and explained heuristically in Section 3. Section 4 describes a kind of strategy. Sections 5 and 6 describe", "authors": ["Robert Hooke", "T. A. Jeeves"], "id": "c1d66a6d27efabbaaa1005a860d220824c1eaaec", "title": "`` Direct Search'' Solution of Numerical and Statistical Problems", "references": []}, {"date": "1952", "abstract": "An iterative algorithm is given for solving a system Ax=k of n linear equations in n unknowns. The solution is given in n steps. It is shown that this method is a special case of a very general method which also includes Gaussian elimination. These general algorithms are essentially algorithms for finding an n dimensional ellipsoid. Connections are made with the theory of orthogonal polynomials and continued fractions.", "authors": ["Magnus R. Hestenes", "Eduard Stiefel"], "id": "466daddfb6340c28cb8da548007028c8cc5df687", "title": "Methods of conjugate gradients for solving linear systems", "references": ["acdde7735330da19ba951b338f0917ed1c45145a"]}, {"date": "1950", "abstract": "The author was led to the study given in this paper from a consideration of large scale computing machines in which a large number of operations must be performed without a single error in the end result. This problem of \u201cdoing things right\u201d on a large scale is not essentially new; in a telephone central office, for example, a very large number of operations are performed while the errors leading to wrong numbers are kept well under control, though they have not been completely eliminated. This has been achieved, in part, through the use of self-checking circuits. The occasional failure that escapes routine checking is still detected by the customer and will, if it persists, result in customer complaint, while if it is transient it will produce only occasional wrong numbers. At the same time the rest of the central office functions satisfactorily. In a digital computer, on the other hand, a single failure usually means the complete failure, in the sense that if it is detected no more computing can be done until the failure is located and corrected, while if it escapes detection then it invalidates all subsequent operations of the machine. Put in other words, in a telephone central office there are a number of parallel paths which are more or less independent of each other; in a digital machine there is usually a single long path which passes through the same piece of equipment many, many times before the answer is obtained.", "authors": ["Richard Wesley Hamming"], "id": "ff4324c1bb61c3903ea0d94701549a45900fdbd9", "title": "Error detecting and error correcting codes", "references": []}, {"date": "1977", "abstract": "Brief Statement of the Problem: Stated most generally, the proposed research is concerned with understanding and representing the teleological structure of engineered devices. More specifically, I propose to study the teleological structure of computer programs written in LISP which perform a wide range of non-numerical computations. The major theoretical goal of the research is to further develop a formal representation for teleological structure, called plans, which will facilitate both the abstract description of particular programs, and the compilation of a library of programming expertise in the domain of non-numerical computation. Adequacy of the theory will be demonstrated by implementing a system (to eventually become part of a LISP Programmer's Apprentice) which will be able to recognize various plans in LISP programs written by human programmers and thereby generate cogent explanations of how the programs work, including the detection of some programming errors. Working Papers are informal papers intended for internal use.", "authors": ["Charles Rich"], "id": "d22000e27a064baa027fa685353abb216b0d06da", "title": "Plan Recognition in a Programmer's Apprentice", "references": ["cf213e9ac5f8445a1f1c8a37fd03cf6bb68c42de", "816e9b69c6adb73205e841be8e69b3651d3bfbbf", "f5e796dc458978f1a01d1b1698636df9c5181934", "59e5c4c530e996e2ec75ac88a0402e87ce85370e", "a3b7d7c4ef3ee8bf775e9f2bc13f469512b6a3a9", "e64db9777a991f55029bd13c9d03741fa2b17046", "df12f998fa2160c2b04fce5aea143052a6b1f837", "02256c909265b66403b6cb08102174aa0b6ede1d"]}, {"date": "1976", "abstract": "This paper explains, in an introductory fashion, the method of specifying the correct behavior of a program by the use of input/output assertions and describes one method for showing that the program is correct with respect to those assertions. An initial assertion characterizes conditions expected to be true upon entry to the program and a final assertion characterizes conditions expected to be true upon exit from the program. When a program contains no branches, a technique known as symbolic execution can be used to show that the truth of the initial assertion upon entry guarantees the truth of the final assertion upon exit. More generally, for a program with branches one can define a symbolic execution tree. If there is an upper bound on the number of times each loop in such a program may be executed, a proof of correctness can be given by a simple traversal of the (finite) symbolic execution tree. However, for most programs, no fixed bound on the number of times each loop is executed exists and the corresponding symbolic execution trees are infinite. In order to prove the correctness of such programs, a more general assertion structure must be provided. The symbolic execution tree of such programs must be traversed inductively rather than explicitly. This leads naturally to the use of additional assertions which are called \"inductive assertions.\"", "authors": ["Sidney L. Hantler", "James C. King"], "id": "92dfbf02fc68df56b7ae3b27188b0314797d072e", "title": "An Introduction to Proving the Correctness of Programs", "references": ["bf15ce3d1575d124527496cb249dc1249eee0acb"]}, {"date": "1965", "abstract": "A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n 41) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on to the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the Hessian matrix in the neighbourhood of the minimum, needed in statistical estimation problems.", "authors": ["John A. Nelder", "Ronald Mead"], "id": "017ddb7e815236defd0566bc46f6ed8401cc6ba6", "title": "A Simplex Method for Function Minimization", "references": ["755c4d6ca13e371e2f684c06caa348162190004b", "027e3321f34c9b2af721f9d102b9d9eb85147dcf", "f7e5e6fc240e984329215532120152ba61bdfffc", "52d44a7937d076ea82726911cb79ce76e6597e40", "b2f499747eb3c6da7bc400950d12833f8965c468"]}, {"date": "1957", "abstract": "The rate at which industrial processes are improved is limited by the present shortage of technical personnel. Dr Box describes a method of process improvement which supplements the more orthodox studies and is run in the normal course of production by plant personnel themselves. The basic philosophy is introduced that industrial processes should be run so as to generate not only product, but also information on how the product can be improved.", "authors": ["George E. P. Box"], "id": "8fd6a3faa76b6a7dbcb63b599dbd07eb5a98aba9", "title": "Evolutionary Operation: a Method for Increasing Industrial Productivity", "references": ["a5d7932955bbcfd2fddcfc48849212e4a127119d", "bd2f9a0da65cb7a68041eb54e05407645d045db3", "2174c746aa328023ab1a2c3d8b1f77fc1a3efa77", "669961522912fc8b7c3a0e4233dc9ccd572928c3"]}, {"date": "1977", "abstract": "CLU is a new programming language designed to support the use of abstractions in program construction. Work in programming methodology has led to the realization that three kinds of abstractions, procedural, control, and especially data abstractions, are useful in the programming process. Of these, only the procedural abstraction is supported well by conventional languages, through the procedure or subroutine. CLU provides, in addition to procedures, novel linguistic mechanisms that support the use of data and control abstractions.\n This paper provides an introduction to the abstraction mechanisms in CLU. By means of programming examples, we illustrate the utility of the three kinds of abstractions in program construction and show how CLU programs may be written to use and implement abstractions. We also discuss the CLU library, which permits incremental program development with complete type-checking performed at compile-time.", "authors": ["Barbara Liskov", "Alan Snyder", "Russell Atkinson", "Craig Schaffert"], "id": "17fe58e6115711ce4d5ceef941c60eb6d6898dcf", "title": "Abstraction mechanisms in CLU", "references": []}, {"date": "1944", "abstract": "The standard method for solving least squares problems which lead to non-linear normal equations depends upon a reduction of the residuals to linear form by first order Taylor approximations taken about an initial or trial solution for the parameters.2 If the usual least squares procedure, performed with these linear approximations, yields new values for the parameters which are not sufficiently close to the initial values, the neglect of second and higher order terms may invalidate the process, and may actually give rise to a larger value of the sum of the squares of the residuals than that corresponding to the initial solution. This failure of the standard method to improve the initial solution has received some notice in statistical applications of least squares3 and has been encountered rather frequently in connection with certain engineering applications involving the approximate representation of one function by another. The purpose of this article is to show how the problem may be solved by an extension of the standard method which insures improvement of the initial solution.4 The process can also be used for solving non-linear simultaneous equations, in which case it may be considered an extension of Newton's method. Let the function to be approximated be h{x, y, z, \u2022 \u2022 \u2022 ), and let the approximating function be H{oc, y, z, \u2022 \u2022 \u25a0 ; a, j3, y, \u25a0 \u2022 \u25a0 ), where a, /3, 7, \u2022 \u25a0 \u25a0 are the unknown parameters. Then the residuals at the points, yit zit \u2022 \u2022 \u2022 ), i = 1, 2, \u25a0 \u2022 \u2022 , n, are", "authors": ["Kenneth Levenberg"], "id": "b1afd5740cdb03295f14e6c993b8d36844956dce", "title": "A METHOD FOR THE SOLUTION OF CERTAIN NON \u2013 LINEAR PROBLEMS IN LEAST SQUARES", "references": []}, {"date": "1943", "abstract": "As Henri Poincare once remarked, \"solution of a mathematical problem\" is a phrase of indefinite meaning. Pure mathematicians sometimes are satisfied with showing that the non-existence of a solution implies a logical contradiction, while engineers might consider a numerical result as the only reasonable goal. Such one sided views seem to reflect human limitations rather than objective values. In itself mathematics is an indivisible organism uniting theoretical contemplation and active application. This address will deal with a topic in which such a synthesis of theoretical and applied mathematics has become particularly convincing. Since Gauss and W. Thompson, the equivalence between boundary value problems of partial differential equations on the one hand and problems of the calculus of variations on the other hand has been a central point in analysis. At first, the theoretical interest in existence proofs dominated and only much later were practical applications envisaged by two physicists, Lord Rayleigh and Walther Ritz ; they independently conceived the idea of utilizing this equivalence for numerical calculation of the solutions, by substituting for the variational problems simpler approximating extremum problems in which but a finite number of parameters need be determined. Rayleigh, in his classical work\u2014Theory of sound\u2014and in other publications, was the first to use such a procedure. But only the spectacular success of Walther Ritz and its tragic circumstances caught the general interest. In two publications of 1908 and 1909 [39], Ritz, conscious of his imminent death from consumption, gave a masterly account of the theory, and at the same time applied his method to the calculation of the nodal lines of vibrating plates, a problem of classical physics that previously had not been satisfactorily treated. Thus methods emerged which could not fail to attract engineers and physicists; after all, the minimum principles of mechanics are more suggestive than the differential equations. Great successes in applications were soon followed by further progress in the understanding of the theoretical background, and such progress in turn must result in advantages for the applications.", "authors": ["Richard Courant"], "id": "efa32848ae69aba45d37e02b872272b87e36d020", "title": "Variational methods for the solution of problems of equilibrium and vibrations", "references": ["64e1a60f2a49eac29c39c650a232de81de4ecf96", "ec1abc5e3fb1a7f76497d6aadce6b9c9d146ec0d", "b2a77070a435cb349ba4303095d18b1bec7136a6", "941f8837e10b140af23d8d6631998f554784b86a"]}, {"date": "1973", "abstract": "Program verification refers to the idea that the intent or effect of a program can be stated in a precise way that is not a simple \"rewording\" of the program itself, and that one can prove (in the mathematical sense) that a program actually conforms to a given statement of intent. This thesis describes a software system which can verify (prove) some non-trivial programs automatically. The system described here is organized in a novel manner compared to most other theorem-proving systems. IL has a great deal of specific knowledge about integers and arrays of integers, yet it is not \"special-purpose\", since this knowledge is represented in procedures which are separate from the underlying structure of the system. It also incorporates some knowledge, gained by the author from both experiment and introspection, about how programs are often constructed, and uses this knowledge to guide the proof process. It uses its knowledge, plus contextual information from the program being verified, to simplify the theorems dramatically as they are being constructed, rather than relying on a super-powerful proof procedure. The system also provides for interactive editing of programs and assertions, and for detailed human control of the proof process when the system cannot produce a proof (or counterexample) on its own.", "authors": ["L. Peter Deutsch"], "id": "bf15ce3d1575d124527496cb249dc1249eee0acb", "title": "An interactive program verifier", "references": ["efa59953df102e903e882f3589c23c9a505d7aa8", "43aa639d8df2285bd805531889d1d5f49563c17d", "bac85a0e8836b887c9b62156a8fd70df8264b7c2", "5b292d2f10da7585dd3487a197bf2852928f1c70", "e3aa3d449a62500dc04ce4a1f8074142e38963cf"]}, {"date": "1975", "abstract": "Publisher Summary This chapter provides an overview of SOPHIE, an intelligent instructional system that reflects a major attempt to extend Carbonell's notion of mixed-initiative Computer Aided Instruction (CAI) for the purpose of encouraging a wider range of student initiatives. Unlike previous AI-CAI systems that attempt to mimic the roles of a human teacher, SOPHIE tries to create a reactive environment in which the student learns by trying out his ideas rather than by instruction. SOPHIE incorporates a strong model of its knowledge domain along with numerous heuristic strategies for answering a student's questions, providing him with critiques of his current solution paths, and generating alternative theories to his current hypotheses. In essence, SOPHIE enables a student to have a one-to-one relationship with an expert who helps the student create, experiment with, and debug his own ideas. SOPHIE's expertise is derived from an efficient and powerful inferencing scheme that uses multiple representations of knowledge including (1) simulation models of its microcosm, (2) procedural specialists that contain logical skills and heuristic strategies for using these models, and (3) semantic nets for encoding time-invariant factual knowledge.", "authors": ["John Seely Brown", "Richard R. Burton"], "id": "b5aea3ab2364c257835ae2c9d961aa8956fa8eda", "title": "MULTIPLE REPRESENTATIONS OF KNOWLEDGE FOR TUTORIAL REASONING", "references": []}, {"date": "1976", "abstract": "The programming language Alphard is designed to provide support for both the methodologies of \u201cwell-structured\u201d programming and the techniques of formal program verification. Language constructs allow a programmer to isolate an abstraction, specifying its behavior publicly while localizing knowledge about its implementation. The verification of such an abstraction consists of showing that its implementation behaves in accordance with its public specifications; the abstraction can then be used with confidence in constructing other programs, and the verification of that use employs only the public specifications.\n This paper introduces Alphard by developing and verifying a data structure definition and a program that uses it. It shows how each language construct contributes to the development of the abstraction and discusses the way the language design and the verification methodology were tailored to each other. It serves not only as an introduction to Alphard, but also as an example of the symbiosis between verification and methodology in language design. The strategy of program structuring, illustrated for Alphard, is also applicable to most of the \u201cdata abstraction\u201d mechanisms now appearing.", "authors": ["William A. Wulf", "Ralph L. London", "Mary Shaw"], "id": "7d06bf84338e89456f609896de4e41f61086d98e", "title": "An introduction to the construction and verification of Alphard programs", "references": ["a5dc91c06f08197e342c44d93d80c8e016469f9c"]}, {"date": "1976", "abstract": "Abstract : This paper describes a theoretical framework and a LISP implementation for describing and simulating the cause-effect behavior of mechanisms. For the purposes of this research, a mechanism is defined to be any physical device, complex or simple, which exhibits cause and effect relationships useful to humans. Ordinarily, this will mean any purposively constructed object, such as a vacuum cleaner, a pencil, a button, a lightbulb or a computer. However, the authors also include in the definition any naturally-occurring physical devices and principles whose cause and effect relationships are of use to humans. Also considered are information-manipulating 'mechanisms' such as computer programs, as though they were physical in nature.", "authors": ["Chuck Rieger", "M A Grinberg"], "id": "e494e85bda74b856f20eee75bd274ea74e4861aa", "title": "The Causal Representation and Simulation of Physical Mechanisms.", "references": []}, {"date": "1976", "abstract": "Semantic Scholar extracted view of \"Applications of Meta Level Knowledge to the Construction\" by R. Jeffrey Davis", "authors": ["R. Jeffrey Davis"], "id": "7cfee4257cd03c70151af5ba71915b1d0f608484", "title": "Applications of Meta Level Knowledge to the Construction", "references": []}, {"date": "1976", "abstract": "Abstract Plan synthesis and language comprehension, or more generally, the act of discovering how one perception relates to others, are two sides of the same coin, because they both rely on a knowledge of cause and effect\u2014algorithmic knowledge about how to do things and how things work. I will describe a new theory of representation for commonsense algorithmic world knowledge, then show how this knowledge can be organized into larger memory structures, as it has been in a LISP implementation of the theory. The large-scale organization of the memory is based on structures called bypassable causal selection networks. A system of such networks serves to embed thousands of small commonsense algorithm patterns into a larger fabric which is directly usable by both a plan synthesizer and a language comprehender. Because these bypassable networks can adapt to context, so will the plan synthesizer and a language comprehender. I will propose that the model is an approximation to the way humans organize and use algorithmic knowledge, and as such, that it suggests approaches not only to problem solving and language comprehension, but also to learning. I'll describe the commonsense algorithm representation, show how the system synthesizes plans using this knowledge, and trace through the process of language comprehension, illustrating how it threads its way through these algorithmic structures.", "authors": ["Chuck Rieger"], "id": "0dc8b5d3452ddfd26b998cd923f4c46cdd80faca", "title": "An Organization of Knowledge for Problem Solving and Language Comprehension", "references": ["8e64b0378e22f1df9d73630e9ad62c57ea25c5c3", "6d801505d744dff6bb787b284ded9c2ef901ebbc", "67a69a1d0124e014d3cf8e7c214d395f1befdf95", "1efcd7be3b52e46de800e06e17268ce7d535d9a7", "438c44ab6270d8b221fca2b94c73d3673a3cb16e", "e64db9777a991f55029bd13c9d03741fa2b17046", "a0adea7988254f3d0740b587334c8ca6357cdd8b", "2cfe1adca4b3fd0b20eb37260d7013303f112111"]}, {"date": "1981", "abstract": "Abstract Many in Artificial Intelligence have noted the common concerns of problem-solving and language-comprehension research. Both must represent large bodies of real world knowledge, and both must use such knowledge to infer new facts from old. Despite this the two subdisciplines have, with minor exceptions, kept arm's length. So, for example, many in language comprehension have adopted some form of \u2018frame\u2019 representation, while problem-solving people have tended to use predicate calculus. In this paper I will first show that this is not merely idiosyncratic behavior, but rather stems from the different issues stressed by the two areas, problem solvers being primarily concerned with deep inferences in narrow domains, while language comprehenders being more concerned with shallow inference in broader areas. I will then suggest a compromise position which will use both frames and predicate calculus, and then show how this representation has features desired by both camps.", "authors": ["Eugene Charniak"], "id": "6aa3a9605c073b6d5f925c08136e0ef352293abd", "title": "A Common Representation for Problem-Solving and Language-Comprehension Information", "references": []}, {"date": "1980", "abstract": "If cognitive science does not exist then it is necessary to invent it. That slogan accommodates any reasonable attitude about the subject. One attitude-an optimistic one-is that cognitive science already exists and is alive and flourishing in academe: we have all in our different ways been doing it for years. The gentleman in Moliere\u2019s play rejoiced to discover that he had been speaking prose for forty years without realizing it: perhaps we are merely celebrating a similar discovery. And, if we just keep going on in the same way, then we are bound to unravel the workings of the mind. Another attitude-my own-is more pessimistic: experimental psychology is not going to succeed unaided in elucidating human mentality; artificial intelligence is not going to succeed unaided in modelling the mind; nor is any other discipline-linguistics, anthropology, neuroscience, philosophy-going to have any greater success. If we are ever to understand cognition, then we need a new science dedicated to that aim and based only in part on its contributing disciplines. Yet pessimism should not be confused with cynicism. We should reject the view that cognititie science is merely a clever ruse dreamed up to gain research funds-that it is nothing more than six disciplines in search of a grant-giving agency. Cognitive science does not quite exist: its precursors do, but it lacks a clear identity. Perhaps the major function of this conference should be to concentrate our minds on what that identity might be. At present, there appear to be two distinct ideas wrapped up in it: one topic-oriented, and the other methodological. The topic-oriented idea is that workers from several disciplines have converged upon a number of central problems and explanatory concepts. George Miller and 1 became aware of this convergence when we were caught in the toils", "authors": ["Philip N. Johnson-Laird"], "id": "3cb699fddc1018222e58b9bddc4bb98ca94a74b2", "title": "Mental Models in Cognitive Science", "references": ["55f8b67183b82eb364471963f6af37dec95cd6dd", "a1cb3eaa1747b43ad0b044a2389510428fa85e6b", "aa2393e520ba147271cacca71eae7b169d20988c", "e280d2855df983093523af744cc0e4e7ca25e57e", "7f2c73def1bb7549f0d259a3c059be37d1d394e1", "8c113945eb49d93b95e3975a4d565e81b40ed684", "619deb188c3c6d7d50fe9ea948ada007455705af", "a479441f4774cef5ad898181f0b15fcc4dfb7fd8", "2c51669f7c7a083dbb851e7cdaaf341219654076", "d09e73d42f2aa42a0abc4aa27d84e72faf712cc7"]}, {"date": "1952", "abstract": "Report presenting the effects of approximately 50-percent biaxial stretch-forming on the tensile and crazing properties of polymethyl methacrylate. Results indicated that biaxially stretch-forming polymethyl methacrylate approximately 50 percent does not affect its tensile strength of secant modulus of elasticity in tension. Results regarding standard tensile tests, stress-solvent crazing tests, long-time tensile tests, discussion of mechanism of crazing, accelerated weathering tests, additional experimental work, and possible applications of stretch-forming are provided.", "authors": ["Benjamin M. Axilrod", "Mark Andrew Sherman", "Cohen", "Irvin Wolock"], "id": "acdde7735330da19ba951b338f0917ed1c45145a", "title": "Effects of moderate biaxial stretch-forming on tensile and crazing properties of acrylic plastic glazing", "references": ["b1e51c1c8056a55bb42db62e975874f33864ca5b", "6f4b0d1087099c80f91eb84dce6f8254fd63c34e", "8d86eb04ce153dce0f36e4b10a554af617195414", "4386457bcf1368fc6f4b6285e3a7ba726fd731ad", "6ab3ef0c6138c2ad0dcda259826a518dace9bc87", "8bd6e90f9e85a8eacfc8fa5e58dded12e34e7d77"]}, {"date": "1977", "abstract": "This paper discusses the problems of representing and reasoning with information about knowledge and action. The first section discusses the importance of having systems that understand the concept of knowledge, and how knowledge is related to action. Section 2 points out some of the special problems that are involved in reasoning about knowledge, and section S presents a logic of knowledge based on the idea of possible worlds. Section 4 integrates this with a logic of actions and gives an example of reasoning in the combined system. Section 5 makes some concluding comments.", "authors": ["Robert C. Moore"], "id": "6a6bfc2d27fe3c3a88d8fd5c21d9c3e2cc93ccaf", "title": "Reasoning About Knowledge and Action", "references": ["c2709beadedb8455021267c61d314f72c7b5abbd", "4435a35f7d425a29a7653daf0b4b98c842aeda5d", "f350dd6a3fb1aa05b856c439cb2e64537277a23f", "512cd16c84f83fae6129bfc22b7a500172846860"]}, {"date": "1977", "abstract": "Abstract : Although large amounts of programming knowledge are available to human programmers in the form of books and articles, very little of this knowledge is available in a form suitable for use by a machine in performing programming tasks automatically. The principal goal of the research reported here is the explication of programming knowledge to a sufficient level of detail that it can be used effectively by a machine. The programming task considered in this experiment is that of constructing concrete implementations of abstract algorithms in the domain of symbolic programming. Knowledge about several aspects of symbolic programming has been expressed as a collection of four hundred refinement rules. The rules deal primarily with collections and mappings and ways of manipulating such structures, including several enumeration, sorting and searching techniques. The principal representation techniques covered include the representation of sets as linked lists and arrays (both ordered and unordered), and the representation of mappings as tables, sets of pairs, property list markings, and inverted mappings (indexed by range element). In addition to these general constructs, many low-level programming details are covered (such as the use of variables to store values).", "authors": ["David R. Barstow"], "id": "47330daa19a95f093b7628e325bee238439abc40", "title": "Automatic Construction Of Algorithms And Data Structures Using A Knowledge Base Of Programming Rules", "references": []}, {"date": "1977", "abstract": "Abstract : SPADE is a theory of the design of computer programs in terms of complementary planning and debugging processes. An overview of the author's recent research on this theory is provided. SPADE borrows tools from computational linguistics -- grammars, augmented transition networks (ATN's), chart-based parsers -- to formalize planning and debugging. The theory has been applied to parsing protocols of programming episodes, constructing a grammar-based editor in which programs are written in a structured fashion. (Author)", "authors": ["Mark L. Miller", "Ira P. Goldstein"], "id": "df12f998fa2160c2b04fce5aea143052a6b1f837", "title": "Overview of a Linguistic Theory of Design.", "references": []}, {"date": "1977", "abstract": "Abstract : This report describes progress to date in the ability of a computer system to understand and reason about actions. A new method of representing actions within a computer's memory has been developed, and this new representation, called the \"procedural net,\" has been employed in developing new strategies for solving problems and monitoring the execution of the resulting solutions. A set of running computer programs, called the NOAH (Nets Of Action Hierarchies) system, embodies this representation. Its major goal is to provide a framework for storing expertise about the actions of a particular task domain, and to impart that expertise to a human in the cooperative achievement of nontrivial tasks. A problem is presented to NOAH as a statement that is to be made true by applying a sequence of actions in an initial state of the world. The actions are drawn from a set of actions previously defined to the system. NOAH first creates a one-step solution to the problem, then it progressively expands the level of detail of the solution, filling in ever more detailed actions. All the individual actions, composed into plans at differing levels of detail, are stored in the procedural net. The system avoids imposing unnecessary constraints on the order of the actions in a plan. Thus, plans are represented as partial orderings of actions, rather than as linear sequences. The same data structure is used to guide the human user through a task. Since the system has planned the task at varying levels of detail, it can issue requests for action to the user at varying levels of detail, depending on his/her competence and understanding of the higher level actions. If more detail is needed than was originally planned for, or if an unexpected event causes the plan to go awry, the system can continue to plan from any point during execution. In essence, the structure of a plan of actions is as important for problem solving and execution monitoring as the nature of the actions themselves.", "authors": ["Earl D. Sacerdoti"], "id": "9b19b116dfb01a59c5bcc0fe7f792162a2cb3614", "title": "A Structure For Plans And Behavior", "references": []}, {"date": "1954", "abstract": "The methods most commonly employed for the statistical analysis of observations are based on the assumption that the number of observations was decided on in advance. The number of observations is indeed chosen in advance in many types of experiment or observational inquiry. In an agricultural field experiment, the number of plots and their treatments must be completely specified long before any observations can be taken; and (apart from possible failures which will be recorded as \"missing observations\") the number of observations eventually obtained is precisely the number chosen at the outset. Many chemical determinations are made in triplicate or quadruplicate or some other fixed number of times, according to a definite rule; and so the number of readings obtained in each determination is fixed in advance. A sample survey of households in a town may be based on inquiries at every hundredth house in a list; here again the number of observations is fixed in advance, provided we include the instances of non-response. There are other sorts of inquiry where the number of observations is not fixed in advance, and where the experimenter, if asked just before he began, would not be able to say how many observations he would take. He may, for example, be following some recognized type of sequential sampling rule, such as olne of A. Wald's sequential tests, or the inverse sampling of J. B. S. Haldane and M. C. K. Tweedie. In that case, when reporting his observations he will presumably state what the sampling rule was, and he will use a method of statistical analysis specially designed for that sampling rule. Most commonly, however, when the number of observations is not fixed in advance, this is because at the outset the experimenter has not fully made up his mind as to his requirements or resources or the nature of the material being studied; and so he does not decide to make a certain fixed number of observations nior to follow a definite sequential sampling rule, but proposes simply to take observations until such time as it shall seem appropriate to stop. S;ometimes indeed", "authors": ["Francis John Anscombe"], "id": "a5d7932955bbcfd2fddcfc48849212e4a127119d", "title": "Fixed-Sample-Size Analysis of Sequential Observations", "references": []}, {"date": "1955", "abstract": "When a set of observations form a time series it is wrong to use the ordinary random sampling theory for calculating standard errors. Dr Jowett discusses the problem and shows what should be done.", "authors": ["Geoffrey H. Jowett"], "id": "669961522912fc8b7c3a0e4233dc9ccd572928c3", "title": "The Comparison of Means of Industrial Time Series", "references": []}, {"date": "1955", "abstract": "This is a sequel to an article which recently appeared in this journal [1] and had the same general title. The previous article described a number of applications of newly developed techniques [2] for the study of response surfaces. The present article shows how study of the form of the empirical surface can throw important light on the basic mechanism operating and can thus make possible developments in the fundamental theory of a process. This idea is illustrated in some detail with an example previously discussed only from the empirical standpoint. A theoretical surface, based on reaction kinetics is now derived, rate constants are estimated from the data and the theoretical surface is compared with the empirical surface previously obtained. It is then shown how the canonical variables of the empirical surface can relate to the basic physical laws controlling the system. In this connection the problem of suitable choice of metrics for the variables is discussed. In a final section some general remarks on the process of scientific investigation are appended.", "authors": ["George E. P. Box", "P. V. Youle"], "id": "2174c746aa328023ab1a2c3d8b1f77fc1a3efa77", "title": "The Exploration and Exploitation of Response Surfaces: An Example of the Link between the Fitted Surface and the Basic Mechanism of the System", "references": []}, {"date": "1988", "abstract": "Abstract : Effort was directed toward showing that the techniques that have emerged for constructing sophisticated problem-solving programs also provide us with new, strong tools for constructing theories of human thinking. They allow us to merge the rigor and objectivity associated with behaviorism with the wealth of data and complex behavior associated with the gestalt movement. To this end their key feature is not that they provide a general framework for understanding problem-solving behavior (although they do that too), but that they finally reveal with great clarity that the free behavior of a reasonably intelligent human can be understood as the product of a complex but finite and determinate set of laws. Although we know this only for small fragments of behavior, the depth of the explanation is striking. (Author)", "authors": ["Allen Newell", "Herbert A. Simon"], "id": "02256c909265b66403b6cb08102174aa0b6ede1d", "title": "GPS, a program that simulates human thought", "references": ["e50129abcbd13b2fe4196681590026f7ce7a6bb6", "e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772", "97876c2195ad9c7a4be010d5cb4ba6af3547421c", "2edc8083073837564306943aab77d6dcc19d0cdc", "224f5b57675009dcf3b82452fba16dd922181038", "f8117e2fa9be7f65d0c0cd19d1473ef0bdf2de69", "8b2d4d160571bc54dedfade5244d37e082618a53", "0012e189a719cc29c3fb1ac597286e9052c0ba82"]}, {"date": "1972", "abstract": "This paper describes a research effort and programming system designed to facilitate the production of programs. Unlike automated programming, which focuses on developing systems that write programs, automated programmering involves developing systems which automate (or at least greatly facilitate) those tasks that a programmer performs other than writing programs: e.g., repairing syntactical errors to get programs to run in the first place, generating test cases, making tentative changes, retesting, undoing changes, reconfiguring, massive edits, et al., plus repairing and recovering from mistakes made during the above. When the system in which the programmer is operating is cooperative and helpful with respect to these activities, the programmer can devote more time and energy to the task of programming itself, i.e., to conceptualizing, designing and implementing. Consequently, he can be more ambitious, and more productive.", "authors": ["Warren Teitelman"], "id": "e3aa3d449a62500dc04ce4a1f8074142e38963cf", "title": "Automated programmering: the programmer's assistant", "references": []}, {"date": "1971", "abstract": "The problem of the mutual exclusion of several independent processes from simultaneous access to a \u201ccritical section\u201d is discussed for the case where there are two distinct classes of processes known as \u201creaders\u201d and \u201cwriters.\u201d The \u201creaders\u201d may share the section with each other, but the \u201cwriters\u201d must have exclusive access. Two solutions are presented: one for the case where we wish minimum delay for the readers; the other for the case where we wish writing to take place as early as possible.", "authors": ["Pierre-Jacques Courtois", "F. Heymans", "David Lorge Parnas"], "id": "43aa639d8df2285bd805531889d1d5f49563c17d", "title": "Concurrent control with \u201creaders\u201d and \u201cwriters\u201d", "references": ["3fea018ca5e6fecf60a90c2612391f9805c86c15"]}, {"date": "1971", "abstract": "The architecture of most interactive systems is based on the general strategy that suitable terminal service can be provided by a central processor that is timemultiplexed among all the active terminals. In order to achieve adequate response time in an interactive environment, the CPU is usually time sliced. Other major system facilities such as I/O channels and secondary storage units are also shared among the users, and multiprogramming techniques are employed to keep all the major system resources as fully utilized as possible. An operating system is usually developed which performs these functions as well as supervising the terminal communications, implementing a system-wide filing subsystem, handling user commands, etc. The result of combining these and other functions into a time-sharing operating system is a highly complex software system which transforms what is basically a batch processing computer structure into a multi-terminal system with significant limitations that are an outgrowth of this strategy. While a failure can occur in any section of the hardware or software, we know that hardware failures are more likely to occur in the electromechanical and core memory sectors than in solid state logic, and that software failures tend to be concentrated in the more complex areas of code. Failures of hardware components may require modification of the operating system in order to regain operational status since the allocation strategies may need more than parametric modification when system resources are affected.", "authors": ["Herbert B. Baskin", "Barry R. Borgerson", "Roger Roberts"], "id": "bac85a0e8836b887c9b62156a8fd70df8264b7c2", "title": "PRIME: a modular architecture for terminal-oriented systems", "references": []}, {"date": "1976", "abstract": "This paper is concerned with proving properties of programs which use data structures. The goal is to be able to prove that all instances of a class (e.g. as defined in Simula) satisfy some property. A method of proof which achieves this goal, generator induction, is studied and compared to other proof rules and methods: inductive assertions, recursion induction, computation induction, and, in some detail, structural induction. The paper concludes by using generator induction to prove a characteristic property of an implementation of hashtables.", "authors": ["Ben Wegbreit", "Jay M. Spitzen"], "id": "a5dc91c06f08197e342c44d93d80c8e016469f9c", "title": "Proving Properties of Complex Data Structures", "references": []}, {"date": "1966", "abstract": "A programming language similar in many respects to ALGOL 60, but incorporating a large number of improvements based on six years' experience with that language, is described in detail. Part I consists of an introduction to the new language and a summary of the changes made to ALGOL 60, together with a discussion of the motives behind the revisions. Part II is a rigorous definition of the proposed language. Part III describes a set of proposed standard procedures to be used with the language, including facilities for input/output. A preliminary version of this report was originally drafted by the first author on an invitation made by IFIP Working Group 2.1 at its meeting in May, 1965 at Prince-ton. It incorporated a number of opinions and suggestions made at that meeting and in its subcommittees, and it was distributed to members of the Working Group as \"Proposal for a Report on a Successor of ALGOL 60\" it was felt that the report did not represent a sufficient advance on A~GOL 60, either in its manner of language definition or in the content of the hmguage itself. The draft therefore no longer had the status of an official Working Document of the Group and by kind permission of the Chairman it was released for wider publication. At that time the authors agreed to collaborate on revising and supplementing the draft. The main changes were: (1) verbal improvements and clarifications, many of which were kindly suggested by recipients of the original draft; (2) additional or altered language features, in particular the replacement of tree structures by records as proposed by the second author; (3) changes which appeared desirable in the course of designing a simple trod efficient implementation of the 1 anguage; (4) addition of introductory and explanatory material , and further suggestions for standard procedures, in particular on input/output; (5) use of a convenient notational facility to abbreviate the description of syntax, as suggested by van Wijn-gaarden in \"Orthogonal Design and Description of a Fornial Language\" (MR76, Mathemat.ical Centre, Am-sterdam, Oct. 1965). The incorporation of the revisions is not intended to reinstate the report as a candidate for consideration as a successor to ALGOL 60. However, it is believed that its publication will serve three purposes: (1) To present to a wider public a view of the general direction in which the development of ALGOL is proceeding; (2) To provide an opportunity \u2026", "authors": ["Niklaus Wirth", "C. A. R. Hoare"], "id": "5b292d2f10da7585dd3487a197bf2852928f1c70", "title": "A contribution to the development of ALGOL", "references": []}, {"date": "1974", "abstract": "Semantic Scholar extracted view of \"Assimilation of New Information by a Natural Language Understanding System\" by D. McDermott", "authors": ["D. McDermott"], "id": "8e64b0378e22f1df9d73630e9ad62c57ea25c5c3", "title": "Assimilation of New Information by a Natural Language Understanding System", "references": []}, {"date": "1975", "abstract": "This paper describes a context mechanism for a natural language understanding system. Since no sentence is ever perceived outside some context, it is reasonable to inquire into the nature of context as it affects the interpretation of sentence meaning at a deep conceptual level. A theory, called conceptual oyerlays, is described. This theory (1) defines C(T1,..., Ti), the context established by the meaningful sequence of thouqhts T1...Tj: (2) defines I(Tj+1, C(T1,... Tj)), the high-level interpretation of Ti+i in the context established by Ti,.,.,Ti; and (3) specifies an effective algorithm and data structure for computing I(t,K) for arbitrary thought T in context K. In particular, a prototype LISP system, EX-SPECTRE-1, which solves simple cases of I(T2., C(T1)) is described. The system is based on an expectancy/fulfillment paradigm. Expectancies are spontaneously activated by a pattern-directed invocation technique. Each expectancy implicitly references large chunks of common-sense algorithms. A collection of such implicitly activated algorithms constitutes context, and the interpretive process is one of identifying future input as steps in these algorithms. Context switching and uses of I(T,K) in a language comprehension system are discussed.", "authors": ["Charles J. Rieger"], "id": "67a69a1d0124e014d3cf8e7c214d395f1befdf95", "title": "Conceptual Overlays: A Mechanism for the Interpretation of Sentence Meaning in Context", "references": ["a0adea7988254f3d0740b587334c8ca6357cdd8b"]}, {"date": "1974", "abstract": "meaning is best ignored in the study of logical form because they think the concept of meaning is occult, unexplanatory, and hence methodologically unacceptable.2 We are not directly concerned with the issue between inten sionalism and extensionalism in the present paper.3 Rather, we assume the intensionalist framework and concern ourselves instead with the choice between the two most recent forms intensionalist theory has taken: the meaning postulate approach introduced by Carnap4 and the semantic theory approach introduced by Katz and Fodor.5 These two approaches are competing proposals about the kind of rules best suited to explicating meaning or logical form in natural language. The present paper focuses on the differences between the kinds of rules proposed by each approach. It provides arguments for claiming that the semantic theory approach offers a better basis for an intensionalist theory of the logical structure of natural languages. We try to establish this claim by showing that systems of meaning postulates fail reasonable conditions of adequacy on a theory of the semantic structure of natural languages, and further, that this is not because the present acccount of meaning postulate is not sophisticated enough, but because, in principle, meaning postulates are unable to cope with the semantic structures that intensionalists are required to explain.", "authors": ["Jerrold J. Katz", "Richard Nagel", "Objectives"], "id": "55f8b67183b82eb364471963f6af37dec95cd6dd", "title": "MEANING POSTULATES AND SEMANTIC THEORY", "references": []}, {"date": "1973", "abstract": "This paper describes D-Script, a language for representing knowledge in artificial intelligence (AI) programs. D-Script contains a powerful formalism for descriptions, which permits the representation of statements that are problematical for other systems. Particular attention is paid to problems of opaque contexts, time contexts, and knowledge about knowledge. The design of a deductive system for this language is also considered.", "authors": ["Robert C. Moore"], "id": "512cd16c84f83fae6129bfc22b7a500172846860", "title": "D-Script: A Computational Theory of Descriptions", "references": ["19306a3d8bc9c7732fb6d25f605ace765198bdd2", "8e64b0378e22f1df9d73630e9ad62c57ea25c5c3", "4a2986f8a3b4a385ef410bfac509ace84401e961", "10f7507b8408bf35125b8e04254ad890c8d45e1d", "e97795382386ecd24300f3a6449ed5732b200bfa", "494aedf82da4755badc1fe74e4d21cf5fc029e9d", "62d0535077cdc622f6e68b9f49f59b97dd482a11"]}, {"date": "1973", "abstract": "Semantic Scholar extracted view of \"A Computational Model of Skill Acquisition\" by Gerald J. Sussman", "authors": ["Gerald J. Sussman"], "id": "438c44ab6270d8b221fca2b94c73d3673a3cb16e", "title": "A Computational Model of Skill Acquisition", "references": []}, {"date": "1975", "abstract": "Semantic Scholar extracted view of \"Reasoning from Incomplete Knowledge in a Procedural Deduction System\" by Robert C. Moore", "authors": ["Robert C. Moore"], "id": "f350dd6a3fb1aa05b856c439cb2e64537277a23f", "title": "Reasoning from Incomplete Knowledge in a Procedural Deduction System", "references": []}, {"date": "1975", "abstract": "We describe a theoretical system intended to facilitate the use of knowledge In an understanding system. The notion of script is introduced to account for knowledge about mundane situations. A program, SAM, is capable of using scripts to understand. The notion of plans is introduced to account for general knowledge about novel situations.", "authors": ["Roger C. Schank", "Robert P. Abelson"], "id": "1efcd7be3b52e46de800e06e17268ce7d535d9a7", "title": "Scripts, Plans and Knowledge", "references": ["bb20f121c979b535bbeade5ac06676d627d4ad7d", "5a211a174b98e1769c6af93c0d36258a36633274", "9b5f006a9edb63ff101535281fd772d40d0e5dc7", "56e27eea812ff76e90cfabf511d2501361f22ff3", "975672653d06a1976345776fd2d7170c32791bfe", "3490c242c3d6b5a735792a75058de0d108d04731", "f16418fbaf212b8c6d6b911812fec8bfe0d255b3", "7a8fddf346860f70a3c01e219ba9ef5674c68168", "3e3725292a28029f308034c6110bd4d9e0dad4ce"]}, {"date": "1952", "abstract": "For everybody, if you want to start joining with others to read a book, this knowledge and belief is much recommended. And you need to get the book here, in the link download that we provide. Why should be here? If you want other kind of books, you will always find them. Economics, politics, social, sciences, religions, Fictions, and more books are supplied. These available books are in the soft files.", "authors": ["Norman Malcolm"], "id": "4435a35f7d425a29a7653daf0b4b98c842aeda5d", "title": "On Knowledge and Belief", "references": []}, {"date": "1969", "abstract": "In the philosophy of logic a distinction is often made between the theory of reference and the theory of meaning.1 In this paper I shall suggest (inter alia) that this distinction, though not without substance, is profoundly misleading. The theory of reference is, I shall argue, the theory of meaning for certain simple types of language. The only entities needed in the so-called theory of meaning are, in many interesting cases and perhaps even in all cases, merely what is required in order for the expressions of our language to be able to refer in certain more complicated situations. Instead of the theory of reference and the theory of meaning we perhaps ought to speak in some cases of the theory of simple and of multiple reference, respectively. Quine has regretted that the term \u2018semantics\u2019, which etymologically ought to refer to the theory of meaning, has come to mean the theory of reference.1 I submit that this usage is happier than Quine thinks, and that large parts of the theory of meaning in reality are \u2014 or ought to be \u2014 but semantical theories for notions transcending the range of certain elementary types of concepts.", "authors": ["Jaakko Hintikka"], "id": "c2709beadedb8455021267c61d314f72c7b5abbd", "title": "Semantics for Propositional Attitudes", "references": []}, {"date": "1950", "abstract": "The effect of various variables, such as type of stress, stress magnitude, duration of stressing, and environment on the initiation and development of crazing in linear polymers is discussed. The basic nature of crazing is investigated in some detail for polystyrene specimens by means of the light microscope, the electron microscope and the x\u2010ray spectrometer. The results of these observations and their bearing on the fundamental group structure of polystyrene molecules is presented.The relationship between crazing and orientation is discussed, as well as the effect of both of these factors on the mechanical properties. The experimental results are compared, wherever possible, with the previously reported data of other investigators. A short discussion is then given of a theory of crazing from the point of view of the molecular structure of the material.", "authors": ["Ching C. Hsiao", "John A. Sauer"], "id": "8bd6e90f9e85a8eacfc8fa5e58dded12e34e7d77", "title": "On Crazing of Linear High Polymers", "references": []}, {"date": "1949", "abstract": "Semantic Scholar extracted view of \"Rheological Properties of Polystyrene below 80\u00b0 C.\" by Bryce Maxwell et al.", "authors": ["Bryce Maxwell", "L. F. Rahm"], "id": "6ab3ef0c6138c2ad0dcda259826a518dace9bc87", "title": "Rheological Properties of Polystyrene below 80\u00b0 C.", "references": []}, {"date": "1950", "abstract": "Semantic Scholar extracted view of \"Crazing of Cast Polymethyl Methacrylate\" by E. W. Russell", "authors": ["E. W. Russell"], "id": "4386457bcf1368fc6f4b6285e3a7ba726fd731ad", "title": "Crazing of Cast Polymethyl Methacrylate", "references": []}, {"date": "1963", "abstract": "Semantic Scholar extracted view of \"Empirical explorations with the logic theory machine\" by Allen Newell et al.", "authors": ["Allen Newell", "J. C. Shaw", "Hans-Arno. Auteur ou responsable intellectuel Simon"], "id": "8b2d4d160571bc54dedfade5244d37e082618a53", "title": "Empirical explorations with the logic theory machine", "references": []}, {"date": "1961", "abstract": "An information processing model of elementary human symbolic learning is given a precise statement as a computer program, called Elementary Perceiver and Memorizer (EPAM). The program simulates the behavior of subjects in experiments involving the rote learning of nonsense syllables. A discrimination net which grows is the basis of EPAM's associative memory. Fundamental information processes include processes for discrimination, discrimination learning, memorization, association using cues, and response retrieval with cues. Many well-known phenomena of rote learning are to be found in EPAM's experimental behavior, including some rather complex forgetting phenomena. EPAM is programmed in Information Processing Language V.\n H. A. Simon has described some current research in the simulation of human higher mental processes and has discussed some of the techniques and problems which have emerged from this research. The purpose of this paper is to place these general issues in the context of a particular problem by describing in detail a simulation of elementary human symbolic learning processes.\n The information processing model of mental functions employed is realized by a computer program called Elementary Perceiver and Memorizer (EPAM). The EPAM program is the precise statement of an information processing theory of verbal learning that provides an alternative to other verbal learning theories which have been proposed. It is the result of an attempt to state quite precisely a parsimonious and plausible mechanism sufficient to account for the rote learning of nonsense syllables. The critical evaluation of EPAM must ultimately depend not upon the interest which it may have as a learning machine, but upon its ability to explain and predict the phenomena of verbal learning.\n I should like to preface my discussion of the simulation of verbal learning with some brief remarks about the class of information processing models of which EPAM is a member.\n a. These are models of mental processes, not brain hardware. They are <u>psychological</u> models of mental function. No physiological or neurological assumptions are made, nor is any attempt made to explain information processes in terms of more elementary neural processes.\n b. These models conceive of the brain as an <u>information processor</u> with sense organs as input channels, effector organs as output devices, and with internal programs for testing, comparing, analyzing, rearranging, and storing information.\n c. The central processing mechanism is assumed to be serial; i.e., capable of doing only one (or a very few) things at a time.\n d. These models use as a basic unit the <u>information symbol</u>; i.e., a pattern of bits which is assumed to be the brain's internal representation of environmental data.\n e. These models are essentially <u>deterministic</u>, not probabilistic. Random variables play no fundamental role in them.", "authors": ["Edward A. Feigenbaum"], "id": "0012e189a719cc29c3fb1ac597286e9052c0ba82", "title": "The simulation of verbal learning behavior", "references": []}, {"date": "1958", "abstract": "This paper traces the development of digital computer programs that play chess. The work of Shannon, Turing, the Los Alamos group, Bernstein, and the authors is treated in turn. The efforts to program chess provide an indication of current progress in understanding and constructing complex and intelligent mechanisms.", "authors": ["Allen Newell", "J. C. Shaw", "Herbert A. Simon"], "id": "f8117e2fa9be7f65d0c0cd19d1473ef0bdf2de69", "title": "Chess-Playing Programs and the Problem of Complexity", "references": []}, {"date": "1965", "abstract": "A number of mainly independent sequential-cyclic processes with restricted means of communication with each other can be made in such a way that at any moment one and only one of them is engaged in the \"critical section\" of its cycle.", "authors": ["Edsger W. Dijkstra"], "id": "3fea018ca5e6fecf60a90c2612391f9805c86c15", "title": "Solution of a problem in concurrent programming control", "references": []}, {"date": "1961", "abstract": "A modern, high-speed digital computer has been used to simulate the behavior of individual human subjects in a classical psychological experiment where the subject is asked to predict a series of binary events. The representation of models of human behavior in the form of computer programs has permitted the construction and study of more realistic hypothesis-testing models of behavior in this experiment rather than the oversimplified conditioning models previously proposed. A model for one subject is described in detail, and the problem of comparing the behavior of the model to the behavior of the subject is also discussed.", "authors": ["Julian Feldman"], "id": "e50129abcbd13b2fe4196681590026f7ce7a6bb6", "title": "Simulation of behavior in the binary choice experiment", "references": []}, {"date": "1959", "abstract": "Semantic Scholar extracted view of \"Report on a general problem-solving program\" by Allen Newell et al.", "authors": ["Allen Newell", "J. C. Shaw", "Herbert A. Simon"], "id": "97876c2195ad9c7a4be010d5cb4ba6af3547421c", "title": "Report on a general problem-solving program", "references": []}, {"date": "1959", "abstract": "Introduction Few of those who have seen a modern high-speed digital computer digest and transform a mass of data in less time than it takes to follow the process in the mind can suppress a certain amount of speculation concerning the future of such machines. Under the assumption that the computer is operating at the mere threshhold of its capacity in performing the tasks we have thus far delegated to it, a long-range program directed at the problem of \"intelligent\" behavior and learning in machines has been established at the IBM Research Center in New York (Gelernter and Rochester, 1958). In particular the technique of heuristic programming is under detailed investigation as a means to the end of applying large-scale digital computers to the solution of a difficult class of problems currently considered to be beyond their capabilities; namely those problems that seem to require the agent of human intelligence and ingenuity for their solution. It is difficult to characterize such problems further, except, perhaps, to remark rather vaguely that they generally involve complex decision processes in a potentially infinite and uncontrollable environment. If, however, we should restrict the universe of problems to those that amount to the discovery of a proof for a theorem in some well-defined formal system, then the distinguishing characteristics of those problems of special interest to us are brought clearly into focus. We should like our machine to be able to prove many of the theorems presented to it in a formal system that is manifestly undecidable. Further, as the machine 134 gains \"experience\" in proving theorems, we should expect it to be able to solve problems that were earlier beyond its capabilities. The requirement that a machine should deal with undecidable systems places a fundamental restriction on its modus operandi. Finding a suitable algorithm, the obvious technique for the solution of problems on a digital computer, is no longer acceptable for the simple reason that no such algorithm exists. An exhaustive search for the initial axioms and theorems of the proof, combined with exhaustive development of the proof sequence by systematically applying the rules of transformation until the required proof has been produced, has been shown to be much too time-consuming for so simple a logic as propositional calculus (Newell, Shaw and Simon, 1957 a). It is a fortiori out of the question for any of the more interesting logics. A remaining alternative is \u2026", "authors": ["H. Gelernter"], "id": "2edc8083073837564306943aab77d6dcc19d0cdc", "title": "Realization of a geometry theorem proving machine", "references": []}, {"date": "1957", "abstract": "Semantic Scholar extracted view of \"A Study of Thinking\" by Fred W. Householder et al.", "authors": ["Fred W. Householder", "Jerome S. Bruner", "Jacqueline J. Goodnow", "George A. Austin"], "id": "224f5b57675009dcf3b82452fba16dd922181038", "title": "A Study of Thinking", "references": []}, {"date": "1951", "abstract": "Note presenting an investigation of the loss of strength of tensile specimens of polymethyl methacrylate as a result of stress-solvent crazing at 23 degrees Celsius and 50-percent relative humidity. the materials tested were commercial cast polymethyl-methacrylate sheets of both heat-resistant and ordinary grades from each of two manufacturers.", "authors": ["Benjamin M. Axilrod", "Martha A. Sherman"], "id": "6f4b0d1087099c80f91eb84dce6f8254fd63c34e", "title": "Effect of stress-solvent crazing on tensile strength of polymethyl methacrylate", "references": []}, {"date": "1950", "abstract": "This paper presents a study of the sequence of events by which cracks ``grow.'' A careful investigation was made of the origin, development, and characteristic pattern of level\u2010difference markings left by rapidly moving fractures in a wide variety of materials. Characteristic markings indicating discontinuous propagation are found in fractures of plastics, polycrystalline and single\u2010crystal materials, coal, mica, photographic film from which the emulsion was stripped, etc. Thus, the continued occurrence of essentially discontinuous initiations which comprise fracturing becomes clear. These considerations are sufficient to explain a number of well known fracture markings and to clarify what is mean by brittleness and fracture velocity.", "authors": ["Joseph A. Kies", "Aaron M. Sullivan", "George Rankin Irwin"], "id": "8d86eb04ce153dce0f36e4b10a554af617195414", "title": "Interpretation of Fracture Markings", "references": []}, {"date": "1950", "abstract": "Polyethylene of such molecular weight and structure that it readily fibers or cold draws to 300\u2013600 percent elongation by usual uniaxial, tensile stressing may react quite differently under biaxial tension. When biaxial tension in 1:1 ratio is applied to a diaphragm, some polymers show brittle fracture with <20 percent elongation at break. However, if the average molecular weight of such polyethylenes is shifted upward by crude fractionation, or an initially higher average is used, the polymers orient under complex stresses. Then, they usually elongate several hundred percent before rupture. Variations in crystallinity are also significant, although most technical polyethylene soon attains at room temperature enough crystallinity so that this factor does not cause big differences.Although the whole study is so far preliminary, x\u2010ray scattering of stressed samples suggests that preferred glide on certain crystallite planes tends to occur as the yield point approaches. These are such as to inhibit smooth al...", "authors": ["Irving L. Hopkins", "William O. Baker", "John B. Howard"], "id": "b1e51c1c8056a55bb42db62e975874f33864ca5b", "title": "Complex Stressing of Polyethylene", "references": []}, {"date": "1973", "abstract": "Semantic Scholar extracted view of \"Computer Models of Thought and Language\" by Roger C. Schank et al.", "authors": ["Roger C. Schank", "Kenneth Mark Colby"], "id": "3e3725292a28029f308034c6110bd4d9e0dad4ce", "title": "Computer Models of Thought and Language", "references": []}, {"date": "1973", "abstract": "This paper presents a critique of contemporary research which uses the notion of a mental image as a theoretical construct to describe one form of memory representation. It is argued that an adequate characterization of \u2018what we know\u2019 requires that we posit abstract mental structures to which we do not have conscious access and which are essentially conceptual and propositional, rather than sensory or pictorial, in nature. Such representations are more accurately referred to as symbolic descriptions than as images in the usual sense. Implications of using an imagery vocabulary are examined, and it is argued that the picture metaphor underlying recent theoretical discussions is seriously misleading \u2014 especially as it suggests that the image is an entity to be perceived. The relative merits of several alternative modes of representation (propositions, data structures, and procedures) are discussed. The final section is a more speculative discussion of the nature of the representation which may be involved when people \u2018use\u2019 visual images.", "authors": ["Zenon W. Pylyshyn"], "id": "d09e73d42f2aa42a0abc4aa27d84e72faf712cc7", "title": "What the Mind\u2019s Eye Tells the Mind\u2019s Brain: A Critique of Mental Imagery", "references": []}, {"date": "1975", "abstract": "Publisher Summary This chapter discusses the scripts, because theory requires a good foundation at the script level. The simplest relevant conceptual script content includes the puuposeful transactional activities of social actors, such as communication with others, enlisting the help of others, and a set of specifications of the realities constraining the possibilities of action\u2013knowledge of transportation and communication systems, the necessary properties of objects, etc. The major type of script is a plan, specified by the actor, the goal state, and the sequence of intended steps to reach the goal. With the aid of conceptual dependency formalism, a small set of primitive act concepts is sufficient to represent almost any describable event. Acts as steps-in-plans have two special features distinguishing them from acts as events-in-reality. Both features stem from the intentionality of plans. First, the reason why low-level primitives such as GRASP are not the most useful components in the cognition of plans is that for human actors they are usually not problematic\u2014they are taken for granted by both planner and observer. The second consideration is closely related to the first. Acts as steps-in-plans are characterized by their intended effects rather than by their physical nature.", "authors": ["Robert P. Abelson"], "id": "f16418fbaf212b8c6d6b911812fec8bfe0d255b3", "title": "CONCEPTS FOR REPRESENTING MUNDANE REALITY IN PLANS", "references": []}, {"date": "1975", "abstract": "Publisher Summary \nThis chapter describes the analyzer that serves as the front end for the MARGIE system. Describing the analyzer in detail means specifying two aspects of it that are the theory and the program. The program can be specified by presenting the functions with which it is built and showing how these functions are used in the task of analysis. The program aspect of the analyzer occupies the latter two-thirds of this description. However, the construction of the program is not haphazard. It is the purpose of the first one-third of the description to present the concepts and motivations about which the program is organized. This is the theoretical aspect of the analyzer. To motivate this theoretical description, it is in turn helpful to talk briefly about the program to which the theory leads. In particular, much can be said about the program from the outside without worrying about its internal machinery. The analyzer has been described by means of a number of examples presented in some detail. The word-oriented nature of the analyzer makes this kind of description necessary. Analysis occurs through the execution of programs, that is, requests that originally spring from individual words. The meaning of a word in this system is, therefore, a very dynamic thing, best illustrated in action.", "authors": ["Christopher Riesbeck"], "id": "975672653d06a1976345776fd2d7170c32791bfe", "title": "CHAPTER 4 \u2013 CONCEPTUAL ANALYSIS", "references": []}, {"date": "1975", "abstract": "A pair of substantially identical body halves are jointed face to face to leave a slot in one surface for a slider. One or both body halves have a resistive track and a collector track including a center terminal. The slider has sliding contacts to connect the resistive track and collector track. A dust shield is mounted so as to be stationary in the body to prevent ingress of dust through the slot. The slider has cooperating body parts on either side of the dust shield so as to be slidable without moving the dust shield.", "authors": ["Daniel G. Bobrow", "Allan Collins"], "id": "7a8fddf346860f70a3c01e219ba9ef5674c68168", "title": "Representation and Understanding: Studies in Cognitive Science", "references": []}, {"date": "1972", "abstract": "Massachusetts Institute of Technology. Dept. of Electrical Engineering. Thesis. 1972. Ph.D.", "authors": ["Eugene Charniak"], "id": "56e27eea812ff76e90cfabf511d2501361f22ff3", "title": "Toward A Model Of Children''s Story Comprehension", "references": []}, {"date": "1975", "abstract": "At the Carbonell Memorial Conference in 1974, there was a good deal of informal discussion of the use by people of analogue simulations in knowledge retrieval or question-answering. We asked each other questions like, \"How many traffic lights are there along your usual route from the railroad station to your house?\" Or, \"Can a salt shaker be used as a stool?\". The former type of question usually gives rise to introspective reports of a mental simulation of the traversal of the requested route, replete with visual imagery. The latter type of question may or may not give rise to a mental simulation. Some people report knowing propositlonally that a salt shaker cannot be used as a stool because its size is insufficient. Others report mentally playing through the motor sequence of sitting on a salt shaker, whence they rudely discover the negative answer.", "authors": ["Robert P. Abelson"], "id": "5a211a174b98e1769c6af93c0d36258a36633274", "title": "Does a story understander need a point of view?", "references": ["6d801505d744dff6bb787b284ded9c2ef901ebbc"]}, {"date": "1975", "abstract": "Publisher Summary This chapter presents some criteria for evaluating ideas for representation. It also presents a rough sketch of a particular version of a frame representation, and discusses the ways in which it can deal with the issues raised. The proceduralists assert that human knowledge is primarily a knowing how. The human information processor is a stored program device, with its knowledge of the world embedded in the programs. The declarativists do not believe that knowledge of a subject is intimately bound with the procedures for its use. They see intelligence as resting on two bases: a quite general set of procedures for manipulating facts of all sorts, and a set of specific facts describing particular knowledge domains. In thinking, the general procedures are applied to the domain-specific data to make deductions. Often this process has been based on the model of axiomatic mathematics. The facts are axioms and the thought process involves proof procedures for drawing conclusions from them.", "authors": ["Terry Winograd"], "id": "9b5f006a9edb63ff101535281fd772d40d0e5dc7", "title": "FRAME REPRESENTATIONS AND THE DECLARATIVE/PROCEDURAL CONTROVERSY", "references": []}, {"date": "1969", "abstract": "Semantic Scholar extracted view of \"A conceptural dependency representation for a computer-oriented semantics\" by Roger C. Schank", "authors": ["Roger C. Schank"], "id": "62d0535077cdc622f6e68b9f49f59b97dd482a11", "title": "A conceptural dependency representation for a computer-oriented semantics", "references": []}, {"date": "1972", "abstract": "A higher level language derives its great power from the fact that it tends to impose structure on the problem solving behavior of the user. Besides providing a library of useful subroutines with a uniform calling sequence, the author of a higher level language imposes his theory of problem solving on the user. By choosing what primitive data structures, control structures, and operators he presents, he makes the implementation of some algorithms more difficult than others, thus discouraging some techniques and encouraging others. So, to be good, a higher level language must not only simplify the job of programming, by providing features which package programming structures commonly found in the domain for which the language was designed, it must also do its best to discourage the use of structures which lead to bad algorithms.", "authors": ["Gerald J. Sussman", "Drew McDermott"], "id": "4a2986f8a3b4a385ef410bfac509ace84401e961", "title": "From PLANNER to CONNIVER: a genetic approach", "references": []}, {"date": "1971", "abstract": "This paper contributes to the discussion whether and how predicate calculus should be used as a deep structure in question-answering programs. The first part of the paper stresses that there are several possible ways of using predicate calculus, and argues that predicate calculus has significant advantages above competing deep structures if the way of using it is carefully selected. The second half gives hints on how various natural-language constructions can be encoded in a consistent way, and how axion sets that define these encodings can be written and debugged.", "authors": ["Erik Sandewall"], "id": "19306a3d8bc9c7732fb6d25f605ace765198bdd2", "title": "Formal Methods in the Design of Question-Answering Systems", "references": []}, {"date": "1977", "abstract": "This paper has four major sections: First, we review the basic arguments offered by Pylyshyn (Psychological Bulletin, 1973, 80, 1\u201324) and others against using imagery as an explanatory construct in psychology. Second, we consider each of these points and find none that speak against any but the most primitive notions of imagery. Third, we review the results of various experiments on imagery. In each case, we compare two explanations of the findings: one which assumes the existence of a surface image manifesting emergent properties, and one which assumes that all internal representations are coded in terms of \u201cabstract propositions.\u201d We find imagery hypotheses to be at least as adequate as those based on propositional representation. Finally, we conclude that debate about the ultimate foundations of internal representation is fruitless; the empirical question is whether images have properties that cannot be derived directly from more abstract propositional structures.", "authors": ["Stephen M. Kosslyn", "James R. Pomerantz"], "id": "2c51669f7c7a083dbb851e7cdaaf341219654076", "title": "Imagery, propositions, and the form of internal representations", "references": []}, {"date": "1977", "abstract": "The concept of imagery has been linked historically to both memory and thought. The memory side apparently originated in Western culture as a mnemonic technique that was invented by Simonides about 2500 years ago. The method was promoted by orators as a way of memorizing speeches, by educators who advocated the technique as a learning device during the middle ages, and by philosophers such as Bruno who wanted to unify all knowledge within memory through the organization of images. Imagery as a theory of thought also goes back to antiquity. Aristotle wrote that it was impossible to think without mental pictures, and used the images of mnemonics to illustrate his statements about imagination and thought. Later, the British empiricists equated images with ideas in their theory of association. Images were subsequently viewed as basic units of consciousness in the introspective psychologies of Wundt and Titchener. Finally, today, both functions of imagery are given a prominent place in some theories of memory and cognition, but with important changes in the assumptions concerning the nature of the concept. I will discuss those differences, presently.", "authors": ["Allan Paivio"], "id": "8c113945eb49d93b95e3975a4d565e81b40ed684", "title": "Images, Propositions, and Knowledge", "references": []}, {"date": "1982", "abstract": "Logicians have only related language to models in various ways; psychologists have only related it to the mind; the real task, however, is to show how language relates to the world through the agency of the mind. This task is, at present, beyond the resources of Cognitive Science, but there is some chance of success by pooling the skills and knowledge of its contributing disciplines. The aim of this paper is accordingly to try to bring together formal and psychological semantics in order to determine the nature of the relations between them. In writing it, I have found myself in effect attempting to make a tunnel that would link up the heights of formal semantics, where the light has an unreal clarity, to the low lying realities of psychology, where the atmosphere is, to say the least, somewhat murky. Like a tunneler, I have tended to work first from one direction to the other, and then, when I reached an impasse, to switch round and work from the other direction. Whether or not the two halves would in the end join up \u2014 and I would, as it were, be able to shake hands with myself \u2014 was something that was unforeseeable at the outset. The project was an experiment: how could I tell what to think until I read what I wrote? Fortunately, there were a number of excellent surveys to guide me, and indeed it would be as well to begin with a brief exploration of the two areas with a view to finding some potential routes between them.", "authors": ["Philip N. Johnson-Laird"], "id": "7f2c73def1bb7549f0d259a3c059be37d1d394e1", "title": "Formal Semantics and the Psychology of Meaning", "references": []}, {"date": "1972", "abstract": "Abstract This paper describes a computer system for understanding English. The system answers questions, executes commands, and accepts information in an interactive English dialog. It is based on the belief that in modeling language understanding, we must deal in an integrated way with all of the aspects of language\u2014syntax, semantics, and inference. The system contains a parser, a recognition grammar of English, programs for semantic analysis, and a general problem solving system. We assume that a computer cannot deal reasonably with language unless it can understand the subject it is discussing. Therefore, the program is given a detailed model of a particular domain. In addition, the system has a simple model of its own mentality. It can remember and discuss its plans and actions as well as carrying them out. It enters into a dialog with a person, responding to English sentences with actions and English replies, asking for clarification when its heuristic programs cannot understand a sentence through the use of syntactic, semantic, contextual, and physical knowledge. Knowledge in the system is represented in the form of procedures, rather than tables of rules or lists of patterns. By developing special procedural representations for syntax, semantics, and inference, we gain flexibility and power. Since each piece of knowledge can be a procedure, it can call directly on any other piece of knowledge in the system.", "authors": ["Terry Winograd"], "id": "bb20f121c979b535bbeade5ac06676d627d4ad7d", "title": "Understanding Natural Language", "references": ["3fdec3403ac6147ebc055dd31bf7375c73c438d8", "c8ed24d86755095c263a2f031752f817e668da3e", "2913075d7fe608ec2964a7e8d68eea1a15d4bb52", "4b4306d839a611b115dd1e922836da76163abe38", "0f149af8e52daa2628e407831d2b955f59242850", "10f7507b8408bf35125b8e04254ad890c8d45e1d", "d057f13d7f9bc2ddc98482df6088e35ea83c9df5", "d5a953f0911553d75d35f55731ec3ff00ac916c6", "27e16f13ce627631900f6354ab04a71c07219326", "3751e88244dab5b8ea6f69ae5056ffecb1856e71"]}, {"date": "1978", "abstract": "Two experiments were carried out on how questions are remembered. Subjects watched a videotape of a series of simple events and then answered 18 questions about these events. The questions were all of the same general syntactic form (e.g., \u201cDid the pencil fall against the jug on A?\u201d, where A refers to a particular location). They were designed to elicit three sorts of answer: \u201cyes,\u201d \u201cno\u201d because the event took place at another location, and \u201cno\u201d because the event did not take place at all. After the subjects had answered the questions, they were given an unexpected test of their ability to recall them. A difference in the memorability of the questions was predicted on the basis of a procedural theory of comprehension and a hypothesis about memory subjects should cease to process a question when they realize that it concerns an event that did not take place, and such questions should be harder to remember because they are processed to a lesser degree than the other sorts of question. Experiment 1 confirmed the predictions, but its results in part could be accounted for by assuming that subjects recalled the original events and used them as a cue to remembering the questions. Experiment 2 eliminated this explanation by showing that when subjects do not have to answer certain questions, their recall of them is very poor. However, the same differences in the memorability of the three sorts of question were obtained for both answered and unanswered questions.", "authors": ["Philip N. Johnson-Laird", "Charles E. Bethell-Fox"], "id": "e280d2855df983093523af744cc0e4e7ca25e57e", "title": "Memory for questions and amount of processing", "references": ["4687231be7f5c040a92bde66da9e06fc2f061f52", "c6b16f9e14a8a1eb20575f21329cf32bd33cfcf4", "9a85a78f50ba752018ad62e6603a82153317c97d", "619deb188c3c6d7d50fe9ea948ada007455705af"]}, {"date": "1973", "abstract": "The syntax and semantics of quantifiers is of crucial significance in current linguistic theorizing for more than one reason. The last statement of his grammatical theories by the late Richard Montague (1 973) is modestly entitled \" The Proper Treatment of Quantification in Ordinary English \". In the authoritative statement of his \" Generative Semantics \" , George Lakoff (1971, especially pp. 238-267) uses as his first and foremost testing-ground the grammar of certain English quantifiers. In particular, they serve to illustrate, and show need of, his use of global constraints governing the derivation of English sentences. Evidence from the behavior of quantifiers (including numerical expressions 1) has likewise played a major role in recent discussions of such vital problems as the alleged meaningdpreservation of transformations , 2 co-reference, 3 the role of surface structure in semantical interpretation, and so on. In all these problems, the behavior of natural-language quantifiers is one of the main issues. Quantifiers have nevertheless entered the Methodenstreit of contemporary linguistics in another major way, too. (These two groups of problems are of course interrelated.) This is the idea that the structures studied iln the so-called quantification theory of symbolic logic-otherwise know as first-order logic, (lower) predicate calculus, or elementary logic-can serve and suffice 6 as semantical representations of English sentences. Views of this general type have been proposed by McCawley (1971)* and Lakoff (1972)' (among others). A related theory of \" Deep Structure as Logical Form \" has been put forward and defended by G. Harman (1972). Theories of this general type may be compared with the traditiomal idea that quantification 'theory can be viewed as an abstraction from the behavior", "authors": ["Jaakko Hintikka"], "id": "a479441f4774cef5ad898181f0b15fcc4dfb7fd8", "title": "Quantifiers vs. Quantification Theory", "references": ["b4351302cfeae8fc1dd540927248af3279c23fae", "c6a211d1b462c093de34c11267d18e67700599e1", "9611cda7f12288567f005907d916d676412e9cca", "d5e7b496c3a20905e5b4a1c69720ee64f567a9cc"]}, {"date": "", "abstract": "Two experiments were carried out in order to try to resolve the controversy about whether the semantic processing of a word necessarily involves all the elements of its meaning. In the first experiment, subjects categorized a list of auditorially presented words according to whether or not they were natural consumable solids le.g., \"apple\"}. They were then given an unexpected test of their ability to recall the whole list, which contained equal numbers of words with none, one, two, or all three of the target components. The results confirmed the prediction that the greater the number of components a word has in common with a target category, the better it will be recalled. The second experiment used a visual presentation of a list of words, which subjects scanned for members of a particular target category. There were four different target categories assigned to independent groups of subjects. The results again confirmed the prediction. On the assumption that memorability in such tasks is determined by amount of processing, we conclude that some elements of the meaning of a word can be processed without having to process all of them.", "authors": ["Nathan P"], "id": "619deb188c3c6d7d50fe9ea948ada007455705af", "title": "Meaning , amount of processing , and memory for words", "references": ["bd705735b128b5edd0467030010c86631a60db34", "ad0d70b7cc299e7af721a079438a11e21834b85c", "4a94d7ae436e4565b58874f2a7857b2e5dbe9c91", "c6b16f9e14a8a1eb20575f21329cf32bd33cfcf4", "2932a16f87dd9bad2cc59145a8263239c6a9cfcc", "9c07b3656297ed82274598c257828dc79d40ff59", "a93ce5fdfbf09463163f1d36e881cafc071ae03d", "c48c9cc30b71ebae2a490c2e50fec8fec156f1f7"]}, {"date": "1973", "abstract": "In this experiment, Ss were visually presented with the names of two animals and were required to throw a switch under the name of the larger animal. Although error rates were relatively low (4.5%), reaction time (RT) was largely an inverse linear function of the logarithm of the estimated difference in animal size. Since RT is similarly related to size differences when Ss make direct perceptual comparisons (e.g., of lengths of line),.it was argued that Ss compare animal names by making an \u201cinternal psychophysical judgment.\u201d A more general model was then proposed for answering, from memory, questions of the form, \u201cWhich is /X/, /A/ or [B/?\u201d where /X/ is any comparative adjective and /A/and /B/ are any concrete nouns.", "authors": ["Robert S. Moyer"], "id": "aa2393e520ba147271cacca71eae7b169d20988c", "title": "Comparing objects in memory: Evidence suggesting an internal psychophysics", "references": []}, {"date": "1971", "abstract": "We define the following classes of program schemata: P = class of schemes using a finite number of simple variables PA = class of schemes using simple and subscripted variables (arrays) PR = class of schemes allowing recursive functions PL = class of schemes allowing labels as values Pm = class of schemes allowing a finite number of special markers as values Ppds = class of schemes using pushdown stores With these, we can also discuss for example PAm, the class of schemes allowing arrays, and special markers as values; and PAL the class of schemes allowing arrays, and labels as values. We argue that PA, PR, and PL faithfully represent mechanisms of subscripting, recursion, and labels as values, that are present in many \"real\" programming languages. We show that P \u2282 PR \u2282 PA \u2261 PAm \u2261 PAL \u2261 Ppdsm. Each of the inclusions and equivalences is effective, except for the equivalences concerning PA. For example, given a scheme in PAm an equivalent scheme in PA exists, but we also prove that you cannot (in general) construct it. We conjecture that PAL, PAm, and Ppdsm are indeed \"universal\".", "authors": ["Robert L. Constable", "David Gries"], "id": "efa59953df102e903e882f3589c23c9a505d7aa8", "title": "On Classes of Program Schemata", "references": []}, {"date": "1978", "abstract": "Two experiments were carried out in which subjects had to draw conclusions from syllogistic premises. The nature of their responses showed that the figure of the syllogisms exerted a strong effect on the accuracy of performance and on the nature of the conclusions that were drawn. For example, premises such as \u201cSome of the parents are scientists; All of the scientists are drivers\u201d tend to elicit the conclusion, \u201cSome of the parents are drivers\u201d rather than its equally valid converse, \u201cSome of the drivers are parents\u201d. In general, premises of the form iE$ created a bias towards conclusions of the form A-C, whereas premises of the form $=fi created a bias towards conclusions of the form C-A. The data cast doubt on current theories of syllogistic inference; a new theory was accordingly developed and implemented as a computer program. The theory postulates that quantified assertions receive an analogical mental representation which captures their logical properties structurally. A simple heuristic generates putative conclusions from the combined representations of premises, and such conclusions are put to logical tests which, if exhaustively conducted, invariably yield a correct response. Erroneous responses may occur if there is a failure to test exhaustively.", "authors": ["Philip N. Johnson-Laird", "Mark Steedman"], "id": "a1cb3eaa1747b43ad0b044a2389510428fa85e6b", "title": "The psychology of syllogisms", "references": ["04c32d0420d32bf144e1a2f99dd350178c64a3e8", "bb20f121c979b535bbeade5ac06676d627d4ad7d", "6e8a70eefa2688aba8801c2962e8d9c44156962c", "11fa4c9340a37256089ccff8a7e43ec4c4b92d34", "f5bedb5be75be235a29fca4dae8c79e61cda3832", "dd0e29a50c1790a59605d51fb2724d4e8c0d1922", "076cf5f7bface707157efd59c6ac3c3f7834c2c8", "a43aa78d78514a0d2fb1d85e38f5355b511ad135", "0a7397da9633c33c39c8004a1db261dc08ccce9c"]}, {"date": "1941", "abstract": "Semantic Scholar extracted view of \"On the First Variation of the Dirichlet-Douglas Integral and on the Method of Gradients.\" by Richard Courant", "authors": ["Richard Courant"], "id": "ec1abc5e3fb1a7f76497d6aadce6b9c9d146ec0d", "title": "On the First Variation of the Dirichlet-Douglas Integral and on the Method of Gradients.", "references": []}, {"date": "1935", "abstract": "Semantic Scholar extracted view of \"On a Minimal Problem in the Theory of Elasticity\" by Alexander Weinstein", "authors": ["Alexander Weinstein"], "id": "64e1a60f2a49eac29c39c650a232de81de4ecf96", "title": "On a Minimal Problem in the Theory of Elasticity", "references": []}, {"date": "", "abstract": "VIII uber den Inhalt im einzelnen unterrichtet das ausfuhrliche Ver zeichnis. Zur Form ist etwas Grundsatzliches zu sagen: Das klassische Ideal einer gewissermassen atomistischen Auffassung der Mathematik ver langt, den Stoff in Form von Voraussetzungen, Satzen und Beweisen zu kondensieren. Dabei ist der innere Zusammenhang und die Motivierung der Theorie nicht unmittelbar Gegenstand der Darstellung. In kom plementarer Weise kann man ein mathematisches Gebiet als stetiges Gewebe von Zusammenhangen betrachten, bei dessen Beschreibung die Methode und die Motivierung in den Vordergrund treten und die Kri stallisierung der Einsichten in isolierte scharf umrissene Satze erst eine sekundare Rolle spielt. Wo eine Synthese beider Auffassungen untunlich schien, habe ich den zweiten Gesichtspunkt bevorzugt. New Rochelle, New York, 24. Oktober 1937. R. Courant. Inhaltsverzeichnis. Erstes Kapitel. Vorbereitung. - Grundbegriffe. I. Orientierung uber die Mannigfaltigkeit der Losungen 2 1. Beispiele S. 2. - 2. Differentialgleichungen zu gegebenen Funk tionenscharen und -familien S. 7. 2. Systeme von Differentialgleichungen 10 1. Problem der Aquivalenz von Systemen und einzelnen Differential 2. Bestimmte, uberbestimmte, unterbestimmte gleichungen S. 10. - Systeme S. 12. J. Integrationsmethoden bei speziellen Differentialgleichungen. . . . . . 14 1. Separation der Variablen S. 14. - 2. Erzeugung weiterer Losungen durch Superposition. Grundlosung der Warmeleitung. Poissons Integral S.16. 4. Geometrische Deutung einer partiellen Differentialgleichung erster Ord nung mit zwei unabhangigen Variablen. Das vollstandige Integral . . 18 1. Die geometrische Deutung einer partiellen Differentialgleichung erster Ordnung S. 18. - 2. Das vollstandige Integral S. 19. - 3. Singulare Integrale S. 20.\"", "authors": ["Richard Courant", "David Hilbert"], "id": "941f8837e10b140af23d8d6631998f554784b86a", "title": "Methoden der mathematischen Physik", "references": []}, {"date": "", "abstract": "JJie Randwertaufgaben der mathematischen Physik erfordern durchweg die Darstellung endlicher, stetiger Funktionen in vorgeschriebenen endlichen Bereichen. Nur ausnahmsweise gelingt hier eine Entwicklung nach Potenzreihen, und noch seltener ist dieselbe im ganzen Bereich numerisch brauchbar. Endlich scheitert, selbst in F\u00e4llen, wo die Entwicklung prinzipiell m\u00f6glich w\u00e4re, ihre Berechnung h\u00e4ufig an dem Umstand, da\u00df sie die L\u00f6sung unendlich vieler linearen Gleichungen mit unendlich vielen Unbekannten erfordert. Sehr 'viel besser eignen sich Entwicklungen nach Polynomen, fownersche Reihen usw. f\u00fcr die Darstellung einer reellen Funktion w ( , y,...) in einem gegebenen Bereich, da hier f\u00fcr die Konvergenz im ganzen Bereich nur Eigenschaften der Stetigkeit usw. gefordert werden, die bei den Randwertaufgaben meist erf\u00fcllt sind. Bei numerisch gegebenem w bietet die Bestimmung der Koeffizienten eines Polynoms ton = au+&!#+'\u00b7* von gegebenem Grade n, derart, da\u00df wn als Approximation von w gelten k\u00f6nne, keinerlei Schwierigkeit, und es kann die Genauigkeit bei gen\u00fcgend gro\u00dfem n unbegrenzt gesteigert werden. Ist aber w als Integral einer Differentialgleichung, unter gewissen Nebenbedingungen, definiert, so gelingt die Berechnung der Koeffizienten at zun\u00e4chst nur in dem sehr speziellen Fall, wo eine Integration durch rasch konvergente Potenzreihen m\u00f6glich ist Es erhebt sich die Forderung, die angen\u00e4herte Darstellung des Integrals im ganzen vorgeschriebenen Bereich durch ein Polynom von gegebenem Grade n auch in diesem Falle allgemein durchzuf\u00fchren, in der Art, da\u00df bei wachsen", "authors": ["Walter Ritz"], "id": "b2a77070a435cb349ba4303095d18b1bec7136a6", "title": "\u00dcber eine neue Methode zur L\u00f6sung gewisser Variationsprobleme der mathematischen Physik.", "references": []}, {"date": "1975", "abstract": "A set of rules (or facts) about program synthesis is presented. The rules are about the process of programming, and are sutficient for the synthesis of an insertion sort program. The use of the rules to write a short LISP program is described. Taken together, the rules are an embodiment of a detailed theory which explains one small part of the programming process. The size of the set of rules suggests the complexity of the process of writing programs and indicates that much work will be required to codify significant amounts of programming knowledge as a step toward the development of program-understanding systems.", "authors": ["C. Cordell Green", "David R. Barstow"], "id": "a3b7d7c4ef3ee8bf775e9f2bc13f469512b6a3a9", "title": "Some Rules For The Automatic Synthesis Of Programs", "references": []}, {"date": "1962", "abstract": "A technique for empirical optimisation is presented in which a sequence of experimental designs each in the form of a regular or irregular simplex is used, each simplex having all vertices but one in common with the preceding simplex, and being completed by one new point. Reasons for the choice of design are outlined, and a formal procedure given. The performance of the technique in the presence and absence of error is studied and it is shown (a) that in the presence of error the rate of advance is inversely proportional to the error standard deviation, so that replication of observations is not beneficial, and (b) that the \u201cefficiency\u201d of the technique appears to increase in direct proportion to the number of factors investigated. It is also noted that, since the direction of movement from each simplex is dependent solely on the ranking of the observations, the technique may be used even in circumstances when a response cannot be quantitatively assessed. Attention is drawn to the ease with which second-o...", "authors": ["W. Spendley", "G. R. Hext", "F. R. Himsworth"], "id": "f7e5e6fc240e984329215532120152ba61bdfffc", "title": "Sequential Application of Simplex Designs in Optimisation and Evolutionary Operation", "references": []}, {"date": "1964", "abstract": "Semantic Scholar extracted view of \"An efficient method for finding the minimum of a function of several variables without calculating derivatives\" by M. J. D. Powell", "authors": ["M. J. D. Powell"], "id": "027e3321f34c9b2af721f9d102b9d9eb85147dcf", "title": "An efficient method for finding the minimum of a function of several variables without calculating derivatives", "references": []}, {"date": "1975", "abstract": "Abstract : It is proposed that the 3-D representation of an object is represented primarily by a stick-figure configuration, where each stick represents one or more axes in the object's generalized cylinder representation. The loosely hierarchical description of a stick figure is interpreted by a special-purpose processor, able to maintain two vector and the gravitational vertical relative to a Cartesian space-frame. It delivers information about the appearance of these vectors.", "authors": ["David C. Marr", "H. K. Hishihara"], "id": "f5e796dc458978f1a01d1b1698636df9c5181934", "title": "Spatial Disposition of Axes in a Generalized Cylinder Representation of Objects That Do Not Encompass the Viewer", "references": []}, {"date": "1951", "abstract": "The work described is the result of a study extending over the past few years by a chemist and a statistician. Development has come about mainly in answer to problems of determining optimum conditions in chemical investigations, but we believe that the methods will be of value in other fields where experimentation is sequential and the error fairly small.", "authors": ["George E. P. Box", "Kenneth B. Wilson"], "id": "bd2f9a0da65cb7a68041eb54e05407645d045db3", "title": "On the Experimental Attainment of Optimum Conditions", "references": []}, {"date": "1969", "abstract": "Semantic Scholar extracted view of \"Planner: a language for manipulating models and proving theorems in a robot\" ijcai-69\" by Carl Hewitt", "authors": ["Carl Hewitt"], "id": "816e9b69c6adb73205e841be8e69b3651d3bfbbf", "title": "Planner: a language for manipulating models and proving theorems in a robot", "references": []}, {"date": "1974", "abstract": "Semantic Scholar extracted view of \"Analysis of algorithm implementations\" by Gregory R. Ruth", "authors": ["Gregory R. Ruth"], "id": "cf213e9ac5f8445a1f1c8a37fd03cf6bb68c42de", "title": "Analysis of algorithm implementations", "references": []}, {"date": "1971", "abstract": "Publisher Summary This chapter describes an imagined interaction between a computer programmer and his machine, which might be made feasible within the next decade. It focuses on an intelligent assistant approach. The chapter also presents the earliest published description of an intelligent programming assistant. It describes an imagined interaction between a computer programmer and an intelligent program verifier assistant; such systems will be feasible within the following decade. The programmer is at an interactive console, designing a program, first in its overall outline, then by successive developments in detail. The computer is, of course, serving its customary role as syntactic analyzer, code generator, program executor, prompter, and file handler. In addition, the computer is continually checking the program, at each level of specification, for consistency with the programmer's stated intentions.", "authors": ["Robert W. Floyd"], "id": "5a061b1cab0f241d6f7226f6c0b12e931cabd90f", "title": "Toward Interactive Design of Correct Programs", "references": []}, {"date": "1976", "abstract": "Abstract : This paper proposes a system which, when implemented, will be able to understand mathematical FORTRAN programs, such as those in the IBM Scientific Subroutine Package. The system takes, as input, a program and annotation of the program. In order to understand the program, the system develops a 'plan' for it. The plan specifies the purpose of each feature of the program, and how these features cooperate in order to create the behavior exhibited by the program. The system can use its understanding of the program to answer questions about it including questions about the ramifications of a proposed modification. It is also able to aid in debugging the program by detecting errors in it, and by locating the features of the program which are responsible for an error. The system should be of significant assistance to a person who is writing a program. (Author)", "authors": ["Richard C. Waters"], "id": "23174bbbc77874d0ebee7b8c2719037d86993170", "title": "A System for Understanding Mathematical FORTRAN Programs", "references": []}, {"date": "1974", "abstract": "Ss determined whether words in a list were members of a relatively general or specific category (e.g., foods vs vegetables), and then they were given an unanticipated free recall test. Assumptions were that: (1) evaluating a word against a general criterion (e.g., food) results in the detection of fewer semantic elements than would evaluating it against a specific criterion (e.g., vegetable), and (2) detecting more semantic elements primes memory. In three studies, free recall scores were highest for the specific search. Instructions to form an image or to think of an association for each word did not improve recall. Telling Ss, just before recall, what categories composed the list increased recall clustering, but not recall.", "authors": ["Lawrence T. Frase", "Richard Kammann"], "id": "c48c9cc30b71ebae2a490c2e50fec8fec156f1f7", "title": "Effects of search criterion upon unanticipated free recall of categorically related words", "references": ["6ccf4974ec9e733bf48cd423fb11615064f74fd2", "c6b16f9e14a8a1eb20575f21329cf32bd33cfcf4", "bd252b15d1ef3515ea9cd7951be4422c04a1e63c"]}, {"date": "1964", "abstract": "SIR is a computer system, programmed in the LISP language, which accepts information and answers questions expressed in a restricted form of English. This system demonstrates what can reasonably be called an ability to \"understand\" semantic information. SIR''s semantic and deductive ability is based on the construction of an internal model, which uses word associations and property lists, for the relational information normally conveyed in conversational statements. A format-matching procedure extracts semantic content from English sentences. If an input sentence is declarative, the system adds appropriate information to the model. If an input sentence is a question, the system searches the model until it either finds the answer or determines why it cannot find the answer. In all cases SIR reports its conclusions. The system has some capacity to recognize exceptions to general rules, resolve certain semantic ambiguities, and modify its model structure in order to save computer memory space. Judging from its conversational ability, SIR is more \"intelligent\" than any existing question-answering system. The author describes how this ability was developed and how the basic features of SIR compare with those of other systems. The working system, SIR , is a first step toward intelligent machine communication. The author proposes a next step by describing how to construct a more general system which is less complex and yet more powerful than SIR . This proposed system contains a generalized version of the SIR model, a formal logical system called SIR1 , and a computer program for testing the truth of SIR1 statements with respect to the generalized model by using partial proof procedures in the predicate calculus. The thesis also describes the formal properties of SIR1 and how they relate to the logical structure of SIR .", "authors": ["Bertram Raphael"], "id": "3fdec3403ac6147ebc055dd31bf7375c73c438d8", "title": "SIR: A COMPUTER PROGRAM FOR SEMANTIC INFORMATION RETRIEVAL", "references": []}, {"date": "1935", "abstract": "Semantic Scholar extracted view of \"An atmosphere effect in formal syllogistic reasoning\" by Robert Sessions Woodworth et al.", "authors": ["Robert Sessions Woodworth", "Saul B. Sells"], "id": "0a7397da9633c33c39c8004a1db261dc08ccce9c", "title": "An atmosphere effect in formal syllogistic reasoning", "references": []}, {"date": "1969", "abstract": "Eight inferences with quantified sentences were judged as valid or invalid. Each inference involved an active and its correlative passive sentence, e.g. \u2018Some medicine cures every disease. Therefore: Every disease is cured by some medicine.\u2019 There were four logically distinct pairs of sentences, and each pair was presented twice: once as an inference from active to passive, and once as the converse inference from passive to active. As predicted, what was crucial in evaluating these inferences was not voice but the position of \u2018some\u2019. An inference from a premise with \u2018some\u2019 in the grammatical subject to a conclusion with \u2018some\u2019 in the grammatical object tended to be judged as valid, whereas an inference in the converse direction tended to be judged as invalid. The results of an earlier study of the ambiguity of the sentences provided a good estimate of performance in the present inferential task.", "authors": ["Philip N. Johnson-Laird"], "id": "076cf5f7bface707157efd59c6ac3c3f7834c2c8", "title": "REASONING WITH AMBIGUOUS SENTENCES", "references": []}, {"date": "1973", "abstract": "Four experiments dealt with the verification of semantic relations. In Experiment I, subjects decided whether an instance was a member of a specified category. For some categories (for example, birds) verification was faster when the target category was a direct superordinate (bird) than a higher level superordinate (animal), while for another category (mammal) this finding reversed. Experiment II obtained ratings of semantic distance that accounted for the previously obtained verification results. Multidimensional scaling of the ratings suggested that semantic distance could be represented as Euclidean distance in a semantic space. Experiments III and IV indicated that semantic distance could predict RTs in another categorization task and choices in an analogies task. These results place constraints on a theory of semantic memory.", "authors": ["Lance J. Rips", "Edward Joseph Jr. Shoben", "Edward E. Smith"], "id": "a93ce5fdfbf09463163f1d36e881cafc071ae03d", "title": "Semantic Distance and the Verification of Semantic Relations.", "references": []}, {"date": "1975", "abstract": "The present paper offers two processing models of how reasoners solve categorical syllogisms. The models are based on traditional statements of the atmosphere effect and the conversion hypothesis. A test of the two models shows that previous studies of formal reasoning have unnecessarily restricted the scope of the hypotheses and have failed to compare them on the critical conditions and in their intended senses. Both models are reasonably accurate in predicting the overall distribution of errors. While the feature selection model is superior to the conversion model in predicting the decisions on a set of critical problems, the underlying assumption of the feature selection model is not supported by the data.", "authors": ["Russell Revlis"], "id": "a43aa78d78514a0d2fb1d85e38f5355b511ad135", "title": "Two models of syllogistic reasoning: Feature selection and conversion", "references": []}, {"date": "1971", "abstract": "This study explored the effects of extralinguistic context on how people process sentences which describe transitive relations between two items. The Ss were given two different tasks; they either had to move both items to make the described arrangement, or they had to move either one of the two items with respect to the other. The former type of task was harder. The hypothesis proposed to account for the results is that Ss initially process these sentences to determine the role of the grammatical subject in the described relation, an analysis which does not explicitly specify the role of the grammatical object. The role of the grammatical object, we suggested, is only determined explicitly when the task requires it.", "authors": ["Janellen Huttenlocher", "Susan L. Weiner"], "id": "f5bedb5be75be235a29fca4dae8c79e61cda3832", "title": "Comprehension of instructions in varying contexts", "references": []}, {"date": "1971", "abstract": "Abstract The experiment attempted to provide evidence that errors in reasoning do not imply that reasoning is not occurring at all. One suggestion has been that errors occur because S s misinterpret syllogistic premises in order to refer to simpler class relations. When S s were given modified syllogisms having these simpler relations as premises, they performed considerably better than when given traditional syllogisms. Furthermore, they responded to the traditional syllogisms as if they were the modified syllogisms, which accounted for their errors. A second source of error was traced to the logical structure of the syllogism. It was found that the more alternatives generated by a set of premises, the more difficult was the syllogism. It was argued that errors in reasoning are not the result of illogical or alogical processes, but are the result of an incomplete analysis of the logical structure of the syllogism.", "authors": ["John Ceraso", "Angela Provitera"], "id": "6e8a70eefa2688aba8801c2962e8d9c44156962c", "title": "Sources of error in syllogistic reasoning", "references": []}, {"date": "1969", "abstract": "An experimental investigation was made into the meaning of eight types of doubly-quantified sentence, e.g. \u201cEvery medicine cures some disease,\u201d \u201cSome disease is cured by every medicine.\u201d All the sentences were ambiguous, depending upon the interpretation of the quantifiers. Subjects classified diagrams representing different specific situations as truthfully or falsely described by the sentences. The classifications revealed that the order of occurrence of the two quantifiers had a crucial effect, causing active and correlative passive to receive different interpretations. This suggested that in the process of understanding an ambiguous sentence a bias towards one intepretation may be created by word order.", "authors": ["Philip N. Johnson-Laird"], "id": "11fa4c9340a37256089ccff8a7e43ec4c4b92d34", "title": "On Understanding Logically Complex Sentences", "references": []}, {"date": "1972", "abstract": "'Is there any other point to which you would wish to draw my attention?' 'To the curious incident of the dog in the night-time.' 'The dog did nothing in the night-time.' 'That was the curious incident, ' remarked Sherlock Holmes. The quotation from A. Conan Doyle with which this book begins, is a delightfully appropriate summation of the authors' point of view garnered from their fifteen years of experiments on the psychology of reasoning. Dr. Wason and Dr. Johnson-Laird are intrigued by the extent to which most individuals can be considered naturally rational thinkers. They present here the surprising results of their comprehensive investigations of how humans draw explicit conclusions from evidence. Given a set of assertions, the authors write, to what extent can the individual appreciate all that follows from them by virtue of logic alone, and remain unseduced by plausible, but fallacious conclusions? We are not concerned with whether these assertions are true or false, nor with whether the individual holds them among his beliefs, nor with whether they are sane or silly. At the core of the Psychology of Reasoning is a vigorous discussion that incorporates various illustrations--some of them humorous, all of them fascinating--of the use of reason under a wide variety of different conditions. Particular emphasis is placed on the difficulties involved in dealing with negatively marked information that must be combined and used with other information for reaching conclusions. Thorough treatment is given as well to the search for plausible contexts that will render anomalous or ambiguous statements sensible. The authors have strived to isolate the components ofinference, the basic steps of any kind of deductive activity, in order to determine the psychological processes involved in them. What has been the outcome of this research? Dr. Wason and Dr. Johnson-Laird conclude, our research has suggested that the individual's logical competence may be either enhanced or limited by performance variables. And, of these, content has turned out to be vitally important for revealing, or obscuring structure. At best, we can all think like logicians; at worst, logicians all think like us.", "authors": ["Peter Cathcart Wason", "Philip N. Johnson-Laird"], "id": "dd0e29a50c1790a59605d51fb2724d4e8c0d1922", "title": "Psychology of Reasoning: Structure and Content", "references": []}, {"date": "1957", "abstract": "Sixteen problems of the three-term series type are presented to each of 64 11-year-old children and 32 16-year-olds. The older children show evidence of solving these problems by articulated sequences of analytical judgments which accord with the varying logical structures of the problems, show no influence of atmosphere effect, show marked practice effects in progressing from one problem to the next, and solve problems involving the relatively abstract \u2018happier-sadder\u2019 relation as quickly as identically structured problems involving the relatively concrete \u2018taller-shorter\u2019 relation. In contrast, the younger children show but slight evidence of being influenced differentially by variations in logical structure, are susceptible to atmosphere effect, show smaller practice effects, and solve \u2018taller-shorter\u2019 problems more rapidly than \u2018happier-sadder\u2019 problems. The results are interpreted as demonstrating that increasing age brings increasing appreciation of the structural characteristics of series as such, together with increasing skill in dealing with serial relations which are progressively more remote from the perceptual-motor level of behaviour.", "authors": ["I. M. Hunter"], "id": "04c32d0420d32bf144e1a2f99dd350178c64a3e8", "title": "The solving of three-term series problems.", "references": []}, {"date": "1981", "abstract": "Semantic Scholar extracted view of \"12. The Psychological Unreality of Semantic Representations\" by Janet Dean Fodor et al.", "authors": ["Janet Dean Fodor", "Jerry A. Fodor", "Merrill F. Garrett"], "id": "4a94d7ae436e4565b58874f2a7857b2e5dbe9c91", "title": "12. The Psychological Unreality of Semantic Representations", "references": []}, {"date": "1972", "abstract": "Semantic Scholar extracted view of \"Conceptual dependency: A theory of natural language understanding \u2606\" by Roger C. Schank", "authors": ["Roger C. Schank"], "id": "2932a16f87dd9bad2cc59145a8263239c6a9cfcc", "title": "Conceptual dependency: A theory of natural language understanding \u2606", "references": []}, {"date": "1969", "abstract": "Semantic Scholar extracted view of \"Differential effects of incidental tasks on the organization of recall of a list of highly associated words.\" by Thomas S. Hyde et al.", "authors": ["Thomas S. Hyde", "James J. Jenkins"], "id": "ad0d70b7cc299e7af721a079438a11e21834b85c", "title": "Differential effects of incidental tasks on the organization of recall of a list of highly associated words.", "references": []}, {"date": "1971", "abstract": "Abstract Perceptual learning involves the learning of distinctive features and higher-order invariants, learning progressing actively toward the most economical features and structure. Features of words are classified as phonological, graphic, semantic and syntactic. Features of these classes are processed independently and sequentially. Ordering of priorities changes with development, and shifts strategically with the demands of the task. Evidence is presented for priority differences for each class of feature depending on task differences.", "authors": ["Eleanor J. Gibson"], "id": "bd705735b128b5edd0467030010c86631a60db34", "title": "Perceptual learning and the theory of word perception", "references": []}, {"date": "1972", "abstract": "Semantic Scholar extracted view of \"Sense impression as an encoding dimension of words\" by Delos D. Wickens et al.", "authors": ["Delos D. Wickens", "Donald B. Reutener", "F. Thomas Eggemeier"], "id": "bd252b15d1ef3515ea9cd7951be4422c04a1e63c", "title": "Sense impression as an encoding dimension of words", "references": []}, {"date": "1975", "abstract": "SUMMARY Ten experiments were designed to explore the levels of processing framework for human memory research proposed by Craik and Lockhart (1972). The basic notions are that the episodic memory trace may be thought of as a rather automatic by-product of operations carried out by the cognitive system and that the durability of the trace is a positive function of \"depth\" of processing, where depth refers to greater degrees of semantic involvement. Subjects were induced to process words to different depths by answering various questions about the words. For example, shallow encodings were achieved by asking questions about typescript; intermediate levels of encoding were accomplished by asking questions about rhymes; deep levels were induced by asking whether the word would fit into a given category or sentence frame. After the encoding phase was completed, subjects were unexpectedly given a recall or recognition test for the words. In general, deeper encodings took longer to accomplish and were associated with higher levels of performance on the subsequent memory test. Also, questions leading to positive responses were associated with higher retention levels than questions leading to negative responses, at least at deeper levels of encoding. Further experiments examined this pattern of effects in greater analytic detail. It was established that the original results did not simply reflect differential encoding times; an experiment was designed in which a complex but shallow task took longer to carry out but yielded lower levels of recognition than an easy, deeper task. Other studies explored reasons for the superior retention of words associated with positive responses on the initial task. Negative responses were remembered as well as positive responses when the questions led to an equally elaborate encoding in the two cases. The idea that elaboration or \"spread\" of encoding provides a better description of the results was given a further boost by the finding of the typical pattern of results under intentional learning conditions, and where each word was exposed for 6 sec in the initial phase. While spread and elaboration may indeed be better descriptive terms for the present findings, retention depends critically on the qualitative nature of the encoding operations performed; a minimal semantic analysis is more beneficial than an extensive structural analysis. Finally, Schulman's (1974) principle of congruity appears necessary for a complete description of the effects obtained. Memory performance is enhanced to the extent that the context, or encoding question, forms an integrated unit with the word presented. A congruous encoding yields superior memory performance because a more elaborate trace is laid down and because in such cases the structure of semantic memory can be utilized more effectively to facilitate retrieval. The article concludes with a discussion of the broader implications of these data and ideas for the study of human learning and memory,", "authors": ["Fergus I. M. Craik", "Endel Tulving"], "id": "9c07b3656297ed82274598c257828dc79d40ff59", "title": "Depth of processing and the retention of words", "references": ["acbd24f22cf9db66bf140e10636ac071f6368ea7", "2f66196293608365eb59137f17cd6e1c4eeeb20b", "e31a771cc15bd4d67bad13a6af0514f80c2d4028", "d2ac4eb014a7a4e3bfafdca2f94fd8e32bef8d46", "33c7326a5b575769dec4475d19ffa736cc36ffbc", "c689f8517c50925c1db9ae563d03635009e1dda7", "5aa8a0fc568de7eb97cfd4778b428c990c87d1b0", "e940dce76e423390383fab059db2a77c3649e2ba", "d2d9ee21203fe513d382cd2fab3672f34a155222", "5f34169675a4fce2a905ee2bc2badb1032e1ce3e"]}, {"date": "1961", "abstract": "Semantic Scholar extracted view of \"An application of games to the completeness problem for formalized theories\" by Andrzej Ehrenfeucht", "authors": ["Andrzej Ehrenfeucht"], "id": "9611cda7f12288567f005907d916d676412e9cca", "title": "An application of games to the completeness problem for formalized theories", "references": []}, {"date": "1970", "abstract": "Semantic Scholar extracted view of \"Measurement of clustering in free recall.\" by E. C. Dalrymple-Alford", "authors": ["E. C. Dalrymple-Alford"], "id": "6ccf4974ec9e733bf48cd423fb11615064f74fd2", "title": "Measurement of clustering in free recall.", "references": []}, {"date": "1969", "abstract": "In manufacture of variegated soap bars, noodles are choke fed into the final plodder through an opening communicating essentially only with a portion of the worm of the plodder which turns downwardly.", "authors": ["James D. McCawley"], "id": "c6a211d1b462c093de34c11267d18e67700599e1", "title": "Where do noun phrases come from", "references": []}, {"date": "1950", "abstract": "The first order functional calculus was proved complete by Godel in 1930. Roughly speaking, this proof demonstrates that each formula of the calculus is a formal theorem which becomes a true sentence under every one of a certain intended class of interpretations of the formal system. For the functional calculus of second order, in which predicate variables may be bound, a very different kind of result is known: no matter what (recursive) set of axioms are chosen, the system will contain a formula which is valid but not a formal theorem. This follows from results of Godel concerning systems containing a theory of natural numbers, because a finite categorical set of axioms for the positive integers can be formulated within a second order calculus to which a functional constant has been added. By a valid formula of the second order calculus is meant one which expresses a true proposition whenever the individual variables are interpreted as ranging over an (arbitrary) domain of elements while the functional variables of degree n range over all sets of ordered n -tuples of individuals. Under this definition of validity, we must conclude from Godel's results that the calculus is essentially incomplete. It happens, however, that there is a wider class of models which furnish an interpretation for the symbolism of the calculus consistent with the usual axioms and formal rules of inference. Roughly, these models consist of an arbitrary domain of individuals, as before, but now an arbitrary class of sets of ordered n -tuples of individuals as the range for functional variables of degree n . If we redefine the notion of valid formula to mean one which expresses a true proposition with respect to every one of these models, we can then prove that the usual axiom system for the second order calculus is complete: a formula is valid if and only if it is a formal theorem.", "authors": ["Leon Henkin"], "id": "d5e7b496c3a20905e5b4a1c69720ee64f567a9cc", "title": "Completeness in the Theory of Types", "references": []}, {"date": "1970", "abstract": "The problem discussed here is to find a basis for a uniform treatment of the relation between pronouns and their antecedents, taking into account both linguists' and philosophers' approaches. The two main candidates would appear to be the linguists' notion of coreference and the philosophers' notion of pronouns as variables. The notion of coreference can be extended to many but not all cases where the antecedent is non-referential. The pronouns-as-variables approach appears to come closer to full generality, but there are some examples of \u2018pronouns of laziness\u2019 which appear to resist either of the two approaches.", "authors": ["Barbara H. Partee"], "id": "b4351302cfeae8fc1dd540927248af3279c23fae", "title": "Opacity, coreference, and pronouns", "references": ["b250f8439864b9ba517566c83e674dd7c887f24f"]}, {"date": "1977", "abstract": "This paper is an empirical and theoretical critique of the depth-of-processing view ( Craik & Lockhart, Journal of Verbal Learning and Verbal Behavior , 1972 , 11 , 671\u2013684). Three new experiments demonstrate that repetition at the phonemic depth of processing does facilitate memory, regardless of whether the repetitions are massed or distributed and regardless of whether the dependent variable is uncued recall, cued recall, or recognition. Previous studies reporting null effects of repetition are reviewed and called into question. These results, combined with logical shortcomings in the principle that deeper processing facilites memory, imply that the current view of depth of processing is not valid as a scientific theory, explanation, or description of the available data.", "authors": ["Thomas O. Nelson"], "id": "9a85a78f50ba752018ad62e6603a82153317c97d", "title": "REPETITION AND DEPTH OF PROCESSING", "references": []}, {"date": "1973", "abstract": "Abstract This paper investigates the hypothesis that the component of sentence processing time directly attributable to syntactic processing depends critically on certain semantic properties of the sentence. Using two different procedures, it is found in a series of experiments that there is little evidence to support this view. Specifically, it is shown that syntactic processing time tends to be constant for sentences of varying semantic plausibility but constant syntactic structure, and further, that reversibility fails to affect sentence processing in a systematic way. These facts are interpreted as indicating that the recovery of the underlying structure of a sentence is controlled by purely syntactic properties of the input.", "authors": ["Kenneth I. Forster", "Ilmar Olbrei"], "id": "4687231be7f5c040a92bde66da9e06fc2f061f52", "title": "Semantic heuristics and syntactic analysis", "references": []}, {"date": "1969", "abstract": "This paper describes a system for \"remembering\" a story or passage in English and for the subsequent retrieval of responses to questions. Although it functions as an information retrieval system, it is also intended as a model for long-term human memory. Information is stored in the form of predicates and propertylists. In translating from natural language, the system carries out a transformational analysis of each sentence and identifies its deep structure interpretation. This grammatical analysisis essential to the identification of the predicates as well as relationships among them. \n \nThe first two subsystems carry out the grammatical analysis. A third identifies the predicates and propertylists, determines a time and priority structure, forms a logical map of relationships among predicates, and eliminates some predicates based on an assignment of priority. The fourth subsystem is used to answer inquiries about the stored information. \n \nThe method is illustrated with references to one story that has been processed and with examples of questions that might be asked.", "authors": ["Alan L. Tharp", "Gilbert K. Krulee"], "id": "3751e88244dab5b8ea6f69ae5056ffecb1856e71", "title": "Using Relational Operators to Structure Long-Term Memory", "references": ["6c4f43d484ac3fc5f277a2fe4305aebb6373970e", "8c8f54ae559d324ea5ccb241bd71a8deaa081694", "dc57c2a9cf6d42dc3424cb4d1ea53f1e87efbd9c", "dd15a1958d5a7bfa45548eb101bef4186acf2299", "4382849df1985a665bdef400625d56a8b728c91a", "69dc76d406bc4a2204e9782e0abc27dd0f37f457", "e685731643f15a82c637b1ef1b745d5618fff3c8", "89d025804988944d6fa4e95f49bff011b33d1418", "56dac50c9b1ac92b8e27583bb4abdbaa66ef326f"]}, {"date": "1965", "abstract": "A solution to the analysis problem for a class of grammars appropriate to the description of natural languages is essential to any system which involves the automatic processing of natural language inputs for purposes of man-machine communication, translation, information retrieval, or data processing. The analysis procedure for transformational grammars described in this paper was developed to explore the feasibility of using ordinary English as a computer control language.", "authors": ["Arnold M. Zwicky", "Joyce Friedman", "Barbara C. Hall", "Donald E. Walker"], "id": "27e16f13ce627631900f6354ab04a71c07219326", "title": "The mitre syntactic analysis procedure for transformational grammars", "references": []}, {"date": "1968", "abstract": "Abstract : Learning a natural language is taken as an improvement in a system's ability to express situations in a natural language. This dissertation describes a computer program, called Zbie, written in IPL-V, which accepts the description of situations in a uniform, structured functional language and tries to express these situations in a natural language. Examples are given for German and, mostly, Russian. At run-time, Zbie builds simple memory structures. Patterns and sets are built on the functional language. The translation rules of the patterns and an in-context vocabulary provide the transition to the natural language. Zbie is a cautious learner, and avoids errors by several mechanisms. Zbie is capable of some evolutionary learning.", "authors": ["Laurent Sikl\u00f3ssy"], "id": "d057f13d7f9bc2ddc98482df6088e35ea83c9df5", "title": "Natural language learning by computer", "references": []}, {"date": "1968", "abstract": "The long-term goal for computational linguistics is to increase our understanding of linguistic and conceptual structures and to formally describe them so that computers can deal effectively with natural languages in such applications as question answering, stylistic and content analysis, essay writing, automated translation, etc. The eventual realization of this goal requires not only a satisfactory model of linguistic structures, but also models for verbal understanding and verbal meaning. In this paper we outline a theory and a model of verbal understanding and describe Protosynthex III, an experimental implementation of the model in the form of a general-purpose language processing system. The effectiveness of the model in representing the process of verbal understanding is demonstrated in terms of Protosynthex III's capability to disambiguate English sentences, to answer a range of English questions and to derive and generate meaning-preserving paraphrases.", "authors": ["Robert F. Simmons", "John F. Burger", "Robert M. Schwarcz"], "id": "d5a953f0911553d75d35f55731ec3ff00ac916c6", "title": "A computational model of verbal understanding", "references": []}, {"date": "1966", "abstract": "An experimental system that uses LISP to make a conceptual dictionary is described. The dictionary associates with each English word the syntactic information, definitional material, and references to the contexts in which it has been used to define other words. Such relations as class inclusion, possession, and active or passive actions are used as definitional material. The resulting structure serves as a powerful vehicle for research on the logic of question answering. Examples of methods of inputting information and answering simple English questions are given. An important conclusion is that, although LISP and other list processing languages are ideally suited for producing complex associative structures, they are inadequate vehicles for language processing on any large scale\u2014at least until they can use auxiliary memory as a continuous extension of core memory.", "authors": ["Robert F. Simmons"], "id": "0f149af8e52daa2628e407831d2b955f59242850", "title": "Storage and retrieval of aspects of meaning in directed graph structures", "references": []}, {"date": "1965", "abstract": "Abstract : A dependency analysis system based on pattern recognition and learning logic was developed to infer word-classes and rules of syntactic combination from experience with text which had been analyzed. The characteristics used to form word-classes are the depth in the dependency tree of each word, the direction of its governor, and the same features for each of its immediate neighbors. Syntactic rules of combination show the relation of a word to its governor in the depth pattern of the sentence. The system was tested on 400 elementary Basic English sentences including 300 used earlier by Knowlton in a different learning parser. The Pattern Learning Parser was able to derive a grammar which in a second pass allowed completely correct parsing of all 400 sentences. After experience with 300 sentences it was able to generalize with 77% accuracy to the next 100. In accumulative learning trials after the first 200 sentences it averaged a probability of .9 for accurately parsing each new sentence it encountered. It was concluded that the system was adequate for learning to parse the bulk of Basic English but that further development is required before conclusions about its application to ordinary English can be stated. The system is operational and available on the ARPA/SDC time-shared Q-32 computing system. (Author)", "authors": ["Keren McConlogue", "Robert F. Simmons"], "id": "4b4306d839a611b115dd1e922836da76163abe38", "title": "Analyzing English syntax with a pattern-learning parser", "references": []}, {"date": "1968", "abstract": "Simmons has presented a survey of some fifteen experimental question-answering and related systems which have been constructed since 1959. These systems take input questions in natural English (subject to varying constraints) and attempt to answer the questions on the basis of a body of information, called the data base, which is stored inside the computer. This process can be conceptually divided into three phases---syntatic analysis, semantic analysis, and retrieval, as illustrated schematically in Figure 1. The first phase consists of parsing the input sentence into a structure which explicitly represents the grammatical relationships among the words of the sentence. Using this information the second component constructs a representation of the semantic content or \"meaning\" of the sentence. The remaining phase consists of procedures for either retrieving the answer directly from the data base, or else deducing the answer from information contained in the data base. The dotted lines in the figure represent the possible use of feedback from the later stages to aid in parsing and semantic interpretation.", "authors": ["William A. Woods"], "id": "2913075d7fe608ec2964a7e8d68eea1a15d4bb52", "title": "Procedural semantics for a question-answering machine", "references": []}, {"date": "1965", "abstract": "As an investigation in artificial intelligence, computer experiments on deductive question-answering were run with a LISP program called DEDUCOM, an acronym for DEDUctive COMmunicator. When given 68 facts, DEDUCOM answered 10 questions answerable from the facts. A fact tells DEDUCOM either some specific information or a method of answering a general kind of question. Some conclusions drawn in the article are: (1) DEDUCOM can answer a wide variety of questions. (2) A human can increase the deductive power of DEDUCOM by telling it more facts. (3) DEDUCOM can write very simple programs (it is hoped that this ability is the forerunner of an ability to self-program, which is a way to learn). (4) DEDUCOM is very slow in answering questions. (5) DEDUCOM's search procedure at present has two bad defects: some questions answerable from the given facts cannot be answered and some other answerable questions can be answered only if the relevant facts are given in the \"right\" order. (6) At present, DEDUCOM's method of making logical deductions in predicate calculus has two bad defects: some facts have to be changed to logically equivalent ones before being given to DEDUCOM, and some redundant facts have to be given to DEDUCOM.", "authors": ["James R. Slagle"], "id": "c8ed24d86755095c263a2f031752f817e668da3e", "title": "Experiments with a deductive question-answering program", "references": []}, {"date": "1972", "abstract": "This paper briefly reviews the evidence for multistore theories of memory and points out some difficulties with the approach. An alternative framework for human memory research is then outlined in terms of depth or levels of processing. Some current data and arguments are reexamined in the light of this alternative framework and implications for further research considered.", "authors": ["Fergus I. M. Craik", "Russell A. Lockhart"], "id": "c6b16f9e14a8a1eb20575f21329cf32bd33cfcf4", "title": "Levels of processing: A framework for memory research", "references": ["362d7fd794ab537c09b3451711c9de31e3501715", "8def1503d588f2dc413e0b3e769be5e033d9e737", "500bc555bbb2344871e61669d686570a8cbc9a90", "b96df0e70e7c9aa041c38996c63e57dbb200efc6", "5ee5b89dc29620fb8e2cc19a885d0b7a13df0715", "7f63bcc1a9013da9e9ca5ca1351b64a937e6c29c", "8d8e1983d277b6182365de3c7bfac9326583bdd8", "fe843bf6a7ee65c58382267e1a0695006c189f1b", "7d4bea05a9a4332b653cce68d47298dc838d16f9"]}, {"date": "2004", "abstract": "SummaryDefining the semantics of programming languages by axioms and rules of inference yields a deduction system within which proofs may be given that programs satisfy specifications. The deduction system herein is shown to be consistent and also deduction complete with respect to Hoare's system. A subgoaler for the deduction system is described whose input is a significant subset of Pascal programs plus inductive assertions. The output is a set of verification conditions or lemmas to be proved. Several non-trivial arithmetic and sorting programs have been shown to satisfy specifications by using an interactive theorem prover to automatically generate proofs of the verification conditions. Additional components for a more powerful verification system are under construction.", "authors": ["Shigeru Igarashi", "Ralph L. London", "David C. Luckham"], "id": "6321686427c86b87e1071497ffd633b71aad6fb6", "title": "Automatic program verification I: A logical basis and its implementation", "references": ["2cf7ae6adfb101ca984b1988bd6bb0be4f9b739f", "405666eddc9d6db81880a195b26d5a66a153cc36", "ed930c69cdc66f983c5abfd041ce9fef3565c08b", "a17e7dce1c39b30d22158e4a8c133d0a4b36f1ca", "5a061b1cab0f241d6f7226f6c0b12e931cabd90f", "244485ba5b8a0fcb678af91dec91d12d13f0f1d3", "5d8056e326d4199d157a17fbeee97a7349d2824c", "5a19db7d827f623143e92bca8c082de5c014d59c"]}, {"date": "1978", "abstract": "In this proposal we describe an interactive programming environment to be used as a tool by the expert programmer in the process of program design and maintenance. This environment (called the. Programmer's Apprentice) will be capable of u.nderstandin., explaining and reaso-ing about the behavior. of real-world :programs, with particular. emphasis on LISP programs involving side effects on complex data-structures. In achieving such a system, our pivotal theoretical committment will be to a. view of programs as engineered devices whose analysis can be carried-out at many level of abstraction . The analysis of a program will lead to a set of logical dependencies between modules which explains how and why modules interact to achieve an overall intention. Such a network of dependencies is a teleological structure which we call a plan; the process of elucidating such a plan stucture and showing that it is coherent and\" that. it achieves its overall intended. behavior we call plan verification. Our research will be concerned with the design of a plan. verification programn called REASON. Given a descriiption of the data flow between modules (specified at any level of abstraction), REASON will determine whether or not these modules cooperate to. achieve their intended net behavior. In so doing, REASON elucidates the plan. structure which, in turn, is used as the pivotal data structure in the pertubation analysis necessary for programr evolution, explanation and debugging. ,This approach to program verification will be sharply contrasted with the traditional Floyd.-Hoare systems. which overly restrict themselves to surface features. of the programming language. More similar in philosophy is. the evolving methodology of languages like CLU or ALPH'ARD which, stress conceptual layering in program structure and therefore have a verification methodology more like our own. Finally, a methodology of .program design will be explored in which man and machine interact in the .formation of -a verified plan at an appropriate level of abstraction. It is hoped that examination of this process will shed light on the theory of \u2022automatic design, thereby aiding in the future development of rich automatic programming systems. The~sis Proposal", "authors": ["Howard E. Shrobe"], "id": "006ae42f9e23ae43afff97082a484de2443c616f", "title": "Plan Verification in a Programmer's Apprentice", "references": ["16e813d1aa3684fa6b4b5b25cf2441f6e13dd3f1", "23533725173200046d037c055779e190febeb4cd", "7c5144af9595b8ff33347906eb28a7ed9f7f55e3", "91a3d5a53961587bb6a5c1fc9fbe8b078013ba44", "ebbec3500ddff64f04a46b0a9a3d48b6e92ba2b5", "a3b7d7c4ef3ee8bf775e9f2bc13f469512b6a3a9", "59e5c4c530e996e2ec75ac88a0402e87ce85370e", "bf15ce3d1575d124527496cb249dc1249eee0acb", "d22000e27a064baa027fa685353abb216b0d06da", "5a19db7d827f623143e92bca8c082de5c014d59c"]}, {"date": "1979", "abstract": "Abstract : This thesis presents a theory of human-like reasoning in the general domain of designed physical systems, and in particular, electronic circuits. One aspect of the theory, causal analysis, describes how the behavior of individual components can be combined to explain the behavior of composite systems. Another aspect of the theory, teleological analysis, describes how the notion that the system has a purpose can be used to aid this causal analysis. The theory is implemented as a computer program, which, given a circuit topology, can construct by qualitative causal analysis a mechanism graph describing the functional topology of the system. This functional topology is then parsed by a grammar for common circuit functions. Ambiguities are introduced into the analysis by the often several possible mechanisms which might describe the circuit's function. These are disambiguated by teleological analysis. The requirement that each component be assigned an appropriate purpose in the functional topology imposes a severe constraint which eliminates all of the ambiguities. Since both analyses are based on heuristics, the chosen mechanism is a rationalization of how the circuit functions, and does not guarantee that the circuit actually does function. This type of coarse understanding of circuits is useful for analysis, design and troubleshooting. (Author)", "authors": ["Johan de Kleer"], "id": "f1f24bb771786c7ae6df4bd087a481c7414dbcde", "title": "Causal and Teleological Reasoning In Circuit Recognition", "references": []}, {"date": "1974", "abstract": "When subjects are asked to monitor sentences for targets defined at different levels, their subsequent recall shows an interaction between recall delay and type of target (phoneme or semantic anomaly). The difficulty of the monitoring task was equal for these two types of target. The results therefore suggest selective interference by phoneme monitoring with the semantic encoding of a sentence which is necessary for it to survive a delay. There was little effect of monitoring tasks on immediate recall; this can apparently draw on either form of encoding.", "authors": ["Anne Treisman", "Julia Tuxworth"], "id": "5f34169675a4fce2a905ee2bc2badb1032e1ce3e", "title": "Immediate and delayed recall of sentences after perceptual processing at different levels", "references": []}, {"date": "1974", "abstract": "These studies ask whether S remembers a picture better the greater the \"depth of processing\" he allots to it. Depth of processing pictures of faces was varied according to judgments of sex (\"superficial\") or judgments of likableness or honesty of the person pictured. Performance on a later recognition memory test was high for pictures judged for likableness or honesty, and low for pictures judged for sex. This ordering held as'true for intentional learners as for incidental learners. A final experiment showed that face recognition memory was not materially affected by a context manipulation: an old test picture was remembered at a level determined by its original depth of processing and independently of how it was tested\u2014either alone, along side an old picture it had been studied with, or with a new picture. In a recent paper, Craik and Lockhart (1972) argued that \"depth of processing\" of stimulus material is a direct determinant of how well that material will be remembered. The underlying assumption in their approach is that a stimulus is processed through a series of stages with different kinds of information being extracted from or triggered off by the stimulus at successive stages. Sensory features of the stimulus are presumably extracted first, whereas associative information (such as the name or meaning of a grapheme) becomes available later. In support of their depth of processing hypothesis, Craik and Lockhart review studies showing higher incidental learning for words which 5s had processed for meaning than for items processed for physical attributes. For instance, Hyde and Jenkins (1969) oriented 5s to answer different questions with respect to a word, either counting the number of letters in it or the number of es, or rating it for pleasantness. Those 5s who did the pleasantness judgments recalled the words later much better than did the other 5s. Similarly, Johnston and Jenkins (1971) showed that 5s required to think of an adjective appropriate to a presented", "authors": ["Gordon H. Bower", "Martin B. Karlin"], "id": "acbd24f22cf9db66bf140e10636ac071f6368ea7", "title": "Depth of processing pictures of faces and recognition memory.", "references": ["324ccd3eacd4680acac33024015c647ce131b78d", "760af8ab0202189b9d545b8ba0c9c1845a1a7c48", "09039e2b8f456d5312b3a98f83910a4f1019dfa8", "0a1a6a7bb7aae5c1bdb9d6d9ba58284bb147b5a7", "c6b16f9e14a8a1eb20575f21329cf32bd33cfcf4", "bf450abe109ac00ef3eb00cd2037642f5993692b", "487432029cd5123d2b327d06d73f4e8707a0bba2", "ad0d70b7cc299e7af721a079438a11e21834b85c", "0cd5de5dbdd02fff2c31aaa2c58752b4bd298d8a", "d32c5cecfa2e847711a66a305464fe8a991988e4"]}, {"date": "1967", "abstract": "Semantic Scholar extracted view of \"On the syntax of preverbs\" by Charles J. Fillmore", "authors": ["Charles J. Fillmore"], "id": "b250f8439864b9ba517566c83e674dd7c887f24f", "title": "On the syntax of preverbs", "references": []}, {"date": "1973", "abstract": "Amount and organization of recall of word lists was shown to depend on the nature of the orienting tasks performed by subjects even when the task varied from word to word within a single list. Subjects heard a cue letter after each word designating the appropriate task to be performed. Experiment I used a list of unrelated words. Words to which subjects applied a semantic task (pleasantness rating) were more often recalled than words to which nonsemantic tasks were applied (estimation of number of letters or occurrences of the letter \u201ce\u201d). No significant difference in recall was observed between groups prewarned about the recall test and those groups not warned. Recalled words did not cluster significantly with respect to task. Experiment II presented a randomized list of high-strength associative pairs. One group of subjects performed the same task for both members of an associative pair; the other group used pair members in different tasks. The semantic task again led to greater recall than nonsemantic tasks. Associative clustering in the same-task condition was greater than in the different-tasks condition. Both meaningfulness of task and task similarity contribute to clustering.", "authors": ["Robert E. Till", "James J. Jenkins"], "id": "d2d9ee21203fe513d382cd2fab3672f34a155222", "title": "The effects of cued orienting tasks on the free recall of words", "references": []}, {"date": "1964", "abstract": "Publisher Summary This chapter reviews experimental methods and findings in the study of short-term retention and incidental learning. There are important continuities between the theoretical and methodological problems in these two areas. Both types of studies are concerned with basic capacities and dispositions that the learner brings to the experimental situation and that determine the initial reception and immediate storage of information. Such experiments are not primarily designed to investigate the laws governing the integration of responses and the growth of associative strength. Rather, they are concerned with a detailed analysis of some of the conditions that limit and bias the subject's (S's) responses in a learning situation. The extended practice takes its departure from these initial dispositions of the learner. In the conventional rote-learning experiment, exposures and tests follow each other in a fixed order within successive trials. When a single presentation of the learning materials is followed without delay by a test of performance, the measures of retention define the amount of immediate memory.", "authors": ["Leo Postman"], "id": "e940dce76e423390383fab059db2a77c3649e2ba", "title": "Short-Term Memory and Incidental Learning", "references": []}, {"date": "1973", "abstract": "Two word lists were prepared for recall experiments. One consisted of moderately associated word pairs, the other of unrelated words. Each list was presented to 11 different groups of subjects (22 groups in all). The control group was simply instructed to remember the words; five groups performed orienting tasks but were not informed that they would have to recall the words; five groups performed the tasks and were informed about subsequent recall. Two orienting tasks required that subjects process the meaning of the words; two tasks required syntactic processing; and one task required processing the orthography of the word. Semantic tasks yielded much greater recall and greater organization of recall than the nonsemantic tasks. Intention to learn was important only with the associated list. Results were discussed in terms of processes involved in tasks rather than responses involved in tasks.", "authors": ["Thomas S. Hyde", "James J. Jenkins"], "id": "5aa8a0fc568de7eb97cfd4778b428c990c87d1b0", "title": "Recall for words as a function of semantic, graphic, and syntactic orienting tasks.", "references": []}, {"date": "1968", "abstract": "Abstract This article discusses the design of automated information-retrieval systems which accept questions in ordinary English and yield as output a numerical answer. Fundamental to the efficient implementation of such a system is the notion of retrieval formulae. These formulae are representations of search strategies in the artificial language of a retrieval automaton. As a first step, a linguistic processor maps English questions into this artificial language. Subsequently, the system will conduct the appropriate search to satisfy the requirements of each formula. The method is illustrated with references to a fortran program which retrieves information from a data base obtained from the field of Astronomy.", "authors": ["Jacques F. Vallee", "Gilbert K. Krulee", "Albert A. Grau"], "id": "56dac50c9b1ac92b8e27583bb4abdbaa66ef326f", "title": "Retrieval formulae for inquiry systems", "references": []}, {"date": "1969", "abstract": "Preliminary experiments showed that 5\"s better recall a noun pair if they generate their own linking sentence for the pair than if they merely read an equivalent linking sentence. Initial attempts to explain this effect in terms of memory search activities or idiosyncratic ally high-associative mediators proved unproductive in later experiments reported here. The hypothesis was then offered that the generate vs. read conditions differ in comprehension of the sentences and that comprehension aids retention. Subsequent experiments on incidental learning showed that recall is excellent when S is set to process a sentence in different ways designed to promote comprehension of its meaning, whereas equivalent exposure to or mouthing of the words in control sentences without comprehension produces relatively little recall. The following experiments are concerned with the facilitation of paired-associate learning produced by embedding each word pair in a sentence. Rohwer (1966) found that an 5\" who hears a linking sentence such as \"The COW chased the BALL\" will recall the COW-BALL pair better than a control 5\" who simply studied the pair without a sentence context. In repeating some of Rohwer's paradigms, another phenomenon was uncovered which led into the present experimental series. The phenomenon is that 5\"s better remembered noun pairs embedded in sentences they generated than they did pairs embedded in sentences E gave them. At the time of study or input, 5\"s in the read condition read aloud a presented sentence (e.g., The COW chased the BALL), whereas those in the generation condition saw the pair COW-BALL and had to make up and say aloud a linking sentence. Although input times were controlled, later recall (of BALL when cued with COW) was about 25-30% higher in the generation condition. This result is quite reliable, having been replicated several times in the experiments reported subsequently. Why does generating a linking sentence", "authors": ["Samuel A. Bobrow", "Gordon H. Bower"], "id": "c689f8517c50925c1db9ae563d03635009e1dda7", "title": "COMPREHENSION AND RECALL OF SENTENCES", "references": ["32e2309faee649c7769e5087a79a3f6149baf466", "c9dfb826ea47273729474435ef96f018bac708ad", "7adb3c40ef03a458d35a3851fa66046936211cc3", "e940dce76e423390383fab059db2a77c3649e2ba", "28524e6b4615d7ed1ac17cb5ffdc527ef94145f6", "f6960020b3be84e9af3012ebfe8311a364b72145", "a0697462f96d14dd00ed57758ae6c833becb8874"]}, {"date": "1964", "abstract": "Semantic Scholar extracted view of \"Childcraft: The How and Why Library.\" by George S Amsbary", "authors": ["George S Amsbary"], "id": "e685731643f15a82c637b1ef1b745d5618fff3c8", "title": "Childcraft: The How and Why Library.", "references": []}, {"date": "1966", "abstract": "The extensive syntactic ambiguity inherent in natural language has been convincingly shown by such systems as the Harvard syntactic analyzer. Furthermore, no semantic techniques are in prospect for satisfactory resolution of this ambiguity by computer. In contrast, well-developed semantic techniques exist for formal languages.", "authors": ["James A. Craig", "Susan C. Berezner", "Homer C. Carney", "Christopher R. Longyear"], "id": "69dc76d406bc4a2204e9782e0abc27dd0f37f457", "title": "DEACON: direct English access and control", "references": []}, {"date": "1961", "abstract": "<u>Baseball</u> is a computer program that answers questions phrased in ordinary English about stored data. The program reads the question from punched cards. After the words and idioms are looked up in a dictionary, the phrase structure and other syntactic facts are determined for a content analysis, which lists attribute-value pairs specifying the information given and the information requested. The requested information is then extracted from the data matching the specifications, and any necessary processing is done. Finally, the answer is printed. The program's present context is baseball games; it answers such questions as \"Where did each team play on July 7?\"", "authors": ["Bert F. Green", "Alice K. Wolf", "Carol L. Chomsky", "K. Ronald Laughery"], "id": "89d025804988944d6fa4e95f49bff011b33d1418", "title": "Baseball: an automatic question-answerer", "references": []}, {"date": "1968", "abstract": "The focus of discussion is a prototype retrieval system with three major components for textprocessing, connectivity and decision operations. Each of these components is based on a distinguishable subtheory. \n \nComputer programs for the first two components have been written for a GE 225 computer. The complete prototype system is now being programmed for operation in a time-shared environment. It is a user-oriented system, with planned capabilities for browsing and man-machine interaction. \n \nA major goal is to develop procedures whereby research workers can conduct an on-line dialogue via terminals with a body of scientific information. Each user-submitted inquiry is a set of sentences without restriction as to vocabulary or form. The system converses with the user to obtain source-derived phrases that elaborate and refine the initial inquiry. The user is led to browse in the general area of his inquiry and to broaden or narrow it as he wishes. The system presents him with the text of documents related to his request as a further aid to request formulation. \n \nEvaluation of system performance is discussed.", "authors": ["Donald J. Hillman"], "id": "4382849df1985a665bdef400625d56a8b728c91a", "title": "Negotiation of inquiries in an on-line retrieval system", "references": []}, {"date": "1968", "abstract": "For the purpose of this paper, a question-answering system is a computer program that has at least the following three characteristics:\n (1) The ability to accept statements of fact and store them in its memory\n (2) The ability to search stored information efficiently and to recognize items that are relevant to a particular query\n (3) The ability to respond appropriately to a question by identifying and presenting the answer if it is present in memory, and by deducing a reasonable logical response from relevant knowledge if the complete answer is not explictly available.", "authors": ["C. Green", "Bertram Raphael"], "id": "dd15a1958d5a7bfa45548eb101bef4186acf2299", "title": "The use of theorem-proving techniques in question-answering systems", "references": []}, {"date": "1966", "abstract": "Semantic Scholar extracted view of \"A PROGRAM FOR TRANSFORMATIONAL SYNTACTIC ANALYSIS.\" by Stanley R. Petrick", "authors": ["Stanley R. Petrick"], "id": "dc57c2a9cf6d42dc3424cb4d1ea53f1e87efbd9c", "title": "A PROGRAM FOR TRANSFORMATIONAL SYNTACTIC ANALYSIS.", "references": []}, {"date": "1967", "abstract": "Semantic Scholar extracted view of \"Answering questions by computers - a logical study\" by J. L. Kuhns", "authors": ["J. L. Kuhns"], "id": "8c8f54ae559d324ea5ccb241bd71a8deaa081694", "title": "Answering questions by computers - a logical study", "references": []}, {"date": "1976", "abstract": "Although the software verification has made important progress during the last ten years, the verification of programs manipulating dynamic memory and complex data structures is still a challenge for the research in this domain. Automatic reasoning about the behaviours of such programs is a challenging problem because the classical techniques (static analysis or model checking) face big scalability issues: the use of the dynamic memory leads to potentially infinite state models. Thus, the specification formalisms used (i.e., Hoare\u2019s logics) has to be able to describe important properties of the dynamic memory and also to allow compositional reasoning which is a key for scalability. The Separation Logic (SL) introduced by Reynolds and O\u2019Hearn [Rey02, BCO04] has such expressiveness and local reasoning properties. However, the entailment checking problem for SL is undecidable in general. Recently, several fragments of SL have been identified to have a decidable entailment checking procedure and still be able to specify programs using interesting data structures, e.g., singly and doubly linked lists [CHO11, PWZ13], nested and overlaid lists [ESS13], and some kind of trees [IRv13]. The techniques used by these decision procedures are various: graph homomorphism in [CHO11, ESS13], reduction to first order logic with reachability and set constraints in [PWZ13], reduction to monadic second order logic in [IRv13]. On other side, efficient techniques based on tree automata have been proposed to specify and reason about programs with dynamic data structures [HHR12]. This internship has as goal to identify a fragment of SL for which the techniques based on graph homomorphism and tree automata may be combined to obtain an efficient decision procedure for the entailment checking problem. This fragment shall include the ones proposed for linked lists (singly, doubly linked and nested) and some restricted kind of trees. The decision procedure may be implemented inside the Celia toolset developed at LIAFA, in the \u201cModeling and Verification\u201d team.", "authors": ["Norihisa Suzuki"], "id": "ebbec3500ddff64f04a46b0a9a3d48b6e92ba2b5", "title": "Automatic Verification of Programs with Complex Data Structures", "references": []}, {"date": "1971", "abstract": "It has been suggested, Hoare (1969), that an axiomatic approach to formal language definition might simultaneously contribute to the clarity and reliability of programs expressed in the language, and to the efficiency of their translation and execution on an electronic digital computer. This paper gives an exa~Lple of the application of the axiomatic method to the definition of procedure and parameter passing features of a high-level progr~mning language. It reveals that ease of demonstrating program correctness and high efficiency of implementation may be achieved simultaneously, provided that the programmer is willing to observe a certain familiar and natural discipline in his use of parameters.", "authors": ["C. A. R. Hoare"], "id": "2cf7ae6adfb101ca984b1988bd6bb0be4f9b739f", "title": "Procedures and parameters: An axiomatic approach", "references": []}, {"date": "1975", "abstract": "feta-evaluetton is a process which symbolically evaluates an actor and checks to see whether the actor fulfills its contract (specification). A formalism for writing contracts for actors with side-effects which allow sharing of data is presented. Typical examples of actors with side-effects are the cell, actor counterparts of the LISP function rplace and rplacd, and procedures whose computation depends upon their input htstory. Meta-evaluation of actors with side-effets is carried out by using sttuattonta toags which denotes a situation (local state of an actor systems at the moment of the transmissions of messages). It is illustrated how the situational tags are used for proving the termination of the activation of actors. This report describes research done at the Artificial Intelligence laboratory of the massachusetts Institute of Technology. Support for the laboratory's artificial intelligence reseach is provided in part by the Advance Research Projects Agency of the Department of Defence under Office of Naval Research contract N000- 14-74-C-0643.", "authors": ["A. Yonezawa"], "id": "91a3d5a53961587bb6a5c1fc9fbe8b078013ba44", "title": "Meta-evaluation of Actors with Side-effects", "references": ["692c91426fa930ce043bee9d1e80410927253139", "23533725173200046d037c055779e190febeb4cd", "c679deecae18fb0e9a7c6f38ca09a534979cd421", "3d53230c66c39eada5d36b16e7ddb28184d94018", "9cf159a7e67143dcda48637391f96ee0d6f7ab36", "bf15ce3d1575d124527496cb249dc1249eee0acb", "fcbfa16e5e92a9cd3d76e768d0db57d417cbb176", "39bcc6ca6a756c5c4425bc1360f00c0c28ee366e", "0398f8eb58efff2bf21ffaced6fe41f33f3bee45", "69ec2b5eaf6c86c1a277e36d7c3194dddb12793f"]}, {"date": "1977", "abstract": "This paper considers the problem of identifying an efficient set of implementations for the abstract constructs in a very high level program description. LIBRA is a system that prunes and expands a tree of partially implemented program descriptions, given a set of refinement rules for generating the tree. Several sets of rules group, order, and select refinements. The analysis of the cost of a program (or program part) at any level of refinement is maintained for cost comparisons between different refinements, for bottleneck identification, and for branch and bound search.", "authors": ["E. Kant"], "id": "7c5144af9595b8ff33347906eb28a7ed9f7f55e3", "title": "The selection of efficient implementations for a high-level language", "references": []}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"Two more incidental tasks that differentially affect associative clustering in recall\" by Carroll D. Johnston et al.", "authors": ["Carroll D. Johnston", "James J. Jenkins"], "id": "d32c5cecfa2e847711a66a305464fe8a991988e4", "title": "Two more incidental tasks that differentially affect associative clustering in recall", "references": []}, {"date": "1973", "abstract": "Publisher Summary The information-processing models of human memory are described in terms of a series of stores. It has been suggested that incoming information is held in a modality-specific sensory store. This store has a large capacity, but the information in it decays and is irretrievably lost unless the subject attends to the stored items. Attended items are passed on to a limited-capacity short-term store (STS) where they are maintained by rehearsal or displaced by further inputs. Rehearsal performs the additional function of transferring information about the items to a permanent or long-term store (LTS), where capacity is unlimited and forgetting follows the laws of interference. The modal model focuses on questions such as the capacity, coding, and forgetting characteristics of the various memory stores. Issues that demand clarification are the registration and retrieval characteristics of each store and the nature of the transfer of information from one store to the next. The model implies various assumptions about the human memory system. This chapter discusses the continuing usefulness of stores or box model approach and suggests an alternative framework for the study of human memory. The chapter discusses the limitations and inconsistencies of the modal model.", "authors": ["Fergus I. M. Craik"], "id": "487432029cd5123d2b327d06d73f4e8707a0bba2", "title": "A \u201cLEVELS OF ANALYSIS\u201d VIEW OF MEMORY", "references": []}, {"date": "1972", "abstract": "The role of context in recognition memory was examined in seven experiments. In the first four experiments a context word was added to or deleted from to-be-remembered units. Recognition was impaired both when the context word added or deleted was associatively related and when it was associatively unrelated. The effects of changing context disappeared when context was only added in Experiment 5, but were still present when context was only deleted in Experiment 6. In Experiment 7, recognition performance was studied over several retention intervals, with critical words tested in changed or unchanged context. The deleterious effects of changing context increased with intralist retention interval. Context effects observed in these experiments are interpreted as evidence for retrieval processes in recognition memory.", "authors": ["Donald M. Thomson"], "id": "0a1a6a7bb7aae5c1bdb9d6d9ba58284bb147b5a7", "title": "Context effects in recognition memory", "references": []}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"Retrieval processes in recognition memory: Effects of associative context\" by Endel Tulving et al.", "authors": ["Endel Tulving", "Donald M. Thomson"], "id": "bf450abe109ac00ef3eb00cd2037642f5993692b", "title": "Retrieval processes in recognition memory: Effects of associative context", "references": []}, {"date": "1972", "abstract": "Abstract Three experiments were performed investigating the extent to which recognition memory for pictures can be predicted by eye-movement patterns on the picture at the time of study. In each experiment, 180 pictures were viewed followed by a yes-no recognition test on all the pictures. Eye movements were recorded at the time of study. Experiment I investigated payoff structure: It was found that higher-valued pictures both received more fixations and were remembered better than low-valued pictures, but when number of fixations was held constant, memory performance was independent of value. Experiment II showed that (a) when pictures are viewed for a fixed amount of time, memory performance is a positive function of number of fixations on the picture, (b) with number of fixations held constant, performance is independent of exposure time, and (c) there is no memory for pictures which were originally viewed only peripherally. In Expt. III, pictures were viewed either normally or while a distracting task (counting backward by threes) was being performed concurrently. The distracting task was found to reduce both number of fixations and memory performance for a picture. When number of fixations was held constant, performance was still better for normally viewed pictures, suggesting that the distracting task was doing more to inhibit encoding besides simply reducing the fixation rate.", "authors": ["Geoffrey R. Loftus"], "id": "0cd5de5dbdd02fff2c31aaa2c58752b4bd298d8a", "title": "Eye fixations and recognition memory for pictures", "references": ["15db31b1e86e3e0142db73b23c9a394d02b545ce", "324ccd3eacd4680acac33024015c647ce131b78d", "ee02d5a38bd392ac65ebb93436aa6668907369f7", "671a8dda6e02e37882dac90b222951a9bdb932db", "f0ac50e73aa8b81177852218ffcf432e836bc237", "aaab50fc20aa3889205c5e4a4ad5e0f2bc38b511", "542dcf8262ad0044584e2667ae6d8cf293fdfbb0", "12d36299968ba63fe9a8ceb73c26a8d7f7ba3743", "24d28713922b7f8f60ef51acc897611377450160", "dbaa315d666c39ff77e471c196adb7b62f4668f4"]}, {"date": "1964", "abstract": "Semantic Scholar extracted view of \"COMPUTERS AND COMPREHENSION\" by Manfred Kochen et al.", "authors": ["Manfred Kochen", "Donald M. MacKay", "M. E. Maron", "Michael Scriven", "Leonard Uhr"], "id": "a0697462f96d14dd00ed57758ae6c833becb8874", "title": "COMPUTERS AND COMPREHENSION", "references": []}, {"date": "1962", "abstract": "Semantic Scholar extracted view of \"Effects of orienting task, practice, and incentive on simultaneous incidental and intentional learning.\" by Arnold Mechanic", "authors": ["Arnold Mechanic"], "id": "f6960020b3be84e9af3012ebfe8311a364b72145", "title": "Effects of orienting task, practice, and incentive on simultaneous incidental and intentional learning.", "references": []}, {"date": "1970", "abstract": "Semantic Scholar extracted view of \"How we remember what we see.\" by Ralph Norman Haber", "authors": ["Ralph Norman Haber"], "id": "09039e2b8f456d5312b3a98f83910a4f1019dfa8", "title": "How we remember what we see.", "references": []}, {"date": "1967", "abstract": "Eight paired nouns were learned by 112 sixth-grade Ss under one or another of seven different experimental conditions in which the task was administered by a study-trial, test-trial method. On every study trial, each pair of nouns was presented in the context of a grammatical string of words and the experimental conditions were distinguished by differences in the composition of these strings. The form class of connectives was varied by comparing conjunctions, prepositions and verbs, and the intralist similarity of the strings was varied by manipulating the number of different words used as connectives within a list (two vs. four vs. eight). Preposition and verb connectives produced more rapid learning than conjunctions, regardless of the number of different connectives used. Thus, intralist similarity was rejected as an explanation of the inferiority of conjunction strings.", "authors": ["William D. Rohwer", "Steve Lynch"], "id": "28524e6b4615d7ed1ac17cb5ffdc527ef94145f6", "title": "Form class and intralist similarity in paired-associate learning", "references": []}, {"date": "1969", "abstract": "Quantitative predictions are made from a model for word recognition. The model has as its central feature a set of \"logogens\": devices which accept information relevant to a particular word response irrespective of the source of this information. When more than a threshold amount of information has accumulated in any logogen, that particular response becomes available for responding. The model is tested against data available on the effect of word frequency on recognition, the effect of limiting the number of response alternatives, the interaction of stimulus and context, and the interaction of successive presentations of stimuli. The implications of the underlying model are largely upheld. Other possible models for word recognition are discussed as are the implications of the Logogen Model for theories of memory.", "authors": ["John J. L. Morton"], "id": "760af8ab0202189b9d545b8ba0c9c1845a1a7c48", "title": "Interaction of information in word recognition.", "references": ["c36d4e698584537e2c8968c887596e2b9aa4aa2c", "85c5cf6d71415101c4004a2fd3977aa0f570b0d9", "a116eed0205ec8b38624a9a445c983bae870fe29", "ac6c4e8d8f38b7def4d2d935d66bdd2ead7bc6c8", "f3cfe7dd4227f1e6a614d9f3b17a1785c60df548", "e8d3d7f35ae695a4112f560713cb9e5f02644a45", "87b00d2a3e84f6b66bc03b63374b281f0e277f29", "1ad13ceb023c1bffadfc1e1318bfe27efed477c4", "417ff490f3e4cdc69bdcadeaa37b9162da4d673e", "e795e1876418f211dc2429d68674139538dc1102"]}, {"date": "1966", "abstract": "The phenomenon of sentential facilitation of paired-associate learning was investigated by dividing 224 sixth-grade S s into 14 groups according to the character of verbal pretraining provided. Subjects were given pretraining that consisted of pre-exposure to verbal strings, each of which contained one of the eight pairs of nouns to be learned. Three properties of these verbal strings were manipulated: Meaningfulness, Syntactic Structure, and Constraint. Pre-exposure to verbal strings facilitated the learning of paired associates only when the strings were characterized by both meaningfulness and syntactic structure. The experiment also produced empirical support for the notion of constraint. A continuum of facilitation was detected, in which verbs produced the greatest facilitation, prepositions a median amount, and conjunctions the least.", "authors": ["William D. Rohwer"], "id": "c9dfb826ea47273729474435ef96f018bac708ad", "title": "Constraint, syntax and meaning in paired-associate learning", "references": []}, {"date": "1968", "abstract": "The task of learning 12 pairs of high-frequency nouns by a study-test method was given to 112 fifth-grade children. In the experimental conditions, the study trial consisted of the presentation of each pair in the context of a three-word sentence wherein the two nouns were connected by a verb. In a three-way design, the variables manipulated were: the amount of overt activity implied by the verbs (action vs. still); the meaningfulness of the sentence (normal vs. anomalous); and the character of the test-trial stimuli (subject nouns vs. verbs vs. subject nouns and verbs). Performance did not vary as a function of the amount of activity implied by the verbs, but normal sentences produced significantly more learning than anomalous ones. Verbs proved inferior to subject nouns as test stimuli, leading to the conclusion that the selection of verbs as functional stimuli during the study trials does not account for the sentential facilitation of noun-pair learning.", "authors": ["William D. Rohwer", "Joel Levin"], "id": "32e2309faee649c7769e5087a79a3f6149baf466", "title": "Action, meaning, and stimulus selection in paired-associate learning", "references": []}, {"date": "1975", "abstract": "The Planner Project is constructing a programming apprentice to assist in knowledge based programming. The programming apprentice is designed for interactive use by expert programmers in the meta-evaluation of implementations in the context of their contracts and background knowledge. Meta-evaluation produces a justification which makes explicit exactly how the module depends on the contracts of other modules and on the background knowledge. The justification is used in answering questions on the behavioral dependencies between modules and in analyzing the implications of perturbations in specifications and/or implementation.", "authors": ["Karl E. Hewitt", "Brian Cantwell Smith"], "id": "23533725173200046d037c055779e190febeb4cd", "title": "Towards a programming apprentice", "references": ["0f3dfcd06fa92c4dfe895c8d4b4151bf162c20c0", "692c91426fa930ce043bee9d1e80410927253139", "c43ebe201e2015d84cf44d8c17438ddbeddf3af9", "fa5b610407ac9296692fcf46b6fe19209ffbee95", "5a061b1cab0f241d6f7226f6c0b12e931cabd90f", "fa53b77026cc5c361b02ad8e6cc209d6c4a880df", "621f8ecb1f571039804b442c4287216424b42f3c", "bf15ce3d1575d124527496cb249dc1249eee0acb", "fcbfa16e5e92a9cd3d76e768d0db57d417cbb176", "a761f26f8239acd88fc83787f28a7f2d2ff9ea22"]}, {"date": "1975", "abstract": "Discusses the importance of formal specifications and surveys a number of promising specification techniques. The role of formal specifications both in proofs of program correctness and in programming methodologies leading to programs which are correct by construction, is explained. Some criteria are established for evaluating the practical potential of specification techniques. The importance of providing specifications at the right level of abstraction is discussed, and a particularly interesting class of specification techniques, those used to construct specifications of data abstractions, is identified. A number of specification techniques for describing data abstractions are surveyed and evaluated with respect to the criteria.", "authors": ["Barbara Liskov", "Stephen N. Zilles"], "id": "16e813d1aa3684fa6b4b5b25cf2441f6e13dd3f1", "title": "Specification techniques for data abstractions", "references": []}, {"date": "1963", "abstract": "Process for eliminating reactive chlorinated organic compounds from aqueous solutions, especially waste waters, which comprises treating the waters at 70 DEG -300 DEG C, preferably 100 DEG -160 DEG C, with ammonia or compounds splitting off the same, and/or primary amines or compounds splitting off the amines, thereby obtaining insoluble reaction products which are mechanically separated from the waters.", "authors": ["Jerrold J. Katz", "Jerry A. Fodor"], "id": "7adb3c40ef03a458d35a3851fa66046936211cc3", "title": "The structure of a semantic theory", "references": []}, {"date": "1967", "abstract": "The S s looked through a series of about 600 stimuli selected at random from an initially larger population. They were then tested for their ability to recognize these \u201cold\u201d stimuli in pairs in which the alternative was always a \u201cnew\u201d stimulus selected at random from the stimuli remaining in the original population. Depending upon whether this original population consisted solely of words, sentences, or pictures, median Ss were able correctly to recognize the \u201cold\u201d stimulus in 90, 88, or 98% of the test pairs, respectively. Estimated lower bounds on the informational capacity of human memory considerably exceed previously published estimates.", "authors": ["Roger N. Shepard"], "id": "324ccd3eacd4680acac33024015c647ce131b78d", "title": "Recognition memory for words, sentences, and pictures", "references": []}, {"date": "1974", "abstract": "Several attempts have been made to produce tools which will help the programmer of complex computer systems. A new approach is proposed which integrates the programmer's intenttons, the program code, and the comments, by relating them to a knowledge base of programming techniques. Our research will extend the work of Sussman, Goldstein, and Hewitt on program descrtption and annotation. A prototype system will be implem,,ted which answers questions and detects bugs in simple LISP programs. Work reported herein was conducted at the Artificial Intelligence Laboratory, a Massachusetts Institute of Technology research program supported in part by the Advanced Research Projects Agency of the Department of Defence and monitored by the Office of Naval Research under Contract Number N00014-70-A-0W62-0005. Working Papers are informal papers intended for internal use.", "authors": ["Charles Rich", "Howard E. Shrobe"], "id": "0398f8eb58efff2bf21ffaced6fe41f33f3bee45", "title": "Understanding LISP Programs: Towards a Programmer's Apprentice", "references": ["b3fe91923c1356a8c6dee2c725dd11fcbfeae903", "e97795382386ecd24300f3a6449ed5732b200bfa", "c43ebe201e2015d84cf44d8c17438ddbeddf3af9", "acb2f7040e21cbe456030c8535bc3f2aafe83b02"]}, {"date": "1975", "abstract": "This paper presents a semantic model for parallel systems with a scheduling mechanism that is useful for expressing and proving a wider range of properties than semantic models that do not consider scheduling.We formally describe a number of properties related to scheduling and deadlock, including \"Fairness\" and \"Fullness\", and show that schedulers with these properties behave in desirable ways.Lastly, we prove and conjecture some proof rules for scheduled systems and outline briefly the relation of this work to modelling protection in parallel systems.", "authors": ["Ellis S. Cohen"], "id": "39bcc6ca6a756c5c4425bc1360f00c0c28ee366e", "title": "A semantic model for parallel systems with scheduling", "references": ["9848efbafec46ae57a7a580a9a82f0a936e67b15"]}, {"date": "1971", "abstract": "A proof is given of the correctness of the algorithm \u201cFind.\u201d First, an informal description is given of the purpose of the program and the method used. A systematic technique is described for constructing the program proof during the process of coding it, in such a way as to prevent the intrusion of logical errors. The proof of termination is treated as a separate exercise. Finally, some conclusions relating to general programming methodology are drawn.", "authors": ["C. A. R. Hoare"], "id": "5a19db7d827f623143e92bca8c082de5c014d59c", "title": "Proof of a program: FIND", "references": []}, {"date": "1973", "abstract": "The PLANNER project is continuing research in natural and effective means for embedding knowledge in procedures. In the course of this work we have succeeded in unifying the formalism around one fundamental concept: the ACTOR. Intuitively, an ACTOR is an active agent which plays a role on cue according to a script. We use the ACTOR metaphor to emphasize the inseparability of control and data flow in our model. Data structures, functions, semaphores, monitors, ports, descriptions, Quillian nets, logical formulae, numbers, identifiers, demons, processes, contexts, and data bases can all be shown to be special cases of actors. All of the above are objects with certain useful modes of behavior. Our formalism shows how all of these modes of behavior can be defined in terms of one kind of behavior: sending messages to actors. An actor is always invoked uniformly in exactly the same way regardless of whether it behaves as a recursive function, data structure, or process.", "authors": ["Carl Hewitt", "Peter Boehler Bishop", "Irene Greif", "Brian Cantwell Smith", "Todd Matson", "Richard Steiger"], "id": "fcbfa16e5e92a9cd3d76e768d0db57d417cbb176", "title": "Actor induction and meta-evaluation", "references": []}, {"date": "1975", "abstract": "The thesis of this dissertation is that an understanding of the ordering constraints that are introduced among events of parallel process is essential to the understanding of synchronization and that therefore any language for specifying synchronization of parallel process should be based on a theory of such orderings. While it is possible to write specifications for systems of communicating parallel processes by reference to the time ordering of some global clock external to the system, such specifications cannot be as useful as ones which are in terms of orderings derivable within the system. Specifications should place constraints on intended behavior of the computer system itself rather than on the possible observations of the system''s behaviors from some global viewpoint which may in fact be totally unrealizable. The dissertation is a development of a specification language. It is based on a model of computation in which an individual process is represented by a totally ordered set of events. Synchronization properties of systems of independent processes are guarantees that in fact the set of events in the system can be ordered by a partial order which properly contains the union of the processes'' total orders. This system ordering can be caused by the presence in a system of side-effect primitives or of synchronization primitives. Thus this model applies equally well both to busy waiting synchronization based on coordinated use of storage cells by independent processes and to non-busy waiting synchronization such as that induced by semaphores and structured synchronization primitives. In addition to applying to a range of types of synchronization, the specification language is also used to define a programming language. The meaning of a program is the specification of the behavior of the system into which that program is compiled. Specifications can be written for synchronization problems and for their implementations in terms of various primitives.", "authors": ["Irene Greif"], "id": "69ec2b5eaf6c86c1a277e36d7c3194dddb12793f", "title": "Semantics of communicating parallel processes", "references": ["3fea018ca5e6fecf60a90c2612391f9805c86c15", "682fbc4a30842faa97adb7c0563a4571033a6c70", "acb2f7040e21cbe456030c8535bc3f2aafe83b02", "afd45a78b319032b19afd5553ee8504ff8319852", "ff568f642a545c7d62df73e7798e3d008b509ea5", "9cf159a7e67143dcda48637391f96ee0d6f7ab36", "5d8056e326d4199d157a17fbeee97a7349d2824c", "a39e894cec4827a39b36630189ba0f7bb9ab5f26", "fcb52bfc5a63dec41969f553eebecd24d3919da8", "a9784da374ca7f0ca091618e602f81831181d9f5"]}, {"date": "1974", "abstract": "Methods for verifying programs written in a higher level programming language are devised and implemented. The system can verify programs written in a subset of PASCAL, which may have data structures and control structures such as WHILE, REPEAT, FOR, PROCEDURE, FUNCTION and COROUTINE. The process of creation of verification conditions is an extension of the work done by Igarashi, London and Luckham which is based on the deductive theory by Hoare. Verification conditions are proved using specialized simplification and proof techniques, which consist of an arithmetic simplifier, equality replacement rules, fast algorithm for simplifying formulas using propositional truth value evaluation, and a depth first proof search process. The basis of deduction mechanism used in this prover is Gentzen-type formal system. Several sorting programs including Floyd''s TREESORT3 and Hoare''s FIND are verified. It is shown that the resulting array is not only well-ordered but also a permutation of the input array.", "authors": ["Norihisa Suzuki"], "id": "3d53230c66c39eada5d36b16e7ddb28184d94018", "title": "Automatic program verification II: verifying programs by algebraic and logical reduction.", "references": ["5d8056e326d4199d157a17fbeee97a7349d2824c"]}, {"date": "1971", "abstract": "Compilers for high-level languages aTe generally constructed to give the complete translation of the programs into machme language. As machines merely juggle bit patterns, the concepts of the original language may be lost or at least obscured during this passage. The purpose of a mathematical semantics is to give a correct and meaningful correspondence between programs and mathematical entities in a way that is entirely independent of an implementation. This plan is illustrated in a very elementary way in the introduction. The first section connects the general method wi th the usual idea of state transformations. The next section shows why the mathematics of functions has to be modified to accommodate recursive commands. Section 3 explains the modifi\u00ad cation. Section 4 introduces the environments for handling variables and identifiers and shows how the semantical equations define equivalence of programs. Section 5 gives an exposition of the new type of mathematical function spaces that are required fOl the semantics of procedures when these are allowed in assignment state\u00ad ments. The conclusion traces some of the background of the project and points the way to future work.", "authors": ["Dana S. Scott", "Christopher Strachey"], "id": "9cf159a7e67143dcda48637391f96ee0d6f7ab36", "title": "Toward a mathematical semantics for computer languages", "references": ["a3ffc76d9d3e310f8ae4248d4b43d60253f2067c"]}, {"date": "1972", "abstract": "The basis for this paper is a logic designed by Dana Scott [1] in 1969 for formalizing arguments about computable functions of higher type. This logic uses typed combinators, and we give a more or less direct translation into typed \u03bb-calculus, which is an easier formalism to use, though not so easy for the metatheory because of the presence of bound variables. We then describe, by example only, a proof-checker program which has been implemented for this logic; the program is fully described in [2]. We relate the induction rule which is central to the logic to two more familiar rules - Recursion Induction and Structural Induction - showing that the former is a theorem of the logic, and that for recursively defined structures the latter is a derived rule of the logic. Finally we show how the syntax and semantics of a simple programming language may be described completely in the logic, and we give an example of a theorem which relates syntactic and semantic properties of programs and which can be stated and proved within the logic.", "authors": ["Robin Milner"], "id": "c679deecae18fb0e9a7c6f38ca09a534979cd421", "title": "Implementation and applications of Scott's logic for computable functions", "references": []}, {"date": "1968", "abstract": "This paper investigates whether the relation between the frequency of occurrence of a word and its sensory threshold can properly be ascribed to response bias. Seven models for the recognition process are tested against data of Brown & Rubenstein (1961). The models are of two kinds: probabilistic single-threshold models and information-processing models. It is concluded that the data can only be accounted for within the former class of models by assuming a differential effect of the stimulus as well as response bias. For the class of information-processing models, the data require the assumptions that there is equal sensitivity for words of all frequencies and a lower criterion for more common words. The relation between the two acceptable models makes it apparent that the notions of \u2018stimulus effect\u2019 and \u2018response effect\u2019 are by no means as clear as has previously been thought.", "authors": ["J. Morton"], "id": "e795e1876418f211dc2429d68674139538dc1102", "title": "A RETEST OF THE RESPONSE-BIAS EXPLANATION OF THE WORD-FREQUENCY EFFECT", "references": []}, {"date": "1968", "abstract": "80 6\"s were shown drawings of common objects and later given verbal recall tests for the object names, followed by visual recognition tests scaled to reflect the accuracy of the visual retention. A 2X2X2 design permitted assessment of the effects of degree of training, of retention interval, and of instructions to verbalize during training. The most significant finding was that the probability of recall of object names is essentially uncorrelated with the accuracy of visual recognition of these same objects by the same 5s. As an explanation the possibility of independent storage systems for the visual and verbal information related to the same objects was considered but rejected as improbable. Verbal recall 2 wk. after training seemed to be based largely upon the retrieval of visual storage. The receding potential of visual storage into a verbal concept appears to be determined by those aspects of the visual storage which distinguish the object class, and not by aspects which distinguish among objects of the same class. Visual storage losses of both aspects occurred during the 2-wk. period examined, but no significant correlation existed between the 2 types of losses. Nonverbal information may be encoded verbally to facilitate storage and retrieval. Some verbal receding seems to occur almost instantaneously at the time of the input; in other instances the information may be stored nonverbally, and the receding is delayed until retrieval becomes necessary. Receding would appear to be", "authors": ["Harry P. Bahrick", "Boswell Boucher"], "id": "dbaa315d666c39ff77e471c196adb7b62f4668f4", "title": "Retention of visual and verbal codes of the same stimuli.", "references": []}, {"date": "1967", "abstract": "The visual fixations of 20 Ss viewing each of two pictures were measured. Each picture was later divided into 64 squares, and 20 other Ss judged their recognizability on a 10-point scale. Both measures gave high readings for unusual details and for unpredictable contours. Although they were judged to be highly recognizable, all the redundant (or predictable) contours received few fixations. Areas of mere texture scored low on both measures. The relations between fixation densities and estimated recognizability suggest that a scene may be divided into informative features and redundant regions. Not only do the eyes have to be aimed, they are usually aimed intelligently, even during the casual inspection of pictures.", "authors": ["Norman H. Mackworth", "Anthony J. Morandi"], "id": "542dcf8262ad0044584e2667ae6d8cf293fdfbb0", "title": "The gaze selects informative details within pictures", "references": []}, {"date": "1970", "abstract": "A continuous paired-associate task was used to examine, the effect of monetary incentive on response probability when incentive was presented at the time a pair was studied, at the time it was tested, at both times, or at neither time. All paired-associate items were assigned either a high or a low value. The S was either cued or not cued with this value at the time he studied the item and again when he was tested. After each test trial, feedback was presented that indicated whether or not the response had been correct and what the value of the item was. The results indicated that presenting the value of an item at the time the item was studied greatly affected the probability of a correct response at test; a study cue of high value led to better performance than a study cue of low value. A similar although smaller effect took place when the value of the item was presented at the time the item was tested; 5s responded more accurately when told that the item on which they were being tested was a high-value as opposed to a low-value item. These data were considered in terms of the memory model of Atkinson and Shiffrin which postulates differential control processes at the time of initial storage and subsequent retrieval.", "authors": ["Geoffrey R. Loftus", "Thomas D. Wickens"], "id": "12d36299968ba63fe9a8ceb73c26a8d7f7ba3743", "title": "Effect of incentive on storage and retrieval processes", "references": ["56c16d9e2a5270ba6b1d83271e2c10916591968d", "a0f7b2d7683d00ad0b1b204f4844af8fd894d8f1", "d65679b0af3d15d83962a97a16d9c2e6a06aef2f", "6c33c645e6e2a0d1f419f2b3de350f9a373b67cc", "781731f7838681f424574ff282234432877ca119", "8b773397e717138ddfca9d7c50ca6f509fa8452c", "e0908a477a6cb1768a8995e00e2894ed39222520", "02e35f77a0aba531890fb929eea287673e53d6c6", "cfe911405dab87fabbcb1f76a0080731cbac8df0", "de159bc421672a5b02b186a4f488679e765c16b7"]}, {"date": "1964", "abstract": "Abstract Subjects read aloud 200-word passages of statistical approximations up to the 8th order. Their eye-movements were recorded together with a trace of the speech output. Speed of reading, using the syllable as the unit of measurement, increased up to the 5th order for slow readers. Fast readers, on the other hand, further increased their speed to the 6th order. This result had been predicted from a hypothesis that fast readers use contextual cues more efficiently. Measures of the material in the eye-voice span showed an increase up to the 8th order. Fast readers had a larger material span than slow readers beyond the 5th order, a result paralleling the differences in speed increase. It is suggested that the eye-voice span measured in time is dependent upon the chosen reading speed and the material span. From the eye-movement records there was no variation of the mean duration of fixation between passages, or between fast and slow readers. The average value was about 240 millisec. The number of both ...", "authors": ["J. Morton"], "id": "417ff490f3e4cdc69bdcadeaa37b9162da4d673e", "title": "The effects of context upon speed of reading, eye movements and eye-voice span", "references": []}, {"date": "1970", "abstract": "A theory of visual pattern perception is proposed, which is intended to explain first how patterns are learned or committed to memory, second how these patterns are recognized when subsequently encountered, and third how the patterns are recognized under unfavorable real-world conditions, for example, when they are distorted, enlarged, or rotated, or are viewed along with other patterns in a cluttered and noisy visual field. The essential idea of the theory is that each pattern is represented in memory as a network of memory traces recording the features of the pattern and the attention shifts required to pass from feature to feature across the visual field. These attention shifts may take the form of saccadic eye movements or they may be executed internally, according to the angular displacement involved. Memorizing and recognizing a pattern are thus seen to be closely analogous to memorizing and repeating a conventional sequence of behavior, each being an alternating sequence of sensory and motor activities. From this analogy come certain predictions concerning the presence of scanpaths in eye movements during pattern perception, and one of these predictions has been verified experimentally.", "authors": ["David Noton"], "id": "24d28713922b7f8f60ef51acc897611377450160", "title": "A Theory of Visual Pattern Perception", "references": ["f70c54029c17f50914996c834930ecbfacda195f", "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7", "78567933f26e3bbe7874e25cc259bb352e235329", "ee02d5a38bd392ac65ebb93436aa6668907369f7", "f0ac50e73aa8b81177852218ffcf432e836bc237", "ed46651b55894c28fb549601a9f2b4fb904dea11", "5029bdf21a5244e60728ef5e2ac699daeb1f6c02", "bc4e084831b9f987826b2cf184356e347ee155f8", "f0ab1b5c3690ddce099e94a6476f0b184a35b714", "dbbabc2af3deb36ed7790cc473cef133c3876554"]}, {"date": "1971", "abstract": "Since the last IJCAI, the PLANNER problem solving formalism has continued to develop. Our eplstemolgy for the foundations for problem solving has been extended. An overview of the formalism Is given from an Information processing view point. A simple example Is explained using snapshots of the state of the problem solving as the example Is worked, finally, current applications for the formalism are listed.", "authors": ["Carl Hewitt"], "id": "a761f26f8239acd88fc83787f28a7f2d2ff9ea22", "title": "Procedural Embedding of knowledge in Planner", "references": ["e91acb2dd6054614bc254376f2cdb464fe3ba073"]}, {"date": "1974", "abstract": "An experimental system for automatically generating certain simple kinds of programs is described. The programs constructed are expressed in a subset of ALGOL containing assignments, function calls, conditional statements, while loops, and non-recursive procedure calls. The input is an environment of primitive programs and programming methods specified in a lnaugage currently used to define the semantics of the output programming language. The system has been used to generate programs for symbolic manipulation, robot control, every day planning, and computing arithmetical functions.", "authors": ["Jack Buchanan", "David C. Luckham"], "id": "0f3dfcd06fa92c4dfe895c8d4b4151bf162c20c0", "title": "On automating the construction of programs.", "references": []}, {"date": "1974", "abstract": "Knowledge Based Programming is programming in an environment which has substantial knowledge of the semantic domain for which the programs are being written and of the purposes that the programs are supposed to satisfy. Actors are a semantic concept in which no active process is ever allowed to treat anything as an object; instead a polite request must be extended to accomplish what the activator desires. Using actors the PLANNER Project is constructing a Programming Apprentice to make it easier for expert programmers to do knowledge based programming. The apprentice is to aid in establishing and maintaining consistency of specifications, validating that modules meet their specifications, answering questions about behavioral dependencies between modules, and analyzing the implications of perturbations in modules and their specifications.", "authors": ["Carl Hewitt", "Peter Boehler Bishop", "Richard Steiger", "Irene Greif", "Brian Cantwell Smith", "Todd Matson", "Roger Hale"], "id": "621f8ecb1f571039804b442c4287216424b42f3c", "title": "Behavioral semantics of nonrecursive control structures", "references": []}, {"date": "1974", "abstract": "An experimental system for automatically generating certain simple kinds of programs is described. The programs constructed are expressed in a subset of ALGOL containing assignments, function calls, conditional statements, while loops, and non-recursive procedure calls. The system has been used to generate programs for symbolic manipulation, robot control, every day planning, and computing arithmetical functions. This system has previously been described in [Buchanan and Luckham 1974]. The present report focuses on the generation of conditional statements and describes applications to mechanical assembly and symbolic manipulation problems.", "authors": ["David C. Luckham", "Jack Buchanan"], "id": "fa5b610407ac9296692fcf46b6fe19209ffbee95", "title": "Automatic generation of programs containing conditional statements", "references": []}, {"date": "1973", "abstract": "We present various heuristic techniques for use in proving the correctness of computer programs. The techniques are designed to obtain automatically the \"inductive assertions\" attached to the loops of the program which previously required human \"understanding\" of the program''s performance. We distinguish between two general approaches: one in which we obtain the inductive assertion by analyzing predicates which are known to be true at the entrances and exits of the loop ($underline{top-down}$ approach), and another in which we generate the inductive assertion directly from the statements of the loop ($underline{bottom-up}$ approach).", "authors": ["Shmuel Katz", "Zohar Manna"], "id": "fa53b77026cc5c361b02ad8e6cc209d6c4a880df", "title": "A Heuristic Approach to Program Verification", "references": ["4022cfec1dee932157741b26091a9d9a1259f8c4"]}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Mathematical concepts in programming language semantics\" by Dana Scott", "authors": ["Dana Scott"], "id": "9848efbafec46ae57a7a580a9a82f0a936e67b15", "title": "Mathematical concepts in programming language semantics", "references": []}, {"date": "2004", "abstract": "SummaryIn this paper the use of semaphores and critical regions to synchronize concurrent processes is compared to a more recent proposal for so-called conditional critical regions. The latter concept permits the programmer to express directly that a process must be delayed until a certain condition holds before entering a critical region. The paper describes a well-structured notation for these. synchronizing concepts and uses both of them to solve a typical resource allocation problem. The two solutions are compared by outlining proofs of their correctness. A fairly long argument is needed to justify the solution which uses semaphores. In contrast, the solution based on conditional critical regions is so obvious that a proof is hardly needed. The paper concludes by explaining why programs using semaphores in general will be much more obscure than programs using conditional critical regions.", "authors": ["Per Brinch Hansen"], "id": "a9784da374ca7f0ca091618e602f81831181d9f5", "title": "A comparison of two synchronizing concepts", "references": ["fcb52bfc5a63dec41969f553eebecd24d3919da8", "f1d5f15fdd8db9af0f3a523dd00f4f31ef14c965", "cf572f6fab8c4c3ce4741992cf58e28b585962c7", "43aa639d8df2285bd805531889d1d5f49563c17d"]}, {"date": "1971", "abstract": "This paper appears in the March, 1972, issue of the Communications of the ACM. Its abstract is reproduced below.\n Formalization of a well-defined synchronization mechanism can be used to prove that concurrently running processes of a system communicate correctly. This is demonstrated for a system consisting of many sending processes which deposit messages in a buffer and many receiving processes which remove messages from that buffer. The formal description makes it very easy to prove that the buffer will neither overflow nor underflow, that senders and receivers will never operate on the same message frame in the buffer nor will they run into a deadlock.", "authors": ["A. Nico Habermann"], "id": "fcb52bfc5a63dec41969f553eebecd24d3919da8", "title": "Synchronization of communicating processes", "references": []}, {"date": "1967", "abstract": "Floyd 3 On page 25, the author states \u201cthis fact offers the possibility of automatic verification of programs, the programmer merely tagging entrances and one edge in each innermost loop; the verifying program would extend the interpretation and verify it, if possible, by mechanical theorem-proving techniques.\u201d Give a brief overview of the state of formal verification (yes, you can just Google this). You may also find it interesting to look at this Digital Library article.", "authors": ["Robert W. Floyd"], "id": "c43ebe201e2015d84cf44d8c17438ddbeddf3af9", "title": "Assigning meaning to programs", "references": []}, {"date": "1969", "abstract": "Semantic Scholar extracted view of \"Fixpoint induction and proofs of program properties\" by David Michael Ritchie Park", "authors": ["David Michael Ritchie Park"], "id": "a3ffc76d9d3e310f8ae4248d4b43d60253f2067c", "title": "Fixpoint induction and proofs of program properties", "references": []}, {"date": "1972", "abstract": "This paper is primarily a user''s manual for LCF, a proof-checking program for a logic of computable functions proposed by Dana Scott in 1969 but unpublished by him. We use the name LCF also for the logic itself, which is presented at the start of the paper. The proof-checking program is designed to allow the user interactively to generate formal proofs about computable functions and functionals over a variety of domains, including those of interest to the computer scientist - for example, integers, lists and computer programs and their semantics. The user''s task is alleviated by two features: a subgoaling facility and a powerful simplification mechanism. Applications include proofs of program correctness and in particular of compiler correctness; these applications are not discussed herein, but are illustrated in the papers referenced in this introduction.", "authors": ["Robin Milner"], "id": "a39e894cec4827a39b36630189ba0f7bb9ab5f26", "title": "Logic for Computable Functions: description of a machine implementation.", "references": []}, {"date": "1973", "abstract": "This paper proposes a modular ACTOR architecture and definitional method for artificial intelligence that is conceptually based on a single kind of object: actors [or, if you will, virtual processors, activation frames, or streams]. The formalism makes no presuppositions about the representation of primitive data structures and control structures. Such structures can be programmed, micro-coded, or hard wired in a uniform modular fashion. In fact it is impossible to determine whether a given object is \"really\" represented as a list, a vector, a hash table, a function, or a process. The architecture will efficiently run the coming generation of PLANNER-like artificial intelligence languages including those requiring a high degree of parallelism. The efficiency is gained without loss of programming generality because it only makes certain actors more efficient; it does not change their behavioral characteristics. The architecture is general with respect to control structure and does not have or need goto, interrupt, or semaphore primitives. The formalism achieves the goals that the disallowed constructs are intended to achieve by other more structured methods. PLANNER Progress \"Programs should not only work, but they should appear to work as well.\" PDP-1X Dogma The PLANNER project is continuing research in natural and effective means for embedding knowledge in procedures. In the course of this work we have succeeded in unifying the formalism around one_ fundamental concept: the ACTOR. Intuitively, an ACTOR is an active agent which plays a role on cue according to a script. We use the ACTOR metaphor to emphasize the inseparability of control and data flow in our model. Data structures, functions, semaphores, monitors, ports, descriptions, Quillian nets, logical formulae, numbers, identifiers, demons, processes, contexts, and data bases can all be shown to be special cases of actors. All of the above are objects with certain useful modes of behavior. Our formalism shows how all of the modes of behavior can be defined in terms of one kind of behavior: sending messages to actors. An actor is always invoked uniformly in exactly the same way regardless of whether it behaves as a recursive function, data structure, or process. \"It is vain to multiply Entities beyond need.\" William of Occam \"Monotheism is the Answer.\" The unification and simplification of the formalisms for the procedural embedding of knowledge has a great many benefits for us: FOUNDATIONS: The concept puts procedural semantics [the theory of how things operate] on a firmer basis. It will now be possible to do cleaner theoretical studies of the relation between procedural semantics and set-theoretic semantics such as model theories of the quantificational calculus and the lambda calculus. LOGICAL CALCULAE: A procedural semantics is developed for the quantificational calculus. The logical constants FOR-ALL, THERE-EXISTS, AND, OR, NOT, and IMPLIES are defined as actors. KNOWLEDGE BASED PROGRAMMING is programming in an environment which has a substantial knowledge base in the application area for which the programs are intended. The actor formalism aids knowledge based programming in the following ways: PROCEDURAL EMBEDDING of KNOWLEDGE, TRACING BEHAVIORAL DEPENDENCIES, and SUBSTANTIATING that ACTORS SATISFY their INTENTIONS. INTENTIONS: Furthermore the confirmation of properties of procedures is made easier and more uniform. Every actor has an INTENTION which checks that the prerequisites and the context of the actor being sent the message are satisfied. The intention is the CONTRACT that the actor has with the outside world. How an actor fullfills its contract is its own business. By a SIMPLE BUG we mean an actor which does not satisfy its intention. We would like to eliminate simple debugging of actors by the META-EVALUATION of actors to show that they satisfy their intentions. Suppose that there is an external audience of actors E which satisfy the intentions.of the actors to which they send messages. Intuitively, the principle of ACTOR INDUCTION states that the intentions of all actions caused by E are in turn satisfied provided that the following condition holds: If for each actor A the' intention of A is satisfied => that the intentions of all actors sent messages by A are satisfied. Computational induction [Manna], structural induction [Burstall], and Peano induction are all special cases of ACTOR induction. Actor based intentions have the following advantages: The intention is decoupled from the actors it describes. Intentions of concurrent actions are more easily disentangled. We can more elegantly write intentions The intentions are written in the same formalism as the Because for dialogues between actors. procedures they describe. Thus for example intentions can have intentions, protection is an intrinsic property of actors, we hope to be able to deal with protection issues in the same straight forward manner as more conventional intentions. Intentions of data structures are handled by the same machinery as for all other actors. COMPARATIVE SCHEMATOLOGY: The theory of comparative power of control structures is I", "authors": ["Carl Hewitt", "Peter Boehler Bishop", "Richard Steiger"], "id": "acb2f7040e21cbe456030c8535bc3f2aafe83b02", "title": "A Universal Modular ACTOR Formalism for Artificial Intelligence", "references": ["628957b7b5708f54ea954ce1385f34d936a4b55f", "7eed64f4f1e8714e1b2a313b2ebaf7c765a8ffcd", "f3e46ad5e182fd3de7e79a828b72c2d82e62a247", "fb4b11202c03ff7855af3e23cf166a2a28c62f26", "97e0e7161f9d0e6762c396d9714f9043b8079b48", "31d75b28132803648b84adb30fde4621361f2f86", "1ca6e22f4c6be216c7f9c6b809ed7ad74096f970"]}, {"date": "1973", "abstract": "This paper attempts to use formal semantics of a class of parallel processes in order to carry out mechanizable proofs about them. The formalism used is LCF (Logic for Computable Functions, Milner [22]), with slight extensions. The processes we consider communicate by sharing memory, rather than by signals on communication lines. Parallelism is treated as nondeterminism. We state properties such as mutual exclusion of critical sections, absence of deadlocks, determinacy, and we show examples of proofs.", "authors": ["J. M. Cadiou", "Jean-Jacques L\u00e9vy"], "id": "682fbc4a30842faa97adb7c0563a4571033a6c70", "title": "Mechanizable Proofs about Parallel Processes", "references": []}, {"date": "2004", "abstract": "SummaryOne of the primary functions of an operating system is to rebuild a machine that must be regarded as non-deterministic (on account of cycle stealing and interrupts) into a more or less deterministic automaton. Taming the degree of indeterminacy in steps will lead to a layered operating system. A bottom layer will be discussed and so will the adequacy of the interface it presents. An analysis of the requirements of the correctness proofs will give us an insight into the logical issues at hand. A \u201cdirector-secretary\u201d relationship will be introduced to reflect a possible discipline in the use of sequencing primitives.", "authors": ["Edsger W. Dijkstra"], "id": "ff568f642a545c7d62df73e7798e3d008b509ea5", "title": "Hierarchical ordering of sequential processes", "references": ["04c12d5bd845611bf06e6808245702cc76c52ac5"]}, {"date": "1968", "abstract": "The relationship between motivation and memory was examined when the motivation was introduced during the retention test. It was found that the paced and free recall of a paired-associates list were not enhanced by the presence of a large monetary reward for correct performance.", "authors": ["Edward A. Wasserman", "Bernard Weiner", "John Porter Houston"], "id": "de159bc421672a5b02b186a4f488679e765c16b7", "title": "Another Failure for Motivation to Enhance Trace Retrieval", "references": ["09953453e1f8c1514c02d8906d26a266a6ad7f16"]}, {"date": "1974", "abstract": "The motivation behind the work in very-high-level languages is to ease the programming task by providing the programmer with a language containing primitives or abstractions suitable to his problem area. The programmer is then able to spend his effort in the right place; he concentrates on solving his problem, and the resulting program will be more reliable as a result. Clearly, this is a worthwhile goal.\n Unfortunately, it is very difficult for a designer to select in advance all the abstractions which the users of his language might need. If a language is to be used at all, it is likely to be used to solve problems which its designer did not envision, and for which the abstractions embedded in the language are not sufficient.\n This paper presents an approach which allows the set of built-in abstractions to be augmented when the need for a new data abstraction is discovered. This approach to the handling of abstraction is an outgrowth of work on designing a language for structured programming. Relevant aspects of this language are described, and examples of the use and definitions of abstractions are given.", "authors": ["Barbara Liskov", "Stephen N. Zilles"], "id": "afd45a78b319032b19afd5553ee8504ff8319852", "title": "Programming with Abstract Data Types", "references": []}, {"date": "1968", "abstract": "This paper discusses the technique of structural induction for proving theorems about programs. This technique is closely related to recursion induction but makes use of the inductive definition of the data structures handled by the programs. It treats programs with recursion but without assignments or jumps. Some syntactic extensions to Landin's functional programming language ISWIM are suggested which make it easier to program the manipulation of data structures and to develop proofs about such programs. Two sample proofs are given to demonstrate the technique, one for a tree sorting algorithm and one for a simple compiler for expressions. (First received April 1968 and in revised form August 1968)", "authors": ["Rod M. Burstall"], "id": "b3fe91923c1356a8c6dee2c725dd11fcbfeae903", "title": "Proving Properties of Programs by Structural Induction", "references": ["27dd189065bd8847a8ec8f27553282df67a42d3e", "c43ebe201e2015d84cf44d8c17438ddbeddf3af9", "427dd6f76ec119aa185f1e2ac82f040082e7d2ff", "2769c203102a875c10bc11affc161891472176d1", "6e3c25ab217c27ce55a19019f856d2fd683cdca1", "e1f9006298bd72d50c0b3a4a1cb4cc9475694fb2", "8f10eb4e5d74c772a9a5a9695e027512327e7c4a", "d8ce4b5489ef14e8878c869101e30432d057599c", "281212cc3d5eaad480dd321daf8ab9de7d5be375"]}, {"date": "1965", "abstract": "Two groups of human Ss were tested in a paired associate learning task using the method of anticipation with 13 pairs of CVC trigrams of low association value. Both groups were told that the learning of trigrams of a certain background color (yellow, white) would result in a reward of 25 cents for each correct response on a predetermined trial. The interval between the stimulus trigram onset and response trigram onset for each pair was 4 sec. for both groups. But the interval between the response trigram onset of one pair and the stimulus trigram onset of the next pair (R-S interval) was 4 sec. for one group and 1.3 sec. for the other group. The 4 sec. R-S interval group learned the high incentive trigrams (HIT) significantly faster than the low incentive trigrams (LIT), but the difference was not significant in the 1.3 sec. group. Also, the length of the R-S interval had a significant effect on the ease of recall of HIT relative to LIT. The amount recalled depended upon the total amount of time permitted for looking at the pairs rather than upon the total number of trials or overt recitations.", "authors": ["Willard F. Harley"], "id": "cfe911405dab87fabbcb1f76a0080731cbac8df0", "title": "The effect of monetary incentive in paired associate learning using a differential method", "references": []}, {"date": "1964", "abstract": "Eye movements of 13 Ss in response to a structured sample of 42 polygons were recorded, using the Brandt Eye Camera, to find out \u201chow people look at nonrepresentational shapes.\u201d The ultimate purpose was to define the criteria for drawing a basic reference axis (dominant direction) for computing various physical parameters of random shapes used in form perception experiments. An experiment consisting of four parts was performed. In Part I Ss were given 8 sec. to judge the degree of symmetry of the stimuli. In Part II the instructions were simply to explore the stimuli for 8 sec. In Parts III and IV stimuli were presented tachistoscopically for .2 sec. (which eliminated eye movements) and Ss' eye movements were recorded for 6 sec. during the post-exposure period. The task was the rating of the degree of symmetry of single stimuli in Part III and the comparative judgment of symmetry of pairs of stimuli in Part IV. The results of Parts I and II did not differ: in simple shapes Ss simply scanned the outlines o...", "authors": ["Leonard Zusne", "Kenneth M. Michels"], "id": "f0ab1b5c3690ddce099e94a6476f0b184a35b714", "title": "NONREPRESENTATIONAL SHAPES AND EYE MOVEMENTS.", "references": []}, {"date": "1971", "abstract": "Subjects learned and recognized patterns which were marginally visible, requiring them to fixate directly each feature to which they wished to attend. Fixed \"scanpaths,\" specific to subject and pattern, appeared in their saccadic eye movements, both intermittently during learning and in initial eye movements during recognition. A proposed theory of pattern perception explains these results.", "authors": ["David Noton", "L. Stark"], "id": "dbbabc2af3deb36ed7790cc473cef133c3876554", "title": "Scanpaths in Eye Movements during Pattern Perception", "references": []}, {"date": "1969", "abstract": "In this paper we give a very weak axiom for the \u201clength of a program,\u201d Q(z) . Without assuming that Q(z) is partial recursive, we then prove a rather surprising result: that for some integer m , there is a finite repertoire of programs which contains the shortest programs for a set of characteristic functions whose domains are of cardinality m , but which is such that the problem of passing from a table for one of these characteristic functions to its shortest program in the repertoire is unsolvable.", "authors": ["David Pager"], "id": "4022cfec1dee932157741b26091a9d9a1259f8c4", "title": "On the Problem of Finding Minimal Programs for Tables", "references": []}, {"date": "1943", "abstract": "ALL this development, both on the transmitting and receiving side, has taken place well inside ten years, and is not solely due to the War, What will be the future of short-wave broadcasting ? Will people still go on listening and searching to see what they can find coming from distant countries ? There is not much doubt that for some time they will, since news must be foremost in everybody's mind for many years to come. The question remains what will happen when and if news becomes a matter of lees pressing importance to the whole world ? Is shortwave broadcasting capable of further development, purely as a means of recreation and enjoyment ? Whether the results obtainable by this means will-ever be equalled by an ordinary direct listener in his home would at first sight seem doubtful, but if the progress in the next ten years is anything like that in the last ten years, we may look forward to the day when reception from far-off countries is almost as good as from the local station, and a few years after that we may even see the addition of pictures.", "authors": ["Judith Scott Clayton"], "id": "e91acb2dd6054614bc254376f2cdb464fe3ba073", "title": "The Future", "references": ["9eed17dd2ad338a2d4ccace774e4ac767284a2d7"]}, {"date": "1969", "abstract": "Haidinger\u2019s brush was used as a method of locating fixation positions on a display. The various experimental patterns studied showed: (1) It is the already organized cortical representation of shape which governs fixation, rather than the peripheral input per se; (2) Acute angles near 20 deg are the most effective angular stimuli: (3) For figures subtending angles less than 5 deg, the eye is directed toward the center of the figure. and not toward its edge; and (4) Removing one segment from a completely enclosed figure may not alter the mean fixation position.", "authors": ["Lloyd Kaufman", "Whitman Richards"], "id": "5029bdf21a5244e60728ef5e2ac699daeb1f6c02", "title": "Spontaneous fixation tendencies for visual forms", "references": []}, {"date": "2002", "abstract": "The objectives for a theory of parallel programming are discussed. Programming constructs for parallelism, including means for expressing resource constraints and synchorinzation, are discussed and formally defined.", "authors": ["C. A. R. Hoare"], "id": "cf572f6fab8c4c3ce4741992cf58e28b585962c7", "title": "Towards a theory of parallel programming", "references": []}, {"date": "1968", "abstract": "Gaze displacements and fixations were recorded by a vectorial method in normal subjects during the free search of a simple (P1) and a complex (P2) visual pattern. \n \nDisplacements are saccadic, characterized by a high intial acceleration, a plateau at the level of the maximum speed and a rather fast deceleration. Some of the characteristics of this deceleration suggest that the end of the course of the movement is servo-controlled by visual input. \n \nFixations represent 85 per cent of search time, their mean duration is 300\u2013400 msec for P1 and 200\u2013300 msec for P2. It follows that the probability of occurrence of a movement during a fixation increases faster when the pattern is complex. But furthermore, this fact suggest that the duration of fixations is not determined only by the analysis of the foveal perceptual material, but also mainly by stimulation of peripheral zones of the retina. The gaze displacement triggered by peripheral stimulation would then provide a response to an expectancy of information which increases progressively during the fixation.", "authors": ["Marc Jeannerod", "Paul G\u00e9rin", "J. Perrier"], "id": "ed46651b55894c28fb549601a9f2b4fb904dea11", "title": "Deplacements et fixations du regard dans l'exploration libre d'une scene visuelle", "references": ["9a76a0cb3cee269c85aaeeda6e8a29fe2b48b314", "848ed3c378a404ea39c59c601970d9abe419cbc4", "6c8c14d131ec8586278330a5f9170d6fef87a1e1", "dc0878d2d191052815ea5b2004ba65c800205017", "f6c4065b3f7c5aa64ff2199554f213aa72209f7c"]}, {"date": "1967", "abstract": "A historical review of past attempts at formulating theories in which efference plays a role in conscious perception is presented. A testable version of such a theory is formulated, and 4 experiments are presented testing implications from this theory. In all of these experiments , conditions in which Ss must learn a new afferent-efferent association are compared with Ss whose physical activity and perceptual experience are very similar but who need not learn a new association between afferent input and relevant efferent output. In all of the experiments significant change in the visual perception of curvature was obtained in the conditions in which the new associations had to be learned. Where no new associations had to be learned, significantly less visual change occurred. The results are consistent with the theoretical position that the efference and efferent readiness activated by visual input helps determine the visual perception of contour. Around the turn of the last century there were a few people who proposed 1 The research reported in this monograph was conducted while all the authors were at Stanford University. While all of us worked together, different ones were involved in different experiments. Experiment III is a condensed version of the dissertation submitted for the PhD at Stanford University by Clarke A. Burnham. Experiment I was conducted by Festinger, Burnham, and Bamber; Ono had primary responsibility for Exp. II; Festinger and Burnham were involved in Exp. IV. We would like to thank Lance Kirkpatrick Canon and Stanley Coren for their help in that motor output was essential to the conscious experience of perception. Munsterberg (1899), for example, elaborated the view that incoming, af-ferent stimulation and outgoing motor innervation were a single, continuous nerve process with no point of separation between them. The motor dis-the experimental work and Douglas H. Lawrence and Charles R. Hamilton for comments , criticisms, and general assistance.", "authors": ["Leon Festinger", "H Ono", "C. A. Burnham"], "id": "78567933f26e3bbe7874e25cc259bb352e235329", "title": "Efference and the conscious experience of perception.", "references": ["d53c0c98386efd1d0e16f33c6782d48f4cf788ad", "c9b6743e19d4b92e2fa6665b5ada2ec395ec4981", "db28e61e614da065d465c52cb870fafcd96fcb8d", "2e959e1c6e27fca0e97b72c441f9105281d1b006", "49e1ea6826464481959ca2cdcbd85a332b41cbdf", "99845addbc6f387682c6a05ecaf62223e92e5486"]}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"The programming language pascal\" by N. Wroth", "authors": ["N. Wroth"], "id": "f1d5f15fdd8db9af0f3a523dd00f4f31ef14c965", "title": "The programming language pascal", "references": []}, {"date": "1959", "abstract": "In this paper, we analyze the activity of single fibers in the optic nerve of a frog. Our method is to find what sort of stimulus causes the largest activity in one nerve fiber and then what is the exciting aspect of that stimulus such that variations in everything else cause little change in the response. It has been known for the past 20 years that each fiber is connected not to a few rods and cones in the retina but to very many over a fair area. Our results show that for the most part within that area, it is not the light intensity itself but rather the pattern of local variation of intensity that is the exciting factor. There are four types of fibers, each type concerned with a different sort of pattern. Each type is uniformly distributed over the whole retina of the frog. Thus, there are four distinct parallel distributed channels whereby the frog's eye informs his brain about the visual image in terms of local pattern independent of average illumination. We describe the patterns and show the functional and anatomical separation of the channels. This work has been done on the frog, and our interpretation applies only to the frog.", "authors": ["Jerome Y. Lettvin", "Humberto R. Maturana", "Warren S. McCulloch", "Walter Pitts"], "id": "bc4e084831b9f987826b2cf184356e347ee155f8", "title": "What the Frog's Eye Tells the Frog's Brain", "references": ["b446e3fb01026b4e5ad978812540f172405fff6f", "6206c28676bc30b30f23c74343df7a06e937861a", "0e9b804adcedfe82d5139b572c5c165fa161efb7", "ec71953a116a869a063cf516fe5e57700fff6567", "929f33d2c4ede915b830e5a47b5007cd4c870386", "22f063bc32cae7adc38ab0f96ea79f6ddb801e6d"]}, {"date": "1971", "abstract": "A certain primitive sequencing operation is defined, applicable as an extension to Algol-like languages. The operation serves the double purpose of \u201cintermediate exit\u201d from or \u201creentry\u201d into procedure-like block instances. A second primitive, which corresponds to a fully symmetric coroutine linkage, is derived from the former. An abstract notation is introduced for the concepts of textual and dynamic enclosure of block instances. The effects of the sequencing primitives are explored in terms of this notation, and certain results are proved. Finally applications are given within the language framework of Simula 67.", "authors": ["Arne Wang", "Ole-Johan Dahl"], "id": "1ca6e22f4c6be216c7f9c6b809ed7ad74096f970", "title": "Coroutine sequencing in a block structured environment", "references": []}, {"date": "1972", "abstract": "Abstract : The manual is an introduction and reference to the latest version of the Conniver programming language, an artificial intelligence language with general control and data-base structures. (Author)", "authors": ["D. McDermott", "Gerald J. Sussman"], "id": "97e0e7161f9d0e6762c396d9714f9043b8079b48", "title": "The Conniver Reference Manual", "references": []}, {"date": "1955", "abstract": "Semantic Scholar extracted view of \"An evaluation of the effect of induced tension on performance.\" by Lyle E. Bourne", "authors": ["Lyle E. Bourne"], "id": "09953453e1f8c1514c02d8906d26a266a6ad7f16", "title": "An evaluation of the effect of induced tension on performance.", "references": []}, {"date": "1967", "abstract": "Semantic Scholar extracted view of \"Correctness of a compiler for algol-like programs\" by Donald M. Kaplan", "authors": ["Donald M. Kaplan"], "id": "281212cc3d5eaad480dd321daf8ab9de7d5be375", "title": "Correctness of a compiler for algol-like programs", "references": []}, {"date": "1966", "abstract": "Both iterative and recursive programs for computing generalizations of functions which obtain the factorial of an integer, which reverse the order of symbols on a list and which obtain the approximate integral of a function are given as recursive definitions using conditional expressions. The equivalence of the iterative and recursive definitions are proved and a general theorem on equivalence, from which some of the results may be deduced, is stated and proved.!", "authors": ["D. C. Cooper"], "id": "8f10eb4e5d74c772a9a5a9695e027512327e7c4a", "title": "The Equivalence of Certain Computations", "references": []}, {"date": "1964", "abstract": "While the study of psychology has offered little in the way of explaining the creative process, Koestler examines the idea that we are at our most creative when rational thought is suspended--for example, in dreams and trancelike states. All who read The Act of Creation will find it a compelling and illuminating book.", "authors": ["Arthur Koestler"], "id": "04c12d5bd845611bf06e6808245702cc76c52ac5", "title": "The Act of Creation", "references": []}, {"date": "1962", "abstract": "Publisher Summary This chapter discusses the mathematical theory of computation. Computation essentially explores how machines can be made to carry out intellectual processes. Any intellectual process that can be carried out mechanically can be performed by a general purpose digital computer. There are three established directions of mathematical research that are relevant to the science of computation\u2014namely, numerical analysis, theory of computability, and theory of finite automata. The chapter explores what practical results can be expected from a suitable mathematical theory. Further, the chapter presents several descriptive formalisms with a few examples of their use and theories that enable to prove the equivalence of computations expressed in these formalisms. A few mathematical results about the properties of the formalisms are also presented.", "authors": ["John R. McCarthy"], "id": "d8ce4b5489ef14e8878c869101e30432d057599c", "title": "A Basis for a Mathematical Theory of Computation", "references": []}, {"date": "1972", "abstract": "Higher-order programming languages (i.e., languages in which procedures or labels can occur as values) are usually defined by interpreters which are themselves written in a programming language based on the lambda calculus (i.e., an applicative language such as pure LISP). Examples include McCarthy's definition of LISP, Landin's SECD machine, the Vienna definition of PL/I, Reynolds' definitions of GEDANKEN, and recent unpublished work by L. Morris and C. Wadsworth. Such definitions can be classified according to whether the interpreter contains higher-order functions, and whether the order of application (i.e., call-by-value versus call-by-name) in the defined language depends upon the order of application in the defining language. As an example, we consider the definition of a simple applicative programming language by means of an interpreter written in a similar language. Definitions in each of the above classifications are derived from one another by informal but constructive methods. The treatment of imperative features such as jumps and assignment is also discussed.", "authors": ["John C. Reynolds"], "id": "31d75b28132803648b84adb30fde4621361f2f86", "title": "Definitional interpreters for higher-order programming languages", "references": ["fcdf3ca85573c8d5c095d885daba64beaf00844f", "8567e9961dd73879d31c860ef18e6548862755f4", "27dd189065bd8847a8ec8f27553282df67a42d3e", "2af0e0213c191779614324336bff716fda34f317", "f07bc62734f02066f0d081d462398fb9ebfde3c4", "466a0abafc20c385397f2aaac14a7ba184995455", "c679deecae18fb0e9a7c6f38ca09a534979cd421", "236e26f19b1eb9fb7ef31a5396c382bf1127c738", "b443e18512181514b19363cd54dd3309c70be20e", "e762c5acd9607907b6dbab223d746ac8f5e884b5"]}, {"date": "1966", "abstract": "Abstract : The paper contains a proof of the correctness of a simple compiling algorithm for compiling arithmetic expressions into machine language.", "authors": ["John McCarthy", "James A. Painter"], "id": "e1f9006298bd72d50c0b3a4a1cb4cc9475694fb2", "title": "CORRECTNESS OF A COMPILER FOR ARITHMETIC EXPRESSIONS", "references": ["5fcdf9ca8638657509e154de3501066f0e9217e0"]}, {"date": "1967", "abstract": "Semantic Scholar extracted view of \"Semantic correctness of a compiler for an algol-like language\" by James A. Painter", "authors": ["James A. Painter"], "id": "6e3c25ab217c27ce55a19019f856d2fd683cdca1", "title": "Semantic correctness of a compiler for an algol-like language", "references": []}, {"date": "1968", "abstract": "Abstract : Results are presented of some applications of universal algebra and automata theory to programming problems. A method of defining some functions as homomorphisms instead of giving recursive or iterative definitions is explained. As a demonstration of the use of this approach, a proof of the correctness of a simple compiler for expressions is given. The method of description is closely related to the methods of proving theorems about the objects described. The report includes a section on basic algebraic concepts as background for the applications, and a section in which functions commonly encountered in programming are expressed as homomorphisms. (Author)", "authors": ["Rodney Martineau Burstall", "P. J. Landin"], "id": "427dd6f76ec119aa185f1e2ac82f040082e7d2ff", "title": "PROGRAMS AND THEIR PROOFS: AN ALGEBRAIC APPROACH,", "references": []}, {"date": "", "abstract": "THE absence of a Department of Agriculture from the complicated scheme of British Government offices leads us to inquire whether it is possible for such a Department in the United States to publish annually eleven or twelve hundred pages of matter useful to the agricultural community, and whether those publications have any considerable circulation in the country.", "authors": ["Philip S. Derfler"], "id": "9eed17dd2ad338a2d4ccace774e4ac767284a2d7", "title": "The United States Department of Agriculture", "references": []}, {"date": "1964", "abstract": "This paper is a contribution to the \"theory\" of the activity of using computers. It shows how some forms of expression used in current programming languages can be modelled in Church's X-notation, and then describes a way of \"interpreting\" such expressions. This suggests a method, of analyzing the things computer users write, that applies to many different problem orientations and to different phases of the activity of using a computer. Also a technique is introduced by which the various composite information structures involved can be formally characterized in their essentials, without commitment to specific written or other representations.", "authors": ["P. J. Landin"], "id": "2769c203102a875c10bc11affc161891472176d1", "title": "The Mechanical Evaluation of Expressions", "references": ["b443e18512181514b19363cd54dd3309c70be20e", "83f054294ba2726d02aa03e471da773c3383b146", "d88f9058b1c3277e5266b9f76cbb067c07d2f363", "9c6e618fe404c84ecb2fcca1dba505e040460f51", "52b99d29c931d9aaf1b3d6f48b31577affef0208", "99338adde12a235040c93333fec78765b0541880"]}, {"date": "1972", "abstract": "Abstract : This report presents a language, called QA4, designed to facilitate the construction of problem-solving systems used for robot planning, theorem proving, and automatic program synthesis and verification. QA4 integrates an omega-order logic language with canonical composition, associative retrieval, and pattern matching of expressions; process structure programming; goal-directed searching; and demons. Thus it provides many useful programming aids. More importantly, however, it provides a semantic framework for common sense reasoning about these problem domains. The interpreter for the language is extraordinarily general, and is therefore an adaptable tool for developing the specialized techniques of intuitive, symbolic reasoning used by the intelligent systems.", "authors": ["Johns F. Rulifson", "Jan A. Derksen", "Richard J. Waldinger"], "id": "fb4b11202c03ff7855af3e23cf166a2a28c62f26", "title": "QA4: A Procedural Calculus for Intuitive Reasoning.", "references": []}, {"date": "1967", "abstract": "The paths of saccadic movements of the left eye were recorded photographically for 12 normal subjects. Movements were elicited by nine small stimulus lights, arranged in a circular pattern. Each stimulus represented a five degree movement in one of eight directions, to or from the primary fixation at the center of the circular pattern. Each direction of movement gave a distinctly different path, yet corresponding movements showed close agreement between subjects. The shapes of the paths suggest that the lateral rectus contracts consistently later than the other five extraocular muscles, regardless of the direction of the stimulus. It is possible that this delay is caused by different signal path lengths or by different propagation velocities in the neural pathways leading to the several nuclei of the oculomotor nerves.", "authors": ["Emma Lewis Thomas", "H. O'Beirne"], "id": "f6c4065b3f7c5aa64ff2199554f213aa72209f7c", "title": "Curvature in the saccadic movement.", "references": []}, {"date": "1966", "abstract": "A family of unimplemented computing languages is described that is intended to span differences of application area by a unified framework. This framework dictates the rules about the uses of user-coined names, and the conventions about characterizing functional relationships. Within this framework the design of a specific language splits into two independent parts. One is the choice of written appearances of programs (or more generally, their physical representation). The other is the choice of the abstract entities (such as numbers, character-strings, list of them, functional relations among them) that can be referred to in the language.\nThe system is biased towards \u201cexpressions\u201d rather than \u201cstatements.\u201d It includes a nonprocedural (purely functional) subsystem that aims to expand the class of users' needs that can be met by a single print-instruction, without sacrificing the important properties that make conventional right-hand-side expressions easy to construct and understand.", "authors": ["Peter J. Landin"], "id": "27dd189065bd8847a8ec8f27553282df67a42d3e", "title": "The next 700 programming languages", "references": []}, {"date": "1959", "abstract": "USING a light microscope, Bruesh and Arey1 found that there were about 15,300 myelinated and 13,700 unmyelinated fibres in the optic nerve of the frog (Rana pipiens), and about 10,200 myelinated and 5,500 unmyelinated fibres in the optic nerve of the toad (Bufo americanus). However, in my electron microscope study of the optic nerve of these and other Anurans (Rana catesbeiana, Bufo terrestris and Hyla cinerea) I have found2 that the unmedullated axons are in a considerably larger number, having been underestimated by a factor of 30 or more, and that most of them could not have been resolved with the light microscope. My observations, though, do not greatly contradict the earlier numbers of myelinated fibres, for these can be counted with reasonable accuracy after myelin stain. In the present communication I only wish to report about the general arrangement of the unmyelinated axons and their number. The full description of the fine anatomy of the optic nerve is deferred to an extensive article now in preparation.", "authors": ["H. R. Maturana"], "id": "22f063bc32cae7adc38ab0f96ea79f6ddb801e6d", "title": "Number of Fibres in the Optic Nerve and the Number of Ganglion Cells in the Retina of Anurans", "references": []}, {"date": "1962", "abstract": "What chiefly distinguishes cerebral cortex from other parts of the central nervous system is the great diversity of its cell types and inter-connexions. It would be astonishing if such a structure did not profoundly modify the response patterns of fibres coming into it. In the cat's visual cortex, the receptive field arrangements of single cells suggest that there is indeed a degree of complexity far exceeding anything yet seen at lower levels in the visual system. In a previous paper we described receptive fields of single cortical cells, observing responses to spots of light shone on one or both retinas (Hubel & Wiesel, 1959). In the present work this method is used to examine receptive fields of a more complex type (Part I) and to make additional observations on binocular interaction (Part II). This approach is necessary in order to understand the behaviour of individual cells, but it fails to deal with the problem of the relationship of one cell to its neighbours. In the past, the technique of recording evoked slow waves has been used with great success in studies of functional anatomy. It was employed by Talbot & Marshall (1941) and by Thompson, Woolsey & Talbot (1950) for mapping out the visual cortex in the rabbit, cat, and monkey. Daniel & Whitteiidge (1959) have recently extended this work in the primate. Most of our present knowledge of retinotopic projections, binocular overlap, and the second visual area is based on these investigations. Yet the method of evoked potentials is valuable mainly for detecting behaviour common to large populations of neighbouring cells; it cannot differentiate functionally between areas of cortex smaller than about 1 mm2. To overcome this difficulty a method has in recent years been developed for studying cells separately or in small groups during long micro-electrode penetrations through nervous tissue. Responses are correlated with cell location by reconstructing the electrode tracks from histological material. These techniques have been applied to CAT VISUAL CORTEX 107 the somatic sensory cortex of the cat and monkey in a remarkable series of studies by Mountcastle (1957) and Powell & Mountcastle (1959). Their results show that the approach is a powerful one, capable of revealing systems of organization not hinted at by the known morphology. In Part III of the present paper we use this method in studying the functional architecture of the visual cortex. It helped us attempt to explain on anatomical \u2026", "authors": ["David H. Hubel", "Torsten N. Wiesel"], "id": "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7", "title": "Receptive fields, binocular interaction and functional architecture in the cat's visual cortex.", "references": ["b446e3fb01026b4e5ad978812540f172405fff6f", "6e56f97881931233c5cf5b83c293cfcf0581f4a9", "6ded76b03bd0612974d86a826ce70d2a2b143fb2", "401185a7ba67af1e01e5278932747e5fd777d3a6", "34898b2e1cb9f7872d2d66fdfcf3a9644948463b", "bc4e084831b9f987826b2cf184356e347ee155f8", "2923e0dca809bf904a653915f1fe1f16db69fb84", "6f20e254e3993538c79e0ff2b9b8f198d3359cb3", "62986ed55e8420663b0157e372df1606e4896bca", "b0b13465c36789ebd0751178c5403efd4dd390b9"]}, {"date": "1940", "abstract": "Appreciation of the form of the retinal image depends upon a correspondence between the distribution of light on the retina and the distribution of activity among the fibers of the optic nerve. This correspondence may be studied directly by recording the activity in single optic nerve fibers in response to illuminating various parts of the retina. A given optic nerve fiber responds to light only if a particular region of the retina receives illumination. This region is termed the receptive field of that fiber. In a previous paper describing the responses in single optic nerve fibers from the cold-blooded vertebrate eye (Hartline, 1938) it was noted that the receptive fields of the optic nerve fibers are of small but appreciable extent, and that their locations on the retina are fixed. It is the purpose of the present paper to describe further the characteristics of receptive fields, and to discuss some of the spatial factors involved in the excitation of the fibers of the optic nerve. METHOD. The method for recording the activity in single optic nerve fibers from the eyes of cold-blooded vertebrates has been described in the previous paper (lot. cit.). An eye is excised, cut open, and small bundles of optic nerve fibers are dissected from the anterior surface of the exposed retina. The action potentials in these bundles are amplified and recorded with an oscillograph. When such a bundle has been split successfully , until only a single active fiber remains, the retina must be searched with a small spot of light to determine the region supplying that fiber. This search is aided by noting the direction, on the retina, from which the nerve fibers in the small bundle come, and by using large spots of light at first to locate the approximate position of the sensitive region. The optical system employed in these experiments has likewise been described. A spot of light of suitable size is projected upon the exposed retina; the coordinates of its position, referred to an arbitrary point of origin on the retina, are obtained from readings of crossed micrometers which control its location. The micrometer readings are reduced to millimeters on the retina by multiplying them by the magnification of the optical system (0.32 or 0.15). Sharpness of focus of the spot on the retina 690", "authors": ["H. Keffer Hartline"], "id": "ec71953a116a869a063cf516fe5e57700fff6567", "title": "THE RECEPTIVE FIELDS OF OPTIC NERVE FIBERS", "references": []}, {"date": "1957", "abstract": "Semantic Scholar extracted view of \"Change of organization in the receptive fields of the cat's retina during dark adaptation.\" by H. B. Barlow et al.", "authors": ["H. B. Barlow", "Richard Fitzhugh", "Stephen W. Kuffler"], "id": "929f33d2c4ede915b830e5a47b5007cd4c870386", "title": "Change of organization in the receptive fields of the cat's retina during dark adaptation.", "references": ["fe46043144b90dad9d09539f6c9904e578717405", "b446e3fb01026b4e5ad978812540f172405fff6f", "df7b6ce93159e91cd042e3f32daf3911cf1bd037", "65eea66366ce91c61e2c8359b6e302c40c42a081", "84abdd5286dfb94fe8799413d56d477fe53b04aa", "46cca334d95fd9e46e96f004af008d1eb2d3850b"]}, {"date": "1947", "abstract": "Two neural mechanisms are described which exhibit recognition of forms. Both are independent of small perturbations at synapses of excitation, threshold, and synchrony, and are referred to partiular appropriate regions of the nervous system, thus suggesting experimental verification. The first mechanism averages an apparition over a group, and in the treatment of this mechanism it is suggested that scansion plays a significant part. The second mechanism reduces an apparition to a standard selected from among its many legitimate presentations. The former mechanism is exemplified by the recognition of chords regardless of pitch and shapes regardless of size. The latter is exemplified here only in the reflexive mechanism translating apparitions to the fovea. Both are extensions to contemporaneous functions of the knowing of universals heretofore treated by the authors only with respect to sequence in time.", "authors": ["Walter C. Pitts", "Warren S. McCulloch"], "id": "0e9b804adcedfe82d5139b572c5c165fa161efb7", "title": "How we know universals; the perception of auditory and visual forms.", "references": []}, {"date": "1964", "abstract": "This is a study of the adaptation of human Ss to long-term exposure to optical distortions produced by wedge prisms. Results indicate that the various prismatic distortions are adapted to at varying rates to varying degrees. Adaptation to the lateral displacement produced by a prism occurs rapidly and completely. Adaptation to certain shape distortions occurs slowly and to a minimal degree.", "authors": ["Herbert L. Pick", "John C. Hay"], "id": "99845addbc6f387682c6a05ecaf62223e92e5486", "title": "Adaptation to prismatic distortion", "references": []}, {"date": "1968", "abstract": "Semantic Scholar extracted view of \"Method and notation for the formal definition of programming languages\" by Peter J. F. Lucas et al.", "authors": ["Peter J. F. Lucas", "Peter E. Lauer", "H. Stigleitner"], "id": "fcdf3ca85573c8d5c095d885daba64beaf00844f", "title": "Method and notation for the formal definition of programming languages", "references": []}, {"date": "1959", "abstract": "Method and device for sealing a continuous product, such as an electrical cable, passing between zones at different pressures during manufacture wherein a liquid such as water, is used to effect sealing, and turbulence is created in the liquid to reduce the dimensions of the seal, e.g. by passing a suitable gas or gas mixture, such as air, through the liquid. The device may be a tube through which the product passes and in which the liquid forms a seal between the product and the tube wall. The gas may pass through suitably directed holes in the tube wall into the liquid. The gas may be directed against the flow of the liquid or may impart a spiral motion to the liquid. The spiral motion may also be produced e.g. by means of spring inserts in the tube.", "authors": ["Gunnar Svaetichin", "E. F. Macnichol"], "id": "6206c28676bc30b30f23c74343df7a06e937861a", "title": "Retinal mechanisms for chromatic and achromatic vision.", "references": []}, {"date": "1962", "abstract": "Semantic Scholar extracted view of \"Computer programs for checking mathematical proofs\" by John McCarthy", "authors": ["John McCarthy"], "id": "5fcdf9ca8638657509e154de3501066f0e9217e0", "title": "Computer programs for checking mathematical proofs", "references": []}, {"date": "1960", "abstract": "A programming system called LISP (for LISt Processor) has been developed for the IBM 704 computer by the Artificial Intelligence group at M.I.T. The system was designed to facilitate experiments with a proposed system called the Advice Taker, whereby a machine could be instructed to handle declarative as well as imperative sentences and could exhibit \u201ccommon sense\u201d in carrying out its instructions. The original proposal [1] for the Advice Taker was made in November 1958. The main requirement was a programming system for manipulating expressions representing formalized declarative and imperative sentences so that the Advice Taker system could make deductions. In the course of its development the LISP system went through several stages of simplification and eventually came to be based on a scheme for representing the partial recursive functions of a certain class of symbolic expressions. This representation is independent of the IBM 704 computer, or of any other electronic computer, and it now seems expedient to expound the system by starting with the class of expressions called S-expressions and the functions called S-functions.", "authors": ["John McCarthy"], "id": "b443e18512181514b19363cd54dd3309c70be20e", "title": "Recursive functions of symbolic expressions and their computation by machine, Part I", "references": []}, {"date": "1968", "abstract": "This paper describes PAL\u2014a new computer language. Given the fact that new languages seem to appear in computer literature at the rate of several per month, it seems incumbent on one who creates a new language to justify having done so. In the present case, there are two important considerations: control and specification. Let us consider each of these in turn.\n By virtue of our having designed PAL, it is ours. There is no PAL Users Group or Committee of Vested Interests concerned with retaining upward compatibility with what was done last year (or last month). This doesn't mean we change the specifications of the language every few weeks (our students are, in a real sense, our Committee of Vested Interests), but it does mean we can make decisions on changes solely on technical grounds. More important, though, we can design the language to meet the criteria we think important. For example, the language almost demands interpretive execution. Since no one writes production programs in PAL we are able to put up with inefficiencies in the implementation that would otherwise be intolerable. Thus we have designed our own language so that we will have control over it.", "authors": ["Arthur Evans"], "id": "236e26f19b1eb9fb7ef31a5396c382bf1127c738", "title": "PAL\u2014a language designed for teaching programming linguistics", "references": []}, {"date": "1962", "abstract": "In this paper I shall discuss the prospects for a mathematical science of computation. In a mathematical science, it is possible to deduce from the basic assumptions, the important properties of the entities treated by the science. Thus, from Newton\u2019s law of gravitation and his laws of motion, one can deduce that the planetary orbits obey Kepler\u2019s laws.", "authors": ["John McCarthy"], "id": "e762c5acd9607907b6dbab223d746ac8f5e884b5", "title": "Towards a Mathematical Science of Computation", "references": ["e128c8750eee8781cdc76c9aa00df037180febf0", "d8ce4b5489ef14e8878c869101e30432d057599c"]}, {"date": "1968", "abstract": "A critical review of recent efforts to automate the writing of translators of programming languages is presented. The formal study of syntax and its application to translator writing are discussed in Section II. Various approaches to automating the postsyntactic (semantic) aspects of translator writing are discussed in Section III, and several related topics in Section IV.", "authors": ["Jerome A. Feldman", "David Gries"], "id": "466a0abafc20c385397f2aaac14a7ba184995455", "title": "Translator writing systems", "references": []}, {"date": "1963", "abstract": "This chapter discusses the elements of mathematical logic. Mathematical logic developed as a result of the application of mathematical methods to the problems of formal logic and as a discipline serving the ends of the foundations of mathematics. Mathematical logic has received diverse technical applications. Contemporary mathematical logic is connected with automation, with machine mathematics and problems of automatic translation from one language to another, with information theory, and with cybernetics. The methods of mathematical logic find wide applications in the theory of electrical networks with switching action. In algebra, numbers\u2014the objects of the study of arithmetic\u2014are denoted by letters. The object of that part of mathematical logic that is known as the propositional calculus is the study of propositions. The propositional calculus is the most elementary part of mathematical logic. Mathematical logic distinguishes carefully between the different ways of using variables and fixes them with the aid of a special symbolism.", "authors": ["I. S. Gradshte\u012dn"], "id": "99338adde12a235040c93333fec78765b0541880", "title": "THE ELEMENTS OF MATHEMATICAL LOGIC", "references": []}, {"date": "1953", "abstract": "THE DISCHARGES carried in the optic nerve fibers contain all the information which the central nervous system receives from the retina. A correct interpretation of discharge patterns therefore constitutes an important step in the analysis of visual events. Further, investigations of nervous activity arising in the eye reveal many aspects of the functional organization of the neural elements within the retina itself. Following studies of discharges in the optic nerve of the eel\u2019s eye by Adrian and Matthews (2,3), Hartline and his colleagues described the discharge pattern in the eye of the Limulus in a series of important and lucid papers (for a summary see 20). In the Limulus the relationship between the stimulus to the primary receptor cell and the nerve discharges proved relatively simple, apparently because the connection between sense cell and nerve fiber was a direct one. Thus, when stimulation is confined to one receptor the discharge in a single Limulus nerve fiber will provide a good indication of excitatory events which take place as a result of photochemical processes. Discharges last for the duration of illumination and their frequency is a measure of stimulus strength. Lately, however, it was shown by Hartline et al. (22) that inhibitory interactions may be revealed when several receptors are excited. On the whole, the Limulus preparation shows many features which are similar to other simple sense organs, for instance, stretch receptors. In the latter, however, instead of photochemical events, stretch-deformation acts as the adequate stimulus on sensory terminals and is translated into a characteristic discharge pattern. The discharge from the cold-blooded vertebrate retina (mainly frogs) proved much more complex. Hartline found three main types when recording from single optic nerve fibers: (i) \u201con\u201d discharges, similar to those in the Limulus, firing for the duration of the light stimulus, (ii) \u201coff\u201d discharges appearing when a light stimulus was withdrawn, and (iii) \u2018con-off\u201d discharges, a combination of the former two, with activity confined mainly to onset and cessation of illumination. The mammalian discharge patterns were studied in a number of species by Granit and his co-workers in the course of their extensive work on the physiology of the visual system (summaries in 13, 15). On the whole, they did not observe any fundamental differences between frog and mammalian discharge types (see later).", "authors": ["Stephen W. Kuffler"], "id": "b446e3fb01026b4e5ad978812540f172405fff6f", "title": "Discharge patterns and functional organization of mammalian retina.", "references": ["9cf0fff7457c954b2d1f769512b3e1552292f84b", "4f10e929ba2576b1198961b581b8609b04c12691", "df7b6ce93159e91cd042e3f32daf3911cf1bd037", "ec71953a116a869a063cf516fe5e57700fff6567", "118ce3604eab68c295cb528ee883a1e4d45fd90b", "61215dcedde6317475e361eb58a6b910e14a6d86", "7eef73a0e100bd61fb88acf0b42db739efb928d8", "2e93efd940bd0e5f7bde557e07ccf455269b52f6"]}, {"date": "1941", "abstract": "The description for this book, The Calculi of Lambda Conversion. (AM-6), will be forthcoming.", "authors": ["Alonzo Church"], "id": "9c6e618fe404c84ecb2fcca1dba505e040460f51", "title": "The calculi of lambda-conversion", "references": []}, {"date": "1959", "abstract": "Publisher Summary This chapter discusses the Lisp-Like machine language. One of the operators that must be assumed to be primitive in Lisp, independently of any class F, is a conditional operator. The new conditional operator is also defined and it is shown how the Lisp conditional operator can be defined in terms of it whenever there is an element representing the truth value truth in the universe U. The quote operator of Lisp is used to create names for members of U that are symbols, in particular for the symbol T used to represent the truth value truth; the value of the quote operator for a symbol is a name for the symbol. Important purposes can be served by defining the semantics of a programming language by defining an abstract computer for which the programming language is the machine language. These purposes include both a better understanding of the language, as well as of the difficulties that are likely to be faced in implementing it. The chapter explains how a recursive definition of a function can be given to the computer in its machine language and how the computer evaluates such a function for a given argument.", "authors": ["Paul C. Gilmore"], "id": "d88f9058b1c3277e5266b9f76cbb067c07d2f363", "title": "An Abstract Computer with a Lisp-Like Machine Language Without a Label Operator", "references": []}, {"date": "1960", "abstract": "Willard Van Orman Quine begins this influential work by declaring, \"Language is a social art. In acquiring it we have to depend entirely on intersubjectively available cues as to what to say and when.\" As Patricia Smith Churchland notes in her foreword to this new edition, with Word and Object Quine challenged the tradition of conceptual analysis as a way of advancing knowledge. The book signaled twentieth-century philosophy's turn away from metaphysics and what Churchland calls the \"phony precision\" of conceptual analysis. In the course of his discussion of meaning and the linguistic mechanisms of objective reference, Quine considers the indeterminacy of translation, brings to light the anomalies and conflicts implicit in our language's referential apparatus, clarifies semantic problems connected with the imputation of existence, and marshals reasons for admitting or repudiating each of various categories of supposed objects. In addition to Churchland's foreword, this edition offers a new preface by Quine's student and colleague Dagfinn Follesdal that describes the never-realized plans for a second edition of Word and Object, in which Quine would offer a more unified treatment of the public nature of meaning, modalities, and propositional attitudes.", "authors": ["Willard van Orman Quine"], "id": "52b99d29c931d9aaf1b3d6f48b31577affef0208", "title": "Word and Object", "references": []}, {"date": "1966", "abstract": "Publisher Summary This chapter focuses on \u03bb-calculus approach. The chapter presents an introduction to a complementary activity is called semantic analysis and explores a mathematical basis for it. This technique consists of establishing a correspondence between the texts of the language to be analyzed and expressions of a structurally simpler language for which the problem of semantic description is less diffuse. This structurally simpler language comprises certain expressions called applicative expressions (AEs). The AEs are characterized in abstract terms, without reference to a particular written representation of them. When presenting specific AEs, one shall be compelled to adopt some conventions about how to write them, but one shall do so informally, and it will be convenient to use different conventions and even different representations of the same AE on different occasions.", "authors": ["P. J. Landin"], "id": "f07bc62734f02066f0d081d462398fb9ebfde3c4", "title": "A \u03bb-CALCULUS APPROACH", "references": []}, {"date": "1952", "abstract": "Semantic Scholar extracted view of \"Accuracy and Sensitivity of the Human Eye\" by M. H. Pirenne et al.", "authors": ["M. H. Pirenne", "E. J. Denton"], "id": "46cca334d95fd9e46e96f004af008d1eb2d3850b", "title": "Accuracy and Sensitivity of the Human Eye", "references": []}, {"date": "1955", "abstract": "A device is described which converts a train of electrical impulses into a succession of hyperbolic wave forms of amplitude proportional to the interval between successive impulses. These wave forms when applied to an oscilloscope furnish a display capable of showing rapid changes in the frequency of an electrical signal. Unlike conventional counting\u2010rate meters response to sudden changes in frequency is immediate so that averaging over several cycles is not required. The instrument as constructed operates in the lower audio\u2010frequency range and is being used in the study of the electrical discharge of single fibers of the optic nerve.", "authors": ["E. F. Macnichol", "Jay A. H. Jacobs"], "id": "84abdd5286dfb94fe8799413d56d477fe53b04aa", "title": "Electronic Device for Measuring Reciprocal Time Intervals", "references": []}, {"date": "1957", "abstract": "Semantic Scholar extracted view of \"Increment thresholds at low intensities considered as signal/noise discriminations.\" by H. B. Barlow", "authors": ["H. B. Barlow"], "id": "65eea66366ce91c61e2c8359b6e302c40c42a081", "title": "Increment thresholds at low intensities considered as signal/noise discriminations.", "references": []}, {"date": "1952", "abstract": "An ophthalmoscope is described for electrophysiological research on the retina. It adds to the usual viewing and illuminating beam two stimulating beams whose patterns may also be viewed and photographed. A large region of the mammalian fundus can be surveyed, the eye being in the center of a spherical coordinate system.The design provides measured adaptive and stimulating parameters over the photopic range, sufficient to study intensity, color, timing, and spatial interaction in the retinal mechanisms. Extension to mesopia and scotopia is possible.Associated optically and mechanically are devices for locating, holding, and directing the eye and head of anaesthetized cats (or monkeys). The eye holder carries a small micromanipulator to control the direction and feed of microelectrodes. This arrangement provides stable electrical contact with single optic ganglion cells for long periods.The method does not interfere with normal metabolism and optical functioning of the eye.", "authors": ["Samuel A. Talbot", "Stephen W. Kuffler"], "id": "df7b6ce93159e91cd042e3f32daf3911cf1bd037", "title": "A multibeam ophthalmoscope for the study of retinal physiology.", "references": []}, {"date": "1957", "abstract": "Granit (1943, 1944) observed that the threshold of retinal ganglion cells in the cat drops when they are allowed to dark-adapt after exposure to bright lights, and that there is a shift in spectral sensitivity analogous to the Purkinje shift in the human. The drop in threshold was rather small compared to that found in human dark adaptation, and the final threshold reached was not nearly as low as one would expect in an eye adapted for nocturnal vision. Pirenne (1954) has emphasized that there is a big discrepancy (a factor of 103-106) between the lowest threshold obtained by electro-physiological methods and the much lower thresholds found by psychophysical methods in humans (Hecht, Shlaer & Pirenne, 1942). In addition, the existence of a random maintained discharge from retinal ganglion cells (Kuffler, FitzHugh & Barlow, 1957) raises some doubts about the relative parts played by the retina and the central nervous system in determining thresholds. We therefore set out to measure the absolute threshold of the large type of ganglion cell (Rushton, 1949) isolated by metallic micro-electrodes of the type used by Granit & Svaetichin (1939). The opportunity was taken of following threshold through the period of dark adaptation, and some observations of the Purkinje shift were made.", "authors": ["H. B. Barlow", "Richard Fitzhugh", "Stephen W. Kuffler"], "id": "fe46043144b90dad9d09539f6c9904e578717405", "title": "Dark adaptation, absolute threshold and Purkinje shift in single units of the cat's retina.", "references": ["b446e3fb01026b4e5ad978812540f172405fff6f", "ebb30df0a932595caf1efc2418f9f30033aab563", "c34da3fd5d71b3a00fcc68801382aa9d0316417e", "df7b6ce93159e91cd042e3f32daf3911cf1bd037", "ec71953a116a869a063cf516fe5e57700fff6567", "878a934bca7c98fdc5ab2079ac88ef784909f6b0", "020e6e056346c97582515268d6e7639574d73c5e", "de39901564a556c9664dd7b9ca2b38c0183665fd", "fd8eb906dcc80191c1dc5d1d549acf1fc4f33744", "83cf69db1a2897e0bf0a250cfe64a0fcf1835eb7"]}, {"date": "1958", "abstract": "This paper represents the beginning of an attempt to improve our understanding of the nervous system by studying normal activity in single cells. The approach we have em\u00ad ployed involves recording from single units in the unanesthetized unrestrained animal. We have done this in order to examine dis\u00ad charge patterns in various states such as wakefulness and natural sleep, and to influ\u00ad ence these patterns, if possible, by physi\u00ad ologic stimuli. We have thus hoped to com\u00ad bine the advantages of two important recent advances in electrophysiology, the develop\u00ad ment of methods for the chronic implanta\u00ad tion of electrodes on the one hand, and the use of microelectrodes on the other. We have felt that such a combination might be espe\u00ad cially promising in studies of the higher cen\u00ad tral nervous system, where such things as anesthetics, brain stem sections, and paralyz\u00ad ing drugs are most likely to distort normal neuronal discharges.", "authors": ["David H. Hubel"], "id": "62986ed55e8420663b0157e372df1606e4896bca", "title": "Cortical unit responses to visual stimuli in nonanesthetized cats.", "references": []}, {"date": "1970", "abstract": "However, Scott does realize that the approach argued for above is simply an argument for an approach that accomodates human understanding of computation and that the operational approach must not be ignored because, as he points out, the machines that the programs of study run on are not capable of dealing with such an abstract level of understanding. That is, the computaional approach should not be abandoned because the machines that we build operate on that lower level.", "authors": ["Dana S. Scott"], "id": "2af0e0213c191779614324336bff716fda34f317", "title": "Outline of a mathematical theory of computation", "references": []}, {"date": "1960", "abstract": "Abstract : Receptive fields of retinal ganglion cells were studied in the light-adapted spider monkey. All fields mapped with white light had a concentric arrangement sililar to that of the cat retinal ganglion cells, with a sharply demarcated 'on' centre surrounded by an antagonistic 'off' periphery, or the reverse. The smallest receptive field centres were found near the fovea, and the size of centres tended to increase with increasing distance from the fovea. The smallest centre had a diameter of 4 minutes of arc (corresponding to about 20 microns on the retina) and was located 4 degrees from the fovea; the largest centre had a diameter of 2 degrees. Three ganglion cells out of about 100 responded in a specific way to coloured stimuli. In these cells light of short wave-length produced an 'on' response and light of long wave-length evoked inhibition followed by an 'off' response. Transition between the two types of response occurred at about 500 millimicrons, light of this wave-length evoked only feebel 'off' responses. Very weak responses were obtained to white light, presumably owing to the antagonism between light of short and long wavelengths. (Author)", "authors": ["David H. Hubel", "Torsten N. Wiesel"], "id": "b0b13465c36789ebd0751178c5403efd4dd390b9", "title": "Receptive fields of optic nerve fibres in the spider monkey.", "references": ["c21024536f3148e65bbf786772a97c8763255411", "b446e3fb01026b4e5ad978812540f172405fff6f", "fe46043144b90dad9d09539f6c9904e578717405", "4e8ec8ecd1a0b15828d862d0ef00a834a1ab1e46", "401185a7ba67af1e01e5278932747e5fd777d3a6", "3088f7fe753b038269f1fd3ceeaf0f219468565b", "663eb5b9acf0e9cc404728d98669012fbfcaa9c2", "3015ecca0e927f0e934b4f1ccce3edd832953ca6", "6f20e254e3993538c79e0ff2b9b8f198d3359cb3", "83cf69db1a2897e0bf0a250cfe64a0fcf1835eb7"]}, {"date": "1960", "abstract": "Surface potentials and single unit potentials in the cat9s cerebral cortex have been recorded in response to diffuse visual stimulation. Surface responses produced by stimulation of the contralateral eye are larger in amplitude than those produced by stimulation of the ipsilateral eye, while those produced by binocular stimulation are larger still. The amplitude of the surface responses increases to a maximum with increasing intensity of stimulation. For any given single cell, there is a constant ratio between the average number of unit discharges in response to contralateral, ipsilateral and binocular stimulation. At different stimulus intensities this ratio is constant, although the absolute numbers of discharges may be different. It has been possible to divide the cells into functional groups according to a ) which eye produces the larger number of discharges, i.e. which eye is \u2018dominant\u2019 and b ) whether binocular stimulation produces more, fewer, or as many discharges as stimulation of the dominant eye alone. All the functionally different types of neurones may be found in all the cellular layers of the cortex. The latency of the unit responses is shorter with binocular than with monocular stimulation, and is shorter when brighter stimuli are used.", "authors": ["Benedict Delisle Burns", "Woodburn Heron", "Bernice Grafstein"], "id": "2923e0dca809bf904a653915f1fe1f16db69fb84", "title": "Responses of cerebral cortex to diffuse monocular and binocular stimulation.", "references": []}, {"date": "1940", "abstract": "In a previous paper (Hartline, 1940) it was shown that a ganglion cell in the peripheral retina of the vertebrate eye is excited by activity in many convergent pathways, from sensory elements distributed over a receptive field covering approximately a square millimeter of retinal area. Illumination of any portion of the receptive field of a retinal ganglion cell will accordingly produce a discharge of impulses in its axon, the strength of the response to illumination of a fixed retinal area usually being greater the higher the intensity of the stimulating light. The present paper will show that the discharge of impulses in a single optic nerve fiber also depends upon the size of the illuminated area. The excitation of a ganglion cell is therefore controlled by the number of active pathways which converge upon it, as well as by the degree of activity in the individual pathways. Spatial summation in the vertebrate retina has previously been demonstrated by Adrian and Matthews (1927-1928). They showed that the latency of the discharge of impulses in the whole optic nerve of the eel was shorter the larger the area of the retina illuminated, and the latency of the response to four spots of light was shorter than the shortest latency obtained with any of the spots singly. This summation was enhanced by the application of strychnine, indicating that it depended upon the nervous interconnections within the retina. The study of the activity in single optic nerve fibers has now furnished more direct evidence for the convergence of excitatory effects within the retina; the present paper is concerned with the extension of this study to an analysis of spatial summation, in terms of the activity of the individual units of the retina. METHOD. The method for studying the activity of single optic nerve fibers in the retinas of cold-blooded vertebrates, and for determining the location and extent of their receptive fields has been described in previous papers (Hartline, 1938, 1940). In the present experiments the eyes from", "authors": ["H. Keffer Hartline"], "id": "7eef73a0e100bd61fb88acf0b42db739efb928d8", "title": "THE EFFECTS OF SPATIAL SUMMATION IN THE RETINA ON THE EXCITATION OF THE FIBERS OF THE OPTIC NERVE", "references": []}, {"date": "1950", "abstract": "Semantic Scholar extracted view of \"An analysis of the response from single visual-purple-dependent elements, in the retina of the cat.\" by K. O. Donner et al.", "authors": ["K. O. Donner", "E. N. Willmer"], "id": "61215dcedde6317475e361eb58a6b910e14a6d86", "title": "An analysis of the response from single visual-purple-dependent elements, in the retina of the cat.", "references": []}, {"date": "1961", "abstract": "Abstract : Cells were recorded with tungsten electrodes in the dorsal lateral geniculate body of the cat. Receptive fields of these units were mapped out, in the light-adapted state, with small sports of light. In their general arrangement geniculate receptive fields resembled those of retinal ganglion cells, having an excitatory ('on') centre and inhibitory ('off') preriphery, or reverse. The two portions of a receptive field were mutually antagonistic; the decrease in centre responses cauded by inclusion of peripheral portions of receptive fields was termed peripheral suppression. Cells recorded in layers A and B of the lateral geniculate body were driven from the contralateral eye; cells in layer A1 from the ipsilateral eye. In penetrations normal to the layers receptive fields of cells in a single layer were close together or superimposed, and from one layer to the next occupied exactly homologous positions in the two retinas. Binocular interaction was not observed in any of the cells studied. All three layers of the lateral geniculate contained both 'on'-centre and 'off'-centre units. Cells in layers A and A1 were similar both in their firing patterns and in average receptive field size. Cells in layer B were more sluggish in their responses to light stimuli, and tended to have larger receptive field centres. Cells with receptive fields within or near the area centralis tended to have smaller field centres and stronger suppression by the receptive field periphery than cells with their fields situated in more peripheral regions of the retina.", "authors": ["David H. Hubel", "Torsten N. Wiesel"], "id": "34898b2e1cb9f7872d2d66fdfcf3a9644948463b", "title": "Integrative action in the cat's lateral geniculate body.", "references": []}, {"date": "1937", "abstract": "It is assumed in most theories of excitation (e. g. Hill 1936 a ) that, when a critical state is reached at a point of the nerve, \u201cexcitation\u201d occurs and then automatically propagates over the whole length of the nerve fibre. Since, however, the self-conduction of an impulse involves stimulation of each element by the simultaneous activity of a finite adjacent region of nerve, it is more natural to suppose that initially also a certain minimal length of nerve must be excited by an applied shock, in order to give rise to a propagated disturbance. A subthreshold stimulus, therefore, by exciting too small a region, might produce a localized response, the spread and size of which are not enough to excite resting points further on. If such a non-conducted response can actually occur, it might be expected (i) to facilitate excitation by a second shock, if applied to the same region shortly after the first, and (ii) to be accompanied by a small local action potential. Both possibilities can be investigated experimentally, though both might be expected to be complicated by the direct effects (electrotonus,\u201clocal potential\u201d) of the first shock.", "authors": ["Bernhard Katz"], "id": "2e93efd940bd0e5f7bde557e07ccf455269b52f6", "title": "Experimental Evidence for a Non-Conducted Response of Nerve to Subthreshold Stimulation", "references": []}, {"date": "1960", "abstract": "In two recent studies of the cat's striate cortex (Hubel, 1959; Hubel & Wiesel, 1959) single units were shown to react to light stimuli in a highly specific manner. Most units responded either feebly or not at all to stimulation of the retina with diffuse light, but gave brisk responses to stationary or moving restricted spots of light. Responses to moving spots often varied with the direction of movement. It was clear that such responses must be the result of complex integrative mechanisms. The present study was undertaken to find out whether similar responses occur in retinal ganglion cells or cells of the dorsal lateral geniculate body. Lateral geniculate units have not previously been studied with restricted light stimulation, and although the cat's retinal ganglion cell has been extensively investigated by Kuffler and his co-workers (Kuffler, 1953; iKuffler, FitzHugh & Barlow, 1957), responses to moving spots were not examined. Thus it has not been possible to say whether the complex activity of cortical units originates in the cortex itself, or at lower levels. Methods for stereotaxic depth recordings in the unanaesthetized unrestrained animal were developed in order to make cortical and depth studies under similar conditions. These techniques make it possible to record from single units from virtually any part of the brain of the freely moving animal.", "authors": ["David H. Hubel"], "id": "401185a7ba67af1e01e5278932747e5fd777d3a6", "title": "Single unit activity in lateral geniculate body and optic tract of unrestrained cats.", "references": ["b446e3fb01026b4e5ad978812540f172405fff6f", "5b503d0bcc84c0be5651296a93f72668474f9741", "f5588fad7a584c6e232f4aa0eb530cbd56f9fcc4", "663eb5b9acf0e9cc404728d98669012fbfcaa9c2", "6f20e254e3993538c79e0ff2b9b8f198d3359cb3", "62986ed55e8420663b0157e372df1606e4896bca", "83cf69db1a2897e0bf0a250cfe64a0fcf1835eb7"]}, {"date": "1949", "abstract": "IT is well known from the work of Granit1 that isolated action potential spikes may be recorded from the eyes of many animals by placing a micro-electrode in contact with suitable spots on the retina through the opened bulb. The cat is particularly suitable. The importance of this work makes it especially desirable to define as precisely as possible the actual retinal structures responsible for the spikes recorded, for such questions as the following come to mind in attempting to interpret and evaluate the full significance of Granit's results. How is it possible to get such good insulation in a packed structure like the retina using a 25-\u00b5 electrode? Why are large isolated spikes only found in some spots and not elsewhere? Is it because some typical structure happens to be exceptionally favourably placed for recording, or is the structure a comparatively rare one? Or are the spikes due to the synchronous firing of a patch of retinal cells, and if so, is this normal or pathological?", "authors": ["W. A. H. Rushton"], "id": "118ce3604eab68c295cb528ee883a1e4d45fd90b", "title": "The Structure Responsible for Action Potential Spikes in the Cat's Retina", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"COMPARATIVE STUDIES ON THE PERIPHERAL AND CENTRAL RETINA: I. On Interaction between Distant Areas in the Human Eye\" by Ra\u01f5nar Granit", "authors": ["Ra\u01f5nar Granit"], "id": "4f10e929ba2576b1198961b581b8609b04c12691", "title": "COMPARATIVE STUDIES ON THE PERIPHERAL AND CENTRAL RETINA: I. On Interaction between Distant Areas in the Human Eye", "references": []}, {"date": "1959", "abstract": "In the central nervous system the visual pathway from retina to striate cortex provides an opportunity to observe and compare single unit responses at several distinct levels. Patterns of light stimuli most effective in influencing units at one level may no longer be the most effective at the next. From differences in responses at successive stages in the pathway one may hope to gain some understanding of the part each stage plays in visual perception. By shining small spots of light on the light-adapted cat retina Kuffler (1953) showed that ganglion cells have concentric receptive fields, with an 'on' centre and an 'off ' periphery, or vice versa. The 'on' and 'off' areas within a receptive field were found to be mutually antagonistic, and a spot restricted to the centre of the field was more effective than one covering the whole receptive field (Barlow, FitzHugh & Kuffler, 1957). In the freely moving lightadapted cat it was found that the great majority of cortical cells studied gave little or no response to light stimuli covering most of the animal's visual field, whereas small spots shone in a restricted retinal region often evoked brisk responses (Hubel, 1959). A moving spot of light often produced stronger responses than a stationary one, and sometimes a moving spot gave more activation for one direction than for the opposite. The present investigation, made in acute preparations, includes a study of receptive fields of cells in the cat's striate cortex. Receptive fields of the cells considered in this paper were divided into separate excitatory and inhibitory ('on' and 'off') areas. In this respect they resembled retinal ganglion-cell receptive fields. However, the shape and arrangement of excitatory and inhibitory areas differed strikingly from the concentric pattern found in retinal ganglion cells. An attempt was made to correlate responses to moving stimuli", "authors": ["David H. Hubel", "Torsten N. Wiesel"], "id": "6f20e254e3993538c79e0ff2b9b8f198d3359cb3", "title": "Receptive fields of single neurones in the cat's striate cortex.", "references": ["b446e3fb01026b4e5ad978812540f172405fff6f", "0e5187790e9de2fdeb10df35830e8420236fa2d5", "6ded76b03bd0612974d86a826ce70d2a2b143fb2", "df7b6ce93159e91cd042e3f32daf3911cf1bd037", "a09fad90b32296e158ed0e15d06996b89696116a", "929f33d2c4ede915b830e5a47b5007cd4c870386", "401185a7ba67af1e01e5278932747e5fd777d3a6", "663eb5b9acf0e9cc404728d98669012fbfcaa9c2", "ddd51dd628e52cd7171db36637d250b5f3d6febe", "345e94cf3db0022721f649d9eb75a443a39c31ba"]}, {"date": "1938", "abstract": "In a recent paper Katz (1937) has described a number of observations which are inconsistent with classical theories of excitation, but which may be explained by assuming that a subthreshold shock can elicit a small and localized action potential in the cathodic part of a nerve fibre. At first the assumption of a subthreshold response seems to be in conflict with the allor-nothing law, but there is in reality no contradiction, since the all-ornothing principle refers to the propagated disturbance, and does not exclude the possibility of graded reactions in the stimulated region. Indeed, the existence of some kind of subliminal action potential is to be expected from the local circuit theory of nervous transmission; for the current generated by the activity of a very short length of nerve would be too weak to excite surrounding regions, so that a shock which activated less than a certain length would fail to produce a self-propagating impulse and would leave behind only a small and localized response (cf. Rosenberg 1937, Rushton 1937).", "authors": ["Alan Lloyd Hodgkin"], "id": "9cf0fff7457c954b2d1f769512b3e1552292f84b", "title": "The subthreshold potentials in a crustacean nerve fibre", "references": []}, {"date": "1961", "abstract": "Semantic Scholar extracted view of \"CHECKING MATHEMATICAL PROOFS BY COMPUTER\" by John McCarthy", "authors": ["John McCarthy"], "id": "e128c8750eee8781cdc76c9aa00df037180febf0", "title": "CHECKING MATHEMATICAL PROOFS BY COMPUTER", "references": []}, {"date": "1973", "abstract": "Publisher Summary This chapter discusses models of various type free calculi. The range of arguments or range of values of a function should consist wholly or partly of functions. The derivative, as this notion appears in the elementary differential calculus, is a familiar mathematical example of a function for which both ranges consist of functions. Formal logic provides other examples; thus the existential quantifier, according to the present account, is a function for which the range of arguments consists of propositional functions, and the range of values consists of truth values. A function could be a \u201cscheme\u201d for a type of process, which would become definite when presented with an argument. The value would be extracted as an end result of the process. Two functions that are extensionally the same might compute, however, by quite different processes.", "authors": ["Dana S. Scott"], "id": "8567e9961dd73879d31c860ef18e6548862755f4", "title": "Models for Various Type-Free Calculi", "references": []}, {"date": "1961", "abstract": "Since the work of Henschen (1923), extended during the 1914 \u20141918 War by Holmes and Lister (1916) and by Pierre Marie and Chatelin (1914\u20131915), it has been generally accepted that localised wounds of the occipital cortex produce good evidence of point to point projection of the visual field on the occipital cortex. This has been strongly supported by the anatomical work of Le Gros Clark and Penman (1934) and of Polyak (1957), but there is not yet agreement even on the exact projection of the macular area on the cerebral cortex. The best physiological work which has been done is that by Talbot and Marshall (1941), who have mapped the exposed postero-lateral surface of the occipital lobe in the monkey. In the present work we have used an essentially similar technique, but by introducing long steel needles we have mapped most of the cortex of the calcarine fissures and can give a quantitative account of the nature of the calcarine visual projection.", "authors": ["David Whitteridge", "Peter M. Daniel"], "id": "6e56f97881931233c5cf5b83c293cfcf0581f4a9", "title": "The Representation of the Visual Field on the Calcarine Cortex", "references": []}, {"date": "1957", "abstract": "For the human retina, Hecht, Shlaer, and Pireune (1942) concluded that statistical fluctuations in response to brief flashes at the absolute threshold could be accounted for entirely by random fluctuations in the number of photons absorbed by the photoreceptors at each flash, and that \"biological noise\" was negligible. However, the presence of large fluctuations in the duration of intervals between successive impulses of the maintained discharge from single ganglion cells in the cat's retina (Granit, 1947; Kuffier, 1952, 1933), both in darkness and for a wide range of light intensities, suggests that biological noise does play a part in limiting the sensitivity of the eye. A similar situation in the infrared receptors of the rattlesnake has been studied by Bullock and Diecke (1956). This paper describes a statistical analysis of single ganglion cell discharges occurring in response to brief flashes of light of near-threshold intensity. The method of analysis used was suggested by a statistical theory of vision in man proposed by Tanner and Swets (1954). On the basis of this analysis the properties of the retina are described in terms of information theory. Finally, these deduced properties are shown to agree approximately with those observed in the experimental measurement of thresholds. * Present address: National Institutes of Health, Bethesda.", "authors": ["Richard FrrZHUGH"], "id": "fd8eb906dcc80191c1dc5d1d549acf1fc4f33744", "title": "THE STATISTICAL DETECTION OF THRESHOLD SIGNALS IN THE RETINA", "references": []}, {"date": "1965", "abstract": "An experiment was designed to determine whether or not the human organism possessed \"outflow\" information derived from monitoring nerve impulses in motor pathways. The experiment focused on the extraocular muscles since proprioceptive input to the central nervous system from these muscles is poor. The results show that in the absence of good proprioceptive information, the presence or absence of \"outflow\" information makes a difference in accuracy of localizing an object in space.", "authors": ["Leon Festinger", "L K Canon"], "id": "49e1ea6826464481959ca2cdcbd85a332b41cbdf", "title": "Information about spatial location based on knowledge about efference.", "references": ["1747d540d68c1342753a1f4f5db1a26a67d66c97", "8a4bd1fcfff8c32aeec3d88026d09c4628248532"]}, {"date": "1960", "abstract": "A cross-section of any vertebrate retina shows a complex pattern in which many types of cells are interconnected. The functions of very few of these cell types are known in any detail. The rods and cones almost certainly absorb the light quanta and are responsible for the initiation of the train of events which culminates in the nerve impulse patterns of the ganglion cells whose axons form the optic nerve. This pat tern of nerve impulses is modified by the intensity, shape, and duration of the illumination falling on the retina. In a few types of vertebrate eyes it is also certain that the color of the stimulating light must modify the response patterns of different ganglion cells in a differential fashion. This is a requirement in any system of color vision, and it is fairly certain from behavioral experiments that these animals are able to distinguish colors. Many species of fish apparently can be trained to recognize colors. For this reason the fish retina is an appropriate place to find out how color information is encoded for transmission along the optic nerve. The response patterns of single ganglion cells of the vertebrate retina have been the subject of extensive study since Hartline's successful isolation of the ganglion cells of the frog retina. These response patterns were classified by Hartl ine (1938) into \"on ,\" \"off,\" and \"on-off ' ' types according to the temporal relationship of the discharge to the illumination. These descriptive terms have been generally accepted by subsequent investigators and are in wide usage today. The response patterns of ganglion cells of a wide variety of other vertebrates have been examined by subsequent investigators. This work has indicated that the discharge patterns are basically similar. The \"on-off ' ' type is most frequently seen and is looked on as playing the dominant role in the functional organization. The analysis of the response relationship", "authors": ["Henry G. Wagner", "E. F. Macnichol", "Myron L. Wolbarsht"], "id": "3015ecca0e927f0e934b4f1ccce3edd832953ca6", "title": "The Response Properties of Single Ganglion Cells in the Goldfish Retina", "references": ["b446e3fb01026b4e5ad978812540f172405fff6f", "97d2b98d8c6c50eee3a67e4f8cf2f36f12f2db32", "6206c28676bc30b30f23c74343df7a06e937861a", "0c0fb0f273ad2c84e51ec132146fd2887e8c4bab", "54ad86d8c1df8a8c50aae24718855df25b1a0931", "9d55a05368809dbb8db60fcc801e2ba7f2f3b8e2", "8a3cca4ce26640825c1c7db3860b3df865288a47", "01a5623b40f63465624f3c9462fdc3a3d5df9059", "3ddd8cd9dbc5906f1d4fc574ea3c43d45720ba2d"]}, {"date": "1957", "abstract": "Nervous activity has been recorded from the unopened eye of decerebrate cats. Recordings were made with metal electrodes or with small micropipettes from ganglion cells or nerve fibers. Continuous maintained discharges were seen in all ganglion cells during steady illumination of their receptive fields, as well as in complete darkness. Possible artefacts, such as electrode pressure, abnormal circulation, anesthetic, and several other factors have been excluded as the source of the maintained discharge. Visual stimuli are therefore transmitted by modulating the ever present background activity. Discharge frequencies were measured following changes of retinal illumination. No consistent patterns of frequency change were found. The maintained discharge frequency may be permanently increased or decreased, or may remain practically unchanged by altering the steady level of illumination. In addition, there were often transient frequency changes during the first 5 to 10 minutes after changing illumination, before a final steady rate was established. A statistical analysis of the impulse intervals of the maintained discharge showed: (a) the intervals were distributed according to the gamma distribution (Pearson's type III), (b) the first serial correlation coefficient of the intervals was between \u20130.10 and \u20130.24, with a mean value of \u20130.17, which is significantly different from zero, (c) the higher order serial correlation coefficients were not significantly different from zero. Thus the firing probability at any time depends on the times of occurrence of the two preceding impulses only, and in such a way as to indicate that each impulse is followed by a transient depression of excitability that outlasts the following impulse. The possible sites at which spontaneous or maintained activity may originate in the retina are discussed.", "authors": ["Stephen W. Kuffler", "Richard Fitzhugh", "H. B. Barlow"], "id": "83cf69db1a2897e0bf0a250cfe64a0fcf1835eb7", "title": "MAINTAINED ACTIVITY IN THE CAT'S RETINA IN LIGHT AND DARKNESS", "references": ["cf44106a5a530f317c1643266687e3207d577a4c", "fe46043144b90dad9d09539f6c9904e578717405", "b446e3fb01026b4e5ad978812540f172405fff6f", "7c1d4560ed7afb5917048602885ff1b49e174d5d", "a4904e88d0ae48937db30e8ebd75abe176988161", "df7b6ce93159e91cd042e3f32daf3911cf1bd037", "c278bcf6c9e053e57670ce077faa240ac29dca4a", "1cafd6a835be70194ab6aac853888cdaca743fad", "bd54f87d5d4df56099e08cb236d4938ff0a2b7e9", "30c7e38b92d3abfc6f5686b16d41573a66b916fd"]}, {"date": "1958", "abstract": "Semantic Scholar extracted view of \"Electric responses from the isolated retinas of fishes.\" by E J Macnichol et al.", "authors": ["E J Macnichol", "Gunnar Svaetichin"], "id": "3088f7fe753b038269f1fd3ceeaf0f219468565b", "title": "Electric responses from the isolated retinas of fishes.", "references": []}, {"date": "1957", "abstract": "THE PRESENT PAPER describes some observations upon the modality and topographical attributes of single neurons of the first somatic sensory area of the cat\u2019s cerebral cortex, the analogue of the cortex of the postcentral gyrus in the primate brain. These data, together with others upon the response latencies of the cells of different layers of the cortex to peripheral stimuli, support an hypothesis of the functional organization of this cortical area. This is that the neurons which lie in narrow vertical columns, or cylinders, extending from layer II through layer VI make up an elementary unit of organization, for they are activated by stimulation of the same single class of peripheral receptors, from almost identical peripheral receptive fields, at latencies ers. It is early These which are not significantly different for the cells of the various layemphasized that this pattern of organization obtains only for the repetitiv neurons \u2018e responses may be rela of ted cortical in quite neurons different to brief peripheral stimuli. organization patterns when analyzed in terms of later discharges. A report of these experiments was made to the American Physiological Society in September, 1955 (10, 17).", "authors": ["Vernon B. Mountcastle"], "id": "6ded76b03bd0612974d86a826ce70d2a2b143fb2", "title": "Modality and topographic properties of single neurons of cat's somatic sensory cortex.", "references": ["5836a706ad0303b3885d90bf4bb70908fafb4073", "a8079740220b16d05664f2d00bd4c58e75776087", "77c6fade0adb43bb52880b257e5555e2889a5421", "6a4279658041e372ee63847376124f402fc90c0c", "ad222f56dfb2fea8054e8bc6b320c00102f0c0e9", "d83a87800ae6721c031398d67399c708843fee4d"]}, {"date": "1960", "abstract": "The monkey, Macaca mulatta, was chosen as the experimental animal for the investigation of the physiological mechanisms of color vision for two reasons: there is excellent behavioral evidence that the rhesus monkey has color vision (this is not true for some of the animals which have been used in similar investigations); furthermore, its color vision system is undoubtedly very similar to, if not identical with, that of man. There is good evidence (Walls, 1942) that color vision systems have evolved several separate times, once in the insects, again in fish, reptiles, and birds, and still again within the primate order. If this is correct, it would be little more than a coincidence ff the way in which information about the wavelength of light is encoded in the visual system of monkey and man were the same as the manner in which it is accomplished in fish or birds. In any case, if one is interested in relating the ways in which various neural elements respond to different wavelengths of light under various conditions to the manner in which these lights are perceived, one cannot go wrong in studying an animal as similar as possible to man, for virtually all of the behavioral results from color discrimination have been obtained on the human observer. The macaque lateral geniculate nucleus (LGN), from which most of the recordings have been obtained, consists of six layers of cells separated by fiber layers. These cells receive connections from the axons of the ganglion cells of the retina and are thus the fourth-order neurons in the visual pathway. Anatomical studies (Glees and Le Gros Clark, 1941) have given little evidence for interconnections at this level; the types of responses recorded here may well be essentially the same as one would obtain from ganglion cells in the retina. However, the highly laminated structure of the LGN, in which the optic projection from each eye splits three ways, presents the possibility of some clues being present as to the nature of the organization of the visual system in the differential responses of cells in the various laminae. For this, as well as for certain technical reasons, the L G N rather than the retina was chosen as the recording site.", "authors": ["Russell L. de Valois"], "id": "4e8ec8ecd1a0b15828d862d0ef00a834a1ab1e46", "title": "Color Vision Mechanisms in the Monkey", "references": []}, {"date": "1958", "abstract": "1. The fish EIRG (Svaetichin's cone action potential) was investigated on Cyprinus auratus with several types of illumination.2. The response amplitude depended strongly upon the area illuminated, becoming very small when the diameter of light spot was decreased to 0.2 mm.3. During passage of the border of light and dark across the site of recording, no abrupt change occurred in the response amplitude.4. Illumination by monochromatic light whose wave-length was changing continuously through the visible spectrum produced various response curves from different regions of the same retina. The result appeared to confirm Svaetichin and Motokawa et al., but details could not be compared due to the difference in experimental conditions.5. Based on the above results, the nature of the response was discussed.", "authors": ["Tsuneo Tomita", "Tsuneo Tosaka", "K. Watanabe", "Yoshihisa Sato"], "id": "c21024536f3148e65bbf786772a97c8763255411", "title": "The fish EIRG in response to different types of illumination.", "references": []}, {"date": "1957", "abstract": "A protective cover of a work vehicle has first and second frame members of a general U configuration connected to the vehicle at spaced apart locations with each frame member generally spanning the width of the vehicle with an operator's station position between said frame members. First and second supporting members of unique configuration are connected to the frame members and support an overlying roof member at preselected spaced apart locations.", "authors": ["David H. Hubel"], "id": "663eb5b9acf0e9cc404728d98669012fbfcaa9c2", "title": "Tungsten Microelectrode for Recording from Single Units.", "references": []}, {"date": "1944", "abstract": "Summary. \n \nDark-adaptation has been followed in cats and guinea pigs by measuring the absolute threshold of single or highly restricted discharges, isolated with micro-electrodes. \n \n \n \nThe curies obtained are of different types, as illustrated in fig. 1. \n \n \n \nThe different adaptation curves can be explained by the assumption that there are two kinds of rods converging in different proportion towards the same fibre in the optic nerve, (i) real rods and (ii) cone-like rods with adaptive properties similar to those of cones.", "authors": ["Ra\u01f5nar Granit"], "id": "de39901564a556c9664dd7b9ca2b38c0183665fd", "title": "The Dark-Adaptation of Mammalian Visual Receptors.", "references": []}, {"date": "1939", "abstract": "Semantic Scholar extracted view of \"Impulses in the pyramidal tract.\" by Edgar Douglas Adrian et al.", "authors": ["Edgar Douglas Adrian", "Giovanni Moruzzi"], "id": "f5588fad7a584c6e232f4aa0eb530cbd56f9fcc4", "title": "Impulses in the pyramidal tract.", "references": []}, {"date": "1956", "abstract": "Semantic Scholar extracted view of \"Chamber for microelectrode studies in the cerebral cortex.\" by Philip W. Davies", "authors": ["Philip W. Davies"], "id": "345e94cf3db0022721f649d9eb75a443a39c31ba", "title": "Chamber for microelectrode studies in the cerebral cortex.", "references": []}, {"date": "1954", "abstract": "Semantic Scholar extracted view of \"Activity of single neurons in the tactile thalamic region of the cat in response to a transient peripheral stimulus.\" by Rose Je et al.", "authors": ["Rose Je", "Mountcastle Vb"], "id": "5b503d0bcc84c0be5651296a93f72668474f9741", "title": "Activity of single neurons in the tactile thalamic region of the cat in response to a transient peripheral stimulus.", "references": []}, {"date": "2004", "abstract": "Ein nicht immer klar gesehenes Problem der allgemeinen Neurophysiologie ist die Erhaltung des Erregungsgleichgewichts im Gehirn durch ttemmungsund Bremsungsvorg\u00e4nge. Ohne solche I-Iemmungsmechanismen w\u00e4re eine geordnete Hirnt~tigkeit unm\u00f6glich und das synaptische Pulverfa\u00df des Gehirns w\u00fcrde mit Millionen aktiver Neurone in Massenentladungen explodieren ~~. Selbst nach starken Sinnesreizen oder elektrischen Einzelreizen gibt das gesunde Gehirn aber eine geordnete Antwort mit spezifischen lokalisierten Impulsen. Trotz der zahllosen Verbindungen der cerebralen Neurone entsteht keine lawinenartige Ausbreitung der Erregung. Deshalb m\u00fcssen hier Regelvorg\u00e4nge am Werke sein, die eine Erregungsbegrenzung mit Stabilisierung, Bremsung und Steuerung der corticalen Entladnngen garantieren. X~r\u00fchere Untersuchungen mit T\u00f6N~I~S ~s 1950 \u00fcber lokale elektrische Cortexreize hat ten Hinweise daf\u00fcr ergeben, da\u00df die normalen I t i rnrhythmen eine solche physiologische Bremsung mit Erhaltung eines mittleren Erregungsniveaus herstellen. \u00dcber die Einzelmeehanismen dieser Vorg~nge konnten wir damals noch nichts aussagen. Es fehlte noch eine Mikrophysiologie des Cortex mit Ableitungen von einzelnen Neuronen. AD~IA~ u. 1V[o~uzzI ~ hat ten zwar schon 1939 einzelne Impulse von Pyramidenneuronen an ihren Axonen in der Oblongata registriert, doch ist es erst in den letzten 3 Jahren gelungen, gute Mikroableitungen aus dem Cortex selbst zu erhalten 2, 3, 4, ~7, 81, a\u00f5 und die ttemmungsph~nomene an einzelnen Neuronen darzustellen 2, 8, 9, 25, 37. Die am optischen Cortex bei ad\u00e4quater Lichtreizung gefundenen Hemmungsund Bremsungsph\u00e4nomene verschiedener Reaktionstypen der Neurone ergaben einige Befunde, deren Darstellung von allgemeinem neurophysiologischem Interesse ist. Die Auswertung dieser Mikroableitungen zeigte zwei grunds\u00e4tzlich verschiedene Arten von Hemmung und Bremsung der corticalen Neuronen.", "authors": ["Richard Jung", "Guenter Baumgartner"], "id": "ddd51dd628e52cd7171db36637d250b5f3d6febe", "title": "Hemmungsmechanismen und bremsende Stabilisierung an einzelnen Neuronen des optischen Cortex", "references": []}, {"date": "1940", "abstract": "Semantic Scholar extracted view of \"A structural analysis of the lateral geniculate nucleus of the cat\" by James L. O'leary", "authors": ["James L. O'leary"], "id": "a09fad90b32296e158ed0e15d06996b89696116a", "title": "A structural analysis of the lateral geniculate nucleus of the cat", "references": []}, {"date": "1937", "abstract": "* Accepted for publication by Leonard Carmichael of the Editorial Board, and received in the Editorial Office on March 23, 1937. 1 Preliminary reports of certain aspects of the present study have been given previously by the writer (70, 74, 75).", "authors": ["K. U. Smith"], "id": "0e5187790e9de2fdeb10df35830e8420236fa2d5", "title": "Visual Discrimination in the Cat: V. The Postoperative Effects of Removal of the Striate Cortex Upon Intensity Discrimination", "references": []}, {"date": "1954", "abstract": "Miscellaneous Publications .903..... ................... 903 Technical Papers The Primary Radical Yield in Irradiated Water: J. Rowbottom. 904 Three-Dimensional Movies without Special Equipment: R. Stuart Mackay. . 905 Incorporation of C14 into Various Carbohydrates of Tobacco Leaves after Different Periods of Photosynthesis in C1402: P _V. Vittorio, G. Krotkov, and G. B. Reed ..............................; 906 Communications A Radiocarbon Date of Peat from James Bay in Quebec: J. E. Potzger and Albert Courtemanche ................................. 908 An Ethical Problem for Scientists in a Divided World: Theodsus Dobzhakky......... 98...........; 908 A Chalenge to Law: Discussions by D. B. Rexworthy and LawsonM. McKenzie ....................... Need for a Standardized International Glossary of Terms in Botany: Divya Darshan Patt .910 Stable Colloidal Sulfur Solutions: M. W. Brenner-and Joseph L. Owades .911 Transplanta,bility of Tumors:, Discussions by Jacob Heiman and Robert H. Foulkkes.911 AlasforHuman Frailties! Joseph H. Friend ................................................. 912 Effects of Itrumil (New Antithyroid Agent) on the Histological Structure of the Rat Thyroid: Walter 0. Bradley .......................................... 912 A Simple Method for the Photographic Reproduction of Pencil' Drawings: E.J. Britten ..................................................... 913", "authors": ["George Wald"], "id": "878a934bca7c98fdc5ab2079ac88ef784909f6b0", "title": "On the mechanism of the visual threshold and visual adaptation.", "references": []}, {"date": "1947", "abstract": "As shown by van der Velden, for the observation of a short and small light flash it is necessary that at least two quanta of light be effectively absorbed by the visual purple within a time \u03c4 (about 0.02 sec.) and within an area corresponding to a visual angle D (about 10\u2032): the two-quanta hypothesis, by which the laws of Ricco, Piper, and Talbot were explained. In the present paper the theoretical foundation is recapitulated and further experiments are described.The extensive experimental results in the present paper are completely covered by the two-quanta hypothesis as far as regards the flash time t<3\u03c4 or the visual angle of the light spot d<2D.It is shown, that for the simultaneous occurrence of long flashes and large visual angles, considerable deviations from the theoretical curves derived from the two-quanta hypothesis occur.From these deviations we conclude that T seconds (about 3\u03c4) after the absorption of light quanta the condition of the retina in the neighborhood of these absorptions (within about 3D) is such as to decrease the chance of observation of a subsequent pair of absorbed quanta.Measurements of the visual acuity are found to be in agreement with the two-quanta case.The experimental results of Hecht and Pirenne and the paper by De Vries are discussed, as is also the note of C. Peyrou and H. Piatier about experiments similar to those of van der Velden.", "authors": ["Maarten A. Bouman", "H A VAN DER VELDEN"], "id": "020e6e056346c97582515268d6e7639574d73c5e", "title": "The two-quanta explanation of the dependence of the threshold values and visual acuity on the visual angle and the time of observation.", "references": []}, {"date": "1960", "abstract": "Semantic Scholar extracted view of \"The absence of position sense in the human eye.\" by Giles Skey Brindley et al.", "authors": ["Giles Skey Brindley", "P. A. Merton"], "id": "1747d540d68c1342753a1f4f5db1a26a67d66c97", "title": "The absence of position sense in the human eye.", "references": []}, {"date": "1954", "abstract": "(1) The interval fluctuation of impulses of the tonic nerve fiber from the muscle spindle of a Japanese toad is investigated by method of the time series analysis.(2) Dividing the interval series into several stationary subseries and computing the serial correlation coefficients, we find that these subseries can be considered as a random process.(3) In these stationary subseries, not only the absolute value of the standard deviation but also its relative value to the mean interval becomes larger with the increase of the mean interval.(4) High correlation among the moments of the interval distribution makes us conclude that each of the distributions obtained from the same preparation can be characterized by a single parameter.(5) Assuming a random fluctuation such as that due to the thermal agitation the equation of distribution is calculated. And comparison is made between the observed and the calculated histograms.", "authors": ["Susumu Hagiwara"], "id": "30c7e38b92d3abfc6f5686b16d41573a66b916fd", "title": "Analysis of interval fluctuation of the sensory nerve impulse.", "references": []}, {"date": "1961", "abstract": "Semantic Scholar extracted view of \"The relationship between saccadic and smooth tracking eye movements.\" by Cyril Rashbass", "authors": ["Cyril Rashbass"], "id": "8a4bd1fcfff8c32aeec3d88026d09c4628248532", "title": "The relationship between saccadic and smooth tracking eye movements.", "references": []}, {"date": "1949", "abstract": "THIS book is not a general treatise on vision, but in deals with some additions to visual physiology in the last fifteen years that have been derived from the study of action potentials of the retina and the optic nerve, and particularly with work in this field with which the author has been closely associated and in which his laboratory has been notably active. It has been possible in many types of visual sense organ to record nerve impulse discharges set up by the stimulus of light ; in work on a simple eye (Limulus), Hartline showed that visual sense cells, like other sensory nerve endings that have been investigated, set up impulses of frequency related to the strength of the stimulus, and Hartline later confirmed this for single opticnerve fibres from the frog's retina. But here the response was complicated by fibres responding when the light was extinguished\u2014the well-known off-discharge found by Adrian and R. Matthews in the eel's optic nerve. Granit's work has extended this investigation to mammalian eyes ; and the response in single optic-nerve fibres in many eyes has been studied and related to the wave-length of the light evoking it.Sensory Mechanisms of the RetinaWith an Appendix or Electroretinography. By Dr. Ragnar Granit. Pp. xxiii+412. (London, New York and Toronto : Oxford University Press, 1947.) 35s. net.", "authors": ["B. H. C. Matthews"], "id": "bd54f87d5d4df56099e08cb236d4938ff0a2b7e9", "title": "Sensory Mechanisms of the Retina", "references": []}, {"date": "1956", "abstract": "Utilizing a silver microelectrode of5-10\u03bcE in diameter insulated by a glass coating, impulse discharges subsequent to adequate stimuli on various parts of the body were recorded from the cervical posterior funiculus of cat and their arrangement was studied.Conclusions are as follows:1) Responses were obtained ipsilaterally and the forms of spikes could be classified into two types. One is thought to be related to nerve fibre and the other to nerve cell.2) Responses to touch on the skin or hairs were most frequently obtained. In them, the amplitude of spikes was generally large and adaptation was very quick. Receptive areas for them were fairly localized, particularly restricted on the distal ends of limbs.3) Impulse discharges responding-to push or pull of the skin were tonic, adapting slowly. The physiological function of these afferents may be partly proprioceptive.4) Among proprioceptive afferents, which generally adapted slowly, those for muscles were far less frequently encountered than those for articular capsules or tendons.5) In Goll's funiculus, responses from tactile afferents were far more frequently obtained than those from proprioceptive afferents. The circumstances were quite opposite in regard to Burdach's funiculus. The majority of responses from the thoracic level seems to be concerned with respiratory movements of the chest.6) From a localized superficial portion of Goll's funiculus closely adjacent to the septum medianum, slowly adapting impulse discharges were obtained subsequent to filling the urinary bladder. Their spike apperances were far smaller than those for tactile afferents.7) The medio-lateral arrangement, upon the transverse section of the second cervical cord, of those parts of the body from which the responses were obtained was as follows:(a) urinary bladder, (b) tail or perineum, (c) toes of hind limbs, (d) distal half of hind limbs except toes, (e) proximal half of hind limbs, (f) loins or abdomen, (g) chest, (h) toes of fore limbs, (i) distal half of fore limbs except toes, (j) proximal half of fore limbs.8) Spontaneous impulse discharges, which were obscure in their receptive fields, were frequently recorded in the lateral half of Burdach's funiculus and in an area between Goll's and Burdach's funiculus. They are assumed to be related to slowly adapting afferents and to be proprioceptive in nature.", "authors": ["Shigeaki Yamamoto", "Sotoo Sugihara", "Masaru Kuru"], "id": "ad222f56dfb2fea8054e8bc6b320c00102f0c0e9", "title": "Microelectrode studies on sensory afferents in the posterior funiculus of cat.", "references": []}, {"date": "1957", "abstract": "1. Photographic records of impulses from single ganglion cells in the cat's retina were made while the retina was stimulated by flashes occurring once a second. Ten flashes at each of several intensities near threshold were used. 2. For the purpose of statistical analysis, the number of impulses (x) falling within a critical period following each flash was used as an index of the response. Histograms of x were plotted and used to calculate rates of transfer of information by the ganglion cell for the case of an ideal experiment, the yes-no choice, in which flashes of intensity I and blanks are to be distinguished. 3. The information rate increased (a) with increasing stimulus intensity and (b) with the number of identical flashes or blanks presented successively in a block. The intensity chosen as threshold by the experimenter, who observed the impulses visually and aurally, corresponded to an average information rate for single flashes of 0.7 bit/flash, compared to the maximum possible rate of 1 bit/flash. A threshold intensity giving 0.4 or more bit/flash, if presented in blocks of six identical flashes, corresponded to 0.95 or more bit/block, or near certainty. Thus the calculation of information rates using the index x provides an estimate of threshold at least as sensitive as those obtained during an experiment, which were made only after observing the responses to five to ten flashes of the same intensity. 4. The index x has statistical properties similar to those of the \"index of neural activity\" used by Tanner and Swets (1954) in their statistical model of human vision, and represents a possible physical interpretation of their index. However, x gave values (0.5 to 1.5) of the parameter called the slope which were consistently smaller than their values (2.1 to 3.1).", "authors": ["Richard Fitzhugh"], "id": "1cafd6a835be70194ab6aac853888cdaca743fad", "title": "THE STATISTICAL DETECTION OF THRESHOLD SIGNALS IN THE RETINA", "references": ["b446e3fb01026b4e5ad978812540f172405fff6f", "9e5e60ac75ea6469d612f9ea293f2e727e3caf3d", "1c253ba1d9619ead06c2cab151aeef6aabbe0be6", "42bab83cc1aa10e18a31c1a8fb04e165ca8180d1", "abd419845a23bce62484bda6f671e77e2d46a383", "df7b6ce93159e91cd042e3f32daf3911cf1bd037", "09dc4bc616b83dcdea522ca261bb8b360571e056", "c34da3fd5d71b3a00fcc68801382aa9d0316417e", "84abdd5286dfb94fe8799413d56d477fe53b04aa", "bd54f87d5d4df56099e08cb236d4938ff0a2b7e9"]}, {"date": "1953", "abstract": "Correlation of sensory testing and histological examination does not support the belief that the morphology of the nerve terminals in the human lip and finger pad determines a sensory I specificity. No \u2018organized endings\u2019 have been found in hairy skin from the abdomen, the dorsum of the finger or the lip, in spite of the use of techniques capable of satisfactorily demonstrating them elsewhere in the same histological section. It is suggested that there may be a simple, consistent and non-specific plan of construction underlying the apparently diverse morphology of both the \u2018organized endings' and the terminals around the hairs.", "authors": ["E. W. Hagen", "H. Knoche", "D. C. Sinclair", "Graham Weddell"], "id": "d83a87800ae6721c031398d67399c708843fee4d", "title": "The role of specialized nerve terminals in cutaneous sensibility", "references": []}, {"date": "1953", "abstract": "Abstract 1. 1. Evoked activity in single cortical units was isolated in the somatosensory areas of cat and monkey under barbiturate anesthesia or chloralose anesthesia combined with d-tubocurarine or B-erythroidin. Peripheral mechanical or nerve stimulation was used. 2. 2. The spikes were predominantly initially negative when recorded with 4\u201312 \u03bc electrodes. Larger positive-negative and positive spikes were recorded with 0.5\u20133 \u03bc electrodes together with smaller initially negative spikes. 3. 3. The latency for the first spike of some units was variable in successive records and varied greatly between units, especially under light anesthesia. Other units showed extraordinary consistency of firing. 4. 4. Some units fired on the commencing limb of the positive primary response which was usually reversed in sign when the electrode tip was 0.5\u20131.0 mm. deep. The earliest evoked activity at depths of 220\u2013350 \u03bc occurred several milliseconds later than the earliest activity at greater depths in the cortex. 5. 5. High frequency repetitive discharges of the same cortical unit were observed following a single peripheral stimulus. However, this can occur as early in the sensory system as the cuneate nuclear relays. Because injury by the electrode can cause an increased number of spikes per discharge, multiple discharges require a careful evaluation. 6. 6. In some units, increases in strength of nerve stimulation or alterations in site of peripheral stimulation, which reduced the latency for the first spike, lead to an increase in rate of firing and number of spikes per discharge. 7. 7. Cells lying within a few micra of one another often fire with different latencies, number of spikes per high frequency discharge, interspike intervals and may be preferentially activated from different afferent sources. 8. 8. The receptive field of some somesthetic association area units included all four limbs, but the timing of unit firing may significantly differ with the site of stimulation. 9. 9. Absolute and relative unresponsive phases are distinguished in single cortical unit responses to paired shocks to the same nerve. Alteration in strength of the test shock may alter duration and degree of these phases. A later supernormal phase is also distinguished. The response to repetitive peripheral stimulation was studied. 10. 10. Forms of inhibitory and occlusive interaction are distinguished following stimuli to two nerves. Interaction may occur with nearly synchronous volleys in peripheral nerves. 11. 11. The significance of single cortical unit recording is discussed as related to spread of activity in the cortex, the mechanism of the primary response, population sampling and transmission of information in the sensory system.", "authors": ["Vah\u00e9E. Amassian"], "id": "77c6fade0adb43bb52880b257e5555e2889a5421", "title": "Evoked single cortical unit activity in the somatic sensory areas.", "references": []}, {"date": "1953", "abstract": "Summary. \nNerve impulses in the medial articular nerve to the knee joint of the cat have been studied. The responses of sensory nerve endings located in the knee to position and movement of the tibia are described. The endings are slowly adapting and seem to be arranged sothat each has an arc of maximum sensitivity covering a few degrees of angular movement but these ranges are different for individual endings. The significance of this is discussed. \n \nThe medial femoro-tibial and the patellar ligaments were found to be equipped with endings which signalled tension. The responses of these endings to steady tensions were measured. \n \nAn increased intracapsular pressure was produced by injecting fluid into the synovial cavity. Simultaneous recordings of pressure and impulse traffic show that a raised pressure was a potent stimulus to slowly adapting endings in the capsule. \n \nThe articular surfaces and the synovial membrane were found insensitive to moderate mechanical and thermal stimulation.", "authors": ["B. L. Andrew", "Eberhard Dodt"], "id": "a8079740220b16d05664f2d00bd4c58e75776087", "title": "The deployment of sensory nerve endings at the knee joint of the cat.", "references": []}, {"date": "1941", "abstract": "Semantic Scholar extracted view of \"Afferent discharges to the cerebral cortex from peripheral sense organs.\" by Edgar Douglas Adrian", "authors": ["Edgar Douglas Adrian"], "id": "6a4279658041e372ee63847376124f402fc90c0c", "title": "Afferent discharges to the cerebral cortex from peripheral sense organs.", "references": []}, {"date": "1953", "abstract": "an en\u00ad entirely new view of the peripheral afferent part of the thermoregulatory system, as well as of many sensory phenomena hitherto the subject of much debate. For that reason it was considered justified to limit this review en\u00ad tirely to thermal receptors. The temperature movements in the skin have been subject to theoretical as well as experimental investigations by Hensel (2, 3), who has developed effective methods for recording the intracutaneous temperature gradient in stationary conditions as well as a precision-flow-ca lorimeter for measuring the steady heat flow given off from small skin areas, the mean error of the method being only \u00b1 0.0001 cal. cm.2 sec.-1 The new \"Stromungskalorime ter\" constructed by Hensel (4) can be used clinically also for the determination of the blood flow through the skin. For details, see Hensel's monograph \"Physiologie der Thermoreception\" in Ergebnisse der Physiologie (5). The depths of cold receptors in the tongue of the cat have been recorded in aIi objective way by Hensel, Strom, and Zotterman (6). This method is based on the recording of the spike potential of specific cold fibres in the lingual nerve set up by applying rapid cold jumps to the tongue and by simul\u00ad taneous recording of the temperature changes. The depth of the cold recep\u00ad tors was calculated from determinations of (a) the threshold, (b) the tempera\u00ad ture change of the thermode, (c) the latency of the specific cold spikes, (d) the thermal diffusion coefficient of the tongue. From a great number of meas\u00ad urements on the tongue of the cat, a mean value of 0.18 mm. \u00b10.04 was ob\u00ad tained. The cold receptors are thus situated closely beneath the epithelium. At this depth an abundance of afferent nerve endings of various configura\u00ad tions have been described by histologists (ef. Sanders (7)). The investigations of the receptive field for thermoceptors has often led to much debate. The question is, as Jenkins (8) formulates it, whether it should be compared with an unsensitive surface with spread circumscript sensitive spots or rather as a continuum with peaks, valleys, and plateaus. The peaks should then cor\u00ad respond to the spots according to the classical view. Hensel (5) maintains 1 The survey of the literature pertaining to this review was concluded in June,", "authors": ["Yngve Zotterman"], "id": "c278bcf6c9e053e57670ce077faa240ac29dca4a", "title": "Special senses: thermal receptors.", "references": []}, {"date": "1942", "abstract": "1. Direct measurements of the minimum energy required for threshold vision under optimal physiological conditions yield values between 2.1 and 5.7 x 10\u201310 ergs at the cornea, which correspond to between 54 and 148 quanta of blue-green light. 2. These values are at the cornea. To yield physiologically significant data they must be corrected for corneal reflection, which is 4 per cent; for ocular media absorption, which is almost precisely 50 per cent; and for retinal transmission, which is at least 80 per cent. Retinal transmission is derived from previous direct measurements and from new comparisons between the percentage absorption spectrum of visual purple with the dim-vision luminosity function. With these three corrections, the range of 54 to 148 quanta at the cornea becomes as an upper limit 5 to 14 quanta actually absorbed by the retinal rods. 3. This small number of quanta, in comparison with the large number of rods (500) involved, precludes any significant two quantum absorptions per rod, and means that in order to produce a visual effect, one quantum must be absorbed by each of 5 to 14 rods in the retina. 4. Because this number of individual events is so small, it may be derived from an independent statistical study of the relation between the intensity of a light flash and the frequency with which it is seen. Such experiments give values of 5 to 8 for the number of critical events involved at the threshold of vision. Biological variation does not alter these numbers essentially, and the agreement between the values measured directly and those derived from statistical considerations is therefore significant. 5. The results clarify the nature of the fluctuations shown by an organism in response to a stimulus. The general assumption has been that the stimulus is constant and the organism variable. The present considerations show, however, that at the threshold it is the stimulus which is variable, and that the properties of its variation determine the fluctuations found between response and stimulus.", "authors": ["Selig Hecht", "Simon Shlaer", "Maurice Henri Pirenne"], "id": "c34da3fd5d71b3a00fcc68801382aa9d0316417e", "title": "ENERGY, QUANTA, AND VISION", "references": ["58c39dd255a8ae76dd3a1d20cf1423d5bbccb210", "a07a8ee6cb1ac36ed63bebf24562a1b97c105fde", "7d834dac179bba2ea12ef96c2cdbc566353f3682", "7a072146327f54c4fba45b6a459043907f3d8621", "d655b2125b7dcaec8e2e6421643df35cc9f83b25", "f4e658705c195e956ccf6ea884e4d262d758fa82", "c62be55de0a0264b3b5882f15ff11569633dae29", "db9317bc2cd3ed86b706801e968603a911c8a089", "48a6a3c61d6df05f2d96e867c25c6bb4dfc505ad", "199dc9e369569dc8fdb2c782c763da82785e9ca9"]}, {"date": "1955", "abstract": "IN A RECENT PAPER one of us (7) described the technique and the results of recording impulses from individual auditory nerve fibers in the modiolus of the guinea pig. The present investigation is an extension to the cochlear nucleus in the medulla oblongata of the same technique of recording action potentials with submicroscopic microelectrodes (6). Action potentials from single elements in the cochlear nucleus have already been successfully recorded by Galambos and Davis (2, 3, 4) with an earlier type of microelectrode. The original purpose of the present investigation was to repeat their previous observations with a finer technique. There is a certain difference in behavior between the elements examined in the present investigation and those studied previously. The \u201cresponse area\u201d of an element is the area on an intensity-frequency plot that includes all tones that evoke a positive response. The response areas of the new elements show a sharp limit on the high-frequency side and very gradual elevation of threshold on the low-frequency side. The elements studied previously had, on the contrary, fairly narrow response areas with a clear maximum of sensitivity for a certain frequency that was characteristic of the element under observation. The new and the previous elements behaved differently in one other aspect: spontaneous discharges of impulses in the new elements were not inhibited by the application of any pure tone, while in the previous elements some tones inhibited and other tones enhanced the spontaneous repetitive activity.", "authors": ["Ichiji Tasaki", "H. Davis"], "id": "cf44106a5a530f317c1643266687e3207d577a4c", "title": "Electric responses of individual nerve elements in cochlear nucleus to sound stimulation (guinea pig).", "references": ["7f50cdaa527fb63fee489901eae5cd355c38a2fc", "4ce1a21503ab4046d02b4dc540a37799c526efed", "3a2424648f4fc52ef937cd67c28a08b68fae71a2", "ea1dd97ddf91a02bf993cba47ae3fe46df05064d", "e0188fabca0d0f3c4b1f580957dc46bbd849f071", "710fa0ffc40fd03f4690ed8575c9e8a08e12ef79", "d13b5fe17d110d783f44963c8088368e2e99cc40", "ecb3afe96d236e5a8c138b62ed654cc54d82f9e4"]}, {"date": "1958", "abstract": "Preliminary experiments have indicated that it is feasible to construct low-level dc differential amplifiers using inexpensive hearing aid transistors. The noise level of such amplifiers was found to be roughly proportional to source impedance; it is less than that of a vacuum tube circuit for sources having an impedance of less than 10,000 ohms. A physiological amplifier having a voltage gain of 10,000 was designed on the basis of these experiments. Several of these units have been constructed which show a bandwidth of about 60 kc and a noise level of about 2 ?v rms with a 50-ohm input. For measuring nerve potentials a pair of electrometer tubes connected as cathode followers are used as an input probe. With this circuit arrangement the noise level is the order of 15 ?v.", "authors": ["E. F. Macnichol", "Theodore A. Bickart"], "id": "3ddd8cd9dbc5906f1d4fc574ea3c43d45720ba2d", "title": "The Use of Transistors in Physiological Amplifiers", "references": []}, {"date": "1941", "abstract": "Semantic Scholar extracted view of \"A Relation between Rod and Cone Substances, Based on Scotopic and Photopic Spectra of Cyprinus, Tinca, Anguilla and Testudo\" by Ra\u01f5nar Granit", "authors": ["Ra\u01f5nar Granit"], "id": "01a5623b40f63465624f3c9462fdc3a3d5df9059", "title": "A Relation between Rod and Cone Substances, Based on Scotopic and Photopic Spectra of Cyprinus, Tinca, Anguilla and Testudo", "references": []}, {"date": "1950", "abstract": "The function of the lateral-line organ of Japanese eel has been examined precisely by means of recording the impulse discharges from a single nerve fiber supplying the organ, which was activated by various kinds of natural and artificial stimulations. The results obtained are summarised, as follows.1) Examinations by water flow.With a calm water flow only thin nerve fibers could be excited. Somewhat higher rates of flow first excite the thicker fibers. The aspect of discharges was not changed by the direction of the water flow. The number of discharges from a single nerve fiber increased with increase of the flow rate. The maximum discharge frequency attained was different in fibers with different diameters. The relation between the average number of spikes and the logarithm of the flow rate was nearly linear in every single fiber examined.2) Examinations by pressure and touch.Applying pressures of various grades on the skin surface along the lateralline, results were obtained, which were similar to those above mentioned qualitatively and quantitatively. However, some very thick fibers of phasic nature were' very sensitive to the impulse shock and had a relatively low thresholds. They showed sometimes inhibitory phenomena during stimulation, and facilitatoryeffect after the cessation of the stimulation. The lateral-line organ of fish has, too, a dual innervation of slow and phasic fibers, as many other sensory organs have.3) Examination by mechanical vibrations.In all fibers examined, the discharges were seen to appear in every cycle of mechanical vibrations, provided that the vibration frequency was relatively low. With very low frequencies we had generally a few spikes in each cycle, which changed into one-for-one relation in the middle range of frequency (20-50 cps.). When the stimulus frequency was further increased, there appeared the discharge deficit oftener, the more frequent the stimulus. The largest frequency producing one-for-one appearance of the spikes was different in different fibers, owing to the different diameters. The thicker the fiber the higher it was. The thinnest fiber could follow the vibrations of under 10 cps. only. On the contrary most of the thicker fibers could follow those of 50 cps., and the thickest one even to those near 100 cps.4) The D. C. stimulation of the receptor easily evokes the iterative firing of the nerve fiber. The average number of spikes had a linear relationship with the logarithm of the current strength. The threshold for repetitive discharges could not be determined precisely, but it was certain that weak currents made the thinner fibers discharge more easily than the thicker fibers. Many fibers were excited by the ingoing current and a few by the outgoing one.5) The A. C. stimulation of the receptor evoked, also, discharges in every stimulus cycle. For the low stimulus frequency there were a few spikes in the negative phase only. With the increase of the stimulus frequency the number of spikes in one stimulus cycle decreased gradually, to become the one-for-one appearance, to random falling out of the spikes, and finally to a stage where there resulted only one impulse in many stimulus cycles. The maximum frequency for one-for-one appearance of the spikes was related to the fiber diameter, the thicker fiber being able to follow a higher frequency. The upper limit could not be determined owing to the disturbances brought by the stimulus current itself.The strength of the stimulating current caused changes in the aspect of the discharges. Namely, an increase in strength brought more spikes to appear in the negative phase. With very low frequency it resulted in some spikes appearing in the positive phase, too. If the stimulus frequency is high, and if the current is sufficiently strong, then the one-for-one appearance of the spikes continued to follow much higher frequency, than when it is weak.", "authors": ["Yasuji Katsuki", "Shizuo Yoshino", "Jung Po Chen"], "id": "7c1d4560ed7afb5917048602885ff1b49e174d5d", "title": "Action currents of the single lateral-line nerve fiber of fish\u30141\u3015on the spontaneous discharge-", "references": ["ca05cbb2199d157a8c506dca01a6742a320e71e7", "8261549162b3d5f85d967a46d679c931a4800850", "995ed6ebef8ebdbd7d4d51bc50dd7ac2bca59543", "f50ec6c608c3a8863105a8fdb286f5e03d86cfc8", "1962f9c6d58b5971c285ff6c30cd76ad12a6fc5c"]}, {"date": "1956", "abstract": "It is shown that the absorption of one quantum can excite a rod in the human retina, but that at least two, and probably many more, excited rods are needed to give a sensation of light. It is suggested that noise in the optic pathway limits its sensitivity, and this idea is subjected to an experimental test. The hypothesis is then formulated quantitatively, and shown to be able to account for the above experiment, and also the disagreement in the literature between those who believe that the absorption of two quanta can cause a sensation, and those who believe that 5 or more are required. The formulation of the hypothesis is used to calculate the maximum allowable noise (expressed as a number x of random, independent events confusable with the absorption of a quantum of light) in the optic pathway for the absorption of various fractions of the total number of quanta incident at the cornea.", "authors": ["H. B. Barlow"], "id": "a4904e88d0ae48937db30e8ebd75abe176988161", "title": "Retinal noise and absolute threshold.", "references": []}, {"date": "1957", "abstract": "Semantic Scholar extracted view of \"Response properties of neurons of cat's somatic sensory cortex to peripheral stimuli.\" by Vernon B. Mountcastle et al.", "authors": ["Vernon B. Mountcastle", "Philip W. Davies", "A. L. Berman"], "id": "5836a706ad0303b3885d90bf4bb70908fafb4073", "title": "Response properties of neurons of cat's somatic sensory cortex to peripheral stimuli.", "references": []}, {"date": "1943", "abstract": "Summary. \n \nThe micro-electrode technique has been used for the recording of spikes of activity from the retina of the cat in order to study the distribution of sensitivity to spectral light of the retinal elements. \n \n \n \nIn dark-adapted cats the distribution of sensitivity corresponds to the absorption curve for visual purple. \n \n \n \nIn light-adapted cats some elements give the broad so-called photopic dominator band with maximum in 0.560 \u03bc, others a narrow curve with maximum in 0.520 \u03bc combined with the dominator in different ratios. \n \n \n \nThe narrow curve is probably identical with the band seen in the eyes of rats and guinea pigs corresponding to an abnormally narrow absorption curve for visual purple, in the cat shifted somewhat to the right because of the added effect of the dominator. \n \n \n \nSome results indicate that back reflexion from the tapetum lucidurn plays a ro1e in determining the shape of the curves in the yellow-green region of the spectrum. \n \n \n \nThe colour vision of the cat has been, discussed in the light of the results arrived at.", "authors": ["Ra\u01f5nar Granit"], "id": "ebb30df0a932595caf1efc2418f9f30033aab563", "title": "The Spectral Properties of the Visual Receptors of the Cat", "references": []}, {"date": "1949", "abstract": "Single spikes, isolated from the retina of decerebrate cats by the micro-electrode technique, have been studied at variable short exposures of illumination in order to find out whether on-and off-discharges overlap and mutually reinforce each other, or whether they are mutually exclusive in the sense that the final common path only can deliver one at a time. The latter was found to be the case.", "authors": ["Ra\u01f5nar Granit"], "id": "97d2b98d8c6c50eee3a67e4f8cf2f36f12f2db32", "title": "III. - The antagonism between the on and off systems in the cat's retina", "references": []}, {"date": "1957", "abstract": "1. Spectral absorption is measured in a single cone from the carp retina, by a specially designed microspectrophotometer.2. Five photopigments having \u03bbmax 490-500m\u03bc, 520-540m\u03bc, 560-580m\u03bc, 620-640m\u03bc and670-680m\u03bc are detected in various cone outer segments. The occurrence frequency is high for the first three.3. As to the rod pigment, the \u03bbmax, is between520-530m\u03bc in the carp and between500-505m\u03bc in the frog.4. A few cones which seem to possess two photopigments are observed in the carp.", "authors": ["Takao Hanaoka", "Kenshiro Fujimoto"], "id": "8a3cca4ce26640825c1c7db3860b3df865288a47", "title": "Absorption spectrum of a single cone in carp retina.", "references": []}, {"date": "1954", "abstract": "Semantic Scholar extracted view of \"A HIGH IMPEDANCE INPUT CIRCUIT SUITABLE FOR ELECTROPHYSIOLOGICAL RECORDING FROM MICRO-PIPETTE ELECTRODES\" by E. F. Macnichol et al.", "authors": ["E. F. Macnichol", "Henry G. Wagner"], "id": "54ad86d8c1df8a8c50aae24718855df25b1a0931", "title": "A HIGH IMPEDANCE INPUT CIRCUIT SUITABLE FOR ELECTROPHYSIOLOGICAL RECORDING FROM MICRO-PIPETTE ELECTRODES", "references": []}, {"date": "1957", "abstract": "h ins ich t l ieh der Ausb i l dung yon vege t a t i ven (Conidientr~tgern) u n d genera t iven F ruk t i f i ka t i onso rganen (Ascogonen). In d iesem labilen Z u s t a n d kOnnen a m gleichen Conidient rgger einige S te r igmen in no rma le r F u n k t i o n Conidien he rvorbr in gen, andere zn s e k u n d g r e n Conid ient rggern auswachsen , u n d wieder andere k6nnen sich di rekt zu Ascogonsch rauben l lmbilden. H y p h e n aus der v e g e t a t i v e n P h a s e gehen also m a n c h m a l in den gene ra t i ven Z u s t a n d fiber. Der um~ gekehr te Verlallf is t b i sher n u r e inmal beobach te t worden : aus e inem morphologisch markier te l l Ascogon s ind s t e r igmenar t ige H y p h e n hervorgewachsen, die Conidien abgl ieder ten. N a c h diesen B e o b a c h t u n g e n sche in t die Zel lwand a m Orte der Sterigmlenu n d Conid ienbi ldung besonders leicht deformierbar zu sein. Das is t gu t zu vers tehen , da an diesen P u n k t e n auch bei no rma le r E n t w i c k l u n g eine s ekundgre V e r g n d e r u n g einsetzt . An der an f angs gesch lossenen W a n d der j u n g e n Blase e n t s t e h e n spg te r die S te r igmen als lokale Auf t r e ibungen , l lnd an tier an f angs gesch lossenen Vgand des S te r igma bi lden sich Con id ien als lokal begrenz te A u s w e i t u n gen. \\ u es sich bestXtigen sollte, dab die H y d r a t u r v e r h a l t n i s s e des N g h r b o d e n s u n d des u m g e b e n d e n R a u m e s hier p r imgr w i r k s a m sind, d a n n is t es \"~ielleicht ke in Zllfall, data eine erhBhte re la t ive D a m p f s p a n n u n g bei Aspergillus repens u n d anderen Pi lzen der A. glaucus-Serie so s t a rke morpholog i sche V e r g n d e r u n g e n hervo r ru fen kann , denn gerade A. glaucus is t u n t e r den Schimmelp i lzen als ve rhg l tn i smaBig xerophi l bekannt~) . Von den sich aus diesen T a t s a e h e n e rgebenden Problemel l f fir die E n t w i c k l u n g s m e c h a n i k , speziell ffir das M e m b r a n w a c h s t u m soleher Pilze sollen einige an andere r Stelle gesonder t darges te l l t werden.", "authors": ["Otto-Joachim Gr\u00fcsser"], "id": "0c0fb0f273ad2c84e51ec132146fd2887e8c4bab", "title": "Rezeptorpotentiale einzelner retinaler Zapfen der Katze", "references": []}, {"date": "1952", "abstract": "A spherical mirror, autocollimating plane grating monochromator is described which is compact, stable, and exhibits spectral resolution which is better than 50 percent of the theoretical value. The spectrometer is 30 inches long and uses a 3-in. wide grating with 30,000 lines/inch. The optical system, although not in use in any modern spectrometer, was described by Ebert in 1889. The aberration corrective arrangement of the spherical mirror leaves only astigmatism as a serious aberration. The use of curved slits to remove this effect of astigmatism is described.The mechanical-optical system is described in detail, and use of the instrument with a photomultiplier tube to record visible and ultraviolet emission spectra is described. Spectra are presented which demonstrate a second-order spectral resolution of 0.05A in the visible and ultraviolet region.", "authors": ["William G. Fastie"], "id": "9d55a05368809dbb8db60fcc801e2ba7f2f3b8e2", "title": "A Small Plane Grating Monochromator", "references": []}, {"date": "1954", "abstract": "A theory of visual detection is developed, based on the model provided by the theory of signal detectability,2 and, more generally, by the theory of statistical decision. Two experiments are reported which test some predictions of the theory for the case of the signal-known-exactly. These experiments demonstrate that the human observer tends toward optimum behavior, where optimum behavior is defined as that behavior which maximizes the expected gain from the decision. Their results show the proportion of correct detections to be dependent upon the proportion of false alarms; they indicate that neural activity is a power function of signal intensity. The data also demand a re-evaluation of the threshhold concept. Predictions are made for the data obtained using two different methods of response, forced-choice end yes-no, and the internal consistency of the theory is demonstrated. The predictions of the theory are compared with contrasting predictions of conventional sensory theory; the data are also related to conventional theory.", "authors": ["Wilson P. Tanner", "John A. Swets"], "id": "09dc4bc616b83dcdea522ca261bb8b360571e056", "title": "The human use of information-I: Signal detection for the case of the signal known exactly", "references": ["fe46d81dd2b53ffb5885aea00bd53a1ecdc250cc", "6c196cdcc0a9d87d57307587f41e963b7a94d50e"]}, {"date": "1952", "abstract": "Semantic Scholar extracted view of \"Neurons in the retina; organization, inhibition and excitation problems.\" by Stephen W. Kuffler", "authors": ["Stephen W. Kuffler"], "id": "1c253ba1d9619ead06c2cab151aeef6aabbe0be6", "title": "Neurons in the retina; organization, inhibition and excitation problems.", "references": []}, {"date": "1954", "abstract": "Two specific cases of signal detection involving uncertainty in the frequency of a sound signal are compared with the case of the signal-known-exactly. In the first case the signal is either of two known frequencies; in the second case the signal is any frequency within a given range. It is suggested that detection behavior that is optimal for the three cases requires a deal mechanism: a combination of a wide-open receiver end a panoramic receiver. Evidence is presented that supports the existence of such a mechanism. Estimates of the bandwidth end seen-rate of the receiver are included.", "authors": ["Wilson P. Tanner", "R. Z. Norman"], "id": "abd419845a23bce62484bda6f671e77e2d46a383", "title": "The human use of information-II: Signal detection for the case of an unknown signal parameter", "references": []}, {"date": "1938", "abstract": "This spectrophotometer was designed specifically to measure in the visible region the absorption spectrum of visual purple, the photosensitive substance of the retinal rods. It is independent of both the photoelectric cell and amplifier characteristics, relying only on the law of crossed polarizing prisms. The sensitivity of the detecting system is high enough so that the measuring light produces no detectable effect upon the measured solution. The entire system can measure a transmission of 1 percent to a precision of about 1 percent.", "authors": ["Simon Shlaer"], "id": "58c39dd255a8ae76dd3a1d20cf1423d5bbccb210", "title": "A Photoelectric Transmission Spectrophotometer for the Measurement of Photosensitive Solutions", "references": []}, {"date": "1952", "abstract": "The maximum rate at which a synaptic link could theoretically transmit information depends on the type of coding used. In a binary modulation system it depends chiefly on the relaxation time, and the limiting capacity equals the maximum attainable impulse rate. In a system using pulse-interval modulation, temporal precision may be a more important limiting factor. It is shown that in a number of typical cases a system of the second type could transmit several times more information per second through a synaptic link than a binary system, and the relation between relative efficiency, relaxation-time, and temporal resolving power is generalized in graphical form. It is concluded, not that interval modulation rather than binary modulation \u201cought\u201d to be the mode of action of the central nervous system, but that the contrary assumption is unsupported by considerations of efficiency.", "authors": ["Donald M. MacKay", "Warren Sturgis McCulloch"], "id": "42bab83cc1aa10e18a31c1a8fb04e165ca8180d1", "title": "The limiting information capacity of a neuronal link", "references": ["9b0e117aadeaeefc9af842b0a38f928074bff6c6", "8bd1c5b7f0e75b26ae8b0dab564698865aeea424"]}, {"date": "1938", "abstract": "FOR the chemical theory of excitation of nerve cells, the following observations as to how the retinal rods are stimulated by the substance visual purple would seem to be of general interest. Their understanding requires knowledge of the fact that after a preceding period of adaptation to sunlight the size of the electrical response of the eye to a constant test light reproduces the curve of regeneration of visual purple (see, for example1,2,3). When the quantity of visual purple, obtainable from an eye after increasingly longer periods of dark adaptation, increases, the size of the electrical response (its b-wave) likewise increases owing\u2014one would have thought\u2014directly to the larger amount of cell stimulant available.", "authors": ["Ra\u01f5nar Granit", "T. Holmberg", "M. Zewi"], "id": "48a6a3c61d6df05f2d96e867c25c6bb4dfc505ad", "title": "Mode of Action of Visual Purple", "references": []}, {"date": "1948", "abstract": "Semantic Scholar extracted view of \"Action Potentials From Single Auditory-Nerve Fibers?\" by R\u00f3bert Galambo\u0161 et al.", "authors": ["R\u00f3bert Galambo\u0161", "H. Davis"], "id": "e0188fabca0d0f3c4b1f580957dc46bbd849f071", "title": "Action Potentials From Single Auditory-Nerve Fibers?", "references": []}, {"date": "1937", "abstract": "The course of dark adaptation of the human eye varies with the intensity used for the light adaptation which precedes it. Preadaptation to intensities below 200 photons is followed only by rod adaptation, while preadaptation to intensities above 4000 photons is followed first by cone adaptation and then by rod adaptation. With increasing intensities of preadaptation, cone dark adaptation remains essentially the same in form, but covers an increasing range of threshold intensities. At the highest preadaptation the range of the subsequent cone dark adaptation covers more than 3 log units. Rod dark adaptation appears in two types\u2014a rapid and a delayed. The rapid rod dark adaptation is evident after preadaptations to low intensities corresponding to those usually associated with rod function. The delayed rod dark adaptation shows up only after preadaptation to intensities which are hundreds of times higher than those which produce the maximal function of the rods in flicker, intensity discrimination, and visual acuity. The delayed form remains essentially constant in shape following different intensities of preadaptation. However, its time of appearance increases with the preadaptation intensity; after the highest preadaptation, it appears only after 12 or 13 minutes in the dark. These two modes of rod dark adaptation are probably the expression of two methods of formation of visual purple in the rods after its bleaching by the preadaptation lights.", "authors": ["Selig Hecht", "Charles Haig", "Aurin M. Chase"], "id": "199dc9e369569dc8fdb2c782c763da82785e9ca9", "title": "THE INFLUENCE OF LIGHT ADAPTATION ON SUBSEQUENT DARK ADAPTATION OF THE EYE", "references": ["93dcf2163619e7c71247f2f37f3656c40f437e1b", "55f8a443d0f0829c91c386805005a05077389fc7", "bf26d3110015654b38af3fe00694a6f174eb3df0", "e895851690178506e1a89b88a9c6b08be84663ea", "43582587e7ce7b409ed7a83ac672ad375f3675aa", "119d3dd7584f92c9da21ec10fe2183164b85a716", "713d139317c19c5e96094e1aed5b85258654d069", "ece42dca735e91857223564ed336d2ce43363124", "f187d363ddb6643e0387a8e0a37b65159db59827"]}, {"date": "1954", "abstract": "Recognizing the mannerism ways to get this books Probability And Information Theory With Applications To Radar International Series Of Monographs On Electronics And Instrumentation Volume 3 is additionally useful. You have remained in right site to start getting this info. get the Probability And Information Theory With Applications To Radar International Series Of Monographs On Electronics And Instrumentation Volume 3 link that we manage to pay for here and check out the link.", "authors": ["Philip M. Woodward"], "id": "9e5e60ac75ea6469d612f9ea293f2e727e3caf3d", "title": "Probability and Information Theory, with Applications to Radar", "references": ["433354f30d9aa6f1bf4937247f171bd801db7cd8"]}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Pacemakers in relation to aspects of behavior\" by Hudson Hoagland", "authors": ["Hudson Hoagland"], "id": "995ed6ebef8ebdbd7d4d51bc50dd7ac2bca59543", "title": "Pacemakers in relation to aspects of behavior", "references": []}, {"date": "1940", "abstract": "Single-fibre discharges have been recorded from the horizontal ampulla of the isolated labyrinth of Raja. A basis is provided for the interpretation of labyrinthine tone, and of the reflex responses to rotation of unilaterally operated animals. The sensory activity of the semicircular canal provides a framework accounting for nystagmus and after-nystagmus.", "authors": ["Otto L\u00f6wenstein", "Alec Sand"], "id": "1962f9c6d58b5971c285ff6c30cd76ad12a6fc5c", "title": "The mechanism of the semicircular canal. A study of the responses of single-fibre preparations to angular accelerations and to rotation at constant speed", "references": ["995ed6ebef8ebdbd7d4d51bc50dd7ac2bca59543"]}, {"date": "1938", "abstract": "The recognition of the morphological and developmental relationship of the vertebrate auditory organ and the lateral-line system of fishes and aquatic Amphibia rests on the foundation of a large volume of com\u00ad parative researches. The main outlines of this generalization were already laid down forty years ago, and Cole\u2019s work on the cranial nerves and lateral sense organs of fishes (1898) contains a comprehensive treatment of the history of the subject. The acustico-lateral or neuromast system embraces, in addition to the labyrinth and the lateral-line canals, the pit organs found to a greater or less extent in most fishes, the vesicles of Torpedo , and the ampullary canal system of Selachians and Holocephali. Concerning these Cole wrote: \u201cThe history of our knowledge of the phylogeny of the sensory canals is coincident with three discoveries\u2014the discovery that the\u2018mucus\u2019 canals contain sense organs, the discovery of Savi\u2019s vesicles, and the dis\u00adcovery of the ampullae of Lorenzini.... We now know that all three types belong to the lateral line system, and I shall suggest that they represent three stages in the development of a canal\u2014the most superficial condition, represented by the pit organs and Savi\u2019s vesicles; the full development, represented by the canal; and the intermediate type, forming neither a Savi vesicle nor yet a canal, represented by the ampullae of Lorenzini\u201d (p. 187). This conception has remained valid to the present day. The ampullae of Lorenzini, with which I am here principally concerned, are briefly described in current text-books as transitional or specialized neuromasts, and the implication always is that structurally and functionally they do not differ significantly from the neuromasts of the lateral-line canals. For example, in their recent exhaustive treatise on the vertebrate nervous system Kappers, Huber and Crosby (1936) state with reference to the lateral-line canals, the Savi vesicles and the ampullae of Lorenzini: \u201cthus in the various animals there is a transition between an open and a closed system for perceiving vibrations\" (p. 438).", "authors": ["Alec Sand"], "id": "8261549162b3d5f85d967a46d679c931a4800850", "title": "The function of the ampullae of Lorenzini, with some observations on the effect of temperature on sensory rhythms", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"QUANTITATIVE ANALYSIS OF RESPONSES FROM LATERAL-LINE NERVES OF FISHES. II\" by Hudson Hoagland", "authors": ["Hudson Hoagland"], "id": "ca05cbb2199d157a8c506dca01a6742a320e71e7", "title": "QUANTITATIVE ANALYSIS OF RESPONSES FROM LATERAL-LINE NERVES OF FISHES. II", "references": []}, {"date": "1949", "abstract": "Semantic Scholar extracted view of \"The normal membrane potential of frog sartorius fibers.\" by Gilbert Ning Ling et al.", "authors": ["Gilbert Ning Ling", "Ralph Waldo Gerard"], "id": "ecb3afe96d236e5a8c138b62ed654cc54d82f9e4", "title": "The normal membrane potential of frog sartorius fibers.", "references": []}, {"date": "1937", "abstract": "For a long time after their discovery in the seventeenth century the lateral-line canals of fishes were considered to be mucus-secreting organs. In 1850 Leydig described sense organs in the lateral-line canals, and this discovery stimulated a keen interest in the investigation of both the morphological and functional features of the lateral-line system. Morphological studies have yielded a thorough understanding of the structure of these organs (Ewart and Mitchell 1892; Cole 1896; Johnson 1917; von Woellwarth 1933). Physiological studies, though numerous, have been less fruitful. An account of the older work was given by Baglioni (1913), and the more recent work is reviewed by Dykgraaf (1933). The only technique until recently available has been the elimination of the sensory system by nerve section and cauterization, and the comparison of the behaviour of intact and operated fishes in response to various stimuli. With so diffuse a structure as the lateral-line system, receiving its nerve supply from the fifth, seventh, ninth and tenth cranial nerves, this method is particularly inadequate, and involves a violent mutilation of the animal. When one considers the crudity of many of these operations, it is not the uncertainty of the results which is remarkable, but rather that some of the conclusions reached should remain valid to-day in the light of far more penetrating experimental analysis. This method of organ elimination could yield at best only an indication of the kind of stimulus that is effective in evoking the excitation of lateral-line receptors. In current textbooks the conclusion of Parker (1904) that the effective stimulus for the lateral line is low-frequency vibration, and that of Hofer (1907) that it is movement of water (i. e. local currents) have received most notice. The observations of Dykgraaf (1933), who employed the more refined methods of von Frisch\u2019s futterdressur technique, support Hofer\u2019s conclusion, and to some extent also Parker\u2019s. Dykgraaf considers the lateral-line system to be an organ of Ferntastsinn , and if this is taken to mean a mechanoreceptor of such sensitivity that it can function both as a touch organ and as a receptor for disturbances coming from a distance, it is undoubtedly a true description, for it is fully confirmed by the direct electrophysiological studies of Hoagland (1933 a, b, c and d ) and of Schriever (1935). The latter, apparently unacquainted with Hoagland\u2019s work, did little more than to confirm several of his observations.", "authors": ["Alec Sand"], "id": "f50ec6c608c3a8863105a8fdb286f5e03d86cfc8", "title": "The Mechanism of the Lateral Sense Organs of Fishes", "references": []}, {"date": "1952", "abstract": "Pairs of very small electrodes were placed in two or more turns of the cochlea of the guinea pig. The cochlear microphonic from a short segment (about 1 mm) of the cochlear partition can thus be recorded, and without contamination by action potentials. The outputs of the second, third, and fourth turns were compared with that of the first turn with respect to both amplitude and phase as a function of frequency. The space\u2010time pattern thus revealed is a traveling wave which passes up the cochlea to a distance that depends on the frequency. The pattern agrees well with that of mechanical movement (Bekesy) except that the cochlear microphonic shows relatively greater amplitude (voltage) in the basal turn. Low frequencies travel farther up the cochlea than do high. The velocity (for a 750 cps tone) is about 100 m/sec in the basal turn and about 2 m/sec in the fourth turn. Phase differences of as much as 5\u03c0 were observed, by means of Lissajous patterns, between the responses of the first and the third turn. Ce...", "authors": ["Ichiji Tasaki", "H. Davis", "J.-P. Legouix"], "id": "d13b5fe17d110d783f44963c8088368e2e99cc40", "title": "The Space\u2010Time Pattern of the Cochlear Microphonics (Guinea Pig), as Recorded by Differential Electrodes", "references": []}, {"date": "1954", "abstract": "Semantic Scholar extracted view of \"Action potentials from individual elements in cat geniculate and striate cortex.\" by Ichiji Tasaki et al.", "authors": ["Ichiji Tasaki", "Edward H. Polley", "Fernando Orrego"], "id": "710fa0ffc40fd03f4690ed8575c9e8a08e12ef79", "title": "Action potentials from individual elements in cat geniculate and striate cortex.", "references": []}, {"date": "1953", "abstract": "Abstract : A theory of visual detection is presented in which the visual system is considered a communication channel with internal noise. Experimental results confirmed the role of variables which were not purely sensory. The ability of subjects to detect visual signals was measured in one session in which the probability of signal existence was 0.8 and in a second session in which the probability was 0.4. In the second session the actual criteria of seeing changed and the subjects reported seeing things which they were unable to see. The assumptions of this new theory were compared with those of the conventional psychophysical theory.", "authors": ["Wilson P. Tanner", "John A. Swets", "Homer William Welch"], "id": "6c196cdcc0a9d87d57307587f41e963b7a94d50e", "title": "A new theory of visual detection", "references": []}, {"date": "1954", "abstract": "An apparatus is described which automatically controls the presentation of light stimuli and records subjects\u2019 responses in visual threshold experiments. The apparatus includes: (a) A tape reader which controls the magnitude and time of occurrence of light stimuli, in accordance with the location of holes in a programming tape, and (b) a punch recorder which makes a permanent record of the conditions of each light stimulus and of the responses made by each of four subjects. The accuracy of the stimulus-producing equipment is monitored. Electric counters tally the correct responses made by each subject for each magnitude of light stimulus.", "authors": ["H. Richard Blackwell", "Benjamin S. Pritchard", "James G. Ohmart"], "id": "fe46d81dd2b53ffb5885aea00bd53a1ecdc250cc", "title": "Automatic apparatus for stimulus presentation and recording in visual threshold experiments.", "references": []}, {"date": "1951", "abstract": "\u201cTone\u2010pips\u201d were produced by brief rectangular electrical pulses being delivered through two sound\u2010effects filters in cascade with both high and low cut\u2010offs set at 2000 cps. Nearly all of the acoustic energy of the final signal was found to be concentrated in a band about an octave wide and centering at 2000 cps. The pulsing frequency was varied independently between 90 and 150 pips per second.Listeners describe the resulting sound as a \u201cmetallic buzz.\u201d Listeners vary greatly in their ability to identify the two \u201cpitches\u201d present in this sound and in the accuracy with which they match with a pure tone either the pulsing frequency (about 130 per second) or the band\u2010pass frequency (2000 cps in the present series). Errors of exactly one octave are particularly common.In the theoretical discussion we argue that the \u201cpitch\u201d of a pure tone is a double attribute compounded of \u201cbuzz\u201d (correlated with frequency of volleys of nerve impulses) and \u201cbody\u201d (correlated with position of maximum stimulation on the basila...", "authors": ["H. Davis", "Seth R. Silverman", "D. R. McAuliffe"], "id": "ea1dd97ddf91a02bf993cba47ae3fe46df05064d", "title": "Some Observations on Pitch and Frequency", "references": []}, {"date": "1954", "abstract": "To UNDERSTAND how the cochlea analyzes and responds to complex sound stimuli, it is desirable to know what kind of message individual nerve fibers carry from the ear to the central nervous system. In 1942, Galambos and and Davis (9) recorded electric responses of single ganglion cells in the cochlear nucleus, which they once believed to be from single auditory nerve fibers. Those responses, recorded from the secondary neurons in the auditory system, indicate that each element responds to a tone of a particular frequency with a particularly high sensitivity. Galambos (8) also described an inhibitory interaction between the responses to two different sound stimuli in the cochlear nucleus. It is therefore possible that the selective response of each element at this level of the auditory system to a particular frequency could be largely due to some complicated interaction among nerve impulses arriving at the cochlear nucleus over a large number of primary auditory nerve fibers. Quite recently, it has been shown in this Institute (21,22) that the basal turn of the guinea pig cochlea responds to practically all frequencies in the audible range, while the upper parts of the cochlea respond only to sounds of low frequencies. By the method of differential recording of the microphonic response across the cochlear partition, it was shown that when the response of the upper part of the cochlea (to low-frequency sounds) had been eliminated by a local injection of an isotonic KC1 solution there were still good normal responses in the basal turn, both microphonics and nerve action potentials. These results exclude any sharp localization of vibratory motion in the cochlea. The \u201cresonance curve\u201d obtained by Bekesy (3), which correlates mechanical displacement of the cochlear partition of the dead human and animal ears with place in the cochlea at various frequencies, is not very sharp. Nevertheless, the large microphonic and nerve action potentials induced by low-frequency sound in the basal turn still seem difficult to reconcile with Bekesy\u2019s curves, which show a fairly rapid decay in amplitude of vibration toward the basal turn. More direct information as to the nerve impulses in the primary auditory neurons seemed to offer a solution of this difficulty.", "authors": ["Ichiji Tasaki"], "id": "3a2424648f4fc52ef937cd67c28a08b68fae71a2", "title": "Nerve impulses in individual auditory nerve fibers of guinea pig.", "references": ["7f50cdaa527fb63fee489901eae5cd355c38a2fc", "ea1dd97ddf91a02bf993cba47ae3fe46df05064d", "ec2c6cad20eda70ca8ef346d2973362a56acbd0a", "55ec0a124f5ceb2018d4cf26369a2be186d77009", "2b690570512365ee0a2f4e1f4e9e9db5350ee35e", "7485132b878588ac7135b3e610ac5cb0fd853d16", "2f5ce95dda4223c7f2aa2dfb75ea17f1a65f3a3b", "4716f0d81346698ae6aeccc5a7afd05f6f86d46f", "d13b5fe17d110d783f44963c8088368e2e99cc40", "0de3a3441b7c7b55d71aa59a5f779ad4ba119317"]}, {"date": "1939", "abstract": "Semantic Scholar extracted view of \"TRANSMISSION OF IMPULSES THROUGH CRANIAL MOTOR NUCLEI\" by N RafaelLorenteDe", "authors": ["N RafaelLorenteDe"], "id": "8bd1c5b7f0e75b26ae8b0dab564698865aeea424", "title": "TRANSMISSION OF IMPULSES THROUGH CRANIAL MOTOR NUCLEI", "references": []}, {"date": "1944", "abstract": "Semantic Scholar extracted view of \"INHIBITION OF ACTIVITY IN SINGLE AUDITORY NERVE FIBERS BY ACOUSTIC STIMULATION\" by R\u00f3bert Galambo\u0161", "authors": ["R\u00f3bert Galambo\u0161"], "id": "4ce1a21503ab4046d02b4dc540a37799c526efed", "title": "INHIBITION OF ACTIVITY IN SINGLE AUDITORY NERVE FIBERS BY ACOUSTIC STIMULATION", "references": []}, {"date": "1949", "abstract": "Semantic Scholar extracted view of \"Homogeneous harmonic functions\" by Hillil Poritsky", "authors": ["Hillil Poritsky"], "id": "433354f30d9aa6f1bf4937247f171bd801db7cd8", "title": "Homogeneous harmonic functions", "references": []}, {"date": "1950", "abstract": "Summary. \n \nThe electrical activity caused by stimulation of the receptors in the vestibular organ was recorded from isolated units in the inferior colliculus and the medial geniculate body. \n \n \n \nThe responses obtained showed that the discharge conducted through these regions mainly followed uncrossed tracts and that, in addition, a smaller portion passed along crossed pathways. \n \n \n \nThus, the experiments demonstrate that the vestibulo-cortical intramesencepharic course of the pathways runs parallel with the acoustic fibres. \n \n \n \nThis work has been supported by a grant from \u201cADOLF ROBBERTS fond\u201d.", "authors": ["Bo E. Gernandt"], "id": "9b0e117aadeaeefc9af842b0a38f928074bff6c6", "title": "Midbrain activity in response to vestibular stimulation.", "references": []}, {"date": "", "abstract": "Informationen zu Angeboten auf der Homepage der Reihe und springer.com/authors Erhaltlich bei Ihrem Buchhandler oder \u2013 Springer Customer Service Center GmbH, Haberstrasse 7, 69126 Heidelberg, Germany \u25b6 Call: + 49 (0) 6221-345-4301 \u25b6 Fax: +49 (0)6221-345-4229 \u25b6 Email: customerservice@springer.com \u25b6 Web: springer.com Handbuch der normalen und pathologischen Physiologie Reihen-Hrsg.: A. Bethe, G.v. Bergmann, A. Ellinger, G. Embden", "authors": ["Albrecht Bethe", "G. v. Bergmann", "G. Embden", "A. Ellinger"], "id": "f187d363ddb6643e0387a8e0a37b65159db59827", "title": "Handbuch der Normalen und Pathologischen Physiologie", "references": []}, {"date": "1943", "abstract": "THE AIM OF the experiments described here was to study the response of single auditory-nerve fibers to acoustic stimulation delivered to the intact ear. Similar studies on other single afferent fibers have remarkably extended knowledge of the mode of action of diverse sensory mechanisms, while at the same time emphasizing their fundamental similarities. The present report describes the behavior of single auditory fibers and stresses similarities to other sensory nerves; in a subsequent paper we hope to relate these findings to a specific theory of action of the mammalian cochlea. METHOD Young cats anesthetized with dial (0.75 cc. per kilo) were used in these experiments. The postero-dorsal aspect of the auditory nerve was exposed by removing the lateral portion of the occipital bone where it meets the petrous bone. Bleeding from the sinus petrosus inferior was stopped by judicious cauterization or by Clotting Globulin. * A Ringer-filled glass micropipette with Ag-AgCl wire inserted as close as possible to the tip served as the active lead from the nerve. It was early established that pipettes with openings greater than 5~ do not allow isolation of the action potentials of single auditory fibers. The 3 to 5~ electrodes used have an impedance of about 1 megohm when tested on a resistance-capacity bridge between 0.6 and 2.5 kc. The microelectrode and the indifferent electrode (a silver plate in the neck muscles) led to a capacity-coupled amplifier (Grass) with an input done photographically", "authors": ["R\u00f3bert Galambo\u0161", "Hallowell Davis"], "id": "7f50cdaa527fb63fee489901eae5cd355c38a2fc", "title": "THE RESPONSE OF SINGLE AUDITORY-NERVE FIBERS TO ACOUSTIC STIMULATION", "references": ["5ea8e8702f08e18cd444b439fb8ce3e70a5b8705", "95816b1158f1723899656f2b0a2471dbb8f135ca", "c9e01d1b42dd2bc9072312f02d11f9dd7b882b2e", "00cb83e7027a2e159c4f733e7e11ea2a45c82313", "a245cac8a737400eac2d68d79926e8c0240c43b8", "ec9cecd24a6c9afcfd22d49254bad238a35d3e01", "4e3da415fb37d6364d0dc7e120a2168049bf538e", "028d4df9ec461a0cdc64c1010fc3a5fcf05f99f4", "31ef5f9ef4ef6b213091de446185b88e793ba2c8"]}, {"date": "1936", "abstract": "1. An optical system is described which furnishes large flickering fields whose brightness, even when reduced with monochromatic filters, is capable of covering the complete range of the relation between critical frequency and intensity. 2. For a centrally located test field of 19\u00b0 diameter, with light from different parts of the spectrum, the data divide into a low intensity section identified with rod function, and a high intensity section identified with cone function. The transition between the two sections is marked by an inflection point which is sharp, except for 450 and 490 m\u00b5 where, though clearly present, it is somewhat rounded. 3. The intensity range covered by the flicker function is smallest in the red, and increases steadily as the wave-length decreases. The increase is due entirely to the extent of the low intensity, rod section which is smallest (non-existent for S. S.) in the red and largest in the violet. The high intensity cone portion for all colors is in the same position on the intensity axis, and the only effect of decreasing wave-length is to shift the rod section to lower intensities without changing its shape. 4. The measurements are faithfully described by two similar equations, one for the rods and one for the cones, both equations being derived from the general stationary state equation already used for various visual functions.", "authors": ["Selig Hecht", "Simon Shlaer"], "id": "ece42dca735e91857223564ed336d2ce43363124", "title": "INTERMITTENT STIMULATION BY LIGHT : V. THE RELATION BETWEEN INTENSITY AND CRITICAL FREQUENCY FOR DIFFERENT PARTS OF THE SPECTRUM", "references": []}, {"date": "1935", "abstract": "1. A theory of visual intensity discrimination is proposed in terms of the photochemical events which take place at the moment when a photosensory system already adapted to the intensity I is exposed to the just perceptibly higher intensity I+\u0394I. Unlike previous formulations this theory predicts that the fraction \u0394I/I, after rapidly decreasing as I increases, does not increase again at high intensities, but reaches a constant value which is maintained even at the highest intensities. 2. The theory describes quantitatively the intensity discrimination data of Drosophila, of the bee, and of Mya. 3. With some carefully considered exceptions the intensity discrimination data of the human eye fall into two classes: those with small test areas or with red light, which form a single continuous curve describing the function of the retinal cones alone, and those with larger areas, and with white, orange, and yellow light, which form a double curve showing a clear inflection point, and represent the separate function of the rods at intensities below the inflection point and of the cones at intensities above it. 4. The theory describes all these data quantitatively by treating the rods and cones as two independently functioning photosensory systems in accordance with the well established duplicity idea. 5. In terms of the theory the data of intensity discrimination give critical information about the order of both the photochemical and dark reactions in each photosensory system. The reactions turn out to be variously monomolecular and bimolecular for the different animals.", "authors": ["Selig Hecht"], "id": "713d139317c19c5e96094e1aed5b85258654d069", "title": "A THEORY OF VISUAL INTENSITY DISCRIMINATION", "references": ["77a77fba796f77d3103430b9b93e7d3f7f920819", "189275dcd1306a028fa1e9146a67a8bf40c20975", "f61bc6af0c4979683049c8dfea4cffb99dc3827a", "55ef6f37e343b20777d4eca80014a46ba110fedd", "b68223017564ce160b65e189ddb736ee26cd3bf4", "394b2f1f2181f20ce735dc45a823ef5e78dec3df", "d655b2125b7dcaec8e2e6421643df35cc9f83b25", "e746c31c40f042bde0566bd060a9e9b839e251b1", "ac5447f3f21ea38984da96a126e18c234f25b73d", "b5b3dcb2cd3d40b03caafe7740d3ab2076f5e1de"]}, {"date": "", "abstract": "1. After a discussion of the sources of error involved in the study of dark adaptation, an apparatus and a procedure are described which avoid these errors. The method includes a control of the initial light adaptation, a record of the exact beginning of dark adaptation, and an accurate means of measuring the threshold of the fovea after different intervals in the dark. 2. The results show that dark adaptation of the eye as measured by foveal vision proceeds at a very precipitous rate during the first few seconds, that most of the adaptation takes place during the first 30 seconds, and that the process practically ceases after 10 minutes. These findings explain much of the irregularity of the older data. 3. The changes which correspond to those in the fovea alone are secured by correcting the above results in terms of the movements of the pupil during dark adaptation. 4. On the assumption that the photochemical effect of the light is a linear function of the intensity, it is shown that the dark adaptation of the fovea itself follows the course of a bimolecular reaction. This is interpreted to mean that there are two photolytic products in the fovea; that they are disappearing because they are recombining to form anew the photosensitive substance of the fovea; and that the concentration of these products of photolysis in the sense cell must be increased by a definite fraction in order to produce a visual effect. 5. It is then suggested that the basis of the initial event in foveal light perception is some mechanism that involves a reversible photochemical reaction of which the \"dark\" reaction is bimolecular. Dark adaptation follows the \"dark\" reaction; sensory equilibrium is represented by the stationary state; and light adaptation by the shifting of the stationary state to a fresh point of equilibrium toward the \"dark\" side of the reaction.", "authors": ["Selig Hecht"], "id": "e895851690178506e1a89b88a9c6b08be84663ea", "title": "THE NATURE OF FOVEAL DARK ADAPTATION", "references": ["356d7b1f5efcacfb7a5968b4597a7a170d0de371", "b3fa455033251b49d96f443d4d18e8d4a33c94b5", "f5366c22ba53fb8f7aff5d3d6f6c5e9ed1793c60"]}, {"date": "1936", "abstract": "1. In the retina, central areas whose diameter is less than 2\u00b0 possess only cones, while larger areas have rods and cones. In conformity with this, the relation of critical fusion frequency to intensity is a single function for centrally fixated areas below 2\u00b0, and a double function for similarly fixated, larger areas. The two sections of such data are easily identified with rod activity at low intensities and with cone activity at high intensities. 2. The curves describing the rod data are the same for all areas, differing only in the values of the associated dimensional constants which control the location of the curves on the coordinate axes. Similarly, the curves for the cone data are the same for all areas; the tendency for an increase in maximal frequency with area is the expression merely of the value of a constant which determines the position of the data on the frequency axis. Area, therefore, does not influence the fundamental nature of the flicker relation through each receptor system, but merely alters the extraneous constants of the relation. 3. The curves which describe the measurements are represented by two equations, one for rods and one for cones; both equations are derived from the stationary state descriptive of the initial event in the photoreceptor process.", "authors": ["Selig Hecht", "E. Lester Smith"], "id": "55f8a443d0f0829c91c386805005a05077389fc7", "title": "INTERMITTENT STIMULATION BY LIGHT: VI. AREA AND THE RELATION BETWEEN CRITICAL FREQUENCY AND INTENSITY", "references": []}, {"date": "1935", "abstract": "1. Carotenoids have been identified and their quantities measured in the eyes of several frog species. The combined pigment epithelium and choroid layer of an R. pipiens or esculenta eye contain about 1\u03b3 of xanthophyll and about 4\u03b3 of vitamin A. During light adaptation the xanthophyll content falls 10 to 20 per cent. 2. Light adapted retinas contain about 0.2\u20130.3 \u03b3 of vitamin A alone. 3. Dark adapted retinas contain only a trace of vitamin A. The destruction of their visual purple with chloroform liberates a hitherto undescribed carotenoid, retinene. The bleaching of visual purple to visual yellow by light also liberates retinene. Free retinene is removed from the isolated retina by two thermal processes: reversion to visual purple and decomposition to colorless products, including vitamin A. This is the source of the vitamin A of the light adapted retina. 4. Isolated retinas which have been bleached and allowed to fade completely contain several times as much vitamin A as retinas from light adapted animals. The visual purple system therefore expends vitamin A and is dependent upon the diet for its replacement. 5. Visual purple behaves as a conjugated protein in which retinene is the prosthetic group. 6. Vitamin A is the precursor of visual purple as well as the product of its decomposition. The visual processes therefore constitute a cycle.", "authors": ["George Wald"], "id": "119d3dd7584f92c9da21ec10fe2183164b85a716", "title": "CAROTENOIDS AND THE VISUAL CYCLE", "references": ["2a638025190cb5cc68f8240089c5bef04dfdac25", "6b7a91a0e6b67b3ad7d1f706b83664d6ec83c0c6", "90d0610506d0320066d15382d9bacd1d632c4ac5", "61d7f4c81e066d1d13731a7576905d78c581e892", "925c89d7f099282b8d847188784848432c24eeff"]}, {"date": "1935", "abstract": "The decrease in threshold shown by the eye during dark adaptation proceeds in two steps. The first is rapid, short in duration, and small in extent. The second is slow, prolonged, and large. The first is probably due to cone function; the second to rod function. In centrally located fields the two parts of adaptation change differently with area. With small, foveal fields the first part dominates and only traces of the second part appear. As the area increases the first part changes a little, while the second part covers an increasing range of intensities and appears sooner in time. Measurements with an annulus field covering only the circumference of a 20\u00b0 circle show most of the characteristics of a 20\u00b0 whole field centrally located. Similarly a 2\u00b0 field located at different distances from the center shows dark adaptation characteristics essentially like those of large centrally located fields whose edges correspond to the position of the central field. Evidently the behavior in dark adaptation of centrally located fields of different size is determined in the main not by area as area, but by the fact that the retina gradually changes in sensitivity from center to periphery, and therefore the larger the field the farther it reaches into peripheral regions of permanently greater sensibility.", "authors": ["Selig Hecht", "Charles Haig", "George Wald"], "id": "93dcf2163619e7c71247f2f37f3656c40f437e1b", "title": "THE DARK ADAPTATION OF RETINAL FIELDS OF DIFFERENT SIZE AND LOCATION", "references": ["8f1c1fc5c79ab4c319f07dbe1a29e5750d93a9f9", "f187d363ddb6643e0387a8e0a37b65159db59827", "e895851690178506e1a89b88a9c6b08be84663ea"]}, {"date": "1939", "abstract": "1. Measurements of visual purple regeneration in solution have been made by a procedure which minimized distortion of the results by other color changes so that density changes caused by the regenerating substance alone are obtained. 2. Bleaching a visual purple solution with blue and violet light causes a greater subsequent regeneration than does an equivalent bleaching with light which lacks blue and violet. This is due to a photosensitive substance which has a gradually increasing effective absorption toward the shorter wavelengths. It is uncertain whether this substance is a product of visual purple bleaching or is present in the solution before illumination. 3. The regeneration of visual purple measured at 560 m\u00b5 is maximal at about pH 6.7 and decreases markedly at more acid and more alkaline pH's. 4. The absorption spectrum of the regenerating material shows only a concentration change during the course of regeneration, but has a higher absorption at the shorter wavelengths than has visual purple before illumination. 5. Visual purple extractions made at various temperatures show no significant difference in per cent of regeneration. 6. The kinetics of regeneration is usually that of a first order process. Successive regenerations in the same solution have the same velocity constant but form smaller total amounts of regenerated substance. 7. In vivo, the frog retina shows no additional oxygen consumption while visual purple is regenerating.", "authors": ["Aurin M. Chase", "E. Lester Smith"], "id": "bf26d3110015654b38af3fe00694a6f174eb3df0", "title": "REGENERATION OF VISUAL PURPLE IN SOLUTION", "references": ["58c39dd255a8ae76dd3a1d20cf1423d5bbccb210", "7d834dac179bba2ea12ef96c2cdbc566353f3682", "961d120af1768287142f0f77c60a33f5c499f2bd", "3f9cf02c77a64c46465b9e4eb52069ecac150a56", "67e10608a5348cdca95cfab913f3b2dbe49d715b", "deba436a8502a60eb3cb8a2a41365f0f3d7ed9b6", "e2f4fa12b3eb898f5ef3ed8262ee011d17ec85ca", "b52c8240d1f66d2afd57be9aca398edf3eb55986", "2d0eaf51dfa68f582c73a7fde1c048be8643eb38", "1dc5b314b222b3482386851db7ac2cf3e78557a0"]}, {"date": "1938", "abstract": "1. The variation of threshold with field area was measured in fields homogeneous in rod-cone composition. At 15\u00b0 above the fovea, an increase in field diameter from 1\u00b0 to 5\u00b0 reduces the threshold sevenfold, at 25\u00b0 above the fovea tenfold. 2. These changes are shown to follow qualitatively from simple statistical properties of the retinal mosaic. Analytic treatment leads to the expression, (A \u2013 nt)k I = C, in which A = area, nt = constant threshold number of elements, I = threshold intensity, and k and C are constants. This equation describes the available data accurately, and is the general form of previous empirical area-threshold formulae.", "authors": ["George Wald"], "id": "f4e658705c195e956ccf6ea884e4d262d758fa82", "title": "AREA AND VISUAL THRESHOLD", "references": ["3305fbeabc5754a3f10055ae44007e9851682005", "65b103df0e8108412d63e772cba4be198d1e06b1"]}, {"date": "1938", "abstract": "1. The properties of rhodopsin in solution have been examined in preparations from marine fishes, frogs, and mammals. 2. The bleaching of neutral rhodopsin in solution includes a photic and at least three thermal (\"dark\") processes. Thermal reactions account for approximately half the total fall in extinction at 500 m\u00b5. 3. Bleaching has been investigated at various pH's from 3.9 to about 11. With increase in pH the velocity of the thermal components increases rapidly. Though the spectrum of rhodopsin itself is scarcely affected by change in pH, the spectra of all product-mixtures following irradiation are highly pH-labile. 4. The spectrum of pure rhodopsin\u2014or of the rhodopsin chromophore\u2014is fixed within narrow limits. The extinction at 400 m\u00b5 lies between 0.20 to 0.32 of that at the maximum. 5. Within the limitations of available data, the spectrum of pure rhodopsin corresponds in form and position with the spectral sensitivity of human rod vision, computed at the retinal surface. 6. The nature of bleaching of rhodopsin in solution, its kinetics, and its significance in the retinal cycle are discussed.", "authors": ["George Wald"], "id": "db9317bc2cd3ed86b706801e968603a911c8a089", "title": "ON RHODOPSIN IN SOLUTION", "references": ["2300a0ac874784d04bf799711142f625289347f1", "a07a8ee6cb1ac36ed63bebf24562a1b97c105fde", "6b1e8b2b3567e14a288ad8ed623f2bf008c42f02", "d655b2125b7dcaec8e2e6421643df35cc9f83b25", "bf26d3110015654b38af3fe00694a6f174eb3df0", "3f9cf02c77a64c46465b9e4eb52069ecac150a56", "71d72b01f7b0366748263440ca6bf8b8ff50b795", "119d3dd7584f92c9da21ec10fe2183164b85a716", "43582587e7ce7b409ed7a83ac672ad375f3675aa", "9cc302f860e60dad82e0a3c5b1c5769a144024a4"]}, {"date": "1938", "abstract": "Quantitative data on the absorption of visible light by the refractive media of the human eye 1 would seem to be important from many points of view. Thus the data may be used for (1) the correction of the photopic ocular visibility curve to a photopic retinal visibility curve, (2) the correction of a scotopic ocular visibility curve to a scotopic retinal visibility curve, (3) a comparison of the absorption spectrum of visual purple with the sensitivity of the eye and (4) the evaluation of various facts and theories concerning vision and color vision. Since an examination of the literature indicated that there were no adequate data available on the absorption spectrum of the ocular media for light of visible wavelengths, we decided that these data should be obtained. Accordingly, about three years ago we set up apparatus, with the aid of which we have determined the combined absorption characteristics", "authors": ["E. J. Ludvigh", "E. F. Mccarthy"], "id": "c62be55de0a0264b3b5882f15ff11569633dae29", "title": "ABSORPTION OF VISIBLE LIGHT BY THE REFRACTIVE MEDIA OF THE HUMAN EYE", "references": []}, {"date": "", "abstract": "THE interesting researches of Prof. S. P. Langley on energy and vision have recently been published in the Memoirs of the American National Academy of Sciences. From this we gather that he was led to in- vestigate the question by the fact that it was not generally recognized how totally different effects may be produced by the same amount of energy in different parts of the spectrum. Two series of experiments were necessary, the first to determine the amount of energy in each ray, the second to observe the corresponding visual effect. The energy was determined as heat by the use of the bolometer, the heat dispersed by a prism being very nearly proportionate to the energy.", "authors": ["P. Lecomte du No\u00fcy"], "id": "7a072146327f54c4fba45b6a459043907f3d8621", "title": "ENERGY AND VISION", "references": []}, {"date": "", "abstract": "1. After a consideration of the existing data and of the sources of error involved, an arrangement of apparatus, free from these errors, is described for measuring the relative energy necessary in different portions of the spectrum in order to produce a colorless sensation in the eye. 2. Following certain reasoning, it is shown that the reciprocal of this relative energy at any wave-length is proportional to the absorption coefficient of a sensitive substance in the eye. The absorption spectrum of this substance is then mapped out. 3. The curve representing the visibility of the spectrum at very low intensities has exactly the same shape as that for the visibility at high intensities involving color vision. The only difference between them is their position in the spectrum, that at high intensities being 48 \u00b5\u00b5 farther toward the red. 4. The possibility is considered that the sensitive substances responsible for the two visibility curves are identical, and reasons are developed for the failure to demonstrate optically the presence of a colored substance in the cones. The shift of the high intensity visibility curve toward the red is explained in terms of Kundt's rule for the progressive shift of the absorption maximum of a substance in solvents of increasing refractive index and density. 5. Assuming Kundt's rule, it is deduced that the absorption spectrum of visual purple as measured directly in water solution should not coincide with its position in the rods, because of the greater density and refractive index of the rods. It is then shown that, measured by the position of the visibility curve at low intensities, this shift toward the red actually occurs, and is about 7 or 8 \u00b5\u00b5 in extent. Examination of the older data consistently confirms this difference of position between the curves representing visibility at low intensities and those representing the absorption spectrum of visual purple in water solution. 6. It is therefore held as a possible hypothesis, capable of direct, experimental verification, that the same substance\u2014visual purple\u2014whose absorption maximum in water solution is at 503 \u00b5\u00b5, is dissolved in the rods where its absorption maximum is at 511 \u00b5\u00b5, and in the cones where its maximum is at 554 \u00b5\u00b5 (or at 540 \u00b5\u00b5, if macular absorption is taken into account, as indeed it must be).", "authors": ["Selig Hecht", "Robert Williams"], "id": "d655b2125b7dcaec8e2e6421643df35cc9f83b25", "title": "THE VISIBILITY OF MONOCHROMATIC RADIATION AND THE ABSORPTION SPECTRUM OF VISUAL PURPLE", "references": ["7a072146327f54c4fba45b6a459043907f3d8621", "48b2f64877ebdc38880c2fa21bd5783f2cced414", "925c89d7f099282b8d847188784848432c24eeff", "f97bffb72c48da2321a172822dc7bdfdbd385af3", "37c70b1055e0b2a85cf49afbf0534d98c247fd7e"]}, {"date": "1953", "abstract": "The ears of anesthetized guinea pigs were exposed to intense tones of 135, 545, 2000, or 8000 cps at sound pressures (at the eardrum) from 138 to 148 db re 0.0002 microbar. The electrical output was recorded by differential intracochlear electrodes before, during, and after the exposures.The injury caused by 8000 cps centers in the basal turn, by 2000 cps in the second turn, by 500 and 185 cps in the third and fourth turns of the cochlea. No single tone, at the intensities and durations employed, injured all of the cochlea.The \u201cthreshold\u201d and also the maximum voltage of the cochlear microphonic are valid indices of the anatomical injury seen under the microscope if the proper test frequencies and positions of electrodes are used.The destructiveness of a tone does not depend on its frequency. Equally severe (probably permanent) injuries were produced by a one\u2010minute exposure at all four frequencies tested when the intensity level was about 144 db at the eardrum.", "authors": ["Hallowell Davis"], "id": "0de3a3441b7c7b55d71aa59a5f779ad4ba119317", "title": "Acoustic Trauma in the Guinea Pig", "references": []}, {"date": "1949", "abstract": "1. \n1. An exponentially rising current excites an isolated single nerve fibre when, and only when, it rises above and crosses the rheobase at a rate of rise greater than the minimal gradient of the fibre. The rheobase is not affected by changes in temperature. The minimal gradient shows a tendency to be increased by cold, but there is a marked hysteresis in the effect of temperature changes on the minimal gradient. \n \n2. \n2. A brief subthreshold shock produces an excitatory state which first rises and then falls after the end of the shock. The time course of this process is retarded by cold. The minimum quantity of electricity needed to excite a single nerve fibre is increased by cold, its temperature coefficient being 1.7 for a change of 10\u00b0C. \n \n3. \n3. At every temperature, the earliest return of excitability occurs immediately after the end of the spike. The process of recovery is affected by temperature changes to the same extent as the spike duration.", "authors": ["Ichiji Tasaki"], "id": "4716f0d81346698ae6aeccc5a7afd05f6f86d46f", "title": "The excitatory and recovery processes in the nerve fibre as modified by temperature changes", "references": []}, {"date": "1937", "abstract": "I. In the last few years there has accumulated a considerable amount of new and highly precise data describing various visual functions. In addition, many of the older measurements have been freshly evaluated, and the whole field of vision has achieved a quantitative form not recognized a generation ago. The first purpose of the present paper is to bring together these recent measurements and evaluations, and to show how easily and strikingly they fall into the pattern of the Duplicity Theory which separates vertebrate vision intlo cone function and rod function. The value of the material is by no means exhausted by this treatment Rather, it is enhanced because of the identification of specific measurements with known morphological units, whose chemical properties may then be investigated. The second purpose of this summary is to deal with the quantitative aspects of the data as they bear on the possible chemical systems present in the retinal receptor elements. II. DUPLICITYTHEORY. The retinas of most vertebrates contain two different types of receptors,cones and rods; and this fact has led to the notion that the vertebrate retina is not one sense organ, but two. Consider the human eye. Structurally the center of the retina is occupied exclusively by cones, while the rest of the retina cont)ains rods and cones, with rods increasingly predominant toward the periphery. Since at high illuminations vision is most efficient with the center of the retina, whereas at low illuminations it is most effective with the periphery, the idea of a double visual organ associates the cones with vision at high light intensities, and the rods with vision at low light intensities. Moreover, since we see color best at high intensities and see no color at low intensities, we may consider the cones as specific receptlors for color, and the rods as the general receptors for light regardless of color. In essentially this form the idea for a retinal double sense organ was proposed by Max Schultze in 1866 on the basis of the histological studies which he made of the vert,ebratle retina, coupled with the physiological", "authors": ["Selig Hecht"], "id": "43582587e7ce7b409ed7a83ac672ad375f3675aa", "title": "RODS, CONES, AND THE CHEMICAL BASIS OF VISION", "references": ["2a638025190cb5cc68f8240089c5bef04dfdac25", "70d3688da247478a181c3da90df8c45db64260b8", "3f9cf02c77a64c46465b9e4eb52069ecac150a56", "f61bc6af0c4979683049c8dfea4cffb99dc3827a", "be5da492a408a8f3fadee4f3cb4f3283c154454a", "b2f56d02bb901b64131ab15eb7e3d5c08ef8f825", "713d139317c19c5e96094e1aed5b85258654d069"]}, {"date": "", "abstract": "Semantic Scholar extracted view of \"A SPACE\u2010TIME PATTERN THEORY OF HEARING\" by Harvey Fletcher", "authors": ["Harvey Fletcher"], "id": "31ef5f9ef4ef6b213091de446185b88e793ba2c8", "title": "A SPACE\u2010TIME PATTERN THEORY OF HEARING", "references": []}, {"date": "1952", "abstract": "EVER SINCE the time of Helmholtz, most physiologists have believed that different parts of the cochlea respond differently to various sound stimuli. Almost all the experimental evidence in support of this belief has been of rather indirect nature, however, and consequently there have been some differences of opinion among physiologists as to just how the behavior of the apical turn of the cochlea differs from that of the basal turn (see 8, 9). The differences might be resolved if we had a technique by which the responses of a part interferin of the g with cochlea could the responses be of recorded, modified or eliminated without the remaining parts of the cochlea. The method of destroying the apical part of the cochlea has frequently been used by previous investigators, but it has several obvious disadvantages. For example, destruction of the apex may change the acoustic properties of the whole cochlea, the method cannot be applied to the other parts of the cochlea, and destruction of the apex may change the excitability of th .e sensory cells in the remaining parts of the cochlea. In the present investigation we undertook to develop micro-techniques by which the condition of a part of the cochlea could be modified in a reversible manner. We have succeeded in recording electrical responses generated in any one of the turns of the cochlea without any significant contamination by the responses from other turns. Administration of an isotonic potassium chloride solution and application of direct current were chosen to modify locally the condition of the cochlea, because these agents are known to change reversibly both resting and action potentials in many other excitable tissues. Actually these agents were found to be very convenient for changing or totally suppressing these electrical signs of cochlear activity in a restricted region where the agents were applied. The results show very clearly that the basal turn of the cochlea generates cochlear microphonics in response to both highand low-frequency sound stimuli, whereas in the apical part high-frequency sound stimuli, applied through the external auditory meatus, do not give any response. Furthermore, the cochlear microphonics recorded from the round window of the guinea pig are the response of the basal turn and give practically no information about the activity of the more distant parts of the cochlea.", "authors": ["Ichiji Tasaki", "C\u00e9sar Fern\u00e1ndez"], "id": "2f5ce95dda4223c7f2aa2dfb75ea17f1a65f3a3b", "title": "Modification of cochlear microphonics and action potentials by KC1 solution and by direct currents.", "references": ["20b5fc21022d808bf1bab0fdd4cf580804eac024", "e8fa4e1afefddbe69516abe396ede3fbb34991ef", "ea1dd97ddf91a02bf993cba47ae3fe46df05064d", "03d7beb512da87419bd79270a55b43522b4b619c", "55ec0a124f5ceb2018d4cf26369a2be186d77009", "f0654cf2ac56ccd94585099569c11d77f5effef2", "7485132b878588ac7135b3e610ac5cb0fd853d16", "447794a882d0b498ba25baecffdd0656fd00a283"]}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Dark-adaptation with especial reference to the problems of night flying.\" by Percy W. Cobb", "authors": ["Percy W. Cobb"], "id": "f5366c22ba53fb8f7aff5d3d6f6c5e9ed1793c60", "title": "Dark-adaptation with especial reference to the problems of night flying.", "references": []}, {"date": "1950", "abstract": "* Contribution No. 1442. t This work was aided by a research grant from the National Heart Institute, of the National Institutes of Health. Dr. Lippman is a Fellow of the John Simon Guggenheim Memorial Foundation. 'Landsteiner, K., The Specificity of Serologial Reactions, Harvard Univ. Press, Cambridge, Mass., 1947. 2 Niven, J. S. F., \"The Action of a Cytotoxic Antiserum on Tissue Cultures,\" J. Path. and Bact., 32, 527 (1929). 3 Lambert, R. A., and Hanes, F. M., \"The Cultivation of Tissues in vitro as a Method for the Study of Cytotoxins,\" J. Exper. Med., 12, 453 (1911). 'Verne, J., and Oberling, C., \"Action des Serums Cytotoxiques sur les Tissus Cultives in vitro,\" Compt. Rend. Soc. Biol., 109, 860 (1932). 6 Harris, M., \"Specificity and Mode of Action of Cytotoxins Produced against Alien Transplants in Rats,\" J. Exper. Zool., 107, 439 (1948). 6 Cameron, G., Tissue Culture Technique, Academic Press, Inc., New York, 1950. 7Chambers, R., and Kempton, R. T., \"Indications of Function of the Chick Mesonephros in Tissue Culture with Phenol Red,\" J. Cell. and Comp. Physiol., 3, 131 (1933). 8 Masugi, M., Sato, Y., Murasawa, S., and Tomizuka, Y., \"tYber die experimentelle Glomerulonephritis durch das spezifische Antinierenserum,\" Tr. Jap. Path. Soc., 22, 614 (1932). 9 Lange, K., Gold, M. M. A., Weiner, D., and Simon, V., \"Autoantibodies in Human Glomerulonephritis,\" J. Clin. Invest., 28, 50 (1949). 10 Addis, T., Glomerular Nephritis, The Macmillan Co., New York, 1948.", "authors": ["H. Davis", "C Fernandez", "D. R. McAuliffe"], "id": "7485132b878588ac7135b3e610ac5cb0fd853d16", "title": "The excitatory process in the cochlea.", "references": ["25d885f95778906cedac840a58efb42f8d135647", "194bf443e1f0deccf20aa79d14b2c09b6aad2f32"]}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Rate of pupillary dilatation and contraction\" by Prentice Reeves", "authors": ["Prentice Reeves"], "id": "356d7b1f5efcacfb7a5968b4597a7a170d0de371", "title": "Rate of pupillary dilatation and contraction", "references": []}, {"date": "1933", "abstract": "I HAVE found vitamin A in considerable concentrations in solutions of the visual purple, in intact retinas, and in the pigment-choroid layers of frogs, sheep, pigs and cattle. The non-saponifiable extracts of these eye tissues display in detail all of the characteristics of vitamin A-containing oils.", "authors": ["George Wald"], "id": "2a638025190cb5cc68f8240089c5bef04dfdac25", "title": "Vitamin A in the Retina", "references": []}, {"date": "", "abstract": "A closing device for movable parts suitable for closing inner edge gaps in the frog and deflection areas of switch points or crossings is disclosed including, in a wedge or filler part, a clamp which is rotatable about an axle and, at the free end thereof facing away from the axis of rotation, has a fishtail-shaped prong end which, on the one hand, engages in a corresponding recess in a bearing surface bottom which is inclined before the recess, a slide rod is guided within a groove, is displaceable along the wedge or filler part, and comprises a barb or hook into which the uppermost prong of the clamp can become engaged so that, during a movement of the slide rod in the direction toward the prong, the clamp will come to be engaged and hence be carried along therewith.", "authors": ["Edgar Douglas Adrian"], "id": "028d4df9ec461a0cdc64c1010fc3a5fcf05f99f4", "title": "The Basis of Sensation", "references": []}, {"date": "1938", "abstract": "Die Regeneration des von der Netzhaut extrahierten Sehpurpurs wurde mit besonderer Rucksicht auf den Gehalt des Extraktes an Stof-fen, die aus dem Pigmentepithel stammen, untersucht, was bis jetzt vollig vernachlassigt geblieben ist. Es ergab sich folgendes: 1. Der Sehpurpur im Extrakt mit pH von 7, 1, der each Z. Saito aus den isolierten Stabchenaussengliedern hergestellt wird, also keine Pigmentepithelstoffe enthalt, oder in gewohnlicher Weise direkt aus der Netzhaut gewonnen, aber mit besonderer Sorgfalt von sicht-baren Pigmentepithelteilen befreit wirde, regeneriert wahrend der Dunkelaufbewahrung nicht, selbst wenn die Bleichung noch nicht das Sehgelb erreicht, sondern zeigt nur eine weitere Bleichung (Nach-bleichung). 2. In einem Extrakt, der in besonderer Weise aus den Stabchen-aussengliedern saint dem Pigmentepithel hergestellt wird, der also gewisse aus dem Pigmentepithel herkommende Stoffe (ausgenommen Fuszinkorner) reichlich enthalten muss, regeneriert der Sehpurpur aus einem gebleichten Zustand, der noch nicht zum Sehgelb gefuhrt ist, und zwar um so vollstandiger, je weniger die Bleichung vorgeschritten ist. Nach einer starkeren Bleichung findet aber auch bei diesem Ex-trakt keine Regeneration mehr statt, sondern nur eine Nachbleichung. Plus diesen Ergebnissen kann geschlossen werden, dass der Seh-purpur nicht nur in der Retina, sondern auch im Extrakt unter be-stimmten Bedingungen aus einem Bleichungszustand regenerieren kann, and dass fur these Regeneration sowohl in der Netzhaut als auch im Extrakt das Vorhandensein gewisser Stoffe aus dem Pigmentepithel, nicht aber das von Pigmentkornern selber, eine notwendige Bedingung darstellt.", "authors": ["Yuji Hosoya", "T. Sasaki"], "id": "1dc5b314b222b3482386851db7ac2cf3e78557a0", "title": "\u00dcber die Regeneration des extrahierten Sehpurpurs", "references": []}, {"date": "1937", "abstract": "Semantic Scholar extracted view of \"AN ACCESSORY PHOTOSENSITIVE SUBSTANCE IN VISUAL PURPLE REGENERATION.\" by Aurin M. Chase", "authors": ["Aurin M. Chase"], "id": "2d0eaf51dfa68f582c73a7fde1c048be8643eb38", "title": "AN ACCESSORY PHOTOSENSITIVE SUBSTANCE IN VISUAL PURPLE REGENERATION.", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"REPORT OF A JOINT DISCUSSION ON VISION\" by Jik", "authors": ["Jik"], "id": "65b103df0e8108412d63e772cba4be198d1e06b1", "title": "REPORT OF A JOINT DISCUSSION ON VISION", "references": []}, {"date": "", "abstract": "1. Ciona possesses two means of responding to an increase in the intensity of illumination. One is by means of a local reaction; the other is by a retraction reflex of the body as a whole. 2. The \"ocelli\" are not photoreceptors. The photosensitive area is in the intersiphonal region containing the neural mass. This area contains no pigment. 3. The reaction time to light is composed of a sensitization period during which Ciona must be exposed to the light, and of a latent period during which it need not be illuminated in order to react to the stimulus received during the sensitization period. 4. The duration of the reaction time varies inversely as the intensity. Analysis shows the latent period to be constant. The relation between the sensitization period and the intensity follows the Bunsen-Roscoe rule. 5. During dark adaptation the reaction time is at first large, then it decreases until a constant minimum is reached. 6. A photochemical system consisting of a reversible reaction is suggested in order to account for the phenomena observed. This system includes a photosensitive substance and its precursor, the dynamics of the reaction following closely the peculiarities of the photosensitivity of Ciona. 7. It is shown that in order to produce a reaction, a constant ratio must be reached between the amount of sensitive substance broken down by the stimulus and the amount previously broken down. 8. From the chemical system suggested certain experimental predictions were made. The actual experiments verified these predictions exactly. 9. The results obtained with regularly repeated stimulation not only fail to show any basis for a learning process or for the presence of a \"higher behavior,\" but follow the requirements of the photochemical system suggested before.", "authors": ["Selig Hecht"], "id": "b3fa455033251b49d96f443d4d18e8d4a33c94b5", "title": "THE PHOTIC SENSITIVITY OF CIONA INTESTINALIS", "references": ["1e00758bfb2d089bde4cce8e9208bc0f07da90ec", "ba6f010219b84a012bdaa30ed0d8da8f5054c1ab", "d3d55c333617049c488857e81066b01b7ef235d9", "3fb57e5a8cca13680528afb4d605bab20683c086", "f899d55c658ed98a0ebb540bc71cc2bb9a919782", "0de396cd5bc75475f9d95df2c33ed13296e0236d", "d903899b58736b34c43b8884372602c537dc4b78"]}, {"date": "", "abstract": "WITH reference to the comment on my lecture at the Royal Society of Arts in NATURE of February 17, I did not give the evidence in favour of the visual purple being the visual stimulus transformer on account of the time at my disposal. This evidence is very strong, and the facts are inexplicable on any other hypothesis. Many physiologists have tried to assign different functions to the rods and cones, but these theories have failed because all the functions which were said to be the exclusive property of the rods have been found, only gradually diminished, in the fovea, in which only cones are present. For instance, von Tschermak, Hering, Hess, Garten, and others, have found the Purkinje phenomenon, the variation in optical white equations by a state of light and dark adaptation, the colourless interval for spectral lights of increasing intensity, the varying phases of the after-image, in the fovea, only gradually diminished. The complete absence of any qualitative change between the foveal and extra-foveal regions is a very important fact in support of the hypothesis that the visual purple is the visual substance. There is also the fact, mentioned by Helmholtz, that a perceptible interval elapses before we see with the fovea, after the rest of the retina, when the eye has been previously some time in darkness. Hess has also pointed out that the recurrent image is present, but retarded at the yellow spot. All these observations and many others agree with my statement that the visual purple can be seen between but not in the cones of the fovea.", "authors": ["Frederick William Edridge-Green"], "id": "3305fbeabc5754a3f10055ae44007e9851682005", "title": "Vision and Colour Vision", "references": []}, {"date": "1936", "abstract": "This paper records the measurement of the absorption curves of a number of new forms of visual purple which have been found among fishes. For reasons which will be given immediately it was hoped that we could correlate these results either with the habits of the species or with the microstructure of their retinae, but the attempt was not successful. We believe the results in themselves to be important because they show that light-sensitive substances of varying absorptions can be found in the retina and it may not be impossible to discover substances which form intermediate links in the perception of colour. The retinae of fishes have attracted considerable attention from histologists because of their large and easily studied visual cells and the extent of their photomechanical changes. The results reported here on the types, distribution, and behaviour of the visual cells in the species examined are not claimed to be either exhaustive or final. The histology was carried out under difficulties and it was only possible to examine a few specimens of each species. It was found, however, that where other workers have also reported on the same retina (as in the elasmobranchs, eel, trout, etc.) the observations were always in agreement.", "authors": ["L. E. Bayliss", "Richard James Lythgoe", "Katharine Tansley"], "id": "2300a0ac874784d04bf799711142f625289347f1", "title": "Some New Forms of Visual Purple Found in Sea Fishes with a Note on the Visual Cells of Origin", "references": []}, {"date": "1936", "abstract": "1. Visual purple from the sea robin, sea bass, and scup is almost identical spectroscopically with that from frogs. The interrelations of this pigment with vitamin A and retinene are also the same as in the frog. 2. In strong acids or at pH > 11, the visual yellow of sea robin retinas is converted irreversibly into a pH indicator, yellow in acid and almost colorless in alkaline solution. Unlike neutral visual yellow, the indicator is not removed to form either vitamin A or visual purple. In the ammoniacal retina the reversion of visual yellow itself to purple is accelerated. 3. The combined pigment epithelium and choroid layer in these fishes contain vitamin A, flavine, and an unidentified xanthophyll.", "authors": ["George Wald"], "id": "71d72b01f7b0366748263440ca6bf8b8ff50b795", "title": "PIGMENTS OF THE RETINA: II. SEA ROBIN, SEA BASS, AND SCUP", "references": []}, {"date": "1936", "abstract": "1. Visual purple from the sea robin, sea bass, and scup is almost identical spectroscopically with that from frogs. The interrelations of this pigment with vitamin A and retinene are also the same as in the frog. 2. In strong acids or at pH > 11, the visual yellow of sea robin retinas is converted irreversibly into a pH indicator, yellow in acid and almost colorless in alkaline solution. Unlike neutral visual yellow, the indicator is not removed to form either vitamin A or visual purple. In the ammoniacal retina the reversion of visual yellow itself to purple is accelerated. 3. The combined pigment epithelium and choroid layer in these fishes contain vitamin A, flavine, and an unidentified xanthophyll.", "authors": ["George Wald"], "id": "9cc302f860e60dad82e0a3c5b1c5769a144024a4", "title": "PIGMENTS OF THE RETINA", "references": ["73f84a8c919fa625b134447d9356e3b21cbe4701", "3f9cf02c77a64c46465b9e4eb52069ecac150a56", "0aaa302f096c1ba0cb08fd95ea6035970c0ea887", "7d39b657c7980bac13f2ef43de8f9e2972a676ec", "297634ffea453e702e1c0364b9cc4e6a2b6b6105", "119d3dd7584f92c9da21ec10fe2183164b85a716"]}, {"date": "1932", "abstract": "On the basis of previous knowledge of the photosensory behavior of Mya it is shown that Talbot's law for the effectiveness of stimulation by intermittent illumination should be valid. Two series of measurements are reported in which the photosensory effects of intermittent and continuous illuminations are compared. The results demonstrate the validity of Talbot's law for Mya.", "authors": ["Selig Hecht", "Ernst Dipl Ing Wolf"], "id": "b2f56d02bb901b64131ab15eb7e3d5c08ef8f825", "title": "INTERMITTENT STIMULATION BY LIGHT : I. THE VALIDITY OF TALBOT'S LAW FOR MYA.", "references": []}, {"date": "1937", "abstract": "1. The reality of a chemical cycle proposed to describe the rhodopsin system is tested with dark adaptation measurements. 2. The first few minutes of rod dark adaptation are rapid following short, slower following long irradiation. As dark adaptation proceeds, the slow process grows more prominent, and occupies completely the final stages of adaptation. 3. Light adaptation displays similar duality. As the exposure to light of constant intensity lengthens, the visual threshold rises, and independently the speed of dark adaptation decreases. 4. These results conform with predictions from the chemical equations.", "authors": ["George Wald", "A. Clark"], "id": "6b1e8b2b3567e14a288ad8ed623f2bf008c42f02", "title": "VISUAL ADAPTATION AND CHEMISTRY OF THE RODS", "references": ["93dcf2163619e7c71247f2f37f3656c40f437e1b", "925c89d7f099282b8d847188784848432c24eeff", "119d3dd7584f92c9da21ec10fe2183164b85a716", "9343cd2e1a4ba35283a6fc0120071edbc34d10f0", "9e206fb53baa54899239195145fa2fe075ad310f", "e3321fa5b05da7fd8eb699d8fd18545bc9336c06", "199dc9e369569dc8fdb2c782c763da82785e9ca9"]}, {"date": "1936", "abstract": "Semantic Scholar extracted view of \"Spatial and Binocular Effects in Human Intensity Discrimination\" by James R. Smith", "authors": ["James R. Smith"], "id": "70d3688da247478a181c3da90df8c45db64260b8", "title": "Spatial and Binocular Effects in Human Intensity Discrimination", "references": []}, {"date": "1932", "abstract": "1. An extension of a previously described method makes possible the measurement of the visibility function of Lepomis at high intensities of spectral illumination. This is accomplished by determining the relative energies of various spectral beams which will just produce a visual orienting response by the animal to the movement of a pattern composed of fine lines. 2. The function so determined is different from that obtained with a pattern composed of wide bars and spaces at a lower intensity level. 3. This difference furnishes direct and quantitative proof that the eye of Lepomis is a physiologically duplex visual system and parallels the known anatomical distinctions between the rods and cones. 4. A comparison of the visibility curves of the two systems indicates that both functions are similar in shape but that the cone curve is shifted to the red. 5. It is suggested that this relation between the two systems, which is also found in the human and the fowl, indicates that the photosensory substance is the same in each case for the rods and cones. According to this hypothesis, the shift of the cone curve is due to a common physical cause which depends on differences in the properties of the solvent media in the cones and in the rods.", "authors": ["Harry Grundfest"], "id": "be5da492a408a8f3fadee4f3cb4f3283c154454a", "title": "THE SPECTRAL SENSIBILITY OF THE SUN-FISH AS EVIDENCE FOR A DOUBLE VISUAL SYSTEM", "references": ["affff404e7de938b81949d9baea9d51ef97fc6c0", "f61bc6af0c4979683049c8dfea4cffb99dc3827a", "d655b2125b7dcaec8e2e6421643df35cc9f83b25", "b68223017564ce160b65e189ddb736ee26cd3bf4", "e6ee1fb9922437326425c389988f0ba59cda95c9", "f187d363ddb6643e0387a8e0a37b65159db59827"]}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Brilliance and Chroma in Relation to Zone Theories of Vision\" by Leonard Thompson Troland", "authors": ["Leonard Thompson Troland"], "id": "37c70b1055e0b2a85cf49afbf0534d98c247fd7e", "title": "Brilliance and Chroma in Relation to Zone Theories of Vision", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"\u00dcber die Wirkung verschiedener Farbstoffe auf das Verhalten des Bromsilbers gegen das Sonnenspectrum\" by Joseph Maria Eder", "authors": ["Joseph Maria Eder"], "id": "f97bffb72c48da2321a172822dc7bdfdbd385af3", "title": "\u00dcber die Wirkung verschiedener Farbstoffe auf das Verhalten des Bromsilbers gegen das Sonnenspectrum", "references": []}, {"date": "1949", "abstract": "Semantic Scholar extracted view of \"The response pattern of the pacinian corpuscle.\" by Dennis Michael Millie Scott", "authors": ["Dennis Michael Millie Scott"], "id": "25d885f95778906cedac840a58efb42f8d135647", "title": "The response pattern of the pacinian corpuscle.", "references": []}, {"date": "1949", "abstract": "Semantic Scholar extracted view of \"The electric response at a sensory nerve ending.\" by Bernard Katz", "authors": ["Bernard Katz"], "id": "194bf443e1f0deccf20aa79d14b2c09b6aad2f32", "title": "The electric response at a sensory nerve ending.", "references": []}, {"date": "1950", "abstract": "A preparation of a single Pacinian corpuscle in the cat\u2019s mesentery has been used to study the initiation of nerve impulses in sensory endings. The minimum movement of a mechanical stimulator required to excite a single corpuscle has been found to be 0\u22c55\u03bc in 100 \u03bcsec. It has been difficult to produce repetitive discharges with rectangular pulses of long duration, either mechanical or of constant current. The latency between a mechanical stimulus and the initiation of an impulse has a value around 1\u22c55 msec, for threshold stimuli, and this decreases to a minimum value around 0\u22c55 msec, as the stimulus is increased; it is altered only slightly, if at all, by changes in the duration of the maintained displacement of the mechanical stimulator. Subthreshold mechanical stimuli have been shown to facilitate stimulation by electrical test shocks. The return of excitability at the ending is independent of the nature of the conditioning stimulus and varies but little with the nature of the test shock. The value of the latency at threshold is unaffected by the relatively refractory state. The relations of these results to various hypotheses are discussed, and it is suggested that these results can all be accounted for in terms of the known properties of axons.", "authors": ["Jeffrey A. Gray", "J Laurence Malcolm"], "id": "447794a882d0b498ba25baecffdd0656fd00a283", "title": "The initiation of nerve impulses by mesenteric Pacinian corpuscles", "references": ["55e8651081cf494d80aa68d7182c77309ab70f6d"]}, {"date": "1939", "abstract": "Semantic Scholar extracted view of \"CHANGES IN RETINAL EXCITABILITY DUE TO POLARIZATION AND SOME OBSERVATIONS ON THE RELATION BETWEEN THE PROCESSES IN RETINA AND NERVE\" by Ra\u01f5nar Granit et al.", "authors": ["Ra\u01f5nar Granit", "Toivo Helme"], "id": "e8fa4e1afefddbe69516abe396ede3fbb34991ef", "title": "CHANGES IN RETINAL EXCITABILITY DUE TO POLARIZATION AND SOME OBSERVATIONS ON THE RELATION BETWEEN THE PROCESSES IN RETINA AND NERVE", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"AUDITORY NERVE IMPULSES.\" by Ernest Glen Wever et al.", "authors": ["Ernest Glen Wever", "C W Bray"], "id": "20b5fc21022d808bf1bab0fdd4cf580804eac024", "title": "AUDITORY NERVE IMPULSES.", "references": []}, {"date": "1983", "abstract": "Semantic Scholar extracted view of \"Hearing, its psychology and physiology\" by S. S. Stevens et al.", "authors": ["S. S. Stevens", "Hallowell Davis"], "id": "03d7beb512da87419bd79270a55b43522b4b619c", "title": "Hearing, its psychology and physiology", "references": []}, {"date": "1951", "abstract": "Semantic Scholar extracted view of \"Theory of hearing\" by E LlanezaRodriguez", "authors": ["E LlanezaRodriguez"], "id": "f0654cf2ac56ccd94585099569c11d77f5effef2", "title": "Theory of hearing", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"POINTS RELATING TO THE WEBER-FECHNER LAW. RETINA; MUSCLE; NERVE\" by Augustus D\u00e9sir\u00e9 Waller", "authors": ["Augustus D\u00e9sir\u00e9 Waller"], "id": "0de396cd5bc75475f9d95df2c33ed13296e0236d", "title": "POINTS RELATING TO THE WEBER-FECHNER LAW. RETINA; MUSCLE; NERVE", "references": []}, {"date": "", "abstract": "here gives of his own and cognate investigations and experiments, and in the conclusions which he feels justified in drawing from them. Starting with a consideration of reflexes, Professor Loeb enquires what part the central ganglia play in their production. He is of opinion that we must deprive them of all specific significance, not only in the case of simple reflexes and instincts, but also in that of spontaneous movements. Their importance lies in the fact that they, in common with other nerve structures, possess in an enhanced degree that irritability and conductibility which is a characteristic of all protoplasm. He seems at times", "authors": ["Jacques Loeb"], "id": "f899d55c658ed98a0ebb540bc71cc2bb9a919782", "title": "Comparative Physiology of the Brain and Comparative Psychology", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Ascidians of the littoral zone of southern California\" by Wm. E. Ritter et al.", "authors": ["Wm. E. Ritter", "Raymond A. Forsyth"], "id": "d903899b58736b34c43b8884372602c537dc4b78", "title": "Ascidians of the littoral zone of southern California", "references": []}, {"date": "1936", "abstract": "1. The interrelations of visual purple, retinene, and vitamin A in the bull frog retina are analyzed in simple experiments, the results of which are presented in a series of automatically recorded spectra. 2. Observations are reported upon the distributions, properties, and concentrations of xanthophyll, vitamin A, and flavine in the pigmented tissues of the eye.", "authors": ["George Wald"], "id": "297634ffea453e702e1c0364b9cc4e6a2b6b6105", "title": "PIGMENTS OF THE RETINA I. THE BULL FROG", "references": ["54403dc9e5e790fc3b51f2e751d0214b04337f31", "7505a75833a905e82f426746d71bcc8c18f1b5f9", "cff861e53b8e23fc2faf96e146917d4b61494367", "c04a1370b3527b749f79f023b7afc2ab45aafad9", "119d3dd7584f92c9da21ec10fe2183164b85a716"]}, {"date": "1936", "abstract": "1. The interrelations of visual purple, retinene, and vitamin A in the bull frog retina are analyzed in simple experiments, the results of which are presented in a series of automatically recorded spectra. 2. Observations are reported upon the distributions, properties, and concentrations of xanthophyll, vitamin A, and flavine in the pigmented tissues of the eye.", "authors": ["George Wald"], "id": "7d39b657c7980bac13f2ef43de8f9e2972a676ec", "title": "PIGMENTS OF THE RETINA", "references": ["cff861e53b8e23fc2faf96e146917d4b61494367", "c04a1370b3527b749f79f023b7afc2ab45aafad9", "119d3dd7584f92c9da21ec10fe2183164b85a716"]}, {"date": "1933", "abstract": "Semantic Scholar extracted view of \"A study of variations in the amount of yellow pigment (xanthophyll) in certain fishes, and of the possible effects upon this of colored backgrounds\" by Francis Bertody Sumner et al.", "authors": ["Francis Bertody Sumner", "Denis L. Fox"], "id": "0aaa302f096c1ba0cb08fd95ea6035970c0ea887", "title": "A study of variations in the amount of yellow pigment (xanthophyll) in certain fishes, and of the possible effects upon this of colored backgrounds", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"2. Some observations on carotenoid colour substances of fishes.\" by Einar Lonnberg", "authors": ["Einar Lonnberg"], "id": "73f84a8c919fa625b134447d9356e3b21cbe4701", "title": "2. Some observations on carotenoid colour substances of fishes.", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"A reexamination of the applicability of the Bunsen\u2010Roscoe law to the phenomena of animal heliotropism\" by Jacques Loeb et al.", "authors": ["Jacques Loeb", "Hardolph A. Wasteneys"], "id": "3fb57e5a8cca13680528afb4d605bab20683c086", "title": "A reexamination of the applicability of the Bunsen\u2010Roscoe law to the phenomena of animal heliotropism", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"ADAPTATION IN THE PHOTOSENSITIVITY OF CIONA INTESTINALIS.\" by Selig Hecht", "authors": ["Selig Hecht"], "id": "d3d55c333617049c488857e81066b01b7ef235d9", "title": "ADAPTATION IN THE PHOTOSENSITIVITY OF CIONA INTESTINALIS.", "references": []}, {"date": "", "abstract": "An apparatus for testing both the light minimum and the light difference in dark adaptation and the use of this apparatus in the early diagnosis of glaucoma have been described in previous publications by Bovie;1by Waite, one of us (G. S. D.) and Kirk,2and by two of us (G. S. D. and P. A. C.) and O'Brien.3In the last of this series of papers, it was shown that a test of the light minimum is of greater value in the diagnosis of glaucoma than is a test of the light difference. Since, therefore, only the light minimum need be tested, it becomes possible to devise an instrument simpler in construction than the one previously described, which may be used in the ordinary dark room of a physician's office. NECESSARY CHARACTERISTICS OF INSTRUMENT The essential and desirable characteristics of an instrument by means of which accurate", "authors": ["George S. Derby", "Paul A. Chandler", "Louise L. Sloan"], "id": "e3321fa5b05da7fd8eb699d8fd18545bc9336c06", "title": "A Portable Adaptometer.", "references": []}, {"date": "1936", "abstract": "Semantic Scholar extracted view of \"Dark Adaptation after Varying Degrees of Light Adaptation\" by Charles P. Winsor et al.", "authors": ["Charles P. Winsor", "A. Clark"], "id": "9e206fb53baa54899239195145fa2fe075ad310f", "title": "Dark Adaptation after Varying Degrees of Light Adaptation", "references": []}, {"date": "1932", "abstract": "1. A test is proposed of the hypothesis that visual purple is the photosensitive substance concerned in dim vision. It is based on the fact that fish visual purple is different from that of other vertebrates. If the hypothesis is correct, the fish dim-visibility function should be different from that of other vertebrates and should be determined by the absorption spectrum of its visual purple. 2. A new method is described for obtaining the visibility function of fish, in quantitative terms. It depends on the measurement of the least amounts of various spectral energies which will produce a visual orienting response to the displacement of a constant background. 3. Data are presented on thirteen animals. It is shown that the maximum of the visibility function is identical with the maximum of the absorption spectrum of fish visual purple. The shapes of the visibility curves obtained are, however, variable and different from that of the absorption spectrum. 4. The possibility that Lepomis visual purple is different from that of other fish is ruled out by a series of measurements which confirm the results of Koettgen and Abelsdorff on other fish. 5. Reasons are given for the conclusion that there are present in Lepomis special conditions which distort the visibility curve out of true agreement with that predictable from the absorption spectrum of its visual purple. The suggestion is made that the presence of light absorbing, but not light sensitive, pigments is responsible for this distortion. One of these pigments may perhaps be carotin while the second is unspecified.", "authors": ["Harry Grundfest"], "id": "e6ee1fb9922437326425c389988f0ba59cda95c9", "title": "THE SENSIBILITY OF THE SUN-FISH, LEPOMIS, TO MONOCHROMATIC RADIATION OF LOW INTENSITIES", "references": ["90153dc373ba0ae5c93538593d80439d3679f840", "affff404e7de938b81949d9baea9d51ef97fc6c0", "f61bc6af0c4979683049c8dfea4cffb99dc3827a", "f187d363ddb6643e0387a8e0a37b65159db59827", "d655b2125b7dcaec8e2e6421643df35cc9f83b25"]}, {"date": "1937", "abstract": "Semantic Scholar extracted view of \"Dark adaptation in the frog eye as determined by the electrical response of the retina\" by Lorrin A. Riggs", "authors": ["Lorrin A. Riggs"], "id": "9343cd2e1a4ba35283a6fc0120071edbc34d10f0", "title": "Dark adaptation in the frog eye as determined by the electrical response of the retina", "references": []}, {"date": "", "abstract": "L Begrenzung der Aufgabe. ]. Q u a l i t a t i v e u n d q u a n t i t a t i v e L i c h t w i r k u n g (S. 5). Unmbglichkeit einer unmittelbaren Messung der Farbwirkung. Das Seebeck-Holmgrensche Prinzip. Notwendigkeit der Untersuchung der quantitativen Lichtwirkung. 2, Zie le de r E r f o r s c h u n g des H e t l i g k e i t s s i n n e s (S. 8). Relative Helligkeitswerte. Relative Empfindlichkeitswerte. Absolute Empfindliehkeitswerte.", "authors": ["H. L. Honigmann"], "id": "affff404e7de938b81949d9baea9d51ef97fc6c0", "title": "Untersuchungen \u00fcber Lichtempfindlichkeit und Adaptierung des Vogelauges", "references": []}, {"date": "1937", "abstract": "Semantic Scholar extracted view of \"Electrical signs of nervous activity\" by Joseph Erlanger et al.", "authors": ["Joseph Erlanger", "Herbert S. Gasser"], "id": "55e8651081cf494d80aa68d7182c77309ab70f6d", "title": "Electrical signs of nervous activity", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Notes on the reactions of bivalve mollusks to changes in light intensity: Image formation in pecten.\" by David Henry Wenrich", "authors": ["David Henry Wenrich"], "id": "ba6f010219b84a012bdaa30ed0d8da8f5054c1ab", "title": "Notes on the reactions of bivalve mollusks to changes in light intensity: Image formation in pecten.", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Heliotropic Animals as Photometers on the Basis of the Validity of the Bunsen-Roscoe Law for Heliotropic Reactions.\" by J Loeb et al.", "authors": ["J Loeb", "John H. Northrop"], "id": "1e00758bfb2d089bde4cce8e9208bc0f07da90ec", "title": "Heliotropic Animals as Photometers on the Basis of the Validity of the Bunsen-Roscoe Law for Heliotropic Reactions.", "references": []}, {"date": "", "abstract": "WHEN Prof. Kayser published the first volume of his \u201cHandbuch der Spectroscopie,\u201d he said that the third volume would be devoted to absorption spectra and cognate phenomena. He has, however, found it necessary to treat the subject in two volumes, the first of which contains the methods of investigation of absorption spectra, the variability of absorption, the connection between absorption and chemical constitution, and, finally, a list of all the measurements of the absorption spectra of inorganic and artificial organic substances. In the next volume the absorption of the natural colouring matters in the animal and vegetable kingdom will be described, together with the relation of dispersion and fluorescence to absorption and, lastly, phosphorescence. The present volume is peculiarly interesting, as it deals to a great extent with the application of spectroscopy to chemical and physicochemical problems.Handbuch der Spectroscopie.By Prof. H. Kayser. Vol. iii. Pp. viii + 604. (Leipzig: S. Hirzel, 1905.)", "authors": ["E. C. S. B."], "id": "48b2f64877ebdc38880c2fa21bd5783f2cced414", "title": "Handbuch der Spectroscopie", "references": []}, {"date": "1931", "abstract": "Semantic Scholar extracted view of \"Specificity in tests for vitamin A. A new conception of the chromogenic constituents of fresh and aged liver oils.\" by Isidor Morris Heilbron et al.", "authors": ["Isidor Morris Heilbron", "A. E. Gillam", "Richard Alan Morton"], "id": "c04a1370b3527b749f79f023b7afc2ab45aafad9", "title": "Specificity in tests for vitamin A. A new conception of the chromogenic constituents of fresh and aged liver oils.", "references": []}, {"date": "1938", "abstract": "Semantic Scholar extracted view of \"AN EXPERIMENT IN HUMAN DIETARY NIGHT-BLINDNESS\" by George Wald et al.", "authors": ["George Wald", "Harold Jeghers", "Joseph Arminio"], "id": "deba436a8502a60eb3cb8a2a41365f0f3d7ed9b6", "title": "AN EXPERIMENT IN HUMAN DIETARY NIGHT-BLINDNESS", "references": []}, {"date": "1936", "abstract": "The invention proposes an apparatus (and method) for transferring data between a first device (1) and a memory area of memory means (3a; F_REG) of a second device (3), the apparatus comprising buffer registers for temporarily storing the data (DATA) to be transferred and the address (ADDR) of the memory area to and/or from which the data are to be transferred, and a control means (EL, CTRL) for controlling said buffer registers, characterized by at least two groups of buffer registers ([DATA_REG1, ADD_REG1], [DATA_REG2, ADD_REG2]) for storing data and associated addresses transmitted in consecutive data transfer operations, and in that said control means (CTRL) is adapted to generate a control signal (ENABLE) for alternately switching between a first group of buffer registers ([DATA_REG1, ADD_REG1]) and a second group of buffer registers ([DATA_REG2, ADD_REG2]) after each of a respective one of consecutive data transfer operations.", "authors": ["Selig Hecht", "Aurin M. Chase", "Simon Shlaer", "Charles Haig"], "id": "e2f4fa12b3eb898f5ef3ed8262ee011d17ec85ca", "title": "THE REGENERATION OF VISUAL PURPLE IN SOLUTION.", "references": []}, {"date": "", "abstract": "1. Visual purple solutions are prepared under such conditions that the bleaching reaction is irreversible. 2. A method is described for the colorimetric estimation of very small quantities of visual purple. By this means the kinetics of the bleaching reaction are investigated. 3. The results show that the course of the decomposition follows that of a monomolecular reaction, without any measurable period of induction or after effect.", "authors": ["Selig Hecht"], "id": "67e10608a5348cdca95cfab913f3b2dbe49d715b", "title": "PHOTOCHEMISTRY OF VISUAL PURPLE I. THE KINETICS OF THE DECOMPOSITION OF VISUAL PURPLE BY LIGHT.", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"A New Recording Spectrophotometer\" by Arthur C. Hardy", "authors": ["Arthur C. Hardy"], "id": "cff861e53b8e23fc2faf96e146917d4b61494367", "title": "A New Recording Spectrophotometer", "references": []}, {"date": "1931", "abstract": "Semantic Scholar extracted view of \"Zur Kenntnis des Vitamins\u2010A aus Fischtranen II\" by Peter Karrer et al.", "authors": ["Peter Karrer", "R. Morf", "K. Schoepp"], "id": "7505a75833a905e82f426746d71bcc8c18f1b5f9", "title": "Zur Kenntnis des Vitamins\u2010A aus Fischtranen II", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"The Determination of Hydrogen Ions\" by Christopher S. McKee", "authors": ["Christopher S. McKee"], "id": "b52c8240d1f66d2afd57be9aca398edf3eb55986", "title": "The Determination of Hydrogen Ions", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Bestimmung von Carotinoiden.\" by Richard J. Kuhn et al.", "authors": ["Richard J. Kuhn", "Hans Brockmann"], "id": "54403dc9e5e790fc3b51f2e751d0214b04337f31", "title": "Bestimmung von Carotinoiden.", "references": []}, {"date": "1937", "abstract": "A NEUTRAL solution of visual purple exposed to bright light bleaches almost instantly to a deep orange colour, then more slowly to pale yellow. This transition has been believed to be wholly photochemical; the orange material a mixture of unbleached visual purple with the final yellow residue1. Both assumptions are mistaken: the orange photo-product is new pigment, which fades to yellow in complete darkness.", "authors": ["George Wald"], "id": "961d120af1768287142f0f77c60a33f5c499f2bd", "title": "Bleaching of Visual Purple in Solution", "references": []}, {"date": "1936", "abstract": "Visual purple from winter frogs shows an intermediate yellow color during bleaching by light; summer extractions do not. This seasonal effect can be duplicated by variations in the hydrogen ion concentration and in the temperature of the solutions. Increasing the pH approximates the summer condition, while decreasing the pH approximates the winter condition. Temperature has no effect on the bleaching of alkaline solutions but greatly influences acid solutions. At low temperatures the bleaching of add solutions resembles the winter condition, while at higher temperatures it resembles the summer condition. A photic decomposition product of frog retinal extractions is an acid-base indicator: it is yellow in acid and colorless in alkaline solution. Its color is not dependent upon light. The hydrogen ion concentration of visual purple solutions does not change under illumination, nor is there a difference in the pH of summer and winter extractions. Bile salt extractions of visual purple are usually slightly acid. The conflicting results of past workers regarding the appearance of \"visual yellow\" may be due to seasonal variation with its differences in temperature, or to the presence of base in the extractions. It is also possible that vitamin A may be a factor in the seasonal variation. The photic decomposition of visual purple in bile salts solution, extracted from summer frogs, follows the kinetics of a first order reaction. Visual purple from winter frogs does not conform to first order kinetics. Photic decomposition of alkaline, winter visual purple extractions also follows a first order equation. Acid, winter extractions appear to conform to a second order equation, but this is probably an artefact due to interference by the intermediate yellow.", "authors": ["Aurin M. Chase"], "id": "3f9cf02c77a64c46465b9e4eb52069ecac150a56", "title": "ANOMALIES IN THE ABSORPTION SPECTRUM AND BLEACHING KINETICS OF VISUAL PURPLE", "references": ["2a638025190cb5cc68f8240089c5bef04dfdac25", "67e10608a5348cdca95cfab913f3b2dbe49d715b", "61d7f4c81e066d1d13731a7576905d78c581e892", "0610515e2f945f964bcf295b101b7191359d5c15"]}, {"date": "1934", "abstract": "SINCE reporting the occurrence of vitamin A in the eye tissues of the frog and several mammals1, I have examined the carotenoids of the frog's eye in detail.", "authors": ["George Wald"], "id": "61d7f4c81e066d1d13731a7576905d78c581e892", "title": "Carotenoids and the Vitamin A Cycle in Vision", "references": []}, {"date": "", "abstract": "It is shown that the velocity of bleaching of visual purple by light, under comparable conditions of concentration, volume, and surface exposed, is directly proportional to the intensity.", "authors": ["Selig Hecht"], "id": "90153dc373ba0ae5c93538593d80439d3679f840", "title": "PHOTOCHEMISTRY OF VISUAL PURPLE", "references": ["a7343bbc77cc73d6e534d725d6c977a2c7cfbd12", "67e10608a5348cdca95cfab913f3b2dbe49d715b", "d655b2125b7dcaec8e2e6421643df35cc9f83b25", "4b2f0c8264dac00ac741c7a92b1d7450c1231974", "925c89d7f099282b8d847188784848432c24eeff"]}, {"date": "1933", "abstract": "The duplicity theory of vision ascribes the sensitivity of the eye to illuminations below 0\u00b701 f. c. to the bleaching of the visual purple contained in tire rods of the retina and the phenomenon of dark adaptation is explained as being due to the regeneration of this substance. It has been shown that there is a close correlation between the rate of regeneration of visual purple and tire rate of dark adaptation and that conditions which affect one will also affect the other (Tansley, 1931). Hecht (1920) by mathematical reasoning from the curves of dark adaptation in man has come to the condition that the regeneration of visual purple must be the result of a bimolecular reaction probably taking place is the retina. He believes that visual purple is broken down under the action of light into two colourless substances which can recombine in darkness to form visual purple again (Hecht, 1919).", "authors": ["Katharine Tansley"], "id": "6b7a91a0e6b67b3ad7d1f706b83664d6ec83c0c6", "title": "Factors Affecting the Development and Regeneration of Visual Purple in the Mammalian Retina", "references": []}, {"date": "", "abstract": "During the dark adaptation of the human eye, its visual threshold decreases to a small fraction of its original value in the light. An analysis of the quantitative data describing this adaptation shows that it follows the course of a bimolecular chemical reaction. On the basis of these findings it is suggested that visual reception in dim light is conditioned by a reversible photochemical reaction involving a photosensitive substance and its two products of decomposition. Accordingly, dark adaptation depends on the course of the \"dark\" reaction during which the two products of decomposition reunite to synthesize the original photosensitive substance.", "authors": ["Selig Hecht"], "id": "925c89d7f099282b8d847188784848432c24eeff", "title": "THE DARK ADAPTATION OF THE HUMAN EYE", "references": ["03697b3dea816ce6ec9847ca5d5da77e8cebd77f", "253f61c801f51f91e1b2dd22e3039f3e9b26b980", "cde3aa2ad7b6a74eac778d7f661eafed5564aa9d"]}, {"date": "1935", "abstract": "1. Vitamin A has been found in the retinas and the combined pigment epithelia and choroid layers of frogs, pigs, sheep, and cattle. The vitamin was identified by (a) its specific absorption at 328 m\u00b5; (b) the blue color yielded with antimony trichloride, associated with an absorption band at about 620 m\u00b5; (c) anti-xerophthalmic and growth-promoting activity; and (d) quantitative relationships among the results of these three types of observation. 2. The mammalian retinas contain about 22\u03b3, the frog retinas about 400\u03b3, and the frog pigmented layers almost 2 mg. of vitamin A per gram of dry tissue. 3. With the possible exception of hepaxanthin, no other carotenoids were found in the mammalian tissues.", "authors": ["George Wald"], "id": "90d0610506d0320066d15382d9bacd1d632c4ac5", "title": "VITAMIN A IN EYE TISSUES", "references": ["2a638025190cb5cc68f8240089c5bef04dfdac25", "d9780629977f55b798e4588eee255a0b1f59b590", "6b7a91a0e6b67b3ad7d1f706b83664d6ec83c0c6"]}, {"date": "1934", "abstract": "Semantic Scholar extracted view of \"A Theoretical Basis for Intensity Discrimination in Vision.\" by Selig Hecht", "authors": ["Selig Hecht"], "id": "8f1c1fc5c79ab4c319f07dbe1a29e5750d93a9f9", "title": "A Theoretical Basis for Intensity Discrimination in Vision.", "references": []}, {"date": "", "abstract": "In the first part of this article, results of an experimental investigation of the action of a light-source in the field of view in changing the adaptation of the fovea of the eye, and in increasing the minimum perceptible brightness difference are presented. In the second part are presented results of a study to determine whether a given light-source in the field of view is an advantage or disadvantage; and also a method is presented for expressing visibility by an exact ratio.", "authors": ["L. L. Holladay"], "id": "b5b3dcb2cd3d40b03caafe7740d3ab2076f5e1de", "title": "Action of a Light-Source in the Field of View in Lowering Visibility", "references": []}, {"date": "1933", "abstract": "1. Bees respond by a characteristic reflex to a movement of their visual field. By confining the field to a series of parallel stripes of two alternating different brightnesses it is possible to determine for any width of stripe, at any brightness of one of the two sets of stripes, the brightness of the second at which the bee will first respond to a displacement of the field. Thus the relations between visual acuity and intensity discrimination can be studied. 2. For each width of stripe and visual angle subtended by the stripe the discrimination power of the bee's eye for different brightnesses was studied. For each visual acuity the intensity discrimination varies with illumination in a characteristic, consistent manner. The discrimination is poor at low illuminations; as the intensity of illumination increases the discrimination increases, and reaches a constant level at high illuminations. 3. From the intensity discrimination curves obtained at different visual acuities, visual acuity curves can be reconstructed for different values of \u0394I/I. The curves thus obtained are identical in form with the curve found previously by direct test for the relation between visual acuity and illumination.", "authors": ["Ernst Dipl Ing Wolf"], "id": "e746c31c40f042bde0566bd060a9e9b839e251b1", "title": "ON THE RELATION BETWEEN MEASUREMENTS OF INTENSITY DISCRIMINATION AND OF VISUAL ACUITY IN THE HONEY BEE", "references": ["189275dcd1306a028fa1e9146a67a8bf40c20975", "b68223017564ce160b65e189ddb736ee26cd3bf4", "394b2f1f2181f20ce735dc45a823ef5e78dec3df", "f61bc6af0c4979683049c8dfea4cffb99dc3827a", "ac5447f3f21ea38984da96a126e18c234f25b73d"]}, {"date": "", "abstract": "1. A study of the historical development of the Weber-Fechner law shows that it fails to describe intensity perception; first, because it is based on observations which do not record intensity discrimination accurately, and second, because it omits the essentially discontinuous nature of the recognition of intensity differences. 2. There is presented a series of data, assembled from various sources, which proves that in the visual discrimination of intensity the threshold difference \u0394I bears no constant relation to the intensity I. The evidence shows unequivocally that as the intensity rises, the ratio See PDF for Equation first decreases and then increases. 3. The data are then subjected to analysis in terms of a photochemical system already proposed for the visual activity of the rods and cones. It is found that for the retinal elements to discriminate between one intensity and the next perceptible one, the transition from one to the other must involve the decomposition of a constant amount of photosensitive material. 4. The magnitude of this unitary increment in the quantity of photochemical action is greater for the rods than for the cones. Therefore, below a certain critical illumination\u2014the cone threshold\u2014intensity discrimination is controlled by the rods alone, but above this point it is determined by the cones alone. 5. The unitary increments in retinal photochemical action may be interpreted as being recorded by each rod and cone; or as conditioning the variability of the retinal cells so that each increment involves a constant increase in the number of active elements; or as a combination of the two interpretations. 6. Comparison with critical data of such diverse nature as dark adaptation, absolute thresholds, and visual acuity shows that the analysis is consistent with well established facts of vision.", "authors": ["Selig Hecht"], "id": "ac5447f3f21ea38984da96a126e18c234f25b73d", "title": "THE VISUAL DISCRIMINATION OF INTENSITY AND THE WEBER-FECHNER LAW", "references": ["b596d04683ffff04c4cfab7f1d0534db479b25e4", "ac53a79101d4f39a7f9e8fcfb2e8d0af19f6f6a5", "043ad48dbb89028fe359155fd0a0877eee77c6b3", "353547611e7d0631250f7872a2820a267ad2e3f5", "925c89d7f099282b8d847188784848432c24eeff"]}, {"date": "1933", "abstract": "1. Bees respond by a characteristic reflex to a movement in their visual field. By confining the field to a series of parallel stripes of different brightness it is possible to determine at any brightness of one of the two stripe systems the brightness of the second at which the bee will first respond to a displacement of the field. Thus intensity discrimination can be determined. 2. The discriminating power of the bee's eye varies with illumination in much the same way that it does for the human eye. The discrimination is poor at low illumination; as the intensity of illumination increases the discrimination increases and seems to reach a constant level at high illuminations. 3. The probable error of See PDF for Equation decreases with increasing I exactly in the same way as does See PDF for Equation itself. The logarithm of the probable error of \u0394I is a rectilinear function of log I for all but the very lowest intensities. Such relationships show that the measurements exhibit an internal self-consistency which is beyond accident. 4. A comparison of the efficiency of the bee's eye with that of the human eye shows that the range over which the human eye can perceive and discriminate different brightnesses is very much greater than for the bee's eye. When the discrimination power of the human eye has reached almost a constant maximal level the bee's discrimination is still very poor, and at an illumination where as well the discrimination power of the human eye and the bee's eye are at their best, the intensity discrimination of the bee is twenty times worse than in the human eye.", "authors": ["Ernst Dipl Ing Wolf"], "id": "394b2f1f2181f20ce735dc45a823ef5e78dec3df", "title": "THE VISUAL INTENSITY DISCRIMINATION OF THE HONEY BEE", "references": ["189275dcd1306a028fa1e9146a67a8bf40c20975", "ac5447f3f21ea38984da96a126e18c234f25b73d", "f61bc6af0c4979683049c8dfea4cffb99dc3827a", "b596d04683ffff04c4cfab7f1d0534db479b25e4"]}, {"date": "", "abstract": "1. Visual acuity varies in a definite manner with the illumination. At low intensities visual acuity increases slowly in proportion to log I; at higher intensities it increases nearly ten times more rapidly in relation to log I; at the highest illuminations it remains constant regardless of the changes in log I. 2. These variations in visual acuity measure the variations in the resolving power of the retina. The retina is a surface composed of discrete rods and cones. Therefore its resolving power depends on the number of elements present in a unit area. The changes in visual acuity then presuppose that the number of elements in the retina is variable. This cannot be true anatomically; therefore it must be assumed functionally. 3. To explain on such a basis the variations of visual acuity, it is postulated that the thresholds of the cones and of the rods are distributed in relation to the illumination in a statistical manner similar to that of other populations. In addition the rods as a whole have thresholds lower than the cones. Then at low intensities the increase in visual acuity depends on the augmentation of the functional rod population which accompanies intensity increase; and at higher intensities the increase in visual acuity depends on the augmentation of the functional cone population. The number of cones per unit foveal area is much greater than the number of rods per unit peripheral area, which accounts for the relative rates of increase of rod and cone visual acuity with intensity. At the highest illuminations all the cones are functional and no increase in visual acuity is possible. 4. If this division into rod visual acuity and cone visual acuity is correct, a completely color-blind person should have only rod visual acuity. It is shown by a study of the data of two such individuals that this is true. 5. The rod and cone threshold distribution has been presented as a purely statistical assumption. It can be shown, however, that it is really a necessary consequence of a photochemical system which has already been used to describe other properties of vision. This system consists of a photosensitive material in reversible relation with its precursors which are its products of decomposition as well. 6. On the basis of these and other data it is shown that a minimal retinal area in the fovea, which can mediate all the steps in such functions as visual acuity, intensity discrimination, and color vision, contains about 540 cones. Certain suggestions with regard to a quantitative mechanism for color vision are then correlated with these findings, and are shown to be in harmony with accurately known phenomena in related fields of physiology.", "authors": ["Selig Hecht"], "id": "b68223017564ce160b65e189ddb736ee26cd3bf4", "title": "THE RELATION BETWEEN VISUAL ACUITY AND ILLUMINATION", "references": ["6330bd177ab52946c622fd2883338ab12609302c", "0543a8095fc22e15c7f3c83009049e432da23389", "155b03402baca8ac4f997b3c641a3f837aa99fc6", "e895851690178506e1a89b88a9c6b08be84663ea", "ac5447f3f21ea38984da96a126e18c234f25b73d", "6e1a5aebd1063e5693e40ec4a8b82776090e9d50"]}, {"date": "", "abstract": "1. Bees respond by a characteristic reflex to a movement in their visual field. By confining the field to a series of parallel dark and luminous bars it is possible to determine the size of bar to which the bees respond under different conditions and in this way to measure the resolving power or visual acuity of the eye. The maximum visual acuity of the bee is lower than the lowest human visual acuity. Under similar, maximal conditions the fineness of resolution of the human eye is about 100 times that of the bee. 2. The eye of the bee is a mosaic composed of hexagonal pyramids of variable apical angle. The size of this angle determines the angular separation between adjacent ommatidia and therefore sets the structural limits to the resolving power of the eye. It is found that the visual angle corresponding to the maximum visual acuity as found experimentally is identical with the structural angular separation of adjacent ommatidia in the region of maximum density of ommatidia population. When this region of maximum ommatidia population is rendered non-functional by being covered with an opaque paint, the maximum visual acuity then corresponds to the angular separation of those remaining ommatidia which now constitute the maximum density of population. 3. The angular separation of adjacent ommatidia is much smaller in the vertical (dorso-ventral) axis than in the horizontal (anterio-posterior) axis. The experimentally found visual acuity varies correspondingly. From this and other experiments as well as from the shape of the eye itself, it is shown that the bee's eye is essentially an instrument for uni-directional visual resolution, functional along the dorso-ventral axis. The resolution of the visual pattern is therefore determined by the vertical angular separation of those ocular elements situated in the region of maximum density of ommatidia population. 4. The visual acuity of the bee varies with the illumination in much the same way that it does for the human eye. It is low at low illuminations; as the intensity of illumination increases it increases at first slowly and then rapidly; and finally at high intensities it becomes constant. The resolving power of a structure like the bee's eye depends on the distance which separates the discrete receiving elements. The data then mean that at low illuminations the distance between receiving elements is large and that this distance decreases as the illumination increases. Since such a moving system cannot be true anatomically it must be interpreted functionally. It is therefore proposed that the threshold of the various ommatidia are not the same but that they vary as any other characteristic of a population. The visual acuity will then depend on the distance apart of those elements whose thresholds are such that they are functional at the particular illumination under investigation. Taking due consideration of the angular separation of ommatidia it is possible to derive a distribution curve for the thresholds of the ommatidia which resembles the usual probability curves, and which describes the data with complete fidelity.", "authors": ["Selig Hecht", "Ernst Dipl Ing Wolf"], "id": "f61bc6af0c4979683049c8dfea4cffb99dc3827a", "title": "THE VISUAL ACUITY OF THE HONEY BEE", "references": ["22787c4915b768dee5ebdb60cd6340580800f8ee", "b68223017564ce160b65e189ddb736ee26cd3bf4", "4147774728108b05f4edbcc4a6cfbbfb9c678538", "fddac6ead64b9401cf441178739330012449ffec"]}, {"date": "", "abstract": "1. The reaction time of Mya to light is composed of two parts. The first, a sensitization period, is an exceedingly short interval of the order of magnitude associated with photographic processes. The second is a latent period of about 1.3 seconds, during which Mya need not remain exposed to the stimulating light. 2. The process of dark adaptation in Mya is orderly. Its progress may be represented by the formation of a photosensitive substance according to the dynamics of a bimolecular reaction. See PDF for Structure 3. Photosensory equilibrium as represented by the light- and dark-adapted conditions finds a rational explanation in terms of the \"stationary state\" of a reversible photochemical reaction involving a photosensitive substance and its two precursors. 4. There are two corollaries to this hypothesis. The first requires that the reaction time at sensory equilibrium for a given intensity should vary inversely with the temperature; the second, that the rate of dark adaptation should vary directly with the temperature. Experiments verified both of these requirements.", "authors": ["Selig Hecht"], "id": "55ef6f37e343b20777d4eca80014a46ba110fedd", "title": "SENSORY EQUILIBRIUM AND DARK ADAPTATION IN MYA ARENARIA", "references": ["dbd0f8f343dd5f103cdd5ae75bde18c28dfbb1ef", "946a0ec6cffcba4ad69d03bc0bcacb71383e1e45", "d3d55c333617049c488857e81066b01b7ef235d9", "e4930af7e0ad2ebf41730dcb892204894a08f3d9", "a5616ad491c871e1d8fd7ce34dc0efaab05efca8"]}, {"date": "", "abstract": "1. A method of experimentation is described which enables one to record objectively and quantitatively the discrimination by Mya between two intensities of illumination to which it is successively exposed. The indicator for this discrimination is a response at a given reaction time. 2. From the data so obtained it is found that the difference, \u0394I, between the two intensities bears no constant relation to the initial intensity, I. Instead, the ratio See PDF for Equation varies in a consistent manner with I. As the latter increases, the ratio decreases to a certain point, after which it increases. 3. The data are analyzed in terms of the photochemical mechanism previously proposed for the sensitivity of Mya to light. It is shown that for the animal to discriminate by means of a given reaction between one intensity and another, the transition from one to the other must be accompanied by the decomposition of a constant amount of photosensitive substance in the sense organ. 4. A mathematical treatment of the behavior of the photochemical mechanism shows not only that the ratio See PDF for Equation cannot be constant as required by the Weber-Fechner law, but that it must vary in the way in which it does. The behavior of Mya under these conditions, therefore, supports the validity of the hypothetical physicochemical mechanism suggested for its sensitivity.", "authors": ["Selig Hecht"], "id": "189275dcd1306a028fa1e9146a67a8bf40c20975", "title": "INTENSITY DISCRIMINATION AND THE STATIONARY STATE", "references": ["03697b3dea816ce6ec9847ca5d5da77e8cebd77f", "1e8e56b0a9c07f800bd149d1f61a12ea3f51c2b9", "6682d0697dc0753b6df84257e14b6683b70b5415"]}, {"date": "1935", "abstract": "Semantic Scholar extracted view of \"THE ACTION POTENTIALS OF THE AUDITORY NERVE\" by A. J. Derbyshire et al.", "authors": ["A. J. Derbyshire", "Hallowell Davis"], "id": "ec9cecd24a6c9afcfd22d49254bad238a35d3e01", "title": "THE ACTION POTENTIALS OF THE AUDITORY NERVE", "references": []}, {"date": "1937", "abstract": "WE have shown1 that the impression of brightness produced by a light ray which forms an image on a fixed area of the retina depends in high degree on the position of entry of the ray in the eye pupil. For the ratio \u03b7 of the apparent brightness for peripheral entry to the apparent brightness for central entry, values as low as 0.2 were found for white light and foveal vision. The following new features of the phenomenon have been revealed by later work :", "authors": ["W. B. Stiles", "B. H. Crawford"], "id": "77a77fba796f77d3103430b9b93e7d3f7f920819", "title": "Luminous Efficiency of Rays entering the Eye Pupil at Different Points", "references": []}, {"date": "1938", "abstract": "Semantic Scholar extracted view of \"Nervous discharges from the olfactory organs of fish.\" by Edgar Douglas Adrian et al.", "authors": ["Edgar Douglas Adrian", "C. C. Ludwig"], "id": "a245cac8a737400eac2d68d79926e8c0240c43b8", "title": "Nervous discharges from the olfactory organs of fish.", "references": []}, {"date": "1933", "abstract": "1. Records of impulses from the lateral-line nerves of catfish show that the lateral-line organs are in a state of continuous activity, producing a massive discharge of impulses. 2. The discharge may be increased during the direct application of pressure on the skin over the lateral-line canal, by ripples in the water, by irregular currents of water, and by movements of the fish's trunk. 3. The asynchronously discharging lateral-line organs respond to vibratory stimuli from tuning-forks by getting into phase with each other and by beating synchronously at frequencies ranging from 20 to 70 per second. The frequency of beating for a given preparation is independent of the frequency of the tuning-fork for the fork frequencies of 100, 200, and 250 double vibrations which were used. 4. The continuous discharge of the lateral-line system is markedly changed by alteration of temperature. The frequency declines on lowering the temperature and rises on increasing it. Spinal and facial nerves in the catfish fail to yield nerve impulses in response to changes of the skin temperature between 0\u00b0 and 28\u00b0C., although the intact animal is known to be sensitive to temperature differences. 5. The action of the lateral-line system of Ameiurus in inhibiting responses initiated through the skin and ears (Parker and Van Heusen, 1917) is discussed in the light of the present experiments.", "authors": ["Hudson Hoagland"], "id": "4e3da415fb37d6364d0dc7e120a2168049bf538e", "title": "ELECTRICAL RESPONSES FROM THE LATERAL-LINE NERVES OF CATFISH. I", "references": ["aaabefc9f7ad1ca13c50d221c8398d07bf67242e", "640409a985a42be57dec268a019bb4d524d35f41", "f91bf52a2bf5759c54312f13aefc34e04dacf85a", "95f48ec98e8fde502ed231ce8306992cf669f724", "0e160a268fe2dc267a1c8a58b16df9e0235d6370"]}, {"date": "", "abstract": "PHYSICAL chemistry in the present-day sense of the term may be said to date from 1887, the year in which Ramsay came to London, Ostwald was appointed to the chair in Leipzig, and the Zeitschrift f\u00fcr physikalische Chemie was founded by Ostwald and van't Hoff. Although many pioneers, amongst whom may be mentioned Deville, Debray, Guldberg and Waage, Gibbs, Horstmann, Berthelot, Thomson, Harcourt and Esson, Gladstone, Le Chatelier, and Lemoine had prepared the way for the new development in chemical science, it was the combined influence of van't Hoff, Arrhenius, Ostwald, and Ramsay that gave direction and strength to the new current of thought and research.A System of Physical Chemistry.By Prof. W. C. McC. Lewis. Two vols. Vol. i., pp. xiv + 523, Vol. ii., pp. vii + 552. (London: Longmans, Green and Co., 1916.) 9s. net each vol.", "authors": ["F. G. Donnan"], "id": "4b2f0c8264dac00ac741c7a92b1d7450c1231974", "title": "A System of Physical Chemistry", "references": []}, {"date": "", "abstract": "1. The effect of temperature on the reaction time of Mya to light is mainly confined to the latent period. The sensitization period, representing a photochemical process, is changed comparatively little. 2. The relation between the latent period and the temperature is adequately expressed by the Arrhenius equation, for temperatures below 21\u00b0C. Above this temperature, the latent period becomes increasingly longer than is required by the Arrhenius formula when \u00b5 = 19,680. 3. These deviations, occurring above the highest environmental temperature of Mya, are explained on the assumption that the principal product formed during the latent period is inactivated by heat. 4. Calculation of the velocity of the hypothetical inactivation reaction at different temperatures shows that it also follows the Arrhenius rule when \u00b5 = 48,500. This value of \u00b5 corresponds to those generally found for spontaneous inactivations and destructions.", "authors": ["Selig Hecht"], "id": "a7343bbc77cc73d6e534d725d6c977a2c7cfbd12", "title": "THE EFFECT OF TEMPERATURE ON THE LATENT PERIOD IN THE PHOTIC RESPONSE OF MYA ARENARIA", "references": ["ae7e0891434f4d3264108f8a8fd59c3969b26dd9", "9dcb0fb221cd9cd928e3f14d3837097984702578", "a5616ad491c871e1d8fd7ce34dc0efaab05efca8", "ce09c55334455b8ec56ef30c0338a316fe310e0c", "8d71f7c2c996f29dca90075ee4dc7f41759c672e", "55ef6f37e343b20777d4eca80014a46ba110fedd"]}, {"date": "", "abstract": "Abstract Xerophthalmia is here considered as a deficiency disease with especial reference to its occurrence in Denmark in 1909\u20131920. The literature of keratomalacia and conjunctival xerosis is reviewed; also the experimental work that has established the importance of fat-soluble vitamin A. The disease may be caused by deficiency thru food lacking this vitamin or because of deficient absorption due to digestive disease, increased consumption of the vitamin during rapid growth or severe disease. Deficiency of the vitamin in the food of the mother during gestation or nursing or in the food of cows depended upon for milk, the use of skimmed milk or of vegetable fats as a substitute for butter also produce it. The responsibility of all these is traced. The mortality and blindness caused are discussed. The treatment includes a corrected diet including large amounts of vitamin. The material on which this paper is based included 434 cases of keratomalacia in children and 13 in adults.", "authors": ["Olaf Blegvad"], "id": "d9780629977f55b798e4588eee255a0b1f59b590", "title": "Xerophthalmia, keratomalacia and xerosis conjunctivae", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Anomalies in the Absorption Spectrum of Visual Purple.\" by Selig Hecht et al.", "authors": ["Selig Hecht", "Aurin M. Chase"], "id": "0610515e2f945f964bcf295b101b7191359d5c15", "title": "Anomalies in the Absorption Spectrum of Visual Purple.", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Die objektiv feststellbaren Lichtwirkungen an der Netzhaut\" by Wilhelm Ernst Theodor Trendelenburg", "authors": ["Wilhelm Ernst Theodor Trendelenburg"], "id": "253f61c801f51f91e1b2dd22e3039f3e9b26b980", "title": "Die objektiv feststellbaren Lichtwirkungen an der Netzhaut", "references": []}, {"date": "", "abstract": "1. In order to produce a response in Mya, the minimum amount of light energy required is 5.62 meter candle seconds. This energy follows the Bunsen-Roscoe law for the relation between intensity and time of exposure. 2. The necessary minimum amount of energy varies but little with the temperature; the temperature coefficient for 10\u00b0C. is 1.06. 3. In view of these facts it is concluded that the initial action of the light is photochemical in nature. This substantiates the hypothesis previously suggested to account for the mechanism of photoreception. 4. The constant energy requirement for stimulation of Mya shows that the traditional division of animals into those which respond to a constant source of light and those which respond to a rapidly augmented light is without any fundamental significance for sensory physiology.", "authors": ["Selig Hecht"], "id": "03697b3dea816ce6ec9847ca5d5da77e8cebd77f", "title": "THE PHOTOCHEMICAL NATURE OF THE PHOTOSENSORY PROCESS", "references": ["c32afd3d389872cd2839c39f391aa4d9975fa92a", "b3fa455033251b49d96f443d4d18e8d4a33c94b5", "a7343bbc77cc73d6e534d725d6c977a2c7cfbd12"]}, {"date": "1939", "abstract": "Semantic Scholar extracted view of \"Afferent impulses from the teeth resulting from a vibratory stimulus.\" by Carl Pfaffmann", "authors": ["Carl Pfaffmann"], "id": "00cb83e7027a2e159c4f733e7e11ea2a45c82313", "title": "Afferent impulses from the teeth resulting from a vibratory stimulus.", "references": []}, {"date": "", "abstract": "THIS volume of 553 pages is the first of the three volumes in which the fifth edition of Prof. Wundt's great work is to appear, The rapid increase in size of the work in each of the successive editions is thus maintained in the present one, and, as in the case of the previous editions, has been necessitated by the rapidity of the growth of the youngest of the natural sciences, experimental or, as Prof. Wundt prefers to call it, physiological psychology. And even the increase in bulk of this book does not by any means fully express the rate of growth of the science, a growth towards which this country has contributed so lamentably little. For the book is primarily a record of the work and the views of the author and of his pupils in the great Leipzig school. Nevertheless, Prof. Wundt has found it necessary to rewrite almost the whole of the book, so that, as he tells us, it must be regarded as almost a new one.Grundz\u00fcge der physiologischen Psychologie.Von Wilhelm Wundt. F\u00fcnfte v\u00f6llig umgearbeitete Auflage. Erster Band. Pp. xv + 553. (Leipzig: W. Engelmann, 1902.) Price 10s. net.", "authors": ["W. McD."], "id": "353547611e7d0631250f7872a2820a267ad2e3f5", "title": "Grundz\u00fcge der physiologischen Psychologie", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Handbuch der physiologischen Optik\" by P. Cz.", "authors": ["P. Cz."], "id": "cde3aa2ad7b6a74eac778d7f661eafed5564aa9d", "title": "Handbuch der physiologischen Optik", "references": []}, {"date": "", "abstract": "SINCE all knowledge comes to us primarily through the evidence of the senses, it is unavoidable that some of the greatest controversies which have occurred in the development of the sciences should have been centred around the modus operandi regulating the translation of the energy of a physical stimulus into the energy of thought. It is also not surprising that the fiercest and most prolonged of these battles, not yet concluded, should have originated in connexion with the sense of sight, whereby we come most widely and most directly into relation with our external surroundings. These struggles have resembled no merely local disputes. They are prolonged campaigns conducted on the common borderland of three great territories; and they cannot cease until \u201cnatural \u201cdelimitations are determined as the result of statesmanship rather than as the result of militant capacity.An Introduction to the Study of Colour Vision.By Sir John Herbert Parsons. (Cambridge Psychological Library.) Second edition. Pp. x + 323. (Cambridge: At the University Press, 1924.) 25s. net.", "authors": ["William Peddie"], "id": "043ad48dbb89028fe359155fd0a0877eee77c6b3", "title": "An Introduction to the Study of Colour Vision", "references": []}, {"date": "", "abstract": "Retrodigitalisierung durch das Zentrum fur Psychologische Information und Dokumentation (ZPID). \n \nBuch (Nachdruck) zur Verfugung gestellt von Gunter Krampen. \n \nEingescanned mit Buchscanner durch UB Trier (Hans-Ulrich Seifert). \n \nNach XHTML konvertiert von Erich Weichselgartner und Theresa Dicke, Juli 2006.", "authors": ["Georg Elias M\u00fcller"], "id": "ac53a79101d4f39a7f9e8fcfb2e8d0af19f6f6a5", "title": "Die Gesichtspunkte und die Tatsachen der psychophysischen Methodik", "references": []}, {"date": "", "abstract": "At the present moment the question whether or not there is a state of \u201call or nothing\u201d activity in reflex arcs seems to be raised, and it is one of importance to the future of investigation of the functions of the nervous system. Of the two views which may be held regarding the manner of the activity of reflex arcs one is that in which it is supposed that the efferent neurone may react with different degrees of intensity in different reflex activities, and that the afferent neurones may play with different degrees of intensity upon efferent neurones or upon interposed neurones.", "authors": ["Thomas Graham Brown"], "id": "6330bd177ab52946c622fd2883338ab12609302c", "title": "On the Question of Fractional Activity: (", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Die Physiologie der facettirten Augen von Krebsen und Insecten\" by Siegmund Exner", "authors": ["Siegmund Exner"], "id": "4147774728108b05f4edbcc4a6cfbbfb9c678538", "title": "Die Physiologie der facettirten Augen von Krebsen und Insecten", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"The migration of the retinal pigment in crustaceans\" by Rudolf Bennitt", "authors": ["Rudolf Bennitt"], "id": "22787c4915b768dee5ebdb60cd6340580800f8ee", "title": "The migration of the retinal pigment in crustaceans", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Der Farbensinn und Formensinn der Biene\" by Karl von Frisch", "authors": ["Karl von Frisch"], "id": "fddac6ead64b9401cf441178739330012449ffec", "title": "Der Farbensinn und Formensinn der Biene", "references": []}, {"date": "", "abstract": "The temperature coefficient of the action of \u03b2-rays from radium upon the egg of Nereis lies between 1.1 and 1.2. This is of a magnitude characteristic of photochemical reactions.", "authors": ["Alfred C. Redfield", "Elizabeth M. Bright"], "id": "e4930af7e0ad2ebf41730dcb892204894a08f3d9", "title": "TEMPERATURE COEFFICIENT OF THE ACTION OF \u03b2-RAYS UPON THE EGG OF NEREIS", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"The Influence of a Power Dam in Modifying Conditions Affecting the Migration of the Salmon.\" by Henry B. Ward", "authors": ["Henry B. Ward"], "id": "0e160a268fe2dc267a1c8a58b16df9e0235d6370", "title": "The Influence of a Power Dam in Modifying Conditions Affecting the Migration of the Salmon.", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"THE RECEPTION OF MECHANICAL STIMULI BY THE SKIN, LATERAL-LINE ORGANS AND EARS IN FISHES, ESPECIALLY IN AMIURUS\" by George Howard Parker et al.", "authors": ["George Howard Parker", "Anne P. Van Heusen"], "id": "95f48ec98e8fde502ed231ce8306992cf669f724", "title": "THE RECEPTION OF MECHANICAL STIMULI BY THE SKIN, LATERAL-LINE ORGANS AND EARS IN FISHES, ESPECIALLY IN AMIURUS", "references": []}, {"date": "", "abstract": "5. It is not possible to shift the critical temperature at which the melanophores expand in the warmth by any previous exposure of the cells to cold. * NATIONAL RESEARCH FELLOW IN THE BIOLOGICAL SCIENCES. Hadley, C. E., \"Color Changes inExcisedPieces of the Integument ofAnolisEquestris,\" Proc. Nat. Acad. Sci., 14, pp. 822-824, 1928. Hogben, L. T., and L. Mirvash, \"The Pigmentary Effector System. V. The Nervous Control of Excitement Pallor in Reptiles,\" Brit. Jour. Exp. Biol., 5, pp. 295-308, 1928. Parker, G. H., \"The Influence of Light and Heat on the Movement of Melanophore Pigment, Especially in the Lizard,\" Jour. Exp. Zo6l., 3, pp. 401-414, 1906. Parker, G. H., and S. A. Starratt, \"TheEffect of Heat on the Color Changes in theSki of Anolis Carolinensis Cuv.,\" Proc. Amer. Acad. Arts and Sci., 40, pp. 457-466, 1904. Redfield, A. C., \"The Physiology of the Melanophores of the Horned Toad, Phrynosoma,\" Jour. Exp. Zool., 26, pp. 275-333, 1918. Smith, D. C., \"The Effect of Temperature on the Melanophores of Fishes,\" Jour. Exp. Zool., 52, pp. 183-234, 1928.", "authors": ["Henry B. Ward"], "id": "f91bf52a2bf5759c54312f13aefc34e04dacf85a", "title": "FURTHER STUDIES ON THE INFLUENCE OF A POWER DAM IN MODIFYING CONDITIONS AFFECTING THE MIGRATION OF THE SALMON.", "references": ["9f1e1028371cfcc72fc2b784e3bbf2ea4a8c7e8e", "0e160a268fe2dc267a1c8a58b16df9e0235d6370"]}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Degeneration and regeneration of the lateral-line organs in Ameiurus nebulosus (Les.)\" by Mary Chambers Brockelbank", "authors": ["Mary Chambers Brockelbank"], "id": "640409a985a42be57dec268a019bb4d524d35f41", "title": "Degeneration and regeneration of the lateral-line organs in Ameiurus nebulosus (Les.)", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Higher Mathematics for Students of Chemistry and Physics, with Special Reference to Practical Work\" by John Williams Mellor", "authors": ["John Williams Mellor"], "id": "8d71f7c2c996f29dca90075ee4dc7f41759c672e", "title": "Higher Mathematics for Students of Chemistry and Physics, with Special Reference to Practical Work", "references": []}, {"date": "", "abstract": "THE publicat\u00edon of Prof. Arrhenius's Silliman lectures on \u201cTheories of Solutions,\u201d delivered at Yale in the spring of 1911, will be welcomed by all who are interested in the present position of physical chemistry. The book is of special value because the author has dealt very lightly with those aspects of his theory of \u201celectrolytic dissociation\u201d which have been discussed over and over again during the last twenty-five years and have occupied so large a space in nearly all recent text-books of physical chemistry. Thus, although many of his illustrations are drawn from electrolytic solutions, only three of the eleven lectures deal specifically with such solutions, namely, those on \u201cThe Theory of Electrolytic Dissociation\u201d \u201cConductivity of Strong Electrolytes,\u201d and \u201cAbnormality of Strong Electrolytes.\u201dTheories of Solutions.Svante Arrhenius. Pp. xx + 247. (New Haven: Yale University Press; London: Oxford University Press, 1912.) Price 12s. 6d. net.", "authors": ["T. M. L."], "id": "ce09c55334455b8ec56ef30c0338a316fe310e0c", "title": "Theories of Solutions", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Forced Movements, Tropisms, and Animal Conduct\" by Jelliffe.", "authors": ["Jelliffe."], "id": "c32afd3d389872cd2839c39f391aa4d9975fa92a", "title": "Forced Movements, Tropisms, and Animal Conduct", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"ON THE MEANING OF VARIATION IN THE MAGNITUDE OF TEMPERATURE COEFFICIENTS OF PHYSIOLOGICAL PROCESSES\" by Charles D. Snyder", "authors": ["Charles D. Snyder"], "id": "9dcb0fb221cd9cd928e3f14d3837097984702578", "title": "ON THE MEANING OF VARIATION IN THE MAGNITUDE OF TEMPERATURE COEFFICIENTS OF PHYSIOLOGICAL PROCESSES", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"A new electrical recording system for physiological work.\" by Bryan Harold Cabot Matthews", "authors": ["Bryan Harold Cabot Matthews"], "id": "aaabefc9f7ad1ca13c50d221c8398d07bf67242e", "title": "A new electrical recording system for physiological work.", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"On the \"heat coagulation\" of proteins.\" by Helen L Chick", "authors": ["Helen L Chick"], "id": "ae7e0891434f4d3264108f8a8fd59c3969b26dd9", "title": "On the ", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Experimentelle Untersuchungen uber die psychophysische Fundamentalformel in Bezug auf den Gesichtsinn\" by A. Konig et al.", "authors": ["A. Konig", "Eugen Brodhun"], "id": "1e8e56b0a9c07f800bd149d1f61a12ea3f51c2b9", "title": "Experimentelle Untersuchungen uber die psychophysische Fundamentalformel in Bezug auf den Gesichtsinn", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"An Analysis of the Relation Between the Temperature and the Duration of a Biological Process.\" by Stefan Hecht", "authors": ["Stefan Hecht"], "id": "a5616ad491c871e1d8fd7ce34dc0efaab05efca8", "title": "An Analysis of the Relation Between the Temperature and the Duration of a Biological Process.", "references": []}, {"date": "", "abstract": "A PROLIFIC investigator does a great service to his brethren when, without waiting to write an elaborate treatise, he collects the gist of some considerable portion of his work into a book; and if the book be a small one, so much the better. This Prof. Loeb has now done, and we are immensely obliged to him. What is more, his volume is but the first of a series, by American writers, all dealing with the wide field of experimental biology, a field in which we at home have done comparatively little, but in which American biologists have greatly distinguished themselves. Among the promised monographs are one by Prof. Morgan on \"Chromosomes and Heredity\"; another, by Dr. Jennings, on \"Pure Line Inheritance\"; a third, by Dr. T. B. Robertson, on \"The Chemical Basis of Growth\"; and a fourth, by Prof. Osterhout, on \"Permeability and Conductivity of Living Tissues.\"In every case (and there are many more besides these) the author has won, and more than won, his right to be heard, and in every case also we feel the need of an authoritative guide to the subject in question.Forced Movements, Tropisms, and Animal Conduct.By Prof. Jacques Loeb. Pp. 209. (Philadelphia and London: J. B. Lippincott Co., 1918.) Price 10s. 6d. net.", "authors": ["D'ARCY W. Thompson"], "id": "946a0ec6cffcba4ad69d03bc0bcacb71383e1e45", "title": "Forced Movements, Tropisms, and Animal Conduct", "references": []}, {"date": "", "abstract": "1. Experiments are described which measure the sensitivity of animals exposed to continued illumination to which they have become adapted. It is shown that the amount of outside light energy necessary to stimulate an adapted animal increases with the intensity of the adapting illumination. 2. The data are analyzed quantitatively in terms of the reversible reaction S \u21cc P + A shown previously to account for the photic sensitivity of these animals. This analysis demonstrates that, though the amount of incident energy necessary for a minimal response varies with the adapting intensity, the actual amount of photochemical decomposition required to set off the sensory mechanism is a constant quantity. 3. The ability of these animals to come into sensory equilibrium with any sustained illumination is accounted for quantitatively by the presence of a stationary state in the reversible photochemical reaction S \u21cc P + A during which the concentrations of the three components are constant. 4. It is shown that the concentrations of these substances at the stationary state are automatically controlled by the outside intensity. Therefore, given the sensory mechanism as a basis, the adaptation of the animals to light and the consequent changes in sensitivity, are determined entirely by the light to which the animals are exposed. 5. Because of the properties of the stationary state, and of the constancy of photochemical decomposition for a minimal effect, it is suggested that the sensory system is not only the traditional receptor system, but is also a protecting layer which stabilizes and buffers the relation between the nervous system and the environment.", "authors": ["Selig Hecht"], "id": "6682d0697dc0753b6df84257e14b6683b70b5415", "title": "SENSORY ADAPTATION AND THE STATIONARY STATE", "references": ["55ef6f37e343b20777d4eca80014a46ba110fedd", "8038da658a7d9a6da86b527d260d9048fbabadf9"]}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Handbuch der physiologischen Optik.\" by Christine Ladd Franklin", "authors": ["Christine Ladd Franklin"], "id": "dbd0f8f343dd5f103cdd5ae75bde18c28dfbb1ef", "title": "Handbuch der physiologischen Optik.", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"The all-or-none principle in nerve.\" by Edgar Douglas Adrian", "authors": ["Edgar Douglas Adrian"], "id": "6e1a5aebd1063e5693e40ec4a8b82776090e9d50", "title": "The all-or-none principle in nerve.", "references": []}, {"date": "", "abstract": "In the December, 1915, issue of this Journal (vol. xxxix, no. 2) a number of the figures accompanying the article by Forbes and Gregg entitled: \"Electrical Studies in Mammalian Reflexes. II. The Co...", "authors": ["Alexander Robert Forbes", "Alan Gregg"], "id": "0543a8095fc22e15c7f3c83009049e432da23389", "title": "ELECTRICAL STUDIES IN MAMMALIAN REFLEXES", "references": []}, {"date": "", "abstract": "On the basis of certain assumptions derived from the all-or-none hypothesis regarding the nature of the nerve processes of the retina, the authors have formulated quantitative expressions for the effect of bright and dark surroundings on the difference-threshold of foveal vision. Comparison of the values calculated from these formulae with the limens for brightness obtained in actual experiment reveals a substantial agreement between the two sets of data. The range of application of the formulae to various conditions in visual experimentation remains still to be determined. From Psych Bulletin 19:09:00581. (PsycINFO Database Record (c) 2012 APA, all rights reserved)", "authors": ["Elliot Quincy Adams", "Percy W. Cobb"], "id": "b596d04683ffff04c4cfab7f1d0534db479b25e4", "title": "The effect on foveal vision of bright (and dark) surroundings. V", "references": []}, {"date": "", "abstract": "IN Part I of the present series one of us (1) described a method of recording nerve action currents by means of a capillary electrometer and a threestage amplifier, together with some preliminary observations on the impulses set up in various types of sensory nerve fibres by stimulation of their end organs. In Part II(2) we gave a more detailed analysis of the sensory impulses produced by stretching a muscle, and we were able to show that in a single nerve fibre the impulses usually recurred in a regular series with a frequency depending on the intensity of the stimulus, that the impulses (or rather their action currents) were all of the s8me intensity and that their frequency was low enough to leave the nerve fibre time for complete recovery between one impulse and the next. As these observations were made on the frog and were confined to one type of sensory ending, we were anxious to extend them to mammals and to some other form of sensation. The results given in Part I had shown that a cutaneous afferent nerve in the cat (the internal saphenous) usually exhibits a series of action currents and that these increase in number when the skin is pricked or pinched. There is, however, a considerable drawback to the use of such forms of stimulation, since their intensity is not readily measured, and to overcome this difficulty we decided to use moderate pressure as the stimulus in the present research. The end organs sensitive to pressure are not known with certainty, but they are generally supposed to be the touch corpuscles in the skin and the Pacinian and other types of corpuscle in the subcutaneous tissues. Since the latter occur singly or in small groups in the mesentery of the cat, we thought at first that the most suitable preparation would be a single Pacinian corpuscle from the mesentery with its nerve fibre isolated and connected to the electrometer. Unfortunately, we found that various technical difficulties stood in our way. In the living animal it is extremely difficult to detect the", "authors": ["Edgar Douglas Adrian", "Yngve Zotterman"], "id": "155b03402baca8ac4f997b3c641a3f837aa99fc6", "title": "The impulses produced by sensory nerve endings: Part 3. Impulses set up by Touch and Pressure.", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"The effect of temperature on the melanophores of fishes\" by Dietrich C. Smith", "authors": ["Dietrich C. Smith"], "id": "9f1e1028371cfcc72fc2b784e3bbf2ea4a8c7e8e", "title": "The effect of temperature on the melanophores of fishes", "references": []}, {"date": "", "abstract": "Resume LA PSYCHOPHYSIOLOGIE DES QUALITES ET DES ATTRIBUTS AUDITIFS L'idee de la mechanique de l'oreille interne suggeree d'abord par Roaf, et developpee par les membres des laboratoires Bell Telephone, peut \u011btre consideree en general satisfaisante. Cependant, les bases neuralogiques des attributs de l'experience auditive exigent encore un eclaircissement. De differentes frequences acoustiques sont actuellement donnees aux groupes separes des fibres des nerfs, selon l'idee de Helmholtz, mais la hauteur est determinee cependant par la frequence des impulsions des nerfs et non pas par l'identite anatomique des fibres. Les difficultes dues a la limitation par des phases refractaires sont enlevees au moyen de partager la conduction de la frequence totale entre les fibres cooperants, quand la frequence est excessivement elevee. La force du ton depend du nombre total d'impulsions passant devant une section fixee, chaque seconde, dans un groupe de fibres cooperants, tandis que le volume a rapport seulement au nom...", "authors": ["Leonard Thompson Troland"], "id": "95816b1158f1723899656f2b0a2471dbb8f135ca", "title": "The Psychophysiology of Auditory Qualities and Attributes", "references": []}, {"date": "1941", "abstract": "This paper is a review of the analysis of color reception with the aid of electrophysiological methods. Microelectrodes have been inserted into the retina to record the discharge of impulses from single or a restricted number of elements in response to illumination with a spectrum of known energy distribution. From the electrodes leads have been taken to amplifier, cathode ray, and loudspeaker. In this manner it has been possible to obtain curves showing the distribution of sensitivity to spectral light of active elements in the eyes of mammals, amphibians, and fishes. Thomas Young\u2019s conception, that the retina possesses elements sensitive to different regions of the spectrum, has been proved to be correct. A number of other results illustrate some fundamental properties of the mechanism of color reception.", "authors": ["Ra\u01f5nar Granit"], "id": "5ea8e8702f08e18cd444b439fb8ce3e70a5b8705", "title": "The Retinal Mechanism of Color Reception", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"ACTION CURRENTS IN THE AUDITORY NERVE IN RESPONSE TO ACOUSTICAL STIMULATION.\" by Ernest Glen Wever et al.", "authors": ["Ernest Glen Wever", "C W Bray"], "id": "c9e01d1b42dd2bc9072312f02d11f9dd7b882b2e", "title": "ACTION CURRENTS IN THE AUDITORY NERVE IN RESPONSE TO ACOUSTICAL STIMULATION.", "references": []}, {"date": "1951", "abstract": "An isolated single nerve fiber of a toad was mounted on the bridge-insulator and the repetitive response to the constant voltage was recorded, as well as measuring the recovery curve and the accommodation. The threshold condition for the repetitive response, the voltage-duration and voltage-frequency relations of the repetitive response were investigated, and it was found that the experimental facts and the author's theory were farely well in accord with each other.", "authors": ["Mitsumoto Sato"], "id": "2b690570512365ee0a2f4e1f4e9e9db5350ee35e", "title": "Repetitive responses of the nerve fiber, as determined by recovery process and accommodation.", "references": ["4fc2b55718276227c974b5e54babe03f3250bda2", "55e8651081cf494d80aa68d7182c77309ab70f6d", "c6d905828cb48089b5ec9261328602743d445c97"]}, {"date": "1949", "abstract": "An experimental technique was developed to measure, for intact middle and inner ears, the volume displacement of the fluid in the cochlea produced by a given sound pressure on the eardrum. In order to determine whether it is possible to immobilize the stapes by increasing the static pressure in the cochlea, the transmission characteristics of the eardrum\u2010to\u2010round\u2010window system were measured with the ear under the maximum static pressure that can be produced in the cochlea without bursting the blood vessels. Since no important change in transmission was observed, it was concluded that immobilization of the stapes cannot be produced by an increase in static pressure in the cochlea.With the same technique the mechanical impedance of the movements of the stapes footplate, the round window, and the fluid in the cochlea were measured singly and combined as in the normal ear. Once the elastic properties of the cochlear partition were known, it was possible to construct an ear model that exhibited patterns of vib...", "authors": ["Georg von B\u00e9k\u00e9sy"], "id": "55ec0a124f5ceb2018d4cf26369a2be186d77009", "title": "The Vibration of the Cochlear Partition in Anatomical Preparations and in Models of the Inner Ear", "references": []}, {"date": "1937", "abstract": "ALTHOUGH it is generally accepted that visual purple plays an essential part in the process of scotopic vision, attempts1 at relating the human scotopic luminosity curve (for equal energy spectrum) with the extinction (absorption) coefficient curve of amphibian or mammalian visual purple have shown a difference in the wave-length position of the maxima. Hecht and Williams2, using a \u2018constant stimulus\u2019 method, found that the maximum of this luminosity curve was at 510 m\u03bc This was a mean value for forty-eight young observers, and there was little variation. Abney and Watson3, using a threshold intensity method, found that the maximum of the luminosity was at 505 m\u03bc. for three monochromats.", "authors": ["H. J. A. Dartnall", "C. F. Goodeve"], "id": "a07a8ee6cb1ac36ed63bebf24562a1b97c105fde", "title": "Scotopic Luminosity Curve and the Absorption Spectrum of Visual Purple", "references": []}, {"date": "1961", "abstract": "Exposure-History as a Factor in Maintaining Stability of Perception and Coordination RICHARD HELD; The Journal of Nervous and Mental Disease", "authors": ["Richard H. Held"], "id": "2e959e1c6e27fca0e97b72c441f9105281d1b006", "title": "Exposure-history as a factor in maintaining stability of perception and coordination.", "references": []}, {"date": "1964", "abstract": "Semantic Scholar extracted view of \"Human position sense and sense of effort.\" by Merton Pa", "authors": ["Merton Pa"], "id": "c9b6743e19d4b92e2fa6665b5ada2ec395ec4981", "title": "Human position sense and sense of effort.", "references": []}, {"date": "1938", "abstract": "The absorption spectra of visual purple solutions extracted by various means were measured with a sensitive photoelectric spectrophotometer and compared with the classical visual purple absorption spectrum. Hardening the retinas in alum before extraction yielded visual purple solutions of much higher light transmission in the blue and violet, probably because of the removal of light-dispersing substances. Re-extraction indicated that visual purple is more soluble in the extractive than are the other colored retinal components. However, the concentration of the extractive did not affect the color purity of the extraction but did influence the keeping power. This suggests a chemical combination between the extractive and visual purple. The pH of the extractive affected the color purity of the resulting solution. Over the pH range from 5.5 to 10.0, the visual purple color purity was greatest at the low pH. Temperature during extraction was also effective, the color purity being greater the higher the temperature, up to 40\u00b0C. Drying and subsequent re-dissolving of visual purple solutions extracted with digitalin freed the solution of some protein impurities and increased its keeping power. Dialysis against distilled water seemed to precipitate visual purple from solution irreversibly. None of the treatments described improved the symmetry of the unbleached visual purple absorption spectrum sufficiently for it to resemble the classical absorption spectrum. Therefore it is very likely that the classical absorption spectrum is that of the light-sensitive group only and that the absorption spectra of our purest unbleached visual purple solutions represent the molecule as a whole.", "authors": ["Aurin M. Chase", "Charles Haig"], "id": "7d834dac179bba2ea12ef96c2cdbc566353f3682", "title": "THE ABSORPTION SPECTRUM OF VISUAL PURPLE", "references": ["58c39dd255a8ae76dd3a1d20cf1423d5bbccb210", "a07a8ee6cb1ac36ed63bebf24562a1b97c105fde", "961d120af1768287142f0f77c60a33f5c499f2bd", "d655b2125b7dcaec8e2e6421643df35cc9f83b25", "bf26d3110015654b38af3fe00694a6f174eb3df0", "3f9cf02c77a64c46465b9e4eb52069ecac150a56", "9ea2293dd8dbfaf4436c6a4c573145f50587d5f5", "119d3dd7584f92c9da21ec10fe2183164b85a716", "2d0eaf51dfa68f582c73a7fde1c048be8643eb38"]}, {"date": "1963", "abstract": "Normal surroundings appear curved when viewed through wedge prism eyeglasses. But prolonged viewing of uniformly curved lines makes them appear less curved. An environment specially patterned to prevent the appearance of curvature when viewed through a prism made possible the demonstration of change in apparent curvature wholly dependent upon the visual feedback accompanying self-produced movement of the prism-wearer.", "authors": ["Richard H. Held", "J H Rekosh"], "id": "d53c0c98386efd1d0e16f33c6782d48f4cf788ad", "title": "Motor-Sensory Feedback and the Geometry of Visual Space", "references": []}, {"date": "1952", "abstract": "In 1951, during the period from February to May, Huxley and I made a series of observations, using glass micro-electrodes (Ling and Gerard, 1949 Nastuk and Hogkin, 1950), on the action potentials recorded from inside the frog motor nerve fiber. A large motor fiber was isolated from a sciatic-gastrocnemius preparation, and the operated region of the preparation, immersed in Ringer's fluid, was brought under a polarizing microscope with an objective lens of water-immersion type. A micro-electrode of below 0.5 micron in diameter, held by means of a micro-manipulator, was pushed into the nerve fiber, and the potential difference between the intra-cellular electrode and the outside fluid medium was recorded with a cathode-ray oscillograph used in conjunction with a DC amplifier. Action potentials were elicited from the fiber by induction shocks applied to the proximal nerve trunk. Muscular contractions were taken as an index of the propagation of impulses across the impaled region. The main features of the results obtained are as follows: 1. When the tip of the micro-electrode was just under the layer of myelin or when a dimpling of myelin into the axis-cylinder was seen at the tip of the micro-electrode, there was practically no measurable resting potential . But a propagated nerve impulse could very often evoke sizable monophasic action potentials (generally not exceeding 30 mV in peak value) . At this stage, propagation of impulses across the impaled region was not impaired. 2. When the tip of the micro-electrode was introduced deep inside the axis-cylinder, a resting potential of 10 to 40 mV was observed. The action potentials observed at this stage were large at the beginning (20 to 40 mV), but soon a notch appeared at the peak of the action potential . As time elapsed, this notch became more pronounced, and generally within 2 to 5 minutes after introduction of the micro-electrode, a stage was reached at which the part of action potential following the notch suddenly disappeared . Propagation across the impaled region was blocked at this moment . The first part of the present investigation is an extension of the abovestated experiments done in England. In order to correlate the action potentials observed with an internal electrode with the action currents recorded with the", "authors": ["Ichiji Tasaki"], "id": "ec2c6cad20eda70ca8ef346d2973362a56acbd0a", "title": "Properties of myelinated fibers in frog sciatic nerve and in spinal cord as examined with micro-electrodes.", "references": []}, {"date": "", "abstract": "In its photosensory effect, the action of light depends on two variables,\u2014intensity and time. If the intensity alone is varied, the photochemical effect is proportional to the logarithm of the intensity. If the time alone is varied, the effect is proportional to the time. Experiments here reported show that when both the intensity and the time are varied, the photochemical effect is equal to the product of their separate activities: E = kt log I. These results furnish the means of expressing directly the relation between the intensity of illumination and the reaction time of Mya.", "authors": ["Selig Hecht"], "id": "8038da658a7d9a6da86b527d260d9048fbabadf9", "title": "TIME AND INTENSITY IN PHOTOSENSORY STIMULATION", "references": ["fe28408c1108c1f2ded89b52d9656d1d12616a92", "2a5c70ea91bcd6edfe0415c6c4f756220b129ab6"]}, {"date": "1965", "abstract": "Resume Histoire d'un malade qui presenta une thrombose de la carotide interne gauche et une stenose de la carotide interne droite. Au stade initial, hemiplegie droite, hemiparesie gauche, troubles du langage, et fixite du regard. Bonne recuperation des troubles moteurs et des troubles du langage, mais persistance d'agnosie spatiale (meconnaissance droite-gauche, perte de la memoire topographique, troubles de la perception de l'espace, profondeur en particulier), de troubles oculo-moteurs (impossibilite de diriger volontairement le regard, de suivre une cible, de converger) sans contexte d'agnosie visuelle. Mise en evidence electro-oculographique des difficultes oculo-motrices, de la persistance des mouvements des yeux lors de la phase onirique du sommeil. Discussion physio-pathologique: pas de troubles sensoriels ou moteurs fondamentaux, mais difficultes a moduler l'afferentation visuelle en passant de la perception maculaire a la perception peripherique et vice-versa. Constatations anatomiques: lesions bilaterales en \u201ccimier de casque\u201d a la jonction des territoires des cerebrales anterieures, moyennes et posterieures, predominant dans les aires parieto-occipitales. Discussion nosologique evoquant les hypotheses d'une fixation spasmodique, d'une apraxie du regard, d'un syndrome de Balint. Ce cas est plus proche du syndrome decrit par Gordon Holmes.", "authors": ["Francisque Michel", "Marc Jeannerod", "Michel Devic"], "id": "dc0878d2d191052815ea5b2004ba65c800205017", "title": "Trouble de L'Orientation Visuelle Dans Les Trois Dimensions de L'Espace: A propos d'un cas anatomique", "references": []}, {"date": "1959", "abstract": "The positive polarity of the human cornea was used to produce signals from marginal electrodes around the eyes. The potentials were amplified with dc networks which produced amplitude-time oscillographic tracings of the horizontal and vertical components of eyeball movement, and also controlled the deflectors of a cathode-ray oscilloscope (CRO) in such a way that the beam moved in the same way as the eyes. An automatic camera photographed the CRO face to produce two-dimensional electro-oculographic (EOG) plots of eyeball movement. Data thus obtained are used for an analysis of eye movements and fixations in a surveillance search task. The paper oscillographic tracings against time show (1) the number of fixations per unit of time, and (2) the duration of the fixations. The cathode-ray EOG shows (1) the order of fixations in search procedure, (2) the lengths of various saccadic jumps, and (3) the areas of neglect and concentration for 5-sec search periods on a circular field subtending 30\u00b0 of visual angle.", "authors": ["Adelbert Ford", "C. T. White", "Martin G. Lichtenstein"], "id": "9a76a0cb3cee269c85aaeeda6e8a29fe2b48b314", "title": "Analysis of eye movements during free search.", "references": []}, {"date": "1952", "abstract": "Argyle, M., & Cook, M. (1976). Gaze and mutual gaze. Cambridge, UK: Cambridge University Press. Henderson, J. M., & Ferreira, F. (Eds.). (2004). The integration of language, vision, and action: Eye movements and the visual world. New York: Psychology Press. Richardson, D. C., Dale, R., & Kirkham, N. Z. (2007). The art of conversation is coordination: Common ground and the coupling of eye movements during dialogue. Psychological Science, 18(5), 407\u2013413.", "authors": ["H. B. Barlow"], "id": "6c8c14d131ec8586278330a5f9170d6fef87a1e1", "title": "Eye movements during fixation.", "references": []}, {"date": "1972", "abstract": "Any user of a computer system is aware that current systems are unreliable because of errors in their software components. While system designers and implementers recognize the need for reliable software, they have been unable to produce it. For example, operating systems such as OS/360 are released to the public with hundreds of errors still in them.", "authors": ["Barbara Liskov"], "id": "f3e46ad5e182fd3de7e79a828b72c2d82e62a247", "title": "A design methodology for reliable software systems", "references": []}, {"date": "1962", "abstract": "Abstract : Several examples of organizational problems dealing with the construction and application of problem-solving programs are examined. The first example is how to store information that is created dynamically and unpredictably during the operation of the program. The second example is how to organize large, complex processes. The third example is how to have many different kinds of goals producing many different kinds of results, and yet be able to use these results in the rest of the problem. The fourth example is how to avoid the rigidities of many special routines when building up highly particular and inhomogeneous collections of data. The last example concerns the general problem of how to remember the past. All of these problems stem from the fact that problem-solving programs are more dynamic and require more flexibility than we know how to provide. By solving these organizational problems in this context we can expect to develop the appropriate ways to organize complex programs that require flexibility in many applied areas as well.", "authors": ["Allen Newell"], "id": "628957b7b5708f54ea954ce1385f34d936a4b55f", "title": "Some Problems Of Basic Organization In Problem-Solving Programs", "references": []}, {"date": "2004", "abstract": "SummaryFeedback mechanisms exist in all the periferal sense organs including the eye, which acts as a highly efficient position control servo system. Histological studies so far have not revealed the precise circuitry of the eye movement control system but some information about it can be obtained by a study of the sources of feedback. Existing theories have considered three types of feedback originating in the oculomotor tract, in the proprioceptive fibres of the extrinsic eye muscles and from retinal image displacement. In the present experiments an optical arrangement has been used to vary or eliminate the amount of information available from retinal image motion, and the response of the eye to simple harmonic displacement of a target has been recorded. The response curves of gain (eyeball movement divided by target motion) against frequency indicate that the system is lion linear when the image falls in the retinal region which is insensitive to position. Outside this area, retinal image position is used as negative feedback but the information from the oculomotor tract must be regenerative. There is also evidence for feedback proportional to the first derivative of eyeball position and this function is ascribed to the proprioceptive signals; this form of feedback appears to saturate for large amplitude movements, thus avoiding heavy damping of the flick movements.A schematic eye movement control system having the same characteristics as the eye is proposed. The transfer function of this system indicates that it should be unstable if the sign of the retinal image feedback loop is reversed. Experiments with this form of feedback show that steady fixation is impossible and the eye performs a pendular nystagmus.", "authors": ["D. H. Fender", "P. W. Nye"], "id": "848ed3c378a404ea39c59c601970d9abe419cbc4", "title": "An investigation of the mechanisms of eye movement control", "references": ["56155ac7656988088e45ffc7612e80570e5e1211"]}, {"date": "1971", "abstract": "Without communication mechanisms, a program is useless. It can neither obtain data for processing nor make its results available. Thus every programming language has contained communication mechanisms. These mechanisms have traditionally been separated into five categories based on the entity with which communication is established. The five entities with which programs can communicate are physical devices (such as printers, card readers, etc.), terminals (although a physical device, they have usually been treated separately), files, other programs, and the monitor. Corresponding to each of these categories are one or more communication mechanisms, some of which may be shared with other categories.", "authors": ["Robert Balzer"], "id": "7eed64f4f1e8714e1b2a313b2ebaf7c765a8ffcd", "title": "PORTS: a method for dynamic interprogram communication and job control", "references": []}, {"date": "1966", "abstract": "The effects of motivation on retention were investigated. Using a short-term memory technique, stimuli were cued for different incentives. At a short time interval there were no differences in recall as a function of the incentive condition. However, after a longer interval, stimuli associated with a five cent reward or shock were recalled significantly more than stimuli for which neither shock nor money was a potential outcome. It was argued that motivation did not affect the strength of original learning, but did influence the temporally subsequent process of trace storage.", "authors": ["P Kernoff", "Bernard Weiner", "Myrna Morrison"], "id": "02e35f77a0aba531890fb929eea287673e53d6c6", "title": "Affect and short-term retention", "references": []}, {"date": "1938", "abstract": "Semantic Scholar extracted view of \"Propri\u00e9t\u00e9s rythmiques de la mati\u00e8re vivante : variations gradu\u00e9es de la polarisation et rythmicit\u00e9s\" by A. Arvanitaki et al.", "authors": ["A. Arvanitaki", "Louis \u00c9douard Lapicque"], "id": "c6d905828cb48089b5ec9261328602743d445c97", "title": "Propri\u00e9t\u00e9s rythmiques de la mati\u00e8re vivante : variations gradu\u00e9es de la polarisation et rythmicit\u00e9s", "references": []}, {"date": "1938", "abstract": "Semantic Scholar extracted view of \"Les variations gradu\u00e9es de la polarisation des syst\u00e8mes excitables : relation avec la n\u00e9gativit\u00e9 propag\u00e9e et signification fonctionnelle dans l'activit\u00e9 rhythmique\" by A. Arvanitaki", "authors": ["A. Arvanitaki"], "id": "4fc2b55718276227c974b5e54babe03f3250bda2", "title": "Les variations gradu\u00e9es de la polarisation des syst\u00e8mes excitables : relation avec la n\u00e9gativit\u00e9 propag\u00e9e et signification fonctionnelle dans l'activit\u00e9 rhythmique", "references": []}, {"date": "1965", "abstract": "The design, apparatus, and stimulus materials of an earlier experiment (Harley, 1965) was duplicated with two exceptions: (1) An absolute method was used in the place of a differential method, and (2) each S was tested to a criterion of 3 consecutive correct trials. The results indicated that the magnitude of incentive had no effect upon learning which was consistant with many animal learning studies summarized by Pubols (1960). Learning was found to be more closely related to learning time (trials x exposure time) than trials.", "authors": ["Willard F. Harley"], "id": "781731f7838681f424574ff282234432877ca119", "title": "The effect of monetary incentive in paired associate learning using an absolute method", "references": []}, {"date": "", "abstract": "In reference to the note by Prof. Fraser-Harris on the \u201cSubjective Demonstration of the Existence of the Muscular Sense\u201d, in Nature of Nov. 23, I beg to point out that so far from the existence of this sense being demonstrated by the simple experiment he describes, he has really got no further than the indication of the nature of the problem. William James was well acquainted with such evidences, yet he decided that the muscular sense was a \u201cneedless encumbrance\u201d. My own position is different from both; the muscular sense, or as I call it, the sense of effort, has an existence, but the affirmation of Prof. Fraser-Harris or the denial of William James has no particular weight, as neither is based on any analysis of a searching character.", "authors": ["Arthur Lynch"], "id": "db28e61e614da065d465c52cb870fafcd96fcb8d", "title": "The Muscular Sense.", "references": []}, {"date": "1966", "abstract": "Semantic Scholar extracted view of \"Motivation and memory.\" by Bernard Weiner", "authors": ["Bernard Weiner"], "id": "e0908a477a6cb1768a8995e00e2894ed39222520", "title": "Motivation and memory.", "references": []}, {"date": "1971", "abstract": "from the National Institutes of Health to the second author. The authors would like to thank several of their co-workers who carried out experiments reported here; each will be acknowledged at the appropriate point in the paper. Thanks are also due to William Cook, Geoffrey Loftus, and John Schnorr for a critical reading of an earlier draft of this paper.", "authors": ["Richard C. Atkinson", "Thomas D. Wickens"], "id": "8b773397e717138ddfca9d7c50ca6f509fa8452c", "title": "Human Memory and the Concept of Reinforcement*", "references": ["f15e0fc35ac4defe084b8bbc65c32089ba1a2863", "0ba1f9fd488b58df71dc823c99c5c6702291cd0e", "4ff6213a556a1fd330312bb5e6e0d6e22887e050", "c01e22dc149ee2454d7c6fa494bba020a4846162", "0b0f819608af7ad8156edb7ed78bf0841e58b712", "9537d2938a7c4e3b4ca781f95f256d6990fabd25", "a0d970ef47cd3f0c5858600196970835494ee528", "450e7ba026de0bad3a43d836567a96ea6d6e1280", "cf05588b61ea0fdfd63c784401463aeada9214a6", "75def8b4dd4e930f3eb1ff9900c5c98d142d1195"]}, {"date": "1965", "abstract": "To UNDERSTAND VISION in physiological terms represents a formidable problem for the biologist. I t am0 unts to learning how the nervous system handles incoming messages so that form, color, movement, and depth can be perceived and interpreted. One approach, perhaps the most direct, is to stimulate the retina with patterns of light while recording from single cells or fibers at various points along the visual pa thway. For each cell the optimum stimulus can be determined, and one can note the charac teristics common to cells at the next. each level in the visual pathway, and compare a given level with", "authors": ["David H. Hubel", "Torsten N. Wiesel"], "id": "f70c54029c17f50914996c834930ecbfacda195f", "title": "RECEPTIVE FIELDS AND FUNCTIONAL ARCHITECTURE IN TWO NONSTRIATE VISUAL AREAS (18 AND 19) OF THE CAT.", "references": ["b446e3fb01026b4e5ad978812540f172405fff6f", "86ea42491b0df0f23b2c23fae5618c216c9dc63d", "6ded76b03bd0612974d86a826ce70d2a2b143fb2", "d7fb932bca642615fcbcf3f5d26b2c26666603d3", "1f7b8ec98d65bd9dd9900ecbffabf5b5220a131b", "25c737626d16d89e79e0e6aab00b52d89a975eb5", "34f989681ef0ed85c9f809e96118e1a360702527"]}, {"date": "", "abstract": "The latent period in the response of Mya to illumination varies inversely as the duration of the exposure to which it is subjected. The reciprocal of the latent period, measuring the velocity of the process which underlies it, is a linear function of the exposure period. Since the duration of the exposure represents the amount of photochemical activity, it is concluded that the substances formed at that time act to catalyze a chemical reaction which determines the duration of the latent period. This explanation is in accord with the previous work on the photochemical reaction and with the effect of temperature on the latent period. As a result of the combined investigations there is presented a concrete hypothesis for the mechanism of photic reception in Mya.", "authors": ["Selig Hecht"], "id": "fe28408c1108c1f2ded89b52d9656d1d12616a92", "title": "THE NATURE OF THE LATENT PERIOD IN THE PHOTIC RESPONSE OF MYA ARENARIA", "references": ["5f6a087f9a9316c14163ee74e1fc871657e2590b", "a5616ad491c871e1d8fd7ce34dc0efaab05efca8"]}, {"date": "1966", "abstract": "Ss recalled CCC trigrams differing in incentive value, temporal position of incentive cue, and length of retention interval. Recall varied as a function of incentive and retention interval, but was unrelated to cue position. Additional evidence suggested that differential learning and covert rehearsal may mediate the facilitative effects of incentive.", "authors": ["Roger M. Tarpy", "Sam Glucksberg"], "id": "6c33c645e6e2a0d1f419f2b3de350f9a373b67cc", "title": "Effects of incentive and incentive-cue position on short-term retention", "references": []}, {"date": "1968", "abstract": "Studies of probability learning, incidental learning, short-term memory, and PA learning have shown that incentive to learn influences recall. In general, the effect is one of negative contrast: The recall of unrewarded materials suffers with no contrasting facilitation in the recall of rewarded materials. It was hypothesized that this incentive effect was due to S 's differential use of organizing responses while learning. The validity of the hypothesis was tested by determining the influence of incentive on recall with various degrees of delay in incentive cuing. Delay in cuing was expected to reduce differential use of organizing responses which, according to the hypothesis, would in turn reduce the incentive effect on recall. Measures of organizing responses conformed to the expectation that cuing delay would reduce differential responding. As predicted, the influence of incentive on recall decreased with degree of cue delay.", "authors": ["Willard F. Harley"], "id": "a0f7b2d7683d00ad0b1b204f4844af8fd894d8f1", "title": "Delay of incentive cues in paired-associate learning and its effect on organizing responses", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Studies on the relative physiological value of spectral lights. II. The sensibility of Volvox to wave-lengths of equal energy content\" by Henry Laurens et al.", "authors": ["Henry Laurens", "Jr. Henry D. Hooker"], "id": "2a5c70ea91bcd6edfe0415c6c4f756220b129ab6", "title": "Studies on the relative physiological value of spectral lights. II. The sensibility of Volvox to wave-lengths of equal energy content", "references": []}, {"date": "1937", "abstract": "A visibility curve depending on about 15 observers has been taken at an intensity comparable to the lowest used in darkroom work. A series of determinations of the wave-length of maximum visibility for several intensities and observers has also been made giving a curve extending from near the dark adapted threshold to about 2 foot lamberts. The observations were all made with the natural pupils and binocular vision. Comparisons with some previous work of other authors are made. The wave-length of maximum visibility at the highest intensity corresponds very closely with that of the standard I.E.S. photopic curve. At the lower levels, values are found approximating those given by Konig in his complete curve, and by Hecht for an intensity 2.7 times the dark adapted threshold.", "authors": ["K. S. Weaver"], "id": "9ea2293dd8dbfaf4436c6a4c573145f50587d5f5", "title": "The Visibility of Radiation at Low Intensities", "references": []}, {"date": "1961", "abstract": "Observers identified monosyllabic words presented in noise. It was found that controlling response bias eliminates the word-frequency effect. However, the magnitude of the word-frequency effect was greater than that predicted by a mathematical model denying stimulus words any role in producing the word-frequency effect.", "authors": ["Charles R. Brown", "Herbert Rubenstein"], "id": "1ad13ceb023c1bffadfc1e1318bfe27efed477c4", "title": "Test of Response Bias Explanation of Word-Frequency Effect", "references": []}, {"date": "1967", "abstract": "On each trial of a short-term memory task, Ss were required to give the correct response to a paired-associate stimulus and to estimate the number of trials intervening between study and test on that stimulus. On trials where an incorrect paired-associate response was made, judgment of the number of intervening items was relatively independent of the actual number of intervening items. On trials where a correct paired-associate response was made, judgment of the number of intervening items was accurate for recent items but incorrect for more distant items.", "authors": ["John W. Brelsford", "Richard D. Freund", "Dewey Rundus"], "id": "d65679b0af3d15d83962a97a16d9c2e6a06aef2f", "title": "Recency judgments in a short-term memory task", "references": []}, {"date": "1963", "abstract": "Summary The intelligibility of monosyllabic words in noise was investigated under a variety of constraints affecting their probability of occurrence (or predictability): verbal context, number of prescribed alternative responses, and word frequency. For all these constraint-types, intelligibility is a simple power function of probability of occurrence. This function varies with S/N ratio in a simple way which ultimately depends upon the nature of the language materials under test\u2014probably upon the number and probability distribution of alternatives. This complex, however, may be summed up by a single constant, the slope of the log I vs log P function at 0 db S/N. As a consequence of this simplicity of interaction among the variables studied, an equation is proposed relating intelligibility, probability of occurrence, and S/N ratio which requires only a single constant to be derived from the materials under consideration.", "authors": ["Herbert Rubenstein", "Irwin Pollack"], "id": "f3cfe7dd4227f1e6a614d9f3b17a1785c60df548", "title": "Word predictability and intelligibility", "references": []}, {"date": "1968", "abstract": "Publisher Summary This chapter presents a general theoretical framework of human memory and describes the results of a number of experiments designed to test specific models that can be derived from the overall theory. This general theoretical framework categorizes the memory system along two major dimensions. The first categorization distinguishes permanent, structural features of the system from control processes that can be readily modified or reprogrammed at the will of the subject. The second categorization divides memory into three structural components: the sensory register, the short-term store, and the long-term store. Incoming sensory information first enters the sensory register, where it resides for a very brief period of time, then decays and is lost. The short-term store is the subject's working memory; it receives selected inputs from the sensory register and also from long-term store. The chapter also discusses the control processes associated with the sensory register. The term control process refers to those processes that are not permanent features of memory, but are instead transient phenomena under the control of the subject; their appearance depends on several factors such as instructional set, the experimental task, and the past history of the subject.", "authors": ["Richard C. Atkinson", "Richard M. Shiffrin"], "id": "56c16d9e2a5270ba6b1d83271e2c10916591968d", "title": "Human Memory: A Proposed System and its Control Processes", "references": []}, {"date": "1964", "abstract": "The errors made in reading passages of statistical approximations were analysed. They fell into categories of omissions, substitutions and errors showing the influence of preceding words and the influence of subsequent words (including errors of transposition). A relationship is shown between errors of transposition and omissions in such a way as to indicate that the material in the eye-voice span was subject to decay in a way similar to that described in some theories of immediate memory. Many of the errors due to effects of preceding words and certain of the substitutions and omissions are shown to be consistent with the transformational model for grammar due to Chomsky.Since it can be shown mathematically and linguistically that natural language sequences cannot be treated fully by information theory, the concept of \u2018thought units' is developed to explain functionally language behaviour which may be described statistically by information theory. These thought units are then linked with the \u2018kernel stri...", "authors": ["J. Morton"], "id": "ac6c4e8d8f38b7def4d2d935d66bdd2ead7bc6c8", "title": "A Model for Continuous Language Behaviour", "references": []}, {"date": "1964", "abstract": "Sequences of 6 letters of the alphabet were visually presented for immediate recall to 387 subjects. Errors showed a systematic relationship to original stimuli. This is held to meet a requirement of the decay theory of immediate memory. \n \nThe same letter vocabulary was used in a test in which subjects were required to identify the letters spoken against a white noise background. A highly significant correlation was found between letters which confused in the listening test, and letters which confused in recall. \n \nThe role of neurological noise in recall is discussed in relation to these results. It is further argued that information theory is inadequate to explain the memory span, since the nature of the stimulus set, which can be defined quantitatively, as well as the information per item, is likely to be a determining factor.", "authors": ["Rainer Conrad"], "id": "87b00d2a3e84f6b66bc03b63374b281f0e277f29", "title": "Acoustic confusions in immediate memory.", "references": []}, {"date": "1953", "abstract": "A system has been devised for causing an image to remain at one point on the retina regardless of eye movements. A beam of light, reflected from a plane mirror on a contact lens, is used to project onto a screen an image of a dark line against a bright background. The screen is viewed by the same eye through an optical system which compensates for the doubling of the angle of rotation of the beam projected from the mirror on the contact lens. Thus, any motion of the eye causes a deviation of the beam such that the retinal image of the projected line undergoes the same displacement as do the retinal receptor cells. By comparison with normal viewing of the same test objects it is found that (1) when first presented, the finest lines are seen with normal or slightly better than normal acuity, (2) within a few seconds the lines begin to disappear, and (3) within one minute even coarse lines are seen only intermittently. The results may be interpreted in terms of local retinal adaptation to a stationary field.", "authors": ["Lorrin A. Riggs", "F. William Ratliff", "Janet C. Cornsweet", "Tom N. Cornsweet"], "id": "56155ac7656988088e45ffc7612e80570e5e1211", "title": "The disappearance of steadily fixated visual test objects.", "references": []}, {"date": "1959", "abstract": "The effect of the frequency of occurrence of words upon their intelligibility in noise was examined under two conditions: (1) in unknown message sets where the specific words under test were initially unknown to the listener; and (2) in known message sets where the specific words under test were known to the listener. Substantial effects of word frequency are observed with unknown message sets, but not with known message sets. In known message sets, the prime factor determining intelligibility is the phonemic interconfusability among the words. In unknown message sets, it is suggested that the important determinant of the intelligibility of a word is its frequency of occurrence relative to the frequencies of the words with which it might be confused.", "authors": ["Irwin Pollack", "Herbert Rubenstein", "Louis R. Decker"], "id": "a116eed0205ec8b38624a9a445c983bae870fe29", "title": "Intelligibility of Known and Unknown Message Sets", "references": []}, {"date": "1964", "abstract": "Visual duration thresholds for words were measured under three conditions: (i) with a highly predictive context; (ii) with a lesser predictive context; (iii) a control condition with no context. The results show that the visual duration threshold for a word is reduced in the presence of a context by an amount depending on the probability of the word given the context. This result agrees with previous related findings. \n \n \n \nThe errors made by the subjects could be classified into three main groups. The first of these groups showed the effect of expectancy, or response bias. Further errors showed the influence of previous stimuli or responses and the third group indicated the role of the stimulus in determining the response. A model for the recognition of words is outlined which accounts for these results and attempts to define what is meant by \u2018the effects of response bias in the perception of words\u2019.", "authors": ["Jason Morton"], "id": "85c5cf6d71415101c4004a2fd3977aa0f570b0d9", "title": "THE EFFECTS OF CONTEXT ON THE VISUAL DURATION THRESHOLD FOR WORDS.", "references": []}, {"date": "1964", "abstract": "Abstract A model is developed for paired-associate learning in which the correct response is not identified on error trials (the noncorrection procedure). The principal assumptions made are (1) that subjects can learn to eliminate error responses and (2) that response elimination and associative processes are all-or-none. A number of statistics including the mean learning curve, sequential statistics, the distributions of the trial of the last error, the total number of errors, and the number of error trials between the kth and (k + 1)st success are presented for the three-response case.", "authors": ["Richard B. Millward"], "id": "cf05588b61ea0fdfd63c784401463aeada9214a6", "title": "An all-or-none model for noncorrection routines with elimination of incorrect responses.", "references": []}, {"date": "1967", "abstract": "Many recent investigators have studied \"Response Bias\" theories of the perception of common vs. uncommon words. 4 different classes of theory are distinguished, and it is demonstrated that 3 of them are inconsistent with previously published and with fresh data. The 4th sense of response bias, however, leads to the prediction that bias on correct responses may be greater than that on errors, and is very accurately consistent with the data. This is the sense of response bias as analogous to the bias of a criterion in a statistical decision.", "authors": ["Donald E. Broadbent"], "id": "e8d3d7f35ae695a4112f560713cb9e5f02644a45", "title": "Word-frequency effect and response bias.", "references": ["c36d4e698584537e2c8968c887596e2b9aa4aa2c", "aee1683e1a07f8baf1e59ba0658b460cfad6bacc", "babbace5b2d417762ab26a6ceb601060844f4f3b", "a116eed0205ec8b38624a9a445c983bae870fe29", "921932e35bdf31d6040d0d0be620ac9c8eb25124", "f3cfe7dd4227f1e6a614d9f3b17a1785c60df548", "be3e86b216033e2f152f9e3208082d37bbc3a140", "2d72e6e9bf285172ffc09cfa5b0a9e2b6333fdee", "da2548b53a561dec5eb42265338fb7da27b7f24c", "8371f2bb81cb9d0588e23c3b9f90779b36d5f384"]}, {"date": "1961", "abstract": "Semantic Scholar extracted view of \"A direct pathway from lateral geniculate body to association cortex.\" by E. F. Vastola", "authors": ["E. F. Vastola"], "id": "25c737626d16d89e79e0e6aab00b52d89a975eb5", "title": "A direct pathway from lateral geniculate body to association cortex.", "references": []}, {"date": "", "abstract": "UNTIL recently, elementary Greek was considered a necessary part of medical education, though it was scarcely possible to justify its inclusion on the ground of utility. Higher mathematics may well take its place, for it becomes increasingly plain that a real working knowledge of it will soon be indispensable for the student of biology or medicine, whether he is content' to follow modern developments or aspires to aid its progress. The present work, Arrhenius's latest con tribution to the science, is convincing proof of this tendency.Quantitative Laws in Biological Chemistry.By Dr. S. Arrhenius. Pp. xi + 164. (London: G. Bell and Sons, Ltd., 1915.) Price 6s. net.", "authors": ["W. W. T."], "id": "5f6a087f9a9316c14163ee74e1fc871657e2590b", "title": "Quantitative Laws in Biological Chemistry", "references": []}, {"date": "1962", "abstract": "Zusammenfassung1.Die Cyto- und Myeloarchitektonik der Areae striata, occipitalis und praeoccipitalis (17, 18, 19 nach Brodmann) wurden bei der Katze an vier Frontal-, drei Sagittal- und zwei Horizontalschnittserien untersucht, die an Nachbarschnitten f\u00fcr Zell- und Markscheidendarstellung gef\u00e4rbt wurden. Zur Frage der Variationsbreite der Windungen der Hemisph\u00e4re, wurden 84 weitere Katzengehirne makroskopisch \u00fcberpr\u00fcft.2.Die untersuchten Rindenfelder des Katzengehirns weisen unterschiedliche charakteristische cyto- und myeloarchitektonische Strukturen auf. Das spezifische Merkmal der Area striata ist die Gliederung der besonders breiten IV. Schicht (Lamina granularis interna) in zwei Unterschichten (IVa + b und IVc) (bistri\u00e4re Form des Calcarinatypus nach Brodmann), die Pr\u00e4gnanz der VI. Schicht und die Schmalheit und Verschmelzung der II. und III. Schicht.In der Area occipitalis ist die III. Schicht sehr breit und in zwei Unterschichten (IIIa und IIIb) differenziert, wobei nur die Unterschicht IIIb gro\u00dfe Pyramidenzellen aufweist.Dagegen zeigt die Area praeoccipitalis keine spezifischen Besonderheiten, au\u00dfer der hellen kleinzelligen IIIa und einzelnen sehr gro\u00dfen Pyramidenzellen in der V.3.Die Areae striata, occipitalis und praeoccipitalis haben im Katzengehirn folgenden Sitz und Lagebeziehungen: Die Area striata nimmt auf der medialen Hemisph\u00e4renfl\u00e4che den Gyrus splenialis, suprasplenialis, postsplenialis und auf der Konvexit\u00e4t einen medialen Streifen des Gyrus lateralis und den Gyrus postlateralis ein. Die Area occipitalis befindet sich nur auf der Konvexit\u00e4t in der Mitte des Gyrus lateralis, direkt lateral an die Area striata anschlie\u00dfend, und erstreckt sich in den Furchengrund des Sulcus postlateralis. Die Area praeoccipitalis umgibt sowohl die Area striata als auch die Area occipitalis. Sie besetzt die mediale Wand und den Grund des Sulcus lateralis sowie den Grund des Sulcus splenialis, oft aber auch einen Teil des Gyrus suprasplenialis posterior.4.Die Gyri lateralis und postlateralis zeigen makroskopisch Variationen, die in vier Haupttypen eingeteilt werden. Der h\u00e4ufigste Typ I besitzt einen Gyrus lateralis ohne L\u00e4ngsfurche. Beim II. Typ, der eine tiefe zus\u00e4tzliche L\u00e4ngsfurche (Sulcus accessorius intralateralis) im Gyrus lateralis aufweist, erscheint die Area occipitalis an der Oberfl\u00e4che schmaler, obwohl die absolute Breite der Area occipitalis infolge ihrer Ausdehnung in dieser Furche gleich ist wie bei den F\u00e4llen ohne zus\u00e4tzliche Furche.5.Die Topographie der Areae striata, occipitalis und praeoccipitalis hat im Katzenhirn folgende Besonderheiten: 1. die Area striata breitet sich weiter rostralw\u00e4rts aus, 2. die Area occipitalis befindet sich nur auf der Konvexit\u00e4t und 3. die Area praeoccipitalis umgibt die Areae striata und occipitalis kontinuierlich.6.Die Besonderheiten der Lageverh\u00e4ltnisse der Areae striata und occipitalis, die anders sind als meist angenommen, sollten bei physiologischen und mikrophysiologischen Untersuchungen ber\u00fccksichtigt werden. Der auff\u00e4llige Faserreichtum und die starke Differenzierung der Area occipitalis wird im Zusammenhang mit der Frage des Rindenfeldes der optischen Wahrnehmung hervorgehoben.", "authors": ["Ryosaku Otsuka", "Rolf Hassler"], "id": "1f7b8ec98d65bd9dd9900ecbffabf5b5220a131b", "title": "\u00dcber Aufbau und Gliederung der corticalen Sehsph\u00e4re bei der Katze", "references": ["b2a2edaef02660afc37aeb3be4815fd630c1cce2", "2707f57a6384a5ce4d2217e573b7f70a49635cdc", "77e57d2752aa570340c4ee7e055c7a2d9509a9aa", "b557f056ea6bb07a6fa2e4e3b1fe70cdb9bdaadb", "47e8a310fb5e6f6f67f0cb4c8a4bbb8fc5902316"]}, {"date": "1967", "abstract": "College student Ss were given two trials on each of 3 lists of stimulus words with either 2 or 4 digits as response alternatives. On immediate outcome items, E said \u201cRight\u201d or \u201cWrong\u201d immediately after S's response. On delayed outcome items, E informed S that his response had been \u201cRight\u201d or \u201cWrong\u201d the next time the item was presented. On no-information items E gave no information on either trial. Delayed \u201cWrong\u201d led to significantly less response repetition than all other conditions. Differences among other conditions were generally not significant.", "authors": ["Alexander M. Buchwald"], "id": "450e7ba026de0bad3a43d836567a96ea6d6e1280", "title": "Effects o immediate vs. delayed outcomes in associative learning", "references": []}, {"date": "1954", "abstract": "Frozen sections of formalin-fixed brains containing surgical lesions, were treated with 15% ethanol for 0.5 hr., soaked in 0.5% phosphomolybdic acid for 0.25\u20131.0 hr., and subsequently treated with 0.05% potassium permanganate for 4\u201310 min. (The duration of the latter treatment is critical and individually variable). Subsequent procedure is as follows: decolorize in a mixture of equal parts of 1% hydroquinone and 1% oxalic acid; wash thoroughly and soak sections in 1.5% silver nitrate for 20\u201330 min.; ammoniacal silver nitrate (silver nitrate 0.9 g., distilled water 20 ml., pure ethanol 10 ml., strong ammonia 1.8 ml., 2.5% sodium hydroxide 1.5 ml.) 0.5\u20131.0 min.; reduce in acidified formalin (distilled water 400 ml., pure ethanol 45 ml., 1% citric acid 13.5 ml., 10% formalin 13.5 ml.) 1 min.; wash, and pass section through 1 % sodium thiosulf ate (0.5\u20131.0 min.); wash thoroughly and pass sections through graded alcohols and xylene (3 changes); cover in neutral synthetic resin.", "authors": ["Walle J. H. Nauta", "Paul Gygax"], "id": "86ea42491b0df0f23b2c23fae5618c216c9dc63d", "title": "Silver impregnation of degenerating axons in the central nervous system: a modified technic.", "references": []}, {"date": "1963", "abstract": "IN A SERIES OF STUDIES on the cat over the past 5 years we have recorded from single cells in the striate cortex and mapped receptive fields using patterned retinal stimulation. The results suggest that connections between geniculate and striate cortex, and between cortical cells, must be highly specific (5). Indeed, cells in the striate cortex respond in such a characteristic way that departures from the normal adult physiology should be easily recognizable. In the present study we have made similar experiments in kittens ranging in age from l-3 weeks. Our purpose was to learn the age at which cortical cells have normal, adult-type receptive fields, and to find out whether such fields exist even in animals that have had no patterned visual stimulation.", "authors": ["David H. Hubel", "Torsten N. Wiesel"], "id": "34f989681ef0ed85c9f809e96118e1a360702527", "title": "RECEPTIVE FIELDS OF CELLS IN STRIATE CORTEX OF VERY YOUNG, VISUALLY INEXPERIENCED KITTENS.", "references": ["345e94cf3db0022721f649d9eb75a443a39c31ba", "b0b13465c36789ebd0751178c5403efd4dd390b9", "843f1ed2050b1667a1c047d3444460c66520b378", "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7"]}, {"date": "1962", "abstract": "Starting at the same time, two timing means produce a series of output pulses at slightly different frequencies. When the two output pulses coincide, a single output pulse is produced. This will occur at a frequency equal to the difference between frequences of two original timing means and the interval between the single outputs will increase as the difference between the frequencies of the two timing means is made smaller.", "authors": ["Frank Restle"], "id": "a0d970ef47cd3f0c5858600196970835494ee528", "title": "The selection of strategies in cue learning.", "references": []}, {"date": "", "abstract": "and environments of the species; perhaps it would be most relevant for studying evolution of behavior. The second approach, however, enables systematic analysis of the influence of various environments and test situations on behavior of the species. The existence of interactions between variables does not imply that general statements cannot be made about genetic , age, or environmental factors per se. Significant main effects may still be the primary concern of the investigator and may frequently emerge. The value of such results increases considerably, however, if it is known that the effect occurs over a wide range of conditions and if the investigator is aware of specific interactions. How is symbolic information retrieved from recent memory? The study of short-term memory (1) has revealed some of the determinants of failures to remember, but has provided little insight into error-free performance and the retrieval processes that underlie it. One reason for the neglect of retrieval mechanisms may be the implicit assumption that a short time after several items have been memorized, they can be immediately and simultaneously available for expression in recall or in other responses, rather than having to be retrieved first. In another vocabulary (2), this is to assume the equivalence of the \"span of immediate mem-ory\" (the number of items that can be recalled without error) and the \"mo-mentary capacity of consciousness\" (the number of items immediately available). The experiments reported here (3) show that the assumption is unwarranted. Underlying the paradigm of these experiments is the supposition that if the selection of a response requires the use of information that is in memory, the latency of the response will reveal something about the process by which 652 References and Notes 1. 6. A longer, more detailed paper describing the complete study, including diallel cross and correlation analyses and a discussion of specific gamete, treatment, and sex effects is in preparation. Among previously undisturbed mice, mode of inheritance interacts with sex, and dominance appears within sexes. This finding is consist-ant with the results of J. H. Bruell: \"Mode of inheritance of emotional defecation in mice,\" unpublished. E the information is retrieved. Of particular interest in the study of retrieval is the effect of the number of elements in memory on the response latency. The subject first memorizes a short series of symbols. He is then shown a test stimulus, and is required to decide whether or not it \u2026", "authors": ["Martin Manosevitz"], "id": "75def8b4dd4e930f3eb1ff9900c5c98d142d1195", "title": "High-Speed Scanning in Human Memory", "references": []}, {"date": "1963", "abstract": "Cells in the cat's striate cortex show marked specificity in their responses to restricted retinal stimulation (Hubel & Wiesel, 1959, 1962). The most effective stimulus shapes are long narrow rectangles of light ('slits'), dark bars against a light background ('dark bars'), and straight-line borders separating areas of different brightness ('edges'). A given cell responds vigorously when an appropriate stimulus is shone on the receptive field or moved across it, provided the stimulus is presented in a specific orientation. This orientation is termed the 'receptive-field axis orientation'. It is critical, and constant for any particular cell, but may differ for different cells. The visual cortex is subdivided into discrete regions or columns extending from surface to white matter, in which all cells have the same receptivefield axis orientation (Hubel & Wiesel, 1962). The present experiments were undertaken with the object of learning more about the anatomical configuration of the columns. We wished to have a clearer idea of their shape, especially if the walls of a column were, as previous work suggested, parallel to the radial fibre bundles of the cortex and perpendicular to the cortical layers, and whether the columns were uniform or irregular in their cross-sectional shape and size. Furthermore, we were curious to know if there was any relationship between the receptive-field axis orientations of neighbouring columns, or whether, on the contrary, the different columns were intermixed in a random way throughout the cortex. These questions were approached: (1) by making several deep, closely spaced, parallel micro-electrode penetrations, placing electrolytic lesions at every shift in receptive-field orientation, and(2) by making many short penetrations in a small cortical area, noting the axis orientation of cells lying in the upper one or two layers of cortex.", "authors": ["David H. Hubel", "Torsten N. Wiesel"], "id": "d7fb932bca642615fcbcf3f5d26b2c26666603d3", "title": "Shape and arrangement of columns in cat's striate cortex.", "references": ["345e94cf3db0022721f649d9eb75a443a39c31ba", "6ded76b03bd0612974d86a826ce70d2a2b143fb2", "b0b13465c36789ebd0751178c5403efd4dd390b9", "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7"]}, {"date": "1967", "abstract": "A memory experiment has been performed with the following procedure. On each trial of the experiment a display of items was presented in a serial order. At the conclusion of each display S was tested for recall on one of the items. The length of the displays varied from 3 to 14 items. Plotted as serial position curves, the results showed an S-shaped recency effect and a smaller primacy effect. A specific version of a memory model formulated by Atkinson and Shiffrin (1965) was presented and applied to the data. The model assumes two memory states: a temporary storage state, called the buffer, from which retrieval is perfect; and a long-term storage state called LTS, from which retrieval is imperfect. Both response data and confidence ratings were accurately fit by the model.", "authors": ["James L. Phillips", "Richard M. Shiffrin", "Richard C. Atkinson"], "id": "9537d2938a7c4e3b4ca781f95f256d6990fabd25", "title": "Effects of list length on short-term memory", "references": []}, {"date": "1968", "abstract": "A theory of human memory is described in which a distinction is made between three memory stores: the sensory register, the short-term store, and the long-term store. Primary emphasis is given to the processes by which information is stored in and retrieved from the long-term store, a store which is considered to be a permanent repository for information. Forgetting and related phenomena are attributed to a failure of the retrieval process, in which the search through some memory area becomes less efficient as new information is placed in it. Storage and retrieval in the long-term store are conceived of as parallel processes, one mirroring the other, and each is divided into three stages for conceptual clarity. The memory trace is viewed as an ensemble of information stored in some memory location, the location of storage determined largely by the components of the ensemble itself. The ability of the system to cope with diverse phenomena is demonstrated by a consideration of a number of selected experimental paradigms.", "authors": ["Richard M. Shiffrin", "Richard C. Atkinson"], "id": "0b0f819608af7ad8156edb7ed78bf0841e58b712", "title": "Search And Retrieval Processes In Long-Term Memory", "references": ["6fa20a427d0a373869b9afa7b5dd29dbc59875d0", "f02bfb2bebd7680d6417ff51404e1c4acde54ae1", "e73f90229c2caec4e833a7ffd782cea411e5102c", "3bd6e99a19cb3ce8caba45f8f942e35edc4dfe95", "9546ba75d031f57d52e33211d95fa2e0bcd7e1cd", "9b0b1d9e91ee9fe38520091c5de74b3e9d4dcc19", "e3d2f8184f06ed486e02f228f169227dbf0a868b"]}, {"date": "1963", "abstract": "Data are presented on the effect of speech\u2010to\u2010noise ratio and context on intelligibility of monosyllabic words. The context for words in noise was provided by parts of noisefree sentences. A formula called the discriminant rule is derived from a multidimensional theory of discrimination. It accounts for the speech\u2010to\u2010noise and context effects both for these data and for word\u2010list effects found in experiments reported elsewhere. Fit of the discriminant rule to both experimental situations shows in what sense context limits the number of alternatives afforded the listener.", "authors": ["Arthur N. Stowe", "William P. Harris", "Donald B. Hampton"], "id": "c36d4e698584537e2c8968c887596e2b9aa4aa2c", "title": "Signal and Context Components of Word\u2010Recognition Behavior", "references": []}, {"date": "1966", "abstract": "The experiment tests an all-or-none theory for concept learning. The assumption tested is that the subject tries out cues (hypotheses) randomly without utilizing information from the past sequence to limit his search. The test condition run involved multiple shifts in relevancy of two cues during the course of learning. Contrary to prediction, this procedure retarded learning. A proposed revision assumes that the subject eliminates inconsistent cues but after awhile forgets that they had been eliminated. Quantitative assumptions to this effect accurately fit the present results. Related research is discussed which also is consistent with the revised sampling rule.", "authors": ["Tom Trabasso", "Gordon H. Bower"], "id": "f15e0fc35ac4defe084b8bbc65c32089ba1a2863", "title": "Presolution dimensional shifts in concept identification: A test of the sampling with replacement axiom in all-or-none models \u2606", "references": ["769d369ce8aeee3fd57dcf809a4f2ced4b8b850d", "b8c9d58c24b443591ef7cfa6e918ae6390639742", "8fdf54f252cc99fe066b3aa0916b231c06aa644d", "2b12b42c7c791aa6313726dc8531e122dddfcdb4", "a0d970ef47cd3f0c5858600196970835494ee528", "925bf4d8e95be4f691edddebabf969568720f0f7", "b77932528d145bd495591f3f0b186696e3015d39"]}, {"date": "1967", "abstract": "Abstract A multiprocess model for memory and learning is applied to the results of two complementary experiments. In Experiment I the subject was required to keep track of the randomly changing responses associated with a fixed set of stimuli. The task involved a lengthy and continuous sequence of trials, each trial consisting of a test on one of the stimuli followed by study on that same stimulus paired with a new response. The size of the stimulus set, s , took on the values 4, 6, and 8. Experiment II differed from Experiment I in that a large number of stimuli were used even though in any experimental condition the subject was required to remember only 4, 6, or 8 stimuli at one time. In both experiments the basic dependent variable was the probability of a correct response as a function of the number of intervening trials between study and test on a given stimulus-response pair (called the \u201clag\u201d). The lag curves were all near 1.0 at lag 0 and monotonically decreased as the lag increased; the lag curves for the three conditions ( s = 4, 6, and 8) decreased at different rates in Experiment I, whereas in Experiment II these curves were identical. Using four estimated parameters the model generated accurate predictions for the various response measures collected.", "authors": ["Richard C. Atkinson", "John W. Brelsford", "Richard M. Shiffrin"], "id": "0ba1f9fd488b58df71dc823c99c5c6702291cd0e", "title": "Multiprocess models for memory with applications to a continuous presentation task", "references": []}, {"date": "1962", "abstract": "This paper reports an attempt to apply a simple association learning model to the analysis of the influence of response and training variables on paired-associate learning. The issues under investigation are old ones but have not been resolved satisfactorily in the past. With the aid of the elementary learning model, the problems are posed clearly and, the data willing, adequately resolved by the use of a few simple and intuitively compelling assumptions about learning. The first problem that led to this investigation concerns the relationship betvven the number of response alternatives (N) and error rate in pairedassociate learning. Experimental results (Noble, 1955; Riley, 1952) are in agreement in showing that the number of errors subjects make before reaching some criterion of learning is greater the larger the number of response alternatives. There is little agreement, however, as to the interpretation of this fact. Two possible factors could be involved. First, the effectiveness of a reinforced trial in increasing performance (i.e., the learning rate constant) may be influenced by N; and second, N may influence the probability of being correct by sheer guessing on items that are yet unlearned. It is a reasonably safe assumption that N has the second effect on chance guessing. Previous data are unclear on whether N also influences the first factor, the", "authors": ["Gordon H. Bower"], "id": "c01e22dc149ee2454d7c6fa494bba020a4846162", "title": "An association model for response and training variables in paired-associate learning.", "references": ["fad10c0532ed3258fe4dfbf6802e0df489805c4e", "0b21768760f1fe27c4868497b15904d0549db4f8", "6b85a45b696aa5b2b740bdc2c1867eb3263d2e88", "49677d4f25784877885fc3b8f7c96ad8224e5cbd", "85cf87f7604fe5efab21c0390ff80f90178e12e6", "69596f0e2adc22555a88816067145ea878f6b8ce", "b0939797fe8e52dfda1c38572975aac8b6d91097"]}, {"date": "1963", "abstract": "In a two-wire to four-wire data switching center, the usual hybrid transformer(s) is replaced by a single transformer with a symmetrical secondary. Interaction between both directions of transmission is avoided by connecting the extremities of the secondary respectively to the output of a first current amplifier and to the input of a second current amplifier. This circuit is intended specifically for use with electronic switching such as that using MOS switching crosspoints and stages.", "authors": ["Leo Postman"], "id": "4ff6213a556a1fd330312bb5e6e0d6e22887e050", "title": "Rewards and punishments in human learning", "references": ["52446b2c27a0cc9002850d3d36ba9c52ef998e87", "6739014b3bbaa84f640679ced7ea12a251aecc6a", "75a36f9ed871bc6ca451ba70a26acdf2fbed66fd", "cc2416019190576578c47a4132f588fc7475d7d0", "422fceaaa2d82c5820bf7ffb01088b1b673b41b9", "179cc285f75519409e546f1e96d545ab360fdeea", "29caa91170188c21644570ea79130d68af2758b1", "cf6403ee11544b1933530b73c7a4f297ca7b0f47", "c119656f113324e36b278d7cfa0217c5a775c71f", "527cb39375b9925105e58b3f264dbe4a8bcce8f9"]}, {"date": "1967", "abstract": "The paper describes a method that has been extensively used to superpose a spot marking the position of the line-of-sight on photographs of stimulus scenes. The spot, reflected from the cornea, determines the line-of-sight with an accuracy of plus or minus 1\u00b0. Some of the useful measurements that can be made of fixation patterns are described. Two basic themes have guided the experimental applications: the relation between attention and the line of sight, and the association between peripheral and central vision. Comparisons among age groups have also demonstrated the erratic and piecemeal nature of children\u2019s visual input. Comparisons among scenes have shown that certain areas of a picture, judged highly informative, receive most of the visual fixations. More than 20 other laboratories have also used the camera in a wide range of research on perception, cognition, and psycholinguistics.", "authors": ["Norman H. Mackworth"], "id": "aaab50fc20aa3889205c5e4a4ad5e0f2bc38b511", "title": "A stand camera for line-of-sight recording", "references": []}, {"date": "1963", "abstract": "Ten men were asked to listen to bursts of noise which were presented to one ear, and which might or might not contain a tone. The other ear received 6 digit numbers simultaneously. The listeners reported their degree of confidence that a tone was present; in one condition they ignored the numbers and in another condition they had to report them as well as their judgement about the tone. In the latter condition they reported the tone with confidence slightly less often when it was present, but also reported it more often when it was in fact absent. Analysis of the results, by a model which supposes the brain to detect signals by a statistical decision, shows that one parameter, \u03b2, is unchanged by division of attention. This parameter measures the subjective probabilities and values associated with signal as opposed to non signal. Another parameter, d', changes when attention is divided. This quantity measures the strength of the signal relative to the random variation within the system . It is concluded that diversion of attention away from a stimulus produces an effect resembling a reduction in the intensity of the stimulus. The ignored event is therefore not blocked altogether and under suitable conditions may nevertheless produce a response from an observer.", "authors": ["Donald E. Broadbent", "Margaret Gregory"], "id": "8371f2bb81cb9d0588e23c3b9f90779b36d5f384", "title": "Division of attention and the decision theory of signal detection", "references": []}, {"date": "1964", "abstract": "Incomplete eight-letter target words in the final position of 9- or 10-word sentences were presented to college students for completion. With separate groups of subjects, the number of context letters of the target word varied from 0 to 6; and, the number of context words preceding the target word varied from 0 to 8. It was found that the obtained proportions of correct word identifications with combined word and letter contexts were substantially greater than predicted by an independence model of interaction between the effects of letter context alone and word context alone. This finding was verified by a letter by letter analysis of the results. Two alternative quantitative descriptions of the interaction of word and letter context in word identification were also examined. The predicted proportions of correct word identifications with these models more nearly approached the obtained scores.", "authors": ["Irwin Pollack"], "id": "da2548b53a561dec5eb42265338fb7da27b7f24c", "title": "Interaction of Two Sources of Verbal Context in Word Identification", "references": []}, {"date": "1964", "abstract": "Functions relating word frequency to recognition threshold and to pseudo-recognition threshold were compared for 24 Dutch Ss. Luminance thresholds were obtained to 10 Turkish words previously shown with frequencies of 1,2, 5, 10, and 25. Pseudo-recognition thresholds were obtained in the absence of stimuli for the same words and the same Ss. Drive was manipulated to determine whether it interacts with frequency, taking this interaction as an indication of response bias. The comparison of the frequency-threshold functions, the absence of a drive-frequency interaction, and the decreasing dependence of response emission on prior training with increasing stimulus information, demonstrated that response bias plays a negligible role in the frequencyrecognition relationship when stimuli are present and when a stringent recognition criterion is employed.", "authors": ["Robert B. Zajonc", "B. Nieuwenhuyse"], "id": "2d72e6e9bf285172ffc09cfa5b0a9e2b6333fdee", "title": "Relationship between word frequency and recognition: Perceptual process or response bias?", "references": []}, {"date": "", "abstract": "Auf Grund obiger Versuche uber die Verteilung der in die Area striata ankommenden peripheren optischen Impulse in der Rinde der gleichseitigen Hemisphare bei der Katze last sich sagen, das eine direkte Ubertragung dieser auf das frontale motorische Gebiet oder zu den ubrigen sensorischen Rindenzentren nicht angenommen werden kann. Es ist vielmehr wahrscheinlich, das diese Impulse, bevor sie zu den genannten Rindenregionen gelangen, in den dazwischengelegenen Rindenabschnitten eine einmalige oder mehrmalige Unterbrechung erfahren.", "authors": ["S. Poljak"], "id": "47e8a310fb5e6f6f67f0cb4c8a4bbb8fc5902316", "title": "Die verbindungen der area striata (intrakemisphaerale, kommissurale, palliodienzephalische, palliotektale Fasern) bei der Katze und deren funktionelle bedeutung", "references": []}, {"date": "1957", "abstract": "Three experiments are reported which give support to an empirical rule which may be used for predicting the entries in a closed confusion matrix for any subset of items drawn from a master set of items with a known confusion matrix. This rule, the constant\u2010ratio rule, states that the ratio between any two entries in a row of a submatrix is equal to the ratio between the corresponding two entries in the master matrix. For this statement of the rule it is assumed that the only variables which differ systematically in obtaining the two matrices are the different sets of messages and the allowable responses. This is an empirical rule which was formulated after examination of three 6\u00d76 master matrices for CV's (consonant\u2010vowel syllables) and six 3\u00d73 submatrices. Two more experiments using monosyllables and digits were then conducted to test further the rule. Although no direct experimental evidence is reported, the use of the constant\u2010ratio rule for predicting a master matrix given some of its possible submatr...", "authors": ["Frank R. Clarke"], "id": "921932e35bdf31d6040d0d0be620ac9c8eb25124", "title": "Constant\u2010Ratio Rule for Confusion Matrices in Speech Communication", "references": []}, {"date": "1955", "abstract": "Semantic Scholar extracted view of \"The organization of the visual cortex in the cat.\" by Donald Sholl", "authors": ["Donald Sholl"], "id": "b557f056ea6bb07a6fa2e4e3b1fe70cdb9bdaadb", "title": "The organization of the visual cortex in the cat.", "references": []}, {"date": "2005", "abstract": "Sehon frtlher war es mir bei Gelegenheit zufalliger ophthalmoskopischer Untersuehungen tier Augen junger Katzen und Hunde aufgefallen, dass die breehenden Medien derselben, namentlich die Cornea, mehr oder weniger getrtibt waren, und dass vor Allem das sonst so strahlende Taper der erwaehsenen Thiere bei jungen auch nicht eine Spur des spi~teren Glanzes zeigte. Daher beschloss ich bei tier naehsten sich mir darbietenden Gelegenheit, die mittelst tier ophthalmoskopischen Untersuehungs-Methoden wahrnehmbaren Veriinderungen der bezeiehneten Organe yon Anfang an zu veffolgen. Die nachfolgenden Beobachtungen wurden an einem kraftigen, 9 Tage alten, schwarzen Kiitzchen mi~nnliehen Geschleehts angestellt, das sich fortdauernd unter meiner Controlle befand and taglieh mindestens ein Mal untersueht wurde.", "authors": ["Dr. Richard Hilbert"], "id": "843f1ed2050b1667a1c047d3444460c66520b378", "title": "Ueber die nach der Geburt eintretenden entwickelungsgeschichtlichen Ver\u00e4nderungen der brechenden Medien und des Augenhintergrundes der Katze", "references": []}, {"date": "1960", "abstract": "Two messages were presented dichotically and subjects were asked to \u201cshadow\u201d whatever they heard on one ear. Somewhere in the middle the two passages were switched to the opposite ears. Subjects occasionally repeated one or two words, at the break, from the wrong ear, but never transferred to it for longer than this. The higher the transition probabilities in the passage the more likely they were to do this. One explanation might be that the \u201cselective filter\u201d (Broadbent, 1958) acts by selectively raising thresholds for signals from the rejected sources rather than acting as an all-or-none barrier.", "authors": ["Anne Treisman"], "id": "be3e86b216033e2f152f9e3208082d37bbc3a140", "title": "Contextual Cues in Selective Listening", "references": []}, {"date": "2005", "abstract": "Semantic Scholar extracted view of \"Bioelektrische Erscheinungen architektonischer Felder. Eine Methode der Lokalisation auf der Gro\u00dfhirnrinde\" by A. E. Kornm\u00fcller", "authors": ["A. E. Kornm\u00fcller"], "id": "77e57d2752aa570340c4ee7e055c7a2d9509a9aa", "title": "Bioelektrische Erscheinungen architektonischer Felder. Eine Methode der Lokalisation auf der Gro\u00dfhirnrinde", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"An experimental study of the association callosal, and projection fibers of the cerebral cortex of the cat\" by Stjepan Poljak", "authors": ["Stjepan Poljak"], "id": "2707f57a6384a5ce4d2217e573b7f70a49635cdc", "title": "An experimental study of the association callosal, and projection fibers of the cerebral cortex of the cat", "references": []}, {"date": "1950", "abstract": "Semantic Scholar extracted view of \"A retrogarde cell degeneration study of the cortical projection field of the pulvinar in the monkey.\" by Kao Liang Chow", "authors": ["Kao Liang Chow"], "id": "b2a2edaef02660afc37aeb3be4815fd630c1cce2", "title": "A retrogarde cell degeneration study of the cortical projection field of the pulvinar in the monkey.", "references": []}, {"date": "1963", "abstract": "The following interpretation of the word frequency effect\u2014the tendency for common words to be perceived correctly at much lower speech to\u2010noise ratios than uncommon words\u2014was proposed and verified experimentally. When a stimulus word is only a few decibels below its intelligibility threshold, many of its features\u2014the number of syllables, for example\u2014are still perceived correctly. If enough such features are heard, then only a small number of English words will be consistent with them. Subjects' incorrect responses will be taken from that small set of words, and furthermore they will usually be the relatively common words in the set. If the stimulus is an uncommon word and there are common English words that are phonetically quite similar to it, then these common words will usually be given as (incorrect) responses except at quite high speech\u2010to noise ratios. This interpretation is confirmed by an analysis of errors in an articulation test. The results show that all subjects tend to give the same incorrect...", "authors": ["Harris B. Savin"], "id": "babbace5b2d417762ab26a6ceb601060844f4f3b", "title": "Word\u2010Frequency Effect and Errors in the Perception of Speech", "references": []}, {"date": "1948", "abstract": "Semantic Scholar extracted view of \"Retroactive and proactive inhibition after 5 and 48 hours.\" by Benton J. Underwood", "authors": ["Benton J. Underwood"], "id": "9b0b1d9e91ee9fe38520091c5de74b3e9d4dcc19", "title": "Retroactive and proactive inhibition after 5 and 48 hours.", "references": []}, {"date": "1964", "abstract": "Semantic Scholar extracted view of \"All-or-none processes in learning and retention.\" by William Kaye Estes", "authors": ["William Kaye Estes"], "id": "9546ba75d031f57d52e33211d95fa2e0bcd7e1cd", "title": "All-or-none processes in learning and retention.", "references": []}, {"date": "1955", "abstract": "Semantic Scholar extracted view of \"Statistical theory of distributional phenomena in learning.\" by William K. Estes", "authors": ["William K. Estes"], "id": "e3d2f8184f06ed486e02f228f169227dbf0a868b", "title": "Statistical theory of distributional phenomena in learning.", "references": []}, {"date": "1964", "abstract": "Semantic Scholar extracted view of \"PRESOLUTION REVERSAL AND DIMENSIONAL SHIFTS IN CONCEPT IDENTIFICATION.\" by Tom Trabasso et al.", "authors": ["Tom Trabasso", "G Bower"], "id": "b8c9d58c24b443591ef7cfa6e918ae6390639742", "title": "PRESOLUTION REVERSAL AND DIMENSIONAL SHIFTS IN CONCEPT IDENTIFICATION.", "references": []}, {"date": "1955", "abstract": "Semantic Scholar extracted view of \"A further analysis of learning without awareness.\" by E B Philbrick et al.", "authors": ["E B Philbrick", "Leo Postman"], "id": "527cb39375b9925105e58b3f264dbe4a8bcce8f9", "title": "A further analysis of learning without awareness.", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"A disproof of the law of effect and a substitution of the laws of emphasis, motivation and disruption.\" by Edward Chace Tolman et al.", "authors": ["Edward Chace Tolman", "Calvin S. Hall", "E. P. Bretnall"], "id": "c119656f113324e36b278d7cfa0217c5a775c71f", "title": "A disproof of the law of effect and a substitution of the laws of emphasis, motivation and disruption.", "references": []}, {"date": "1960", "abstract": "Semantic Scholar extracted view of \"Learning theory and the new \"mental chemistry\".\" by William K. Estes", "authors": ["William K. Estes"], "id": "fad10c0532ed3258fe4dfbf6802e0df489805c4e", "title": "Learning theory and the new ", "references": []}, {"date": "1956", "abstract": "A very large body of experimental results have accumulated in the field of operant, or instrumental, conditioning of the rat, the pigeon, and of other experimental animals. The application to human behavior of the laws generated by such research is most often done by the use of theory. An alternative method is to demonstrate that the manipulation of classes of empirically defined variables that produce specific and highly characteristic changes in the behavior of small experimental animals in Skinner boxes produce similar changes in the behavior of college students. This paper reports procedures for the direct application of the variables defining the paradigm for operant conditioning to human behavior and shows that human beings act very much indeed like experimental animals when they are subjected to the same experimental treatments. It suggests that direct application of conditioning principles to some categories of human behavior may be justified. The procedures are simple and they may be followed by anyone, with a minimum of equipment. That it is possible to condition human motor behavior will surprise few who are concerned with behavior theory. Nevertheless, it has not always been clear what behaviors will act as \"responses,\" what events will prove to be \"reinforcing stimuli,\" or exactly what procedures would most readily yield reproducible results. This paper describes methods that have been worded out for easy and rapid operant conditioning of motor behavior in humans, states characteristic findings, and reports sample results. Developed in a series of exploratory experiments in an elementary laboratory course in psychology, the methods may have a wider utility.", "authors": ["William S. Verplanck"], "id": "cf6403ee11544b1933530b73c7a4f297ca7b0f47", "title": "The operant conditioning of human motor behavior.", "references": []}, {"date": "1937", "abstract": "* Accepted for publication by Buford Johnson of the Editorial Board, and received in the Editorial Office on February 3, 1937.", "authors": ["John Mortimer Stephens", "Joseph Alva Baer"], "id": "29caa91170188c21644570ea79130d68af2758b1", "title": "The Influence of Punishment on Learning when the Opportunity for Inner Repetition is Reduced", "references": []}, {"date": "1939", "abstract": "Semantic Scholar extracted view of \"The effect of \"right\" and \"wrong\" upon the learning of nonsense syllables in multiple choice arrangement.\" by J. W. Tilton", "authors": ["J. W. Tilton"], "id": "179cc285f75519409e546f1e96d545ab360fdeea", "title": "The effect of ", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"A comparison of emphasis upon right and upon wrong responses in learning\" by Wilbur Schofield Hulin et al.", "authors": ["Wilbur Schofield Hulin", "David A. Katz"], "id": "cc2416019190576578c47a4132f588fc7475d7d0", "title": "A comparison of emphasis upon right and upon wrong responses in learning", "references": []}, {"date": "1938", "abstract": "Semantic Scholar extracted view of \"A further investigation of the role of emphasis in learning\" by Eugene L. Porter et al.", "authors": ["Eugene L. Porter", "Calvin S. Hall"], "id": "75a36f9ed871bc6ca451ba70a26acdf2fbed66fd", "title": "A further investigation of the role of emphasis in learning", "references": []}, {"date": "1955", "abstract": "The reinforcing effects of various stimuli presented immediately following a response have been investigated largely with infra-human Ss. In the context of experiments using the operant conditioning paradigm, the accepted definition of the reinforcing stimulus is a stimulus introduced following a response that increases the probability of occurrence of that response. Despite this research utilizing infra-human Ss, there has been relatively little effort to identify reinforcing stimuli for human Ss. Many investigators have conditioned humans, but they have used only a few reinforcing stimuli. Thorndike demonstrated the effectiveness of 'right' and 'wrong' in increasing the frequency of different responses.' Hurlock demonstrated that praise and reproof significantly affected performance in the classroom situation.2 Other investigators have demonstrated that various stimuli would increase the probability of responding.3 Most of the research involving reinforcing stimuli with human Ss has, however, been designed to test problems other than the identification of reinforcing stimuli for human Ss. The primary purpose of this research was to investigate the effect of the introduction and omission of two spoken sounds following a pre-determined response on the frequency of occurrence of that response.", "authors": ["Joel Greenspoon"], "id": "422fceaaa2d82c5820bf7ffb01088b1b673b41b9", "title": "The reinforcing effect of two spoken sounds on the frequency of two responses.", "references": ["aa56f3bea0e5e8900cabf4dc834c8a1e70be652c"]}, {"date": "1955", "abstract": "Semantic Scholar extracted view of \"Compound trial-and-error learning as a function of response availability.\" by Carl E. Noble", "authors": ["Carl E. Noble"], "id": "b0939797fe8e52dfda1c38572975aac8b6d91097", "title": "Compound trial-and-error learning as a function of response availability.", "references": []}, {"date": "1941", "abstract": "Thorndike has presented a large quantity of evidence \"to show that a satisfying after-effect of a connection does in fact strengthen it under conditions equalized in respect of all other forces than the satisfying after-effect.\" 2 The experiments reported here were performed to determine whether or not it is the satisfying after-effect alone, the success of the reaction itself which 'strengthens connections' and thereby brings about repetition of the successful reaction. The Law of Effect has been tested with human subjects only in situations in which it was the obvious task of the subject to learn the rewarded material. Repetition of rewarded responses has so far been shown to occur only where such repetition was sensible; it has not been demonstrated merely as an automatic effect of reward. The experiments to be reported in the present paper were designed to present a situation in which repetition of rewarded responses was possible, while no reason for or against such repetition existed for the subject. Only if under these conditions rewarded responses are still repeated more frequently than by chance, such repetition may be attributed exclusively to an effect of the reward. Such a situation was created by adopting with modifications a procedure of Thorndike's. He had prepared a list consisting of pairs of words and numbers. The numbers were chosen from a limited sequence of numbers and arbitrarily assigned to the words. A word was read to the subject, who was to guess the corresponding number from this sequence. He was immediately told whether his response was correct or incorrect, and the experimenter proceeded to read the next word of the list. When the list was thus completed, the experimenter started again with the first word. Sooner or later in the course of the repeated presentations of the list the subject was bound to make some hits by chance. Once a correct response was found, the subject was to repeat it on subsequent presentations of the list. Correct responses were found to be repeated from presentation to presentation to an extent far exceeding chance expectation. Similar results have been obtained in many previous experiments.", "authors": ["Hans Wallach", "Mary Henle"], "id": "52446b2c27a0cc9002850d3d36ba9c52ef998e87", "title": "An experimental analysis of the law of effect", "references": []}, {"date": "1933", "abstract": "ZusammenfassungIn monotonen Reihen artgleichen Materials, wie sie die klassische Ged\u00e4chtnispsychologie verwendet hat, wirken intensive Kr\u00e4fte, die die entstandene Lernwirkung aufzuheben tendieren. Reihenglieder, die nicht in so monotoner H\u00e4ufung gegeben werden, erreichen deshalb weit h\u00f6here Reproduktionswerte als Glieder in H\u00e4ufungsstellung. Indessen beruht die untersuchte Sch\u00e4digung nicht einfach auf der Nachbarschaft vieler einander artgleicher Glieder, sondern auf Bereichsbildung und auf Absorption der Glieder in Bereichen, die durch gleichm\u00e4\u00dfigen Reihenverlauf beg\u00fcnstigt wird. Pr\u00fcfung des Wiedererkennens anstatt der Reproduktion f\u00fchrt zu demselben Ergebnis, jedoch anscheinend in schw\u00e4cherer Form. \u2014 R\u00fcckwirkende und vorw\u00e4rtswirkende Hemmung sind Nebenformen im Prinzip der gleichen Sch\u00e4digung. Dem niederen Grade, mit welchem die Bereichswirkung bei Pr\u00fcfung des Wiedererkennens wirksam zu werden scheint, d\u00fcrfte Ausbleiben einer vorw\u00e4rtswirkenden und r\u00fcckwirkenden Hemmung bei derselben Pr\u00fcfungsart entsprechen.", "authors": ["Hedwig von Restorff"], "id": "6739014b3bbaa84f640679ced7ea12a251aecc6a", "title": "\u00dcber die Wirkung von Bereichsbildungen im Spurenfeld", "references": []}, {"date": "1954", "abstract": "Semantic Scholar extracted view of \"Acquisition of S-R connections: a test of Hull's and Guthrie's theories.\" by Virginia W. Voeks", "authors": ["Virginia W. Voeks"], "id": "69596f0e2adc22555a88816067145ea878f6b8ce", "title": "Acquisition of S-R connections: a test of Hull's and Guthrie's theories.", "references": []}, {"date": "1960", "abstract": "Semantic Scholar extracted view of \"Conceptional Models of Spatial and Non-Spatial Selective Learning.\" by Kenneth W. Spence", "authors": ["Kenneth W. Spence"], "id": "49677d4f25784877885fc3b8f7c96ad8224e5cbd", "title": "Conceptional Models of Spatial and Non-Spatial Selective Learning.", "references": []}, {"date": "1960", "abstract": "Semantic Scholar extracted view of \"All-or-none and conservation effects in the learning and retention of paired associates.\" by William K. Estes et al.", "authors": ["William K. Estes", "Barbara L. Hopkins", "Edward J. Crothers"], "id": "6b85a45b696aa5b2b740bdc2c1867eb3263d2e88", "title": "All-or-none and conservation effects in the learning and retention of paired associates.", "references": []}, {"date": "1960", "abstract": "Semantic Scholar extracted view of \"A stochastic model for individual choice behavior.\" by Robert J. Audley", "authors": ["Robert J. Audley"], "id": "85cf87f7604fe5efab21c0390ff80f90178e12e6", "title": "A stochastic model for individual choice behavior.", "references": []}, {"date": "1952", "abstract": "Free-recall verbal learning is analyzed in terms of a probability model. The general theory assumes that the probability of recalling a word on any trial is completely determined by the number of times the word has been recalled on previous trials. Three particular cases of this general theory are examined. In these three cases, specific restrictions are placed upon the relation between probability of recall and number of previous recalls. The application of these special cases to typical experimental data is illustrated. An interpretation of the model in terms of set theory is suggested but is not essential to the argument.", "authors": ["George A. Miller", "William J. McGill"], "id": "0b21768760f1fe27c4868497b15904d0549db4f8", "title": "A statistical description of verbal learning", "references": []}, {"date": "1963", "abstract": "These studies investigated the effects of reversal and nonreversal shifts before solution upon performance in a later concept identification task. In 2 experiments, a reversal or a nonreversal shift after an error on a critical trial had no interfering effect upon subsequent learning. The reversal and nonreversal groups made about the same number of errors and required as many trials to learn as did controls who were not shifted. In a 3rd experiment, 1 group of Ss received reversals on every alternate error, but still made the same number of informed errors as did controls who learned with no shifts. These results support the hypothesis that learning is insightful or an all-or-nothing event in simple concept identification. In the typical two-category concept identification experiment, 5 is shown a series of complex patterns which vary in several, binary attributes. As each pattern is presented, 5 attempts to anticipate the correct classification; following his response, he is informed of the correct response. The patterns", "authors": ["G Bower", "Tom Trabasso"], "id": "b77932528d145bd495591f3f0b186696e3015d39", "title": "REVERSALS PRIOR TO SOLUTION IN CONCEPT IDENTIFICATION.", "references": []}, {"date": "1965", "abstract": "Semantic Scholar extracted view of \"ON THE DIFFERENT EFFECTS OF RANDOM REINFORCEMENT AND PRESOLUTION REVERSAL ON HUMAN CONCEPT IDENTIFICATION.\" by S B Holstein et al.", "authors": ["S B Holstein", "David Premack"], "id": "925bf4d8e95be4f691edddebabf969568720f0f7", "title": "ON THE DIFFERENT EFFECTS OF RANDOM REINFORCEMENT AND PRESOLUTION REVERSAL ON HUMAN CONCEPT IDENTIFICATION.", "references": []}, {"date": "1964", "abstract": "Recent quantitative theories of concept identification assume a limited memory during learning. To test this assumption, Ss recalled the stimulus dimensions and the correct responses on each stimulus card in 12 six-card problems. The expected recency effect was obtained but a larger, unexpected primacy effect was also observed. On those problems which were solved, the joint recall of the relevant dimension and the correct response to the cards was above chance. On unsolved problems, the recall of the relevant dimension was independent of the recall of its classification. Average recall on all problems was only slightly above the chance level. These results, with the exception of the primacy effect, support the limited memory assumption.", "authors": ["Tom Trabasso", "Gordon H. Bower"], "id": "2b12b42c7c791aa6313726dc8531e122dddfcdb4", "title": "Memory in concept identification", "references": []}, {"date": "1962", "abstract": "Semantic Scholar extracted view of \"Cue neutralization: the effects of random reinforcements upon discrimination learning.\" by Milton I. Levine", "authors": ["Milton I. Levine"], "id": "8fdf54f252cc99fe066b3aa0916b231c06aa644d", "title": "Cue neutralization: the effects of random reinforcements upon discrimination learning.", "references": []}, {"date": "1961", "abstract": "A theory of cue learning, which gives rise to a system of recurrent events in Feller's sense, is analyzed mathematically. The distribution of total errors and sampling distribution of mean errors are derived, and the learning curve is investigated. Maximum likelihood estimates of parameters and sampling variances of those estimates are derived. Likelihood ratio tests of the usual null hypotheses and approximate tests of goodness of fit of substantive hypotheses are developed. The distinguishing characteristic of these tests is that they are concerned with meaningful parameters of the learning process.", "authors": ["Frank Restle"], "id": "769d369ce8aeee3fd57dcf809a4f2ced4b8b850d", "title": "Statistical methods for a theory of cue learning", "references": []}, {"date": "1967", "abstract": "Abstract An analysis is given of a simple theory of paired-associate learning in which items may be retained for short intervals before they are learned. Theoretical questions are discussed that involve parameters of the theory, and the problem of identifying these parameters is analyzed. Analyses of several experiments lead to the conclusion that either there is no learning on trials when the presented item is already in short-term memory, or an item usually is learned during the intervals between its presentations. Results of one experiment that permits a choice between these two possibilities favor the assumption that learning occurs mainly during the intervals between an item's presentations. Finally, some statistical methods are presented for the general theory and for the special case that is supported best by the data.", "authors": ["James G. Greeno"], "id": "e73f90229c2caec4e833a7ffd782cea411e5102c", "title": "Paired-associate learning with short-term retention: Mathematical analysis and data regarding identification of parameters \u2606", "references": []}, {"date": "1964", "abstract": "Semantic Scholar extracted view of \"PAIRED-ASSOCIATE LEARNING WITH MASSED AND DISTRIBUTED REPETITIONS OF ITEMS.\" by James G. Greeno", "authors": ["James G. Greeno"], "id": "f02bfb2bebd7680d6417ff51404e1c4acde54ae1", "title": "PAIRED-ASSOCIATE LEARNING WITH MASSED AND DISTRIBUTED REPETITIONS OF ITEMS.", "references": []}, {"date": "1965", "abstract": "Abstract A two-process Markov model for paired-associate learning is presented in which stimulus-response associations may pass through an intermediate or short-term memory state before learning is complete. In the short-term state, forgetting may occur, and in the trial-dependent-forgetting (TDF) model, the likelihood that forgetting takes place on any trial is postulated to be a function of the number of S-R pairs remaining to be learned on that trial. To determine the quantitative accuracy of the model, a paired-associate experiment was conducted in which list length was varied. Specific response-sequence frequencies from experimental lists of 9, 15, and 21 items were reasonably well predicted by the TDF model. A much better account of the data was obtained by a revision of the model, in which it was assumed that the probability of learning on a given trial depended on whether an item was still in short-term memory or had been forgotten. Comparative predictions from the linear and all-or-none models, as well as an alternative two-process Markov model, are also presented.", "authors": ["Robert C. Calfee", "Richard C. Atkinson"], "id": "6fa20a427d0a373869b9afa7b5dd29dbc59875d0", "title": "Paired-associate models and the effects of list length", "references": []}, {"date": "1939", "abstract": "Semantic Scholar extracted view of \"Principles of condition in human goal behavior.\" by Anthony J. Mitrano", "authors": ["Anthony J. Mitrano"], "id": "aa56f3bea0e5e8900cabf4dc834c8a1e70be652c", "title": "Principles of condition in human goal behavior.", "references": []}, {"date": "1962", "abstract": "Words, selected from a limited range of word frequency, were presented in noise to listeners. The distribution of word frequencies of the listeners' incorrect responses were determined. The median response frequency decreases with more favourable signal-to-noise ratios, as previously noted, but only when the word frequency of the presented words was lower than the overall median response word frequency.", "authors": ["Irwin Pollack"], "id": "aee1683e1a07f8baf1e59ba0658b460cfad6bacc", "title": "Incorrect Responses to Unknown Messages Restricted in Word Frequency", "references": []}, {"date": "1967", "abstract": "Pattern perception was studied by recording eye movements while Ss visually scanned nine simultaneously presented patterns of asterisks for target patterns. Pattern parameters studied were: similarity of target patterns to non-target patterns (absolute difference in the number of elements), number of target elements, and frequency of targets. Systematic correlation between the first two pattern parameters and eyemovement parameters were found. Mean duration and mean number of fixations on targets and also on non-targets increased with increased similarity. Mean duration and mean number of fixations increased on targets with an increase in the number of target elements. Non-target patterns were perceived more auicfely than targets. Fixations of longer duration were required to perceive the original target than to identify the other target patterns subsequently. The eye-movement results provide the basis for developing inferences about higher order processing of visual stimuli.", "authors": ["John D. Gould"], "id": "f0ac50e73aa8b81177852218ffcf432e836bc237", "title": "Pattern recognition and eye-movement parameters", "references": []}, {"date": "1963", "abstract": "Abstract : A dichotomy of human memory into immediate memory and long-term memory (associative memory, habit) has been widely accepted for many years and has been formally stated by some theorists. This assumed dichotomy of the phenomena of short-term memory and long-term memory is examined and rejected in this paper. First, a number of current issues in learning theory are restated as issues about the formation, storage, and retrieval of memory traces, and the major issue is identified as the question whether short-term memory and long- term memory are points on a continuum, or a dichotomy. Then this major issue is examined in the light of data from recent studies in which the recall of single to-be-remembered alphanumeric items followed a single or very few repetitions. Finally, the issue is examined in the light of new data that relate the slope of the short-term forgetting curve to the number of elements or recoded chunks in the to-be-remembered unit, and also new data that confirm and extend Hebb's finding that there is a specific accumulative strengthening effect of repetitions in the immediate memory situation involving to-be-remembered units beyond the span of immediate memory of human subjects. The principal consequence of the conclusion that a continuum, rather than a dichotomy, is involved in short-term and longterm memory is the rejection of the postulate of autonomous decay of traces in the case of shortterm memory and acceptance of the postulate of permanence of traces, once formed, throughout all varieties of memory. (Author)", "authors": ["Arthur W. Melton"], "id": "3bd6e99a19cb3ce8caba45f8f942e35edc4dfe95", "title": "IMPLICATIONS OF SHORT-TERM MEMORY FOR A GENERAL THEORY OF MEMORY,", "references": ["c5b4337a952f602f90edb7b68995441259cd8674", "62edcc7d7f76dc9b12c9ed6f6864ba1045577452", "fed1acee3ef1d8703c9ac30ddfe164ec5ab473a9", "277ace70085295ca32d4514ce3ab99ef218d0c4b", "c0069916fe269ff59f601dfdf2a091387f883471", "64a86e32d2edb67bd284c4bd43950935a5597064", "d0c96f3fe415103229c34ae4cbf4498db3f04eae", "4023ae0ba18eed43a97e8b8c9c8fcc9a671b7aa3", "e940dce76e423390383fab059db2a77c3649e2ba", "38f60bccc379144fd3e039d8b00470731f1517ef"]}, {"date": "1970", "abstract": "Ss were shown 2,560 photographic Stimuli for 10 sec each; their recognition memory was then tested, using a two-alternative forced-choice task. Performance exceeded 90%, indicating retention of over 2,000 items, even when up to 3 days elapsed between learning and testing. Variants of the experiment showed that the presentation time could be reduced to 1 sec per picture without seriously affecting performance; also, that the stimuli could be reversed in orientation in the test situation without impairing recognition performance appreciably. The orientation of the stimuli could also be learned, although not as well as the identity of the pictures. These results indicate the vast memory for pictures possessed by human beings and emphasize the need to determine mechanisms by which this is accomplished.", "authors": ["Lionel Standing", "Jerry Conezio", "Ralph Norman Haber"], "id": "671a8dda6e02e37882dac90b222951a9bdb932db", "title": "Perception and memory for pictures: Single-trial learning of 2500 visual stimuli", "references": []}, {"date": "1969", "abstract": "Pattern discrimination was studied in a visual-search task by recordingan O\u2019s eye movements while he determined how many of eight patterns, arranged in a square around a standardpattern, matched the standard pattern. The results demonstrate the role of eye movements in visual search and human pattern discrimination. The mean duration of an eye fixation on a pattern, the probability of fixating it, the probability ofrefixating it, and the sequence in which patterns were fixated were all systematically related to various pattern measures. Multivariateanalyses showed modest correlations between the duration of individual eye fixations and various pattern measures. Relative characteristics of patterns influenced performance more than absolute characteristics of patterns. Patterns that matched a standard were fixated more often and longer than patterns that did not match a standard. The order in which patterns were fixated depended upon their relative characteristics. The results were consistent with a model ofpattern discrimination consistingoftwo stagesin which (1) features of a fixated pattern are abstracted and encoded, and (2) these features are then compared with the features ofanother pattern.", "authors": ["John D. Gould", "Amanda B. Dill"], "id": "ee02d5a38bd392ac65ebb93436aa6668907369f7", "title": "Eye-movement parameters and pattern discrimination", "references": []}, {"date": "1975", "abstract": "Work on PLANNER-73 and actors has led to the development of a basis for semantics of programming languages. Its value in describing programs with side-effects, parallelism, and synchronization is discussed. Formal definitions are written and explained for sequences, cells, and a simple synchronization primitive. In addition there is discussion of the implications of actor semantics for the controversy over elimination of side-effects.", "authors": ["Irene Greif", "Carl Hewitt"], "id": "692c91426fa930ce043bee9d1e80410927253139", "title": "Actor semantics of PLANNER-73", "references": []}, {"date": "2004", "abstract": "SummaryProof methods adequate for a wide range of computer programs have been expounded in [1] and [2]. This paper develops a method suitable for programs containing functions, and a certain kind Of jump. The method is illustrated by the proof of a useful and efficient program for table lookup by logarithmic search.", "authors": ["Maurice Clint", "C. A. R. Hoare"], "id": "244485ba5b8a0fcb678af91dec91d12d13f0f1d3", "title": "Program proving: Jumps and functions", "references": ["5d8056e326d4199d157a17fbeee97a7349d2824c", "9983c1923bd2cb5aa26c5f51dae9b99b31fe523d"]}, {"date": "1967", "abstract": "Semantic Scholar extracted view of \"In Mathematical Aspects of Computer Science\" by Robert W. Floyd", "authors": ["Robert W. Floyd"], "id": "405666eddc9d6db81880a195b26d5a66a153cc36", "title": "In Mathematical Aspects of Computer Science", "references": []}, {"date": "1969", "abstract": "The present research attempted to manipulate the encoding modality, pictorial or verbal, of schematic faces with well-learned names by manipulating S\u2019s expectations of the way the material was to be used. On every trial, a single name or face was presented, followed by another one; the S was asked to respond \u201csame\u201d if the stimuli had the same name, and \u201cdifferent\u201d otherwise. The majority of second stimuli of any session was either names or faces. It was hypothesized that if S had encoded the first stimulus in the modality of the second, his judgment would be faster than if he had not appropriately encoded the first stimulus. Significantly slower reaction times were obtained to stimulus pairs where the second stimulus modality was infrequent. Further evidence that encoding of the first stimulus was in the frequent second stimulus modality comes from the finding that \u201cdifferent\u201d responses were shorter when the stimuli differed on more than one attribute in the encoding (second stimulus) modality, regardless of the modality of the stimuli. Thus, evidence is presented that not only can verbal material be pictorially encoded (and vice versa), but that whether either verbal or pictorial material is verbally or pictorially encoded depends on S\u2019s anticipation of what he is to do with the material.", "authors": ["Barbara Tversky"], "id": "15db31b1e86e3e0142db73b23c9a394d02b545ce", "title": "Pictorial and verbal encoding in a short-term memory task", "references": ["5acbf8b91494a23f906fa973ef2da1163ba5e98e", "767c5dfa4e939749f369ac91d44d9614193e8804", "87b00d2a3e84f6b66bc03b63374b281f0e277f29", "f1dd4a4b12b5cc0a836ad0bc84fa18fea72a1cf8", "769e02fe5aacd5876dc16f384e631f89a885c361", "c89e78f325e64cd6fa41e35dc9cfb03d2cea72ed", "95d207af130c9533e6a118d4c38e26658e2d6061", "64d959f0d8609ca03bf7fcb1343be0505645a5bb", "6e22d253b24b405b7331762b6d75dd619f2de887", "3b6cc65066ff34c3fc5833adbe3e7ade2ac6cc4a"]}, {"date": "1972", "abstract": "Presented are successful efforts in proving that computer programs are correct. Included are (i) the methods used, (ii) the wide class of programs (including systems programs) that have been proved, and (iii) implemented computer systems for demonstrating correctness. There is also a partially annotated bibliography.", "authors": ["Ralph L. London"], "id": "a17e7dce1c39b30d22158e4a8c133d0a4b36f1ca", "title": "The current state of proving programs correct", "references": []}, {"date": "1972", "abstract": "Semantic Scholar extracted view of \"An axiomatic definition of the programming language PASCAL\" by C. A. R. Hoare", "authors": ["C. A. R. Hoare"], "id": "ed930c69cdc66f983c5abfd041ce9fef3565c08b", "title": "An axiomatic definition of the programming language PASCAL", "references": []}, {"date": "1968", "abstract": "In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This involves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantage, both theoretical and practical, may follow from a pursuance of these topics.", "authors": ["C. A. R. Hoare"], "id": "5d8056e326d4199d157a17fbeee97a7349d2824c", "title": "An Axiomatic Basis for Computer Programming", "references": []}, {"date": "1964", "abstract": "Summary A series of experiments was designed to investigate the monitoring and storage of the irrelevant message during selective attention to one of two dichotic speech messages, and thus to throw light on the nature and level of selective \u201cfiltering\u201d (Broadbent, 1958) . The experiments extend Cherry's finding (1953) that Ss noticed if the rejected message was identical to the selected one but a few seconds out of step. It was shown that (a) the comparison between the messages must be made at a central level, and required the identification of words and meaning rather than a simple analysis of the sounds, since S s noticed the identity of the messages even when they were in different voices or different languages; (b) their identity is noticed at a 5-sec interval when a selected message of coherent prose leads the rejected one in time, but at 1 or 2 sec only when the order is reversed. The first result supports Treisman's suggestion (1960) that the filter acts by attenuating rather than blocking irrelevant signals. The second may give a measure of the different storage times for selected and rejected material. The critical delay interval for recognition also varied with the verbal content of the selected message: it decreased both when the information content of the words was high, and when the order of the items was crucial to the recognition. The relation of these to other findings on immediate memory is discussed.", "authors": ["Anne Treisman"], "id": "fe843bf6a7ee65c58382267e1a0695006c189f1b", "title": "Monitoring and storage of irrelevant messages in selective attention", "references": []}, {"date": "1970", "abstract": "Semantic Scholar extracted view of \"7 \u2013 A Functional Model for Memory1\" by J. Morton", "authors": ["J. Morton"], "id": "7d4bea05a9a4332b653cce68d47298dc838d16f9", "title": "7 \u2013 A Functional Model for Memory1", "references": []}, {"date": "1968", "abstract": "This paper lists twelve conditions that must be fulfilled by a satisfactory theory of visual pattern recognition in animals and man : such a theory must explain (1) size invariance, (2) position invariance, (3) brightness invariance, (4) the equivalence of outline and filled-in shapes, (5) lack of invariance under most rotations, (6) the known confusions made between patterns, (7) animals\u2019 ability to disregard jitter, (8) the human ability to segment shapes in different ways, (9) the way in which man recognizes complex scenes without recognizing individual details of the scene, (10) perceptual learning, (11) the way in which the brain takes advantage of the redundancy of the visual environment; (12) finally any theory must be consistent with the physiological evidence. The outlines of a theory meeting these conditions are put forward. The model has three parts : (1) A processor that extracts local features from the input picture preserving information about the spatial relationships between the features. (2) A mechanism that induces an abstract description of the output from the processor. (3) A store in which such descriptions are held. Some suggestions are made about the language in which descriptions are framed : it must contain hierarchical elements to allow for the possibility of segmenting a picture in different ways. It is shown how the properties of such a language could account for the facts listed above. In recognizing a picture the output from the store is matched to a stored descriptive rule. What we see is the descriptive rule selected to describe an input picture\u2014we cannot respond to details of the picture not represented in the rule. Since, however, many different rules describe the same input picture, animals and man respond to different aspects of the same picture on different occasions of presentation.", "authors": ["N. S. Sutherland"], "id": "8d8e1983d277b6182365de3c7bfac9326583bdd8", "title": "Outlines of a theory of visual pattern recognition in animals and man", "references": ["244bfff60b1a3070c03d82592feb301947c4ae4b", "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7", "34c0fbcc2f7e7b1cc9594461c27f16ea0747b494", "c5f5311fa1f34159ab3a0a1d58da51cd0340a640", "8907e8431fc2340a6cd2109d0c342929d6b3cc1e", "ba9d3c04e6cbd13eeef7c833888b723911273975", "46aedf6f440399361f1c1264a8402e8e05bab992", "8c698a0e0e54ef70bdb5a41585598b84400db178"]}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"The Relationship among Reported Strategies, Presentation Rate, and Verbal Ability and Their Effects on Free Recall Learning.\" by David M. Stoff et al.", "authors": ["David M. Stoff", "Morris N. Eagle"], "id": "7f63bcc1a9013da9e9ca5ca1351b64a937e6c29c", "title": "The Relationship among Reported Strategies, Presentation Rate, and Verbal Ability and Their Effects on Free Recall Learning.", "references": []}, {"date": "1970", "abstract": "Subjects responded as soon as they heard a preset target in a sequence of nonsense syllables. The target was a complete syllable (e.g., \u201cbaeb\u201d \u201csaeb\u201d) or a phoneme from that syllable, the syllable-initial consonant phoneme for some objects (e.g., \u201cb-\u201d or \u201cs-\u201d), and the medial vowel phoneme for other subjects (e.g., \u201c-ae-\u201d). Subjects responded more slowly to phoneme targets than to syllable targets (by 40 msec for /s-/, 70 msec for /b-/ and 250 msec for medial /ae/). These results indicate that phoneme identification is subsequent to the perception of larger phonological units. The reality of the phoneme is demonstrated independently of speech perception and production by the natural presence of alphabets, rhymes, spoonerisms, and interphonemic contextual constraints.", "authors": ["Harris B. Savin", "Thomas G. Bever"], "id": "b96df0e70e7c9aa041c38996c63e57dbb200efc6", "title": "The nonperceptual reality of the phoneme", "references": []}, {"date": "1970", "abstract": "Semantic Scholar extracted view of \"Memory and Verbal Learning.\" by Endel Tulving et al.", "authors": ["Endel Tulving", "Stephen A. Madigan"], "id": "500bc555bbb2344871e61669d686570a8cbc9a90", "title": "Memory and Verbal Learning.", "references": []}, {"date": "1963", "abstract": "Any system--and I shall not define system more closely than as a group of somehow integrated subordinate units that forms an entity of its own--mus( have a certain architecture, structure, or morphology, which is reasonably constant in time. The way the system is put together, I like to call its \"being.\" This system interacts with its environment, responding to stimuli--whether at the more complex level of organisms giving behavioral responses or simply a rubber band yielding to a weight--and most of these responses are in effect ephemeral, reversible changes in time. The system yields and restores itself, and this I like to call its \"behaving.\" But under certain conditions the interaction of the system and its environment leads to irreversible changes; the system has altered as a result of its experience. I t fixes its experience and so becomes something different, and this I like to call \"becoming.\" So this addiction to alliteration gives us architecture or being, functioning or behaving, and development or becoming. Becoming subsumes, of course, development of the individual, evolution of the species, history of the particular society or social group of any kind, and learning in the individual. And learning may include, if you accept a broad definition, changes as varied as: the hypertrophy of a muscle with exercise; the horny hands of a laborer; and the many other material changes that record the p a s t as in that lovely couplet on weatherbeaten trees: Is it as plainly in our living shown, By slant and twist, which way the wind hath blown?", "authors": ["Ralph Waldo Gerard"], "id": "38f60bccc379144fd3e039d8b00470731f1517ef", "title": "The material basis of memory", "references": []}, {"date": "1960", "abstract": "Can a machine think? The answer to this old chestnut is certainly \"yes\": Computers have been made to play chess and checkers, to prove theorems, to solve intricate problems of strategy. Yet the intelligence implied by such activities has an elusive, unnatural quality. It is not based on any orderly development of cognitive skills. In particular, the machines are not well equipped to select from their environment the things, or the relations, they are going to think about. In this they are sharply distinguished from intelligent living organisms. Every child learns to analyze speech into meaningful patterns long before he can prove any propositions. Computers can find proofs, but they cannot understand the simplest spoken instructions. Even the earliest computers could do arithmetic superbly, but only very recently have they begun to read the written digits that a child recognizes before he learns to add them. Understanding speech and reading print are examples of a basic intellectual skill that can variously be called cognition, abstraction or perception; perhaps the best general termfor it is patternrecognition. Except for their inability to recognize patterns, machines (or, more accurately, the programs that tell machines what to do) have now met most of the classic criteria of intelligence that skeptics have proposed. They can outperform their designers: The checker-playing program devised by Arthur L. Samuel of International Business Machines Corporation (1959a) usually beats him. They are original: The \"Logic Theorist,\" a creation of a group from the Carnegie Institute of Technology and the RAND Corporation [Newell, Simon, and Shaw (1956a, 19576)] has found proofs for many of the theorems in Principia Mathematica, the", "authors": ["Oliver G. Selfridge", "Ulric Neisser"], "id": "8def1503d588f2dc413e0b3e769be5e033d9e737", "title": "Pattern recognition by machine", "references": []}, {"date": "1972", "abstract": "Semantic Scholar extracted view of \"Cognitive basis of language learning in infants.\" by John Macnamara", "authors": ["John Macnamara"], "id": "5ee5b89dc29620fb8e2cc19a885d0b7a13df0715", "title": "Cognitive basis of language learning in infants.", "references": []}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"Notes on Avoiding \"go to\" Statements\" by Donald E. Knuth et al.", "authors": ["Donald E. Knuth", "Robert W. Floyd"], "id": "9983c1923bd2cb5aa26c5f51dae9b99b31fe523d", "title": "Notes on Avoiding ", "references": []}, {"date": "1959", "abstract": "Semantic Scholar extracted view of \"Fate of first-list associations in transfer theory.\" by Janine M. Barnes et al.", "authors": ["Janine M. Barnes", "Benton J. Underwood"], "id": "d0c96f3fe415103229c34ae4cbf4498db3f04eae", "title": "Fate of first-list associations in transfer theory.", "references": []}, {"date": "1962", "abstract": "Summary Three experiments were performed to determine the relationship between certain variables influencing proactive inhibition in long-term retention of lists of verbal items and the influence of these variables on short-term retention of single items. More particularly, retention of single items over 18 sec. should, if the laws of long-term retention are applied, decrease with number of previous items to which S has been exposed. In addition, amount of forgetting should be a direct joint function of number of previous items and length of the retention interval. In Exp. 1 each S was presented consonant syllables singly, with retention being measured after 3, 9, and 18 sec. Forgetting of the first item presented (T-1) was less than for the second (T-2) or third (T-3) item, but forgetting of the latter (T-2 vs. T-3) did not differ. On all three tests forgetting was directly related to length of retention interval, but no interaction was evident between number of previous items and length of retention interval. In Exp. 2 a higher degree of initial learning of the items was achieved. Forgetting increased directly as a function of number of previous items presented. The predicted interaction was indeterminate since retention was essentially 100% on T-1 for all retention intervals. Experiment 3 tested retention of six successive items over 3- and 18-sec. intervals. Retention after 3 sec. showed an initial drop and then a rise over the six tests, the rise suggesting a practice effect. Forgetting over 18 sec. increased directly from T-1 to T-6 and there was no indication that a constant amount of proactive interference had been reached. The interaction between length of retention interval and number of potential proactively interfering items was very evident. The results were interpreted to mean that proactive inhibition in short-term memory of single items follows the same laws as proactive inhibition in long-term memory of lists of items.", "authors": ["Geoffrey Keppel", "Benton J. Underwood"], "id": "64a86e32d2edb67bd284c4bd43950935a5597064", "title": "Proactive inhibition in short-term retention of single items", "references": []}, {"date": "1963", "abstract": "Summary This paper outlines the general approach of drawing inferences about the flow of information inside the organism, and summarizes the evidence for distinguishing short-term from long-term memory. A minor experiment is described, in which short-term memory is compared for items which have been followed by a fixed number of interfering items at fast and at slow speeds. The latter produced inferior performance, which is consistent with a decay theory. The greatest emphasis is placed, however, upon the different role of similarity in short- and in long-term memory.", "authors": ["Donald E. Broadbent"], "id": "c0069916fe269ff59f601dfdf2a091387f883471", "title": "Flow of information within the organism", "references": []}, {"date": "1961", "abstract": "Semantic Scholar extracted view of \"Perseverative neural processes and consolidation of the memory trace.\" by Stephen E. Glickman", "authors": ["Stephen E. Glickman"], "id": "277ace70085295ca32d4514ce3ab99ef218d0c4b", "title": "Perseverative neural processes and consolidation of the memory trace.", "references": []}, {"date": "1966", "abstract": "4 experiments were conducted to determine the time required to make some simple memory-dependent decisions. S's task was to decide whether any of the items of a memorized check list were contained in a visually displayed search list, and to register his decision as quickly as possible by pressing 1 of 2 response keys. RT varied directly both with the number of items in the check list and the number in the search list, and inversely with the number of items common to both lists. Practice reduced RT across conditions, and it also decreased, but did not eliminate, the effects of the independent variables. Decreases in RT with practice were accompanied, in most cases, with increases in the frequency of errors. How does the search of a memory representation of a visual display compare with the search of the visual display itself? Suppose that 6\" is given the task of deciding whether two visually presented sets of characters have any items in common. If one set is presented only after the other has been erased, then the task is memory dependent and may be described either as that of determining whether any of the items in the memory representation are included in the visual display, or, conversely, as that of deciding whether any of the displayed items are included in the memory representation. Consider in particular two contrast", "authors": ["Raymond S. Nickerson"], "id": "3b6cc65066ff34c3fc5833adbe3e7ade2ac6cc4a", "title": "Response times with a memory-dependent decision task.", "references": []}, {"date": "1968", "abstract": "Four sets of paired visual stimuli (OO, XX, XO, or OX) were judged by 48 subjects to be either \u201csame\u201d or \u201cdifferent.\u201d Decision latencies of the same and different judgement were studied as a function of the inter-stimulus interval (ISI). In Experiments I and II, in which stimulus durations were 70 millisec., decision latencies showed marked increases when the ISI was reduced to 100 millisec., but in Experiments III and IV, in which the stimulus durations were only 40 millisec., comparable increases did not occur until the ISI was reduced to 50 millisec. These increases were more marked for \u201csame\u201d than for \u201cdifferent\u201d judgements, although overall decision latencies were generally shorter for \u201csame\u201d judgements. The effects of varying ISIs and stimulus durations are interpreted in terms of masking; they fail to support an hypothesis of central intermittency.", "authors": ["Michael C. Corballis", "Warren J. Lieberman", "Dalbir Bindra"], "id": "64d959f0d8609ca03bf7fcb1343be0505645a5bb", "title": "Discriminability and Central Intermittency in Same-Different Judgements", "references": []}, {"date": "1965", "abstract": "Summary Short-term memory for a list of four letters, followed by a list of eight letters that the S s copied as they were presented, followed by immediate recall of the original four-letter list, was shown to be a function of the acoustic similarity of the intervening list to the original list. An interfering list whose letters have similar pronunciation to the letters in the original list produces greater RI than an interfering list whose letters have a very different pronunciation from the letters in the original list. An interfering list composed of items identical to items in the original list, but in a different order, tends to produce less RI in the recall of items and more RI in the recall of the correct position of these items than an interfering list composed of similar items. These findings for STM are completely consistent with analogous studies of RI as a function of similarity in LTM.", "authors": ["Wayne A. Wickelgren"], "id": "6e22d253b24b405b7331762b6d75dd619f2de887", "title": "Acoustic similarity and retroactive interference in short-term memory", "references": []}, {"date": "1968", "abstract": "When a subject is asked to judge whether two stimuli are \u201csame\u201d or\u201cdifferent,\u201d the time he takes to reach the decision same is frequently unequal to the time he takes to reach the decision different. We studied this discrepancy as a function of several variables, including stimulus modality, \u201ccodability\u201d vs.\u201cnoncodability\u201d of test stimuli, interstimulus interval, and discrimination difficulty. Results of four different experiments performed on a total of 111 subjects showed codability and discrimination difficulty to be the most important factors. Stimuli that are codable (i.e., which can be categorized by absolute judgment) yield a shorter latency for decision same, and noncodable stimuli (i.e., those requiring a reference stimulus for categorization) yield a longer latency for decision same. The modality of test stimuli, the prothetic or metathetic nature of the dimension to be judged, and simultaneous vs. successive presentation of the stimuli appear not to be crucial factors.", "authors": ["Dalbir Bindra", "D. C. Donderi", "Shizuhiko Nishisato"], "id": "95d207af130c9533e6a118d4c38e26658e2d6061", "title": "Decision latencies of \u201csame\u201d and \u201cdifferent\u201d judgments", "references": []}, {"date": "1959", "abstract": "The interpretive cortex has in it a mechanism for instant reactivation of the detailed record of the past. It has a mechanism also for the production of interpretive signals. Such signals could only be significant if past records are scanned and relevant experiences are selected for comparison with present experience. This is a subconscious process. But it may well be that this scanning of past experience and selection from it also renders the relevant past available for conscious consideration as well. Thus, the individual may refer to the record as he employs other circuits of the brain. Access to the record of the past seems to be as readily available from the temporal cortex of one side as from that of the other. Auditory illusions (or interpretations of the distance, loudness, or tempo of sounds) have been produced by stimulation of the temporal cortex of either side. The same is true of illusional emotions, such as fear and disgust. But, on the contrary, visual illusions (interpretations of the distance, dimension, erectness, and tempo of things seen) are only produced by stimulation of the temporal cortex on the nondominant (normally, right) side of the brain. Illusions of recognition, such as familiarity or strangeness, were also elicited only from the nondominant side, except in one case.", "authors": ["Wilder Penfield"], "id": "244bfff60b1a3070c03d82592feb301947c4ae4b", "title": "The Interpretive Cortex", "references": []}, {"date": "1958", "abstract": "First published in 1958, this book has become recognized as a classic in its field. It marked a transition between behaviourist learning theory and the modern 'information processing' or 'cognitive' approach to perception and communication skills. It continues to provide a principal starting point for theoretical and experimental work on selective attention. As Professor Posner writes in his Foreword to the reissue: 'it remains of great interest to view the work in its original form and to ponder those creative moments when the mind first grasps a new insight and then struggles to work out its consequences.", "authors": ["Donald E. Broadbent"], "id": "fed1acee3ef1d8703c9ac30ddfe164ec5ab473a9", "title": "Perception and communication", "references": []}, {"date": "1966", "abstract": "It has been shown that short-term memory (STM) for word sequences is grossly impaired when acoustically similar words are used, but is relatively unaffected by semantic similarity. This study tests the hypothesis that long-term memory (LTM) will be similarly affected. In Experiment I subjects attempted to learn one of four lists of 10 words. The lists comprised either acoustically or semantically similar words (A and C) or control words of equal frequency (B and D). Lists were learned for four trials, after which subjects spent 20 min. on a task involving immediate memory for digits. They were then asked to recall the word list. The acoustically similar list was learned relatively slowly, but unlike the other three lists showed no forgetting. Experiment II showed that this latter paradox can be explained by assuming the learning score to depend on both LTM and STM, whereas the subsequent retest depends only on LTM. Experiment III repeats Experiment I but attempts to minimize the effects of STM during learning by interposing a task to prevent rehearsal between the presentation and testing of the word sequences. Unlike STM, LTM proved to be impaired by semantic similarity but not by acoustic similarity. It is concluded that STM and LTM employ different coding systems.", "authors": ["Alan Baddeley"], "id": "c89e78f325e64cd6fa41e35dc9cfb03d2cea72ed", "title": "The Influence of Acoustic and Semantic Similarity on Long-term Memory for Word Sequences", "references": []}, {"date": "1964", "abstract": "Abstract : Sixteen rats were trained to discriminate between a square and a rectangle: both shapes were presented in different orientations to different animals. All animals were trained in succession with positive shape only, positive and negative shapes presented simultaneously, positive and negative shapes presented successively; finally they were given tests with 18 transfer shapes presented singly. Although the rats showed a strong preference for the square early in training, they later responded more accurately to the rectangle presented on its own than to the square. There was no tendency to transfer from a rectangle in one orientation to rectangles in other orientations, and indeed when a horizontal rectangle was substituted for a vertical or vice versa negative transfer resulted. The rats (unlike octopuses) showed complete transfer to outline shapes. The transfer test results are discussed in relation to recent physiological discoveries about retinal receptive fields. (Author)", "authors": ["N. S. Sutherland", "Anne E. Carr"], "id": "46aedf6f440399361f1c1264a8402e8e05bab992", "title": "SHAPE DISCRIMINATION BY RATS: SQUARES AND RECTANGLES", "references": []}, {"date": "1951", "abstract": "Semantic Scholar extracted view of \"An examination of the electrical field theory of cerebral integration.\" by Karl S. Lashley et al.", "authors": ["Karl S. Lashley", "Kao Liang Chow", "Josephine Semmes"], "id": "ba9d3c04e6cbd13eeef7c833888b723911273975", "title": "An examination of the electrical field theory of cerebral integration.", "references": []}, {"date": "1966", "abstract": "Abstract In goldfish the optic nerve fibers which subserve dorsal and ventral halves of the retina separate anatomically into lateral and medial optic nerve brachia before projecting to different regions of the optic tectum. In this species it is known that surgical interruption of the optic nerve produces only temporary blindness since the optic nerve fibers are able to regenerate and in time re-establish proper connections within the tectum. By cutting one or the other of the brachia, vision was temporarily restricted to one half of the visual field. The fish were then trained to discriminate between a vertical and a horizontal rectangle using one half of the visual field of one eye. The fish were blinded in the other eye and in addition training was always discontinued long before vision was restored to the initially operated brachium. Finally, the initially unoperated brachium was cut. In subsequent behavioral tests all subjects displayed positive intraretinal transfer of the learned discrimination across the horizontal meridian.", "authors": ["J. R. Cronly-Dillon", "N. S. Sutherland", "Jacques Wolfe"], "id": "8907e8431fc2340a6cd2109d0c342929d6b3cc1e", "title": "Intraretinal transfer of a learned visual shape discrimination in goldfish after section and regeneration of the optic nerve brachia", "references": []}, {"date": "1960", "abstract": "The perception of depth involves monocular and binocular depth cues. The latter seem simpler and more suitable for investigation. Particularly important is the problem of finding binocular parallax, which involves matching patterns of the left and right visual fields. Stereo pictures of familiar objects or line drawings preclude the separation of interacting cues, and thus this pattern-matching process is difficult to investigate. More insight into the process can be gained by using unfamiliar picture material devoid of all cues except binocular parallax. To this end, artificial stereo picture pairs were generated on a digital computer. When viewed monocularly, they appear completely random, but if viewed binocularly, certain correlated point domains are seen in depth. By introducing distortions in this material and testing for perception of depth, it is possible to show that pattern-matching of corresponding points of the left and right visual fields can be achieved by first combining the two fields and then searching for patterns in the fused field. By this technique, some interesting properties of this fused binocular field are revealed, and a simple analog model is derived. The interaction between the monocular and binocular fields is also describea. A number of stereo images that demonstrate these and other findings are presented.", "authors": ["B\u00e9la Julesz"], "id": "8c698a0e0e54ef70bdb5a41585598b84400db178", "title": "Binocular depth perception of computer-generated patterns", "references": []}, {"date": "1963", "abstract": "Octopuses were trained in a successive situation to discriminate between vertical and horizontal rectangles: Group S was trained with rectangles of side length 5 \u00d7 1 cm., Group M with 10 \u00d7 2 cm. rectangles, and Group L with 20 \u00d7 4 cm. rectangles. The larger the shapes used, the more readily were they discriminated, both in terms of speed of learning and of the asymptote of performance. After training, each group was given transfer tests with the two pairs of rectangles not used in training and with two further pairs of 2.5 \u00d7 0.5 cm. and 40 \u00d7 8 cm. The results can be summarized in three generalizations: (1) When the size of a shape is changed, performance is worse than on the original training shape. (2) The bigger the change in proportionate size, the less transfer is shown. (3) For corresponding changes of proportionate size, there is better transfer to larger shapes than to smaller. These generalizations are supported by data from earlier experiments on the question of transfer to different sized shapes: some of these data were reworked and are presented in detail here. The theoretical implications of the results are discussed.", "authors": ["N. S. Sutherland", "Anne E. Carr"], "id": "34c0fbcc2f7e7b1cc9594461c27f16ea0747b494", "title": "The Visual Discrimination of Shape by Octopus: The Effects of Stimulus Size", "references": []}, {"date": "1967", "abstract": "The two experiments reported are concerned with short-term memory for digit lists simultaneously presented both auditorily and visually. Results showed (1) that interpolated written and verbal recall differentially affect retention depending on whether the to-be-recalled list was presented auditorily or visually. (2) That input modality appears to be far more important for recall than was directing subjects' attention to a list during input, when that list might or might not have been subsequently required for recall. The results suggest that short-term storage is modality specific. In this case, Broadbent's P and S mechanisms do not adequately describe what happens during simultaneous visual and auditory presentation. Nor would Sperling's suggestion of a final auditory store appear to be supported.", "authors": ["S A Margrain"], "id": "769e02fe5aacd5876dc16f384e631f89a885c361", "title": "Short-Term Memory as a Function of Input Modality", "references": []}, {"date": "1963", "abstract": "Summary Three experiments were carried out to test the generality of the verbal loop hypothesis\u2014the hypothesis that S 's perceptual processing of a stimulus includes a covert verbalization and that the length of the verbalization determines the difficulty of the stimulus. In Exp. I, a relation was demonstrated between verbalization length and the accuracy of report of the binary numbers 0 to 11111111 (decimal 255). Since the binary numbers varied in length from one to eight digits, an alternative explanation based on the physical length of the stimulus was considered. It was found that predictions based on physical length (the number of digits) were almost as effective as predictions on the basis of verbalization length. To settle the issue between the explanation in terms of physical length and verbal loop, a second experiment was carried out which replicated Exp. I in all details except for the form of the stimuli. In Exp. II all the binary numbers were converted into eight-digit form by adding the appropriate supplementary number of zeroes to all binary numbers that had fewer than eight digits (e.g., 10 was converted into 00000010). In effect, physical size was held constant. With this form of the binary numbers, the relation between verbalization length and accuracy was shown to maintain itself. The relation between verbalization length and accuracy is, therefore, the more general one. A third experiment was carried out with the eight-digit form of the binary numbers that replicated the findings of Exp. II.", "authors": ["Murray Glanzer", "William H. Clark"], "id": "767c5dfa4e939749f369ac91d44d9614193e8804", "title": "The verbal loop hypothesis: Binary numbers", "references": []}, {"date": "1967", "abstract": "Abstract Experimental data are considered from a simple task in which an observer looks at letters and then writes them down. Three models are proposed. Model 1 consists of only two components: a visual memory for the letters and a motor translation component to enable copying a visual memory onto paper. Model 1 is inadequate because the visual image is shown not to persist until the time of reproduction. Model 2 corrects this deficiency by incorporating the possibility of subvocal rehearsal of the stimulus letters and an auditory memory for the rehearsal. However, Model 2 cannot account for performance with extremely short duration images because of the limit on the maximum rehearsal rate. The critical improvement in Model 3 is a more detailed specification of scanning, recognition and rehearsal, including a form of memory which is inherent in the process of recognition itself. Model 3 accounts for these data and incidently gives rise to some interesting inferences about the nature of consciousness.", "authors": ["George Sperling"], "id": "f1dd4a4b12b5cc0a836ad0bc84fa18fea72a1cf8", "title": "Successive approximations to a model for short term memory.", "references": []}, {"date": "1957", "abstract": "I know of no one who seriously maintains that interference among tasks is of no consequence in the production of forgetting. Whether forgetting is conceptualized at a strict psychological level or at a neural level (e.g., neural memory trace), some provision is made for interference to account for at least some of the measured forgetting. The many studies on retroactive inhibition are probably responsible for this general agreement that interference among tasks must produce a sizable proportion of forgetting. By introducing an interpolated interfering task very marked decrements in recall can be produced in a few minutes in the laboratory. But there is a second generalization which has resulted from these studies, namely, that most forgetting must be a function of the learning of tasks which interfere with that which has already been learned (19). Thus, if a single task is learned in the laboratory and retention measured after a week, the loss has been attributed to the interference from activities learned outside the laboratory during the week. It is this generalization with which I am concerned in the initial portions of this paper. Now, I cannot deny the data which show large amounts of forgetting produced by an interpolated list in a few minutes in the laboratory. Nor do I deny that this loss may be attributed to interference. But I will try to show 1 Address of the president, Midwestern Psychological Association, St. Louis, Missouri,", "authors": ["Benton J. Underwood"], "id": "c5b4337a952f602f90edb7b68995441259cd8674", "title": "Interference and forgetting.", "references": []}, {"date": "1965", "abstract": "A number of recent studies indicate that short-term memory for letters, digits, and words uses an auditory or speech-motor code. Conrad and Wickelgren have shown that errors in short-term recall of verbal lists tend to have a vowel or consonant phoneme in common with the correct item.' Perceptual errors were eliminated from these data by slow visual presentation or scoring of only items copied correctly during presentation. Proactive and retroactive interference in short-term recall are greater for interference lists consisting of letters that have a vowel phoneme in common with the correct letter(s) in the original list than for interference letters that have no phoneme in common with the correct letter(s).2 Finally, lists of letters that are often confused with each other in auditory recognition are more difficult to recall than lists of letters that are rarely confused with each other in auditory recognition.8 The former letters tend to have phonemes in common, while the latter letters tend to have no common phonemes.", "authors": ["Wayne A. Wickelgren"], "id": "5acbf8b91494a23f906fa973ef2da1163ba5e98e", "title": "Short-term memory for phonemically similar lists.", "references": []}, {"date": "1968", "abstract": "1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded.2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive-field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross-sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent.3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other.4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers (II and the upper two-thirds of III) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer III, and in IV A and IV B. In layer IV B they form a large proportion of the population, whereas complex cells are rare. In layers IV A and IV B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer IV most cells are driven by one eye only; this layer consists of a mosaic with cells of some regions responding to one eye only, those of other regions responding to the other eye. Layers V and VI contain mostly complex and hypercomplex cells, binocularly driven.5. The cortex is seen as a system organized vertically and horizontally in entirely different ways. In the vertical system (in which cells lying along a vertical line in the cortex have common features) stimulus dimensions such as retinal position, line orientation, ocular dominance, and perhaps directionality of movement, are mapped in sets of superimposed but independent mosaics. The horizontal system segregates cells in layers by hierarchical orders, the lowest orders (simple cells monocularly driven) located in and near layer IV, the higher orders in the upper and lower layers.", "authors": ["David H. Hubel", "Torsten N. Wiesel"], "id": "c5f5311fa1f34159ab3a0a1d58da51cd0340a640", "title": "Receptive fields and functional architecture of monkey striate cortex.", "references": ["f70c54029c17f50914996c834930ecbfacda195f", "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7", "d7fb932bca642615fcbcf3f5d26b2c26666603d3", "6ded76b03bd0612974d86a826ce70d2a2b143fb2", "5a7e6b47e6e83592412bce8416f69ed15c03a44c", "32eb84773c5531c816f1eb958a9d85a1ad1de37c", "6f20e254e3993538c79e0ff2b9b8f198d3359cb3", "62986ed55e8420663b0157e372df1606e4896bca", "b0b13465c36789ebd0751178c5403efd4dd390b9", "77734bcbfe2e939b340d870bf676775a879f747a"]}, {"date": "1965", "abstract": "Semantic Scholar extracted view of \"A recognition procedure for transformational grammars.\" by Stanley R. Petrick", "authors": ["Stanley R. Petrick"], "id": "6c4f43d484ac3fc5f277a2fe4305aebb6373970e", "title": "A recognition procedure for transformational grammars.", "references": []}, {"date": "1967", "abstract": "Abstract A model is presented for the limitations of processing information by the human operator which proposes that he acts not as a limited capacity channel with fixed capacity, but as a limited capacity processor. The total capacity of the brain can be allocated to the separate aspects of the tasks, such as reception, recording, emission, storing, etc. Hence from moment to moment the size of the \u2018channel\u2019 in the Shannon sense will appear to vary. In particular parallel processing is possible where the total capacity is not exceeded, and where there is high compatibility. Experimental evidence in support of the model is presented.", "authors": ["Neville Moray"], "id": "362d7fd794ab537c09b3451711c9de31e3501715", "title": "Where is capacity limited? A survey and a model.", "references": []}, {"date": "1958", "abstract": "The hypothesis of decay of the memory trace as a cause of forgetting has been unpopular. The reasons for this unpopularity are criticized and a theory of the memory span, based on this hypothesis, is put forward. Three experiments which test the hypothesis are described. In each, two kinds of stimuli are presented to the subject, viz., \u201crequired\u201d stimuli, which he attempts to remember, and \u201cadditional\u201d stimuli, to which he merely makes responses. The first experiment will show that even when the number of required stimuli is well below the memory span, forgetting occurs if the presentation of additional stimuli delays recall for several seconds. The second shows that the effect of the additional stimuli depends only slightly on their similarity to the required stimuli: it also shows that their effect is negligible when they precede, instead of follow, the required stimuli. The third shows that the effect of additional stimuli interpolated before recall remains considerable even when there is an interval of several seconds between presentation of required and additional stimuli.", "authors": ["John Brown"], "id": "62edcc7d7f76dc9b12c9ed6f6864ba1045577452", "title": "Some Tests of the Decay Theory of Immediate Memory", "references": []}, {"date": "1974", "abstract": "This paper describes research whose goal is to determine the implications of verba] classificatory, judgments for recognition memory and recall. Toward this end, St were required to answer 100 queries of attribution and superordination ds a TWINGE sudden? Is SPINACH ecstatic? Is a CORKSCREW an opener? Is a DUNGEON a scholar? before being tested unexpectedly on their ability to remember either the uppercase \u201ckeywords\u201d or the lowercase \u201cdescriptors.\u201d Lexical memory did not depend on whether a word had been part of an attributive or a superordinate query. But words from \u201cincongruous\u201d queries almost invariably were more poorly remembered-under conditions of free recall, cued recall, and recognition memory-than words from \u201ccongruous\u201d queries. Congruous cues, but not incongruous ones, greatly facilitated recall, with keywords being more effective cues than descriptors. Recognition memory of keywords was uniformly superior to that of descriptors. It is argued that the large and pervasive memorial advantages of congruity arise because a congruous query, unlike an incongruous one, fosters a relational encoding of keyword and descriptor.", "authors": ["Arthur I. Schulman"], "id": "33c7326a5b575769dec4475d19ffa736cc36ffbc", "title": "Memory for words recently classified", "references": []}, {"date": "1973", "abstract": "Two commonplace assumptions about encoding are that sentences are encoded and recognized on the basis of their semantic features primarily and that information regarding form features such as typography is typically ignored or discarded. These assumptions were tested m the present experiment where, within a signal-detection paradigm, S sorted sentences according to whether he had seen them before or not (old vs new) and, if they were old, whether their reappearance was in the same typography as on the first occurrence or a different one. Of the two typographies, one was familiar and the other unfamiliar. Results show that a considerable amount of information regarding surface features is stored for many minutes and that ease of initial encoding is inversely related to likelihood of subsequent recognition: sentences in the unfamiliar typography were remembered better. The results are probably not due to time spent encoding; control tests suggest that time spent encoding a difficult typography does not by itself increase recognition of the semantic content embodied in the typography. Other control tests show that pictorial features or images of the sentences play no significant role in their subsequent recognition. One interpretation of the results is that the analytic activities or cognitive operations that characterize initial acquisition play a significant role in subsequent recognition.", "authors": ["Paul A. Kolers"], "id": "2f66196293608365eb59137f17cd6e1c4eeeb20b", "title": "Remembering operations", "references": []}, {"date": "1973", "abstract": "Several widely accepted models of memory postulate that the adequacy of an item's registration in long-term storage is a positive function of its length of stay in the short-term store. However, when short-term storage times were measured, these times did not predict long-term recall or recognition. Two further experiments showed that neither the length of an item's stay in short-term storage nor the number of overt rehearsals it received was related to subsequent recall. It is concluded that the \u201cmaintenance\u201d and \u201celaborative\u201d aspects of rehearsal can be clearly separated, and that the duration of rehearsal is related to long-term memory and learning only in the latter case. Maintenance rehearsal does not lead to an improvement in memory performance.", "authors": ["Fergus I. M. Craik", "Michael J. Watkins"], "id": "d2ac4eb014a7a4e3bfafdca2f94fd8e32bef8d46", "title": "The role of rehearsal in short-term memory", "references": []}, {"date": "1988", "abstract": "An influence diagram is a network representation for probabilistic and decision analysis models. The nodes correspond to variables which can be constants, uncertain quantities, decisions, or objectives. The arcs reveal the probabilistic dependence of the uncertain quantities and the information available at the time of the decisions. The detailed data about the variables are stored within the nodes, so the diagram graph is compact and focuses attention on the relationships among the variables. Influence diagrams are effective communication tools and recent developments also allow them to be used for analysis. We develop algorithms to address questions of inference within a probabilistic model represented as an influence diagram. We use the conditional independence implied by the diagram's structure to determine the information needed to solve a given problem. When there is enough information we can solve it, exploiting that conditional independence. These same results are applied to problems of decision analysis. This methodology allows the construction of computer tools to maintain and evaluate complex models.", "authors": ["Ross D. Shachter"], "id": "1eb1583c2d2f7f075075cc54b0ef1640d2b7da4b", "title": "Probabilistic Inference and Influence Diagrams", "references": []}, {"date": "1973", "abstract": "Recent changes in prctheorclical orientation toward problems of human memory have brought with them a concern with retrieval processes, and a number of early versions of theories of retrieval have been constructed. This paper describes and evaluates explanations offered by these theories to account for the effect of extralist cuing, facilitation of recall of list items by nonlist items. Experiments designed to test the currently most popular theory of retrieval, the generation-recognition theory, yielded results incompatible not only with generation-recognition models, but most other theories as well: under certain conditions subjects consistently failed to recognize many recallable list words. Several tentative explanations of this phenomenon of recognition failure were subsumed under the encoding specificity principle according to which the memory trace of an event and hence the properties of effective retrieval cue are determined by the specific encoding operations performed by the system on the input stimuli.", "authors": ["Endel Tulving", "Donald M. Thomson"], "id": "e31a771cc15bd4d67bad13a6af0514f80c2d4028", "title": "Encoding specificity and retrieval processes in episodic memory.", "references": ["a00176470d841ea1bd2c3a13dd2991c008ac2e55", "291aeeb01e24a53a5841c96aea1dcc0b78e60ec7", "6ea84e903d70d4bd95999ec6cf0690e2ddbc7c92", "3bd6e99a19cb3ce8caba45f8f942e35edc4dfe95", "4ee237d215c41374cb94227a44d1a51af7d3873f", "2a7518c7d244acba9827b3c11d68f5a505496626", "6ed71ee8857558f4f068191ae0326cd4a432c03f", "a074b5bb4d9ad90a936016f0ec7f4e73f4f58ccc", "e6077dbdd1bdca5892860aef8ba75af1dfbf6b2a", "e940dce76e423390383fab059db2a77c3649e2ba"]}, {"date": "1987", "abstract": "Change in view : , Change in view : , \u06a9\u062a\u0627\u0628\u062e\u0627\u0646\u0647 \u062f\u06cc\u062c\u06cc\u062a\u0627\u0644 \u0648 \u0641\u0646 \u0622\u0648\u0631\u06cc \u0627\u0637\u0644\u0627\u0639\u0627\u062a \u062f\u0627\u0646\u0634\u06af\u0627\u0647 \u0627\u0645\u0627\u0645 \u0635\u0627\u062f\u0642(\u0639)", "authors": ["Ronald Prescott Loui", "Gilbert Harman"], "id": "738b875c6e8237235b038960785bb32521a908c6", "title": "Change in View", "references": []}, {"date": "1990", "abstract": "Causal probabilistic networks (CPNs) have proved to be a useful knowledge representation tool for modeling domains where causal relations-in a broad sense-are a natural way of relating domain concepts and where uncertainty is inherited in these relations. The domain is modeled in a CPN by use of a directed graph where the nodes represent concepts in the domain and the arcs represent causal relations. Furthermore, the quantitative relation between a node and its immediate causes is expressed as conditional probabilities. During the last few years, several schemes based on probability theory for incorporating and propagating new information throughout a CPN has emerged. As long as the domain can be modeled by use of a singly connected CPN (i. e., no more than one path between any pair of nodes), the schemes operate directly in the CPN and perform conceptually simple operations in this structure. When it comes to more complicated structures such as multiply connected CPNs (i. e., more than one path is allowed between pairs of nodes), the schemes operate in derived structures where the embedded domain knowledge no longer is as explicit and transparent as in the CPN. Furthermore, the simplicity in the operations is lost also. This report outlines a scheme-the algebra of Bayesian belief universes-for absorbing and propagating evidence in multiply connected CPNs. The scheme provides a secondary structure, a junction tree, and a simple set of algebraic operations between objects in this structure, Collect Evidence and Distribute Evidence. These are the basic tools for making inference in a CPN domain model and yield a calculus as simple as in the case of singly connected CPNs.", "authors": ["Finn Verner Jensen", "Kristian G. Olesen", "Stig K. Andersen"], "id": "0e7c8ac116f3d3b3052d23fa4b269b0000286105", "title": "An algebra of bayesian belief universes for knowledge-based systems", "references": ["91d574d069b4af5afbe2c23fa6417c1aaba78337", "17d87b9ac0bedad64489022ef415df05829843ad", "bb75e5a3b46ec37a72922c706acd87ebab35b666", "98fe695240b3ab9cd0ef771dd094a1d5f13c28b6", "bb0419bccc2244ed33c9c42341f342511262daa3", "3f9d64ef847e89f9ec454078d532d05ac7d868cf", "0a3767909649cf31d32e087693d93171af28ebe0", "c561aaeb73289589891ffdf803979e55c346f2f5", "373b1817afebdced6119cb6564a6be187b4823a9", "f8d16924d37ac2ad2fe23e641673f9f2b5434733"]}, {"date": "1989", "abstract": "Abstract Stochastic simulation approaches perform probabilistic inference in Bayesian networks by estimating the probability of an event based on the frequency that the event occurs in a set of simulation trials. This paper describes the evidence weighting mechanism, for augmenting the logic sampling stochastic simulation algorithm [Henrion, 1986]. Evidence weighting modifies the logic sampling algorithm by weighting each simulation trial by the likelihood of a network's evidence given the sampled state node values for that trial. We also describe an enhancement to the basic algorithm which uses the evidential integration technique [Chin and Cooper, 1987]. A comparison of the basic evidence weighting mechanism with the Markov blanket algorithm [Pearl, 1987], the logic sampling algorithm, and the evidence integration algorithm is presented. The comparison is aided by analyzing the performance of the algorithms in a simple example network.", "authors": ["Robert M. Fung", "Kuo-Chu Chang"], "id": "9e475eff11c29f0d532f18a0710bfd87010ef44d", "title": "Weighing and Integrating Evidence for Stochastic Simulation in Bayesian Networks", "references": ["3b99793e181ea13b6f79a046b33b65ed692b3614", "5669148e6026516720940558086def2ab92b04e2", "262f353855b0e6cbeda26ba3b19fff5df1d7c1a2", "0a3767909649cf31d32e087693d93171af28ebe0", "89da92a02d878b898ea2b051d34874c8df9d89d5"]}, {"date": "1986", "abstract": "Bayesian belief networks and influence diagrams are attractive approaches for representing uncertain expert knowledge in coherent probabilistic form. But current algorithms for propagating updates are either restricted to singly connected networks (Chow trees), as the scheme of Pearl and Kim, or they are liable to exponential complexity when dealing with multiply connected networks. Probabilistic logic sampling is a new scheme employing stochastic simulation which can make probabilistic inferences in large, multiply connected networks, with an arbitrary degree of precision controlled by the sample size. A prototype implementation, named Pulse, is illustrated, which provides efficient methods to estimate conditional probabilities, perform systematic sensitivity analysis, and compute evidence weights to explain inferences.", "authors": ["Max Henrion"], "id": "262f353855b0e6cbeda26ba3b19fff5df1d7c1a2", "title": "Propagating uncertainty in bayesian networks by probabilistic logic sampling", "references": []}, {"date": "1990", "abstract": "The method of conditioning permits probabilistic inference in multiply connected belief networks using an algorithm by Pearl. This method uses a select set of nodes, the loop cutset, to render the multiply connected network singly connected. We discuss the function of the nodes of the loop cutset and a condition that must be met by the nodes of the loop cutset. We show that the problem of finding a loop cutset that optimizes probabilistic inference using the method of conditioning is NP-hard. We present a heuristic algorithm for finding a small loop cutset in polynomial time, and we analyze the performance of this heuristic algorithm empirically.", "authors": ["Henri Jacques Suermondt", "Gregory F. Cooper"], "id": "4b15bd9477e5a7dd59a1b6cc414d6c530405e19f", "title": "Probabilistic inference in multiply connected belief networks using loop cutsets", "references": ["73ba229d5fe920e33ad0509707602a97ef492ffd", "6710b1fffd6e850e04f972441732ecf66cf372a2", "e55a07e0d51ba0b920883a231a47e5b8631da995", "67bdbc2e4ea332f6932ea8e1998684e2f6c7eff8", "e97661976fb3a854c2e6b63f4067d3a6f62b19a7", "bb0419bccc2244ed33c9c42341f342511262daa3", "9e0daca0acc6ee3baf7573fe2e2b3cc94276e7f4", "0a3767909649cf31d32e087693d93171af28ebe0", "373b1817afebdced6119cb6564a6be187b4823a9", "5669148e6026516720940558086def2ab92b04e2"]}, {"date": "1977", "abstract": "Using the concepts of stimulus and response frames of scheduled Knowledge source instantiations, competition among alternative responses, goals, and the desirability of a knowledge source instantiation, a general attentional control mechanism is developed. This general focusing mechanism facilitates the experimental evaluation of a variety of specific attentional control policies (such as best-first, bottom-up, and top-down search strategies) and allows the modular addition of specialized heuristics for the speech understanding task. Empirical results demonstrate the effectiveness of the focusing principles, and possible directions for future research are considered.", "authors": ["Frederick Hayes-Roth", "Victor R. Lesser"], "id": "87613876fde8ae60c0c3024a5f1fa9772fd33481", "title": "Focus of Attention in the Hearsay-II Speech Understanding System", "references": ["7a4a434543c67975a6d6dcba5457fe3e8bf87391", "10c8c8ec01f4a0dd8c0d864063c0c2a1b2ca21cb"]}, {"date": "1975", "abstract": "Abstract : The Hearsay II speech-understanding system (HSII) (Lesser, et al., 1974; Fennell, 1975; Erman and Lesser, 1975) is an implementation of a knowledge-based multiprocessing AI problem-solving organization. HSII is intended to represent a problem-solving organization which is applicable for implementation in a multiprocessing environment, and is, in particular, currently being implemented on the C.mmp multiprocessor system (Bell, et al., 1971) at Carnegie-Mellon University. The object of this paper is to explore several of the ramifications of such a problem-solving organization by examining the mechanisms and policies underlying HSII which are necessary for supporting its organization as a multiprocessing problem-solving system. First, an abstract description of a class of problem-solving systems is given using the Production System model of Newell (1973). Then, the HSII problem-solving organization is described in terms of this model. The various decisions made during the course of design necessitated the introduction of various multiprocessing mechanisms (e.g., mechanisms for maintaining data localization and data integrity), and these mechanisms are discussed. Finally, a simulation study is presented which details the effects of actually implementing such a problem-solving organization for use in a particular application area, that of speech understanding.", "authors": ["Richard D. Fennell", "Victor R. Lesser"], "id": "4597f9a93809a01462ee895d524df485583aeff4", "title": "Parallelism in AI Problem Solving: A Case Study of Hearsay 2", "references": ["f1a93cd7f9302e93462c7d694a84527ae23bab7f", "320cefa46a705ef20ace952267a34eaa0d2c929f", "8ef1568b4377fce96f9d350d6d46a619d71a1462", "13b55dfb568050d86191b755c4d9d6e5bd09ac78", "73ed12a7b3019f266adca7beef47163b8b737402", "006521402572c1d0fcded90f8d16803b640803cd", "f027ce53a12f36f93897a2b5733549ca323c18d0", "89729006f2b9358b46b8748b1958c7e8f0d6ffd8", "af52d8878f388ad5818fd6da1770e2ab9ef2335a"]}, {"date": "1962", "abstract": "Spectral response behavior of single units was investigated in the visual cortex of the unanesthetized macaque monkeys with test stimuli of 0.5\u00b0 in visual angle. 1. Cortical and optic radiation units were distinguished by the configuration of spikes and spontaneous discharge rate. 2. \u201cOn\u201d units responding to a narrow-band of the spectrum were designated as \u201cchromatic\u201d or C type units. Their response maxima were found in three selected regions of the spectrum, orange-red (600-640 m\u03bc), green (520-540 m\u03bc) and blue (460 m\u03bc) respectively. Most units showed one or two submaxima besides the dominant peak in their spectral response curves. 3. Some units responded to a certain limited region of the spectrum with \u201con\u201d or \u201coff\u201d, but to another with \u201coff\u201d or \u201con\u201d, thus changing discharge type depending on the wavelengths. These units were designated as \u201copponent\u201d or O type units. 4. Some units responded to the whole parts of the spectrum, but showed no clear-cut response maxima anywhere in the visible parts of the spectrum. 5. Some \u201con-off\u201d units and pure \u201coff\u201d units showed two response maxima towards both ends of the spectrum. 6. Two kinds of units, A and B, relating to rod receptors were distinguished. The common feature of these units was that the sensitivity maximum was found at about 500 m\u03bc The unit A had a large receptive field and gave \u201con-off\u201d discharges. Its photosensitivity is low compared with that of B. The unit B was always linked with photopic units and worked in a range of low intensities in which the linked photopic unit was almost inactive. Its receptive field was very small as compared with that of A.", "authors": ["Koiti Motokawa", "Norio Taira", "Junna Okuda"], "id": "32eb84773c5531c816f1eb958a9d85a1ad1de37c", "title": "Spectral responses of single units in the primate visual cortex.", "references": ["ccacc50bbf195d0f10aa57d984f5f7e8acc3a898"]}, {"date": "1966", "abstract": "1. It is known that an object is less detectable when it is viewed against a background containing structures similar to the object. The effect of changing the orientation between the object and background is investigated.2. Gratings of variable contrast were generated on two oscilloscopes; these were superimposed optically. The angle of orientation between them could be changed. The threshold of one grating, the test grating, was determined in the presence of the other, the masking grating.3. When the gratings were presented with the same orientation (and locked in phase) the increment threshold of the test grating was found to be proportional to the suprathreshold contrast of the masking grating.4. As the angle between the test and masking gratings was increased the masking effect fell exponentially.5. At 12 degrees on either side of a vertical test grating the masking effect was reduced by a factor of two with respect to its maximum value. This angle was independent of the contrast level of masking, the focus, and also the phase coherence of the masking grating.6. If the test grating was presented obliquely the effect of masking was slightly less.7. The narrow orientationally tuned channels found psychophysically by this masking technique are compared with the orientationally sensitive cells discovered electrophysiologically in the visual cortex of the cat.", "authors": ["Fergus William Campbell", "Janus J. Kulikowski"], "id": "5a7e6b47e6e83592412bce8416f69ed15c03a44c", "title": "Orientational selectivity of the human visual system.", "references": []}, {"date": "1986", "abstract": "In general diagnostic problems multiple disorders can occur simultaneously. AI systems have traditionally handled the potential combinatorial explosion of possible hypotheses in such problems by focusing attention on a few \"most plausible\" ones. This raises the issue of establishing what makes one hypothesis more plausible than others. Typically a hypothesis (a set of disorders) must not only account for the given manifestations, but it must also satisfy some notion of simplicity (or coherency, or parsimony, etc) to be considered. While various criteria for simplicity have been proposed in the past, these have been based on intuitive and subjective grounds. In this paper, we address the issue of if and when several previously-proposed criteria of parsimony are reasonable in the sense that they are guaranteed to at least identify the most probable hypothesis. Hypothesis likelihood is calculated using a recent extension of Bayesian classification theory for multimembership classification in causal diagnostic domains. The significance of this result is that it is now possible to decide objectively a priori the appropriateness of different criteria for simplicity in developing an inference method for certain classes of general diagnostic problems.", "authors": ["Yun Peng", "James A. Reggia"], "id": "edb4f2b7751aa8b4d479d8333c63101305bc8b9b", "title": "Plausibility of Diagnostic Hypotheses: The Nature of Simplicity", "references": ["bb8ba5d61f3610955f00c1254fd32e9036f9d4c5", "37f4fe1978432bb71d9073a2322813dd53636938", "b25ec86e382b937e8a0cccdf86b0eb3c0315e3ba", "4cd91c51098783ec972f6a0ab430cacdd634a5b2", "a6dd51bc649918e7ecc236f7b905ec02264b9c09", "2b386a5745a287700408805a5a68e4dae80b97ed", "7fc3df4dcccbc301ad021054947eb8979ed38301", "f884c9cd761732a2b41849c1ec0e0a33585ae854", "373b1817afebdced6119cb6564a6be187b4823a9", "c3dc50296f24afb1e141b72898ea45fe76ea1dff"]}, {"date": "1975", "abstract": "An organization is presented for implementing solutions to knowledge-based AI problems. The hypothesize-and-test paradigm is used as the basis for cooperation among many diverse and independent knowledge sources (KS's). The KS's are assumed individually to be errorful and incomplete. \n \nA uniform and integrated multi-level structure, the blackboard, holds the current state of the system. Knowledge sources cooperate by creating, accessing, and modifying elements in the blackboard. The activation of a KS is data-driven, based on the occurrence of patterns in the blackboard which match templates specified by the knowledge source. \n \nEach level in the blackboard specifies a different representation of the problem space; the sequence of levels forms a loose hierarchy in which the elements at each level can approximately be described as abstractions of elements at the next lower level. This decomposition can be thought of as an a prion framework of a plan for solving the problem; each level is a generic stage in the plan. \n \nThe elements at each level in the blackboard are hypotheses about some aspect of that level. The internal structure of an hypothesis consists of a fixed set of attributes; this set is the same for hypotheses at all levels of representation in the blackboard. These attributes are selected to serve as mechanisms for implementing the data-directed hypothesize-and-test paradigm and for efficient goal-directed scheduling of KS's. Knowledge sources may create networks of structural relationships among hypotheses. These relationships, which are explicit in the blackboard, serve to represent inferences and deductions made by the KS's about the hypotheses; they also allow competing and overlapping partial solutions to be handled in an integrated manner. \n \nThe Hearsay II speech-understanding system is an implementation of this organization; it is used here as an example for descriptive purposes.", "authors": ["Lee D. Erman", "Victor R. Lesser"], "id": "13b55dfb568050d86191b755c4d9d6e5bd09ac78", "title": "A Multi-Level Organization For Problem Solving Using Many, Diverse, Cooperating Sources Of Knowledge", "references": ["1a4c5e53e250194a258441b791364f0a725ec9ee", "04ffb20cbfa502d3d2611dcfe027cfa94b45a629", "8ef1568b4377fce96f9d350d6d46a619d71a1462", "4597f9a93809a01462ee895d524df485583aeff4", "f027ce53a12f36f93897a2b5733549ca323c18d0", "3eed9d1d505e5ad021f8e3d77689750cc3c15013", "fb4b11202c03ff7855af3e23cf166a2a28c62f26", "e97795382386ecd24300f3a6449ed5732b200bfa", "68cba25ae59c2dacef771c08c5d413872d8dd00b", "af52d8878f388ad5818fd6da1770e2ab9ef2335a"]}, {"date": "1965", "abstract": "BEFORE A KITTEN OPENS ITS EYES, and long before the eyes are used in visual exploration, single cells of the primary visual cortex respond to natural stimulation with the same specificity as is found in the adult (5). This suggests that the anatomical connections between retina and striate cortex are for the most part innate. During the first 3 months of life the connections are highly susceptible to the effects of visual deprivation, to the extent that exclusion of all form and some light from one eye leads to a severe decline in the ability of that eye to influence cortical cells. Anatomical and physiological evidence suggests that the defect is chiefly, though not entirely, a cortical one (7-9). The object of the present study was to influence cortical connections by some means less drastic than covering one or both eyes. We wished if possible to alter the input in such a way that there would be no question of effects on the visual pathway below the level of the striate cortex. A method was suggested by the well-known clinical observation that a child with a squint (strabismus or nonparallel visual axes) may suffer a deterioration of vision in one eye (amblyopia ex anopsia). Since the visual pathways from the two eyes are for practical purposes separate up to the level of the striate cortex, it is unlikely that in these children the defect is in the retina or geniculate. An artificial squint therefore seemed to provide a possible means of obtaining a cortical defect while sparing the retina and lateral geniculate body. Accordingly , we produced a divergent strabismus by cutting one of the extraocular muscles in each of four newborn kittens, with the plan of testing vision and recording from single cortical cells after several months to a year. When at length each eye was tested in these kittens by observing the ani-mal's behavior with the other eye covered the results were disappointing: there was not the slightest suggestion of any defect in vision in either eye. This was not entirely unexpected, since with both eyes uncovered the animals had appeared to fix at times with one eye and at times with the other. At this stage there seemed to be little point in proceeding further, for there- .", "authors": ["David H. Hubel", "Torsten N. Wiesel"], "id": "77734bcbfe2e939b340d870bf676775a879f747a", "title": "Binocular interaction in striate cortex of kittens reared with artificial squint.", "references": ["f70c54029c17f50914996c834930ecbfacda195f", "d7fb932bca642615fcbcf3f5d26b2c26666603d3", "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7", "75d37ea521e3fcff3706458e4aa6fcbe3fac446c", "3041e2f5116cab83b024d5a66dc85779ab082995", "6f20e254e3993538c79e0ff2b9b8f198d3359cb3", "5a0c4a618425328ba0bbb9737b02e2ac18f94646", "34f989681ef0ed85c9f809e96118e1a360702527"]}, {"date": "1977", "abstract": "Partial matching is a comparison of two or more descriptions that identifies their similarities. Determining which of several descriptions is most similar to one description of interest is called the best match problem. Partial and best matches underlie several knowledge system functions, including: analogical reasoning, inductive inference, predicate discovery, pattern-directed inference, semantic interpretation, and speech and image understanding. Because partial matching is both combinatorial and ill-structured, admissible algorithms are elusive. Economical solutions require very effective use of constraints that, apparently, can be provided only by globally organized knowledge bases. Examples of such organizations are provided, and promising avenues of research are proposed.", "authors": ["Frederick Hayes-Roth"], "id": "e262c54e0eca34a3146b911134b5a0fcb4e20133", "title": "The role of partial and best matches in knowledge systems", "references": []}, {"date": "1975", "abstract": "An augmentation of semantic networks is presented in which the various nodes and arcs are partitioned into \"net spaces.\" These net spaces delimit the scopes of quantified variables, distinguish hypothetical and imaginary situations from reality, encode alternative worlds considered in planning, and focus attention at particular levels of detail.", "authors": ["Gary G. Hendrix"], "id": "bc587390dcf5198671fb0fa9304f52dea5afdacb", "title": "Expanding the Utility of Semantic Networks Through Partitioning", "references": ["071e316fc7de5cac931900ae390ab1d13fb099e9", "05b2ce14226ac47418954b2e48887aa4612c8e32"]}, {"date": "1972", "abstract": "After a grouping pretest, 128 S s each learned a 40-word free-recall (FR) list containing 5 words representing each of 8 taxonomic categories and beginning with each of 8 first letters, followed by additional recall and transfer tests. Taxonomic cues and blocking both facilitated FR performance and categorical clustering, whereas both first-letter cues and blocking impaired FR performance but produced less alphabetical than categorical clustering. Taxonomic cues on a final cued test trial produced enhanced FR performance, but letter cues thereupon produced substantial FR losses. Besides demonstrating a marked superiority of categorical over alphabetical bases for FR organization, these results also indicate the necessity of appropriate storage for effective coding and FR retrieval, and argue against independent storage of individual items.", "authors": ["Patricia A. Lauer", "William F. Battig"], "id": "e6077dbdd1bdca5892860aef8ba75af1dfbf6b2a", "title": "Free recall of taxonomically and alphabetically organized word lists as a function of storage and retrieval cues", "references": []}, {"date": "1980", "abstract": "The need to make default assumptions is frequently encountered in reasoning about incompletely specified worlds. Inferences sanctioned by default are best viewed as beliefs which may well be modified or rejected by subsequent observations. It is this property which leads to the non-monotonicity of any logic of defaults. \n \nIn this paper we propose a logic for default reasoning. We then specialize our treatment to a very large class of commonly occuring defaults. For this class we develop a complete proof theory and show how to interface it with a top down resolution theorem prover. Finally, we provide criteria under which the revision of derived beliefs must be effected.", "authors": ["Raymond Reiter"], "id": "93bdca51c9c0477121ba9708ffe2747855b93aef", "title": "A Logic for Default Reasoning", "references": ["b35aa4d90f7368fefaf05ca94f76edf03134eee7", "f8ddb251bf94e4b055c6f520f21816e403c30e2a", "da6eb135261dc3b3f7df8ff741796688898b5cd4", "0eaf153cf0e613845e44d4543a7e1c12916f8afe", "76c880bd5bfa3c627a9497cd6f1ee0afa093e131", "06162f180234f4e1e190784236157cc52c79a0b5", "2255db1a8ada12287fb175f52805f4c5bac26873", "3430d7929dc9c0c9278dca858e785ee3d89ce2b0", "f78390e72ac8f4cdb34a2f953a586b11a6cd67bd", "b8938098d8c8bb2594cb6d02a55f5f8f203f4af4"]}, {"date": "1984", "abstract": "On definit et on etudie la notion d'hypergraphe decomposable en montrant qu'un tel hypergraphe est toujours conforme", "authors": ["Steffen L. Lauritzen", "Terence P. Speed", "K. Vijayan"], "id": "c561aaeb73289589891ffdf803979e55c346f2f5", "title": "Decomposable graphs and hypergraphs", "references": []}, {"date": "1976", "abstract": "Abstract : The work described in this paper is part of an investigation of the issues involved in making expert problem solving programs for engineering design and for maintenance of engineered systems. In particular, the paper focuses on the troubleshooting of electronic circuits. Only the individual properties of the components are used, and not the collective properties of groups of components. The concept of propagation is introduced which uses the voltage-current properties of components to determine additional information from given measurements. Two propagated values can be discovered for the same point. This is called a coincidence. In a faulted circuit, the assumptions made about components in the coinciding propagations can then be used to determine information about the faultiness of these components. In order for the program to deal with actual circuits, it handles errors in measurement readings and tolerances in component parameters. This is done by propagating ranges of numbers instead of single numbers. Unfortunately, the comparing of ranges introduces many complexities into the theory of coincidences. In conclusion, we show how such local deductions can be used as the basis for qualitative reasoning and troubleshooting. (Author)", "authors": ["J Dekleer"], "id": "697817d6ebacf03edd547d97bfa6d66582e4b40e", "title": "Local Methods for Localizing Faults in Electronic Circuits.", "references": []}, {"date": "1985", "abstract": "Because digital systems operate over time, hardware descriptions should be based on formalisms suited to temporal reasoning. One such notation, interval temporal logic, offers a natural basis for the specification of devices and digital signals. As computer systems continue to grow in complexity, the distinction between hardware and software is becoming increasingly blurred. This situation has produced an increasing awareness of the need for behavioral models suited to specifying and reasoning about both digital devices and programs. Contemporary hardware description languages (for example, Barbacci, Parker and Wallace,2 and Su et al. 3) are not sufficient because of various limitations:", "authors": ["Ben C. Moszkowski"], "id": "4ae93f8cae228353e07bf549c45b47ed622397eb", "title": "A Temporal Logic for Multilevel Reasoning about Hardware", "references": ["66a302e05a154fb8a937e353c5ee75407c0b93c2", "2d206d2ee6f24c42ecb7e07005004650577e7966", "139036327823c07958686bb46b214ede5fa3928b", "f6279cd6ab20542b793044a1b4ffc45b506dc0e6", "f8c02120710b877d802114e360d723b63a5ea7e0", "3ebe8ffa7e5cc5fa713c1e2faeb8db1f944d131e", "1f95ed618f677f4bf6062f896a2ba69d794c540f", "bb47920d8ad380b3718665abf3eeb99f9920e0b7", "116621c83f1bb7b331f769564e336b6b339ef2e5"]}, {"date": "1985", "abstract": "Publisher Summary This chapter discusses the intelligent probabilistic inference. The analysis of practical probabilistic models on the computer demands a convenient representation for the available knowledge and an efficient algorithm to perform inference. The chapter focuses on arbitrary conditional probability distributions from a given probabilistic model. With given qualitative information about the dependence of the random variables in the model, for a specific conditional expression, quantitative information needed to determine the desired conditional probability distribution can be specified precisely. In this way, the conditional independence present in the model can be exploited to avoid having to construct or manipulate the full joint distribution. These results are extended to include maximal processing when the information available is incomplete and optimal decision making in an uncertain environment. One main application of this research is in the construction of a decision system, an automated tool to assist a decision maker. The influence diagrams processed by the algorithm can be constructed and interpreted by programs within the system. The ability to determine the information needed to answer a given question is critical in such an environment.", "authors": ["Ross D. Shachter"], "id": "3b99793e181ea13b6f79a046b33b65ed692b3614", "title": "Intelligent Probabilistic Inference", "references": ["365d4a85981f5b9d1bd297baa6e9f5b139f304bb", "1e78284bc0f4ca19969d7924a8201f1210a81df8", "9e0daca0acc6ee3baf7573fe2e2b3cc94276e7f4"]}, {"date": "1987", "abstract": "Abstract Stochastic simulation is a method of computing probabilities by recording the fraction of time that events occur in a random series of scenarios generated from some causal model. This paper presents an efficient, concurrent method of conducting the simulation which guarantees that all generated scenarios will be consistent with the observed data. It is shown that the simulation can be performed by purely local computations, involving products of parameters given with the initial specification of the model. Thus, the method proposed renders stochastic simulation a powerful technique of coherent inferencing, especially suited for tasks involving complex, nondecomposable models where \u201cballpark\u201d estimates of probabilities will suffice.", "authors": ["Judea Pearl"], "id": "5669148e6026516720940558086def2ab92b04e2", "title": "Evidential Reasoning Using Stochastic Simulation of Causal Models", "references": ["ccb09e3a0ac30cc904d3eb879ad1569e1843b437", "4e9c168f6d744174efad3764e03522fe55be5ada", "c779f855137fe3e3525ac5d8cc7566980c0048df", "f6d556419aa02c6dacd0165cf01ad5aa05ca3a8e", "262f353855b0e6cbeda26ba3b19fff5df1d7c1a2", "ff568f642a545c7d62df73e7798e3d008b509ea5", "88956404ff1325e024ff606934856116b88c5c2d", "bb0419bccc2244ed33c9c42341f342511262daa3", "7e9b24417378323edfa0b3211126c847067b66ba", "373b1817afebdced6119cb6564a6be187b4823a9"]}, {"date": "1975", "abstract": "Abstract : Two important problems in speech understanding are how to effectively integrate multiple sources of knowledge within the system and how to control the activities of the system to arrive at appropriate interpretations tor utterances. This paper first describes the roles played by acoustics, syntax, semantics, and discourse, and shows how a language definition is used to integrate them into a system in a way that allows the interactions to be easily visible. The second part of the paper describes an executive that uses information from these knowledge sources in its control strategy,", "authors": ["William H. Paxton", "Ann E. Robinson"], "id": "10c8c8ec01f4a0dd8c0d864063c0c2a1b2ca21cb", "title": "System Integration and Control in a Speech Understanding System", "references": []}, {"date": "1987", "abstract": "Data-dependencies of the type \"x can tell us more about y given that we already know z\" can be represented in various formalisms: Probabilistic Dependencies, Embedded-Multi-Valued Dependencies, Undirected Graphs and Directed-Acyclic Graphs (DAGs). This paper provides an axiomatic basis, called a semigraphoid, which captures the structure common to all four types of dependencies and explores the expressive power of DAGs in representing various types of data dependencies. It is shown that DAGs can represent a richer set of dependencies than undirected graphs, that DAGs completely represent the closure of their specification bases, and that they offer an effective computational device for testing membership in that closure as well as inferring new dependencies from given inputs. These properties might explain the prevailing use of DAGs in causal reasoning and semantic nets.", "authors": ["Judea Pearl", "Thomas Verma"], "id": "e97661976fb3a854c2e6b63f4067d3a6f62b19a7", "title": "The Logic of Representing Dependencies by Directed Graphs", "references": []}, {"date": "1989", "abstract": "This study describes a general framework and several algorithms for reducing Bayesian networks with loops (i.e., undirected cycles) into equivalent networks which are singly connected. The purpose of this conversion is to take advantage of a distributed inference algorithm (6). The framework and algorithms center around one basic operation, node aggregation. In this operation, a cluster of nodes in a network is replaced with a single node without changing the underlying joint distribution of the network. The framework for us ing this operation includes a node aggregation theorem which describes whether a cluster of nodes can be combined, and a complexity analysis which estimates the computational require ments for the resulting networks. The algorithms described include a heuristic search algorithm which finds the set of node aggregations that makes a network singly connected and allows inference to execute in minimum time, and a \"graph-directed\" algorithm which is guaranteed to find a feasible but not necessary optimal solution and with less computation than the search algorithm.", "authors": ["Kuo-Chu Chang", "Robert M. Fung"], "id": "af115c60376909328560fa9312ef0e274ea1e7fa", "title": "Node Aggregation for Distributed Inference in Bayesian Networks", "references": ["89da92a02d878b898ea2b051d34874c8df9d89d5", "70ef29e6f0ce082bb8a47fd85b9bfb7cc0f20c93", "bb0419bccc2244ed33c9c42341f342511262daa3", "213343078313486f74600cb09952cd3860c337d9", "0a3767909649cf31d32e087693d93171af28ebe0", "3b99793e181ea13b6f79a046b33b65ed692b3614"]}, {"date": "1947", "abstract": "Semantic Scholar extracted view of \"Sensory mechanisms of the retina : with an appendix on electroretinography\" by Ra\u01f5nar Granit", "authors": ["Ra\u01f5nar Granit"], "id": "ccacc50bbf195d0f10aa57d984f5f7e8acc3a898", "title": "Sensory mechanisms of the retina : with an appendix on electroretinography", "references": []}, {"date": "1973", "abstract": "Since the early years of the digital computer era, there has been a continuing attempt to gain processing power by organizing hardware processors so as to achieve some form of parallel operation. One important thread has been the use of an array of processors to allow a single control stream to operate simultaneously on a multiplicity of data streams; the most ambitious effort in this direction has been the ILLIAC IV project. Another important thread has been the partitioning of problems so that several control streams can operate in parallel. Often functions have been unloaded from a central processor onto various specialized processors; examples include data channels, display processors, front-end communication processors, on-line data preprocessors---in fact, I/O processors of all sorts. Similarly, dual processor systems have been used to provide load sharing and increased reliability. Still another thread has been the construction of pipeline systems in which sub-pieces of a single (generally large) processor work in parallel on successive phases of a problem. In some of these pipeline approaches the parallelism is \"hidden\" and the user considers only a single control stream.", "authors": ["Frank E. Heart", "Severo M. Ornstein", "William R. Crowther", "W. B. Barker"], "id": "006521402572c1d0fcded90f8d16803b640803cd", "title": "A new minicomputer/multiprocessor for the ARPA network", "references": []}, {"date": "1988", "abstract": "A circulation control airfoil having an edge slot formed by upper and lower duct panels. A plurality of struts are spaced along the length of the edge slot and are adjustably attached to the upper and lower duct to maintain the edge slot height. A reinforcing saddle is provided at each attachment position on the panels for distributing load applied to the struts over a large area of the duct panels. Each strut is provided with a differential lead thread system which provides high resolution adjustment capability for the edge slot.", "authors": ["Rosalba Mancinas Chavez", "Gregory Cooper"], "id": "67bdbc2e4ea332f6932ea8e1998684e2f6c7eff8", "title": "A fully - polynomial randomized approximation scheme for the bayesian inferencing problem", "references": []}, {"date": "1972", "abstract": "Progress in Artificial Intelligence has traditionally been accompanied by advances in special purpose programming techniques and languages. Virtually all of this development has been concentrated in languages and systems oriented to list processing. As the efforts of Artificial Intelligence researchers began to turn from purely symbolic problems toward interaction with the real world, certain features of algebraic languages became desirable. There were several attempts (notably LISP2 and FORMULA ALGOL) to combine the best features of both kinds of language. At the same time, designers of algebraic languages began to include features for non-numerical computation. No new general purpose language without some sort of list processing facility has been suggested for several years. We have followed a tack somewhat different from either of these in the design of SAIL and in its subsequent modifications.", "authors": ["Jerome A. Feldman", "James R. Low", "D. C. Swinehart", "R. H. Taylor"], "id": "320cefa46a705ef20ace952267a34eaa0d2c929f", "title": "Recent developments in SAIL: an ALGOL-based language for artificial intelligence", "references": []}, {"date": "1973", "abstract": "A g e n e r a l system to s i m u l a t e human c o g n i t i v e p r o cesses is d e s c r i b e d . The f o u r p a r t system compr ises a nodespace to s t o r e t he ne twork s t r u c t u r e ; a s u p e r v i s o r ; a t r a n s i t i o n network p a r s e r ; and an i n t e r p r e t e r . The method by wh ich noun phrases ope ra te and t h e p rocess f o r t he d e t e r m i n e r \" t h e \" i s p r e s e n t e d . A n a n a l y s i s o f ve rb s t r u c t u r e s i l l u s t r a t e s how network s t r u c t u r e s can b e c o n s t r u c t e d f rom p r i m i t i v e ve rb d e f i n i t i o n s t h a t ge t a t t h e u n d e r l y i n g s t r u c t u r e s o f p a r t i c u l a r v e r b s . The paper conc ludes w i t h an i l l u s t r a t i o n o f a p rob lem i n q u e s t i o n a s k i n g . A Model of Human Memory We have c o n s t r u c t e d a l a r g e g e n e r a l s i m u l a t i o n of human language and l o n g t e r m memory on t h e p remise t h a t t h e s t u d y o f t he i n t e r r e l a t i o n s h i p s among p s y c h o l o g i c a l p rocesses w i l l l ead t o more i n s i g h t i n t o human cog n i t i o n and memory. The g e n e r a l i m p l e m e n t a t i o n i s ba s i c a l l y c o m p l e t e , and a v a r i e t y o f use rs a r e s t a r t i n g t o s t u d y s p e c i f i c p s y c h o l o g i c a l t a s k s ( language under s t a n d i n g ; c h i l d r e n ' s development o f l anguage ; p r i m i t i v e v e r b s t r u c t u r e ; r e a d i n g ; i n f e r e n c e ; game p l a y i n g G o and Gomoku; v i s u a l r e p r e s e n t a t i o n and memory; l e a r n i n g ; and q u e s t i o n a n s w e r i n g ) . I t i s s t i l l too e a r l y t o r e p o r t o n t h e r e s u l t s o f t h e p s y c h o l o g i c a l i n v e s t i g a t i o n . . T h e r e f o r e , t h i s paper i s a p r o g r e s s r e p o r t o n t h e s y s tem and t h e u n d e r l y i n g p s y c h o l o g i c a l p r i n c i p l e s . The ma jo r g u i d e l i n e s have come f rom our a t t e m p t s to r e p r e s e n t l o n g t e r m memory s t r u c t u r e s . We know t h a t peop le r a p i d l y f o r g e t t h e d e t a i l s about t h e s u r f a c e s t r u c t u r e o f a n e x p e r i e n c e b u t r e t a i n t h e meaning o r i n t e r p r e t a t i o n o f t h a t e x p e r i e n c e i n d e f i n i t e l y . W e a l so know t h a t r e t r i e v a l o f an e x p e r i e n c e f rom memory i s u s u a l l y a r e c o n s t r u c t i o n wh ich i s h e a v i l y b i a s e d b y t h e p e r s o n ' s g e n e r a l knowledge o f t h e w o r l d . Thus , g e n e r a l w o r l d knowledge shou ld i n t e r a c t w i t h s p e c i f i c event knowledge in such a way t h a t d i s t i n c t i o n between the two i s n o t p o s s i b l e . The r e p r e s e n t a t i o n shou ld a l l o w p a r a p h r a s e . F i n a l l y , t h e l i m i t a t i o n s o f human w o r k i n g s t o r a g e ( o r s h o r t t e r m memory) p r o b a b l y compr i se a f u n damenta l p r o p e r t y o f t h e sys tem, one t h a t shou ld be v iewed as an e s s e n t i a l , p o s i t i v e component , n o t as s imp l y a per fo rmance l i m i t a t i o n .", "authors": ["David E. Rumelhart", "Donald A. Norman"], "id": "05b2ce14226ac47418954b2e48887aa4612c8e32", "title": "Active Semantic Networks as a Model of Human Memory", "references": []}, {"date": "1973", "abstract": "This paper considers various factors affecting system em organization for speech understanding research. The structure of the Hearsay system based on a set of cooperating, independent processes using the hypothesize-and-test paradigm is presented. Design considerations for the effective use of multiprocessor and network achitectures in speech understanding systems ems are presented: control of processes, interprocess communication and data sharing, resource allocation, and debugging are discussed.1", "authors": ["Lee D. Erman", "Richard D. Fennell", "Victor R. Lesser", "Raj Reddy"], "id": "68cba25ae59c2dacef771c08c5d413872d8dd00b", "title": "System Organizations for Speech Understanding: Implications of Network and Multiprocessor Computer Architectures for AI", "references": ["70e69265cb050a5050e75ccb308e00b6d9571ab0", "8ef1568b4377fce96f9d350d6d46a619d71a1462", "4597f9a93809a01462ee895d524df485583aeff4", "be2ccd19a3906c54172027d17cfc2c017c978572", "7fc3a6862f746c9988d82c023d2b7aaf0e089168", "13b55dfb568050d86191b755c4d9d6e5bd09ac78", "73ed12a7b3019f266adca7beef47163b8b737402", "f027ce53a12f36f93897a2b5733549ca323c18d0", "5444939ad7935c0302f414c136c0d4f09c202e67", "4448051cadba1a198f1fe1d705780cb43c8ca302"]}, {"date": "1973", "abstract": "Visual and speech perception tasks, which can be performed with no apparent effort by people, have proved to be difficult for machines. This may be in part due to the absence of cognitive models of perception of the type proposed above by Jakobson. In this paper we attempt to give a unified view of the research in machine perception of speech and vision in the hope that a clear appreciation of similarities and differences may lead to better information processing models of perception. Being active in research in both computer vision and speech, we have found it useful to look at the problems that have arisen in one domain and anticipate corresponding problems in the other (Reddy, 1969). Thus, this paper represents a comparitive study of the issues, systems and unsolved problems that are, at present, of interest to visual and speech recognition research.", "authors": ["Raj Reddy"], "id": "f1a93cd7f9302e93462c7d694a84527ae23bab7f", "title": "Eyes and Ears for Computers", "references": ["fda841df3b0620c9322514d4434ca1dcccddd1f8", "8ef1568b4377fce96f9d350d6d46a619d71a1462", "c78a6bb3ff2ea09bd4a4cf971984f2ccd4e08e1a", "79b72cbd43608729524b5e41384094949a6064ce", "73ed12a7b3019f266adca7beef47163b8b737402", "1374222f975da69b26b0c34d4fac6d139a065e4d", "236dcee7c12e0a19ed200a02e95e55c561266347", "36421f1bcc37d6b46c8a4d80c588569decfce59e", "c2e41b4445a5988f59136c62fc061958c088e731", "e3c63e00a1bc1042406ef08775b6482ccda0fa1b"]}, {"date": "1977", "abstract": "Deductive question-answering system generally evaluate queries under one of two possible assumptions which we in this paper refer to as the open and closed world assumptions. The open world assumption corresponds to the usual first order approach to query evaluation: Given a data base DB and a query Q, the only answers to Q are those which obtain from proofs of Q given DB as hypotheses. Under the closed world assumption, certain answers are admitted as a result of failure to find a proof. More specifically, if no proof of a positive ground literal exists, then the negation of that literal is assumed true. In this paper, we show that closed world evaluation of an arbitrary query may be reduced to open world evaluation of so-called atomic queries. We then show that the closed world assumption can lead to inconsistencies, but for Horn data bases no such inconsistencies can arise. Presented at the Workshop on Logic and Data Bases, Toulouse, France, November 16-18, 1977.", "authors": ["Raymond Reiter"], "id": "b8938098d8c8bb2594cb6d02a55f5f8f203f4af4", "title": "On Closed World Data Bases", "references": []}, {"date": "1975", "abstract": "Semantic Scholar extracted view of \"Partitioned networks for the mathematical modeling of natural language semantics.\" by Gary G. Hendrix", "authors": ["Gary G. Hendrix"], "id": "071e316fc7de5cac931900ae390ab1d13fb099e9", "title": "Partitioned networks for the mathematical modeling of natural language semantics.", "references": []}, {"date": "1978", "abstract": "This paper surveys a number of kinds of default reasoning in Artificial Intelligence, specifically, default assignments to variables, the closed world assumption, the frame default for causal worlds, exceptions as defaults, and negation in Artificial Intelligence programming languages. Some of these defaults provide clear representational and computaional advantanges over their corresponding first order theories. Finally, the paper discusses various difficulties associated with default theories.", "authors": ["Raymond Reiter"], "id": "f78390e72ac8f4cdb34a2f953a586b11a6cd67bd", "title": "On reasoning by default", "references": ["4e895eed5fb7ded7baffd36f8aff455603787676", "2255db1a8ada12287fb175f52805f4c5bac26873"]}, {"date": "1981", "abstract": "Publisher Summary Minsky introduced the terminology of frames to unify and denote a loose collection of related ideas on knowledge representation. Frames are sometimes understood at the metaphysical level. One aspect of frame reasoning which is often considered to lie outside of logic is the idea of a default value: a value that is taken to be the slot filler in the absence of explicit information to the contrary. A close analysis of what defaults mean shows that they are intimately connected with the idea of observations: additions of fresh knowledge into a data-base. Their role in inference \u2014 the drawing of consequences of assumptions \u2014 is readily expressible in logic, but their interaction with observation requires that the role of the state of the system's own knowledge is made explicit. This requires not a new logic but an unusual ontology and some new primitive relations. One needs to be able to talk about the system itself, in its own language, and to involve assumptions about itself in its own processes of reasoning. An emphasis on the analysis of such processes of reflexive reasoning is one of the few positive suggestions which the frames movement has produced.", "authors": ["Patrick J. Hayes"], "id": "06162f180234f4e1e190784236157cc52c79a0b5", "title": "The Logic of Frames", "references": []}, {"date": "1978", "abstract": "Semantic Scholar extracted view of \"An introduction to decision analysis.\" by Wayne W. Daniel et al.", "authors": ["Wayne W. Daniel", "Sherry A. Terrell"], "id": "1e78284bc0f4ca19969d7924a8201f1210a81df8", "title": "An introduction to decision analysis.", "references": []}, {"date": "1986", "abstract": "Semantic Scholar extracted view of \"Concurrency in systems with neighborhood constraints\" by V. C. Barbosa", "authors": ["V. C. Barbosa"], "id": "88956404ff1325e024ff606934856116b88c5c2d", "title": "Concurrency in systems with neighborhood constraints", "references": []}, {"date": "1982", "abstract": "This book investigates the application of logic to problem-solving and computer programming. It assumes no previous knowledge of these fields, and may be Karl duncker in addition to make difficult fill one of productive. The unifying epistemological virtues of program variables tuples in different terminologies he wants. Functional fixedness which appropriate solutions are most common barrier. Social psychologists over a goal is represented can take. There is often largely unintuitive and, all be overcome standardized procedures like copies? Functional fixedness it can be made possible for certain fields looks. In the solution paths or pencil. After toiling over the ultimate mentions that people cling rigidly to strain on. Luckily the book for knowledge of atomic sentences or fundamental skills. Functional fixedness is a problem solving techniques such.", "authors": ["Robert A. Kowalski"], "id": "3430d7929dc9c0c9278dca858e785ee3d89ce2b0", "title": "Logic for problem solving", "references": []}, {"date": "1984", "abstract": "The problem of resolving conflicts between processes in distributed systems is of practical importance. A conflict between a set of processes must be resolved in favor of some (usually one) process and against the others: a favored process must have some property that distinguishes it from others. To guarantee fairness, the distinguishing property must be such that the process selected for favorable treatment is not always the same. A distributed implementation of an acyclic precedence graph, in which the depth of a process (the longest chain of predecessors) is a distinguishing property, is presented. A simple conflict resolution rule coupled with the acyclic graph ensures fair resolution of all conflicts. To make the problem concrete, two paradigms are presented: the well-known distributed dining philosophers problem and a generalization of it, the distributed drinking philosophers problem.", "authors": ["K. Mani Chandy", "Jayadev Misra"], "id": "7e9b24417378323edfa0b3211126c847067b66ba", "title": "The drinking philosophers problem", "references": []}, {"date": "1981", "abstract": "Abstract A computer program capable of acting intelligently in the world must have a general representation of the world in terms of which its inputs are interpreted. Designing such a program requires commitments about what knowledge is and how it is obtained. Thus, some of the major traditional problems of philosophy arise in artificial intelligence. More specifically, we want a computer program that decides what to do by inferring in a formal language that a certain strategy will achieve its assigned goal. This requires formalizing concepts of causality, ability, and knowledge. Such formalisms are also considered in philosophical logic. The first part of the paper begins with a philosophical point of view that seems to arise naturally once we take seriously the idea of actually making an intelligent machine. We go on to the notions of metaphysically and epistemo-logically adequate representations of the world and then to an explanation of can, causes, and knows in terms of a representation of the world by a system of interacting automata. A proposed resolution of the problem of freewill in a deterministic universe and of counterfactual conditional sentences is presented. The second part is mainly concerned with formalisms within which it can be proved that a strategy will achieve a goal. Concepts of situation, fluent, future operator, action, strategy, result of a strategy and knowledge are formalized. A method is given of constructing a sentence of first-order logic which will be true in all models of certain axioms if and only if a certain strategy will achieve a certain goal. The formalism of this paper represents an advance over McCarthy (1963) and Green (1969) in that it permits proof of the correctness of strategies that contain loops and strategies that involve the acquisition of knowledge; and it is also somewhat more concise. The third part discusses open problems in extending the formalism of part 2. The fourth part is a review of work in philosophical logic in relation to problems of artificial intelligence and a discussion of previous efforts to program \u2018general intelligence\u2019 from the point of view of this paper.", "authors": ["John McCarthy"], "id": "76c880bd5bfa3c627a9497cd6f1ee0afa093e131", "title": "SOME PHILOSOPHICAL PROBLEMS FROM THE STANDPOINT OF ARTI CIAL INTELLIGENCE", "references": ["d807145539dc1ebffc0a67182748ef5ce8c6aaab", "1a6eacdbf4e881a91cf6b76a9f70f53ccc290ae1", "dc668c1c80a7bc533ad7b20ead1f737e953f79f9", "349e996dcd41195d862d57b8ca230affdd1e344c", "46bed4c578e96e05fa3e5704620c4ffa0746d78f", "75ffaa9fd92e72c10c046f2cab543d46d3b25a35", "b1b61c2bf83a33531a6b8503c9963a48fd3df0cf", "2d504759b4e1571075c85813d03304f0faf8e2fe", "9010d8c1850605bbfed06f60be208b6639c95aff", "e762c5acd9607907b6dbab223d746ac8f5e884b5"]}, {"date": "1984", "abstract": "Mechanisms for the automation of uncertainty are required for expert systems. Sometimes these mechanisms need to obey the properties of probabilistic reasoning. We argue that a purely numeric mechanism, like those proposed so far, cannot provide a probabilistic logic with truth functional connectives. We propose an alternative mechanism, Incidence Calculus, which is based on a representation of uncertainty using sets of points, which might represent situations models or possible worlds. Incidence Calculus does provide a probabilistic logic with truth functional connectives.", "authors": ["Alan Bundy"], "id": "f6d556419aa02c6dacd0165cf01ad5aa05ca3a8e", "title": "Incidence calculus: A mechanism for probabilistic reasoning", "references": ["1ebb0cfd4509b229d3d3739fe468616d4d11e0ba", "c8e38cee27196aebba894bba680e74f23bf9c560", "beb266397808b817d6178f19bab4800fec522044", "7c3ffa7def5c7cc5794aa0211d3f1cd4e3d821f1"]}, {"date": "1968", "abstract": "Semantic Scholar extracted view of \"A formal basis f or the heuristic determination of minimum cost paths in graphs\" by Peter E. Hart et al.", "authors": ["Peter E. Hart", "Nils J. Nilsson", "Bertram Raphael"], "id": "213343078313486f74600cb09952cd3860c337d9", "title": "A formal basis f or the heuristic determination of minimum cost paths in graphs", "references": []}, {"date": "1983", "abstract": "Semantic Scholar extracted view of \"On Representing And Solving Decision Problems\" by Scott M. Olmsted", "authors": ["Scott M. Olmsted"], "id": "365d4a85981f5b9d1bd297baa6e9f5b139f304bb", "title": "On Representing And Solving Decision Problems", "references": []}, {"date": "1981", "abstract": "We consider the problem of maintaining communication between the nodes of a data network and a central station in the presence of frequent topological changes as, for example, in mobile packet radio networks. We argue that flooding schemes have significant drawbacks for such networks, and propose a general class of distributed algorithms for establishing new loop-free routes to the station for any node left without a route due to changes in the network topology. By virtue of built-in redundancy, the algorithms are typically activated very infrequently and, even when they are, they do not involve any communication within the portion of the network that has not been materially affected by a topological change.", "authors": ["Eli Gafni", "Dimitri P. Bertsekas"], "id": "c779f855137fe3e3525ac5d8cc7566980c0048df", "title": "Distributed Algorithms for Generating Loop-Free Routes in Networks with Frequently Changing Topology", "references": ["0f389b20703fd956cd7450f0e925bbb544677b69"]}, {"date": "1981", "abstract": "The Instruction Set Processor Specifications (ISPS) computer description language is an evolutionary step towards the formalization of the digital design process at the higher or behavioral levels. It has been used as a design tool, which covers a wider area of application than any other hardware description language. Thus, besides simulation and synthesis of hardware, software generation program verification, and architecture evaluation and control are among the current applications based on ISPS. The range of current and contemplated application areas are proof of the usefulness of the notation and its extension mechanisms. ISPS supports a wide range of applications, rather than a wide range of design levels. Thus, this paper is divided into two parts. The first part describes the notation, its intended use, and the extension mechanisms which allow multiple applications or areas of research to co-exit and share machine descriptions. The second part describes some of the current applications for ISPS.", "authors": ["Mario Barbacci"], "id": "116621c83f1bb7b331f769564e336b6b339ef2e5", "title": "Instruction set processor specifications (ISPS): The notation and its applications", "references": ["a9188a74a750ecd4217056fa0a5237c6b0d9ac65"]}, {"date": "1981", "abstract": "This paper describes the SLIDE language\u2014a hardware-descriptive language desigued for the description of input/output, interfaces, and interconnected digital systems. The language allows the description of asynchronous, concurrent processes which can communicate. Timing and synchronization mechanisms are capable of being described as well. Current research applications include the use of the SLIDE language for I/O simulation and for verification of synchronization mechanisms.", "authors": ["Alice C. Parker", "John J. Wallace"], "id": "1f95ed618f677f4bf6062f896a2ba69d794c540f", "title": "Slide: An I/O Hardware Descriptive Language", "references": ["164ce547a4f3855840f4b8a8ff8212167a670be5"]}, {"date": "1980", "abstract": "This paper is concerned with the analysis of design errors that lead to unpredictable response of digital systems. Besides classical topics, such as hazards and races, the analysis of malfunctions in real circuits is also included. After defining the notion of behavior and nondeterministic response, a general approach for detecting such design problems through algebraic analysis is presented. Compared with existing simulation methods, the algebraic technique provides results of improved accuracy. Another basic advantage is the ability to accomodate modular synthesis of digital systems. Examples show how the proposed methods deal with sequential circuits under various delay assumptions. In particular, analysis of designs based on nominal delay parameters and on window delays is presented. A novel method, aiming at spike detection, is also presented. The ability of the algebraic analysis to detect errors in a modular design environment is illustrated by means of an example. Finally, the topic of nondeterministic behavior at RTL is briefly discussed. Notably, an algebraic method for deriving setup and hold time constraints from the circuit delay parameters is proposed.", "authors": ["Sany M. Leinwand", "T. Lamdan"], "id": "3ebe8ffa7e5cc5fa713c1e2faeb8db1f944d131e", "title": "Algebraic Analysis of Nondeterministic Behavior", "references": []}, {"date": "1983", "abstract": "Abstract : Predicate logic is a powerful and general descriptive formalism with a long history of development. However, since the logic's underlying semantics have no notion of time, statements such as I increases by 2 and The bit signal X rises from 0 to 1 can not be directly expressed. The author presents a formalism called interval temporal logic (ITL) that augments standard predicate logic with time-dependent operators. ITL is like discrete linear-time temporal logic but includes time intervals. The behavior of programs and hardware devices can often be decomposed into successively smaller intervals of activity. State transitions can be characterized by properties relating the initial and final values of variables over intervals. Furthermore, these time periods provide a convenient framework for introducing quantitative timing details. After giving some motivation for reasoning about hardware, he presents the propositional and first-order syntax and semantics of ITL. Demonstrated is ITL's utility for uniformly describing the structure and dynamics of a wide variety of timing-dependent digital circuits. Devices discussed include delay elements, adders, latches, flip-flops, counters, random-access memories, a clocked multiplication circuit and the Am2901 bit slice. ITL also provides a means for expressing properties of such specifications. Also examine are such concepts as device equivalence and internal states. Propositional ITL is shown to be undecidable although useful subsets are of relatively reasonable computational complexity. (Author)", "authors": ["Benjamin Charles Moszkowski"], "id": "bb47920d8ad380b3718665abf3eeb99f9920e0b7", "title": "Reasoning About Digital Circuits", "references": []}, {"date": "1971", "abstract": "This paper describes a system which solves the puzzle \"Instant Insanity\". The puzzle consists of four multicolored cubes. The solution involves arranging the cubes in a tower so that no side of the tower reveals more than one face of a given color. Our system, which runs as eight (multitask) jobs under the PDP-10 time-sharing system, uses a TV camera to locate four objects and, having verified that they are cubes, to find the color of each face. A mechanical arm turns the cubes over to expose all faces to the TV. Having found the solution, the arm then stacks the cubes into a tower to demonstrate it.", "authors": ["Jerome A. Feldman", "Karl K. Pingle", "Thomas O. Binford", "Gilbert Falk", "Alan C. Kay", "R. Paul", "Robert F. Sproull", "Jay M. Tenenbaum"], "id": "c2e41b4445a5988f59136c62fc061958c088e731", "title": "The Use of Vision and Manipulation to Solve the ", "references": ["c3576b28daab769c8ece29fed47e6f95f83f4777", "70e69265cb050a5050e75ccb308e00b6d9571ab0", "89729006f2b9358b46b8748b1958c7e8f0d6ffd8", "10eae4058bd6a68600afc4a068e32eaf741f1e74", "e979c7bf95e924f71ed6697da3cb5bac3ea926b7", "ae12520c730c4588f79f1751f04b9675fd10e3d9", "18836cbf140d542022ec5f04805fa47d9d0772f9", "9957429e56e0ac0a906b769476c0518732051261"]}, {"date": "1967", "abstract": "A system for obtaining a phonemic transcription from a connected speech sample entered into the computer by a microphone and an analog\u2010to\u2010digital converter is described. A feature\u2010extraction program divides the speech utterance into segments approximately corresponding to phonemes, determines pitch periods of those segments where pitch analysis is appropriate, and computes a list of parameters for each segment. A classification program assigns a phoneme\u2010group label (vowellike segment, fricativelike segment, etc.) to each segment, determines whether a segment should be classified as a phoneme or whether it represents a phoneme boundary between two phonemes, and then assigns phoneme label to each segment that is not rejected as being a phoneme boundary. About 30 utterances of 1\u20132 sec duration were analyzed using the above programs on an interconnected IBM 7090\u2010PDP1 system. Correct identification of many vowel and consonantal phonemes was achieved for a single speaker using the same speech material that was ...", "authors": ["D. Raj Reddy"], "id": "36421f1bcc37d6b46c8a4d80c588569decfce59e", "title": "Computer recognition of connected speech.", "references": []}, {"date": "1971", "abstract": "A method is presented for the visual analysis of objects by computer. It is particularly well suited for opaque objects with smoothly curved surfaces. The method extracts information about the object''s surface properties, including measures of its specularity, texture, and regularity. It also aids in determining the object''s shape. The application of this method to a simple recognition task, (the recognition of fruit) is discussed. The results on a more complex smoothly curved object, a human face, are also considered.", "authors": ["Lawrence J. Krakauer"], "id": "e3c63e00a1bc1042406ef08775b6482ccda0fa1b", "title": "COMPUTER ANALYSIS OF VISUAL PROPERTIES OF CURVED OBJECTS", "references": []}, {"date": "1971", "abstract": "The frame problem arises in considering the logical structure of a robot''s beliefs. It has been known for some years, but only recently has much progress been made. The problem is described and discussed. Various suggested methods for its solution are outlined, and described in a uniform notation. Finally, brief consideration is given to the problem of adjusting a belief system in the face of evidence which contradicts beliefs. It is shown that a variation on the situation notation of (McCarthy and Hayes, 1969) permits an elegant approach, and relates this problem to the frame problem.", "authors": ["Patrick J. Hayes"], "id": "4e895eed5fb7ded7baffd36f8aff455603787676", "title": "The frame problem and related problems in artificial intelligence", "references": []}, {"date": "1967", "abstract": "An information-processing system that scans stimuli serially, part-by-part, and attempts \u2018simple\u2019 interpretations of the parts would experience a number of the well-known perceptual illusions that human subjects report. The hypothesized system has the same basic characteristics as systems which have been used to explain a wide range of cognitive phenomena. The description of the system is proposed as an explanation of some of the mechanisms for human perceptual processing.", "authors": ["Herbert A. Simon"], "id": "236dcee7c12e0a19ed200a02e95e55c561266347", "title": "An information-processing explanation of some perceptual phenomena.", "references": []}, {"date": "1963", "abstract": "This paper describes the method and the system investigated to solve the problem encountered in the automatic recognition of speech sound. From research in the automatic analyzer of speech sound, a monosyllable recognition system was constructed in which the phoneme is used as the basic recognition unit. Recently this system has been developed to accept the conversational speech sound with unlimited vocabulary. The mechanical recognition of conversational speech sound requires two basic operations. One is the segmentation of the continuous speech sound into several discrete intervals (or segments), each of which may be thought to correspond to a phoneme, and the other is the pattern recognition of such segments. For segmentation, by defining two criteria, ``stability'' and ``distance,'' the properties of the time pattern obtained by the analysis of input speech sound may be examined. The principle of the recognition is based on the mechanism of the articulation in our speech organ. Corresponding to this, the machine has the functions called phoneme classification, vowel analysis and consonant analysis. A conversational speech recognition system with the phonetic contextual approach is also applied to the vowel recognition where the time pattern of input speech is matched with the stored standard patterns in which the phonetic contextual effects are taken into consideration. The time pattern which has great variety may be effectively expressed by the new representation of ``sequential pattern'' and ``weighting pattern.''", "authors": ["Toshiyuki Sakai", "Shuji Doshita"], "id": "79b72cbd43608729524b5e41384094949a6064ce", "title": "The Automatic Speech Recognition System for Conversational Sound", "references": ["5f87bb3dc34be8c90498b31c3bdbf43f539f1a8f"]}, {"date": "1969", "abstract": "Abstract : The paper describes techniques and methodology which are useful in achieving close to real-time recognition of speech by a computer. To analyze connected speech utterances, any speech recognition system must perform the following processes: preprocessing, segmentation, segment classification, recognition of words, recognition of sentences. The paper presents implemented solutions to each of these problems which achieved accurate recognition in all the trial cases. (Author)", "authors": ["Pierre J. Vicens"], "id": "1374222f975da69b26b0c34d4fac6d139a065e4d", "title": "Aspects of speech recognition by computer", "references": []}, {"date": "1977", "abstract": "Rule-based inference systems allow judgmental knowledge about a specific problem domain to be represented as a collection of discrete rules. Each rule states that if certain premises are known, then certain conclusions can be inferred. An important design issue concerns the representational form for the premises and conclusions of the rules. We describe a rule-based system that uses a partitioned semantic network representation for the premises and conclusions.", "authors": ["Richard O. Duda", "Peter E. Hart", "Nils J. Nilsson", "Georgia L. Sutherland"], "id": "7c3ffa7def5c7cc5794aa0211d3f1cd4e3d821f1", "title": "Semantic network representations in rule-based inference systems", "references": []}, {"date": "1982", "abstract": "One common feature of most knowledge-based expert systems is that they must reason based upon evidential information. Yet there is very little agreement on how this should be done. The authors present their current understanding of this problem and some partial solutions. They begin by characterizing evidence as a body of information that is uncertain, incomplete, and sometimes inaccurate. Based on this characterization, the authors conclude that evidential reasoning requires both a method for pooling multiple bodies of evidence to arrive at consensus opinion and some means of drawing the appropriate conclusions from that opinion. This approach, based on a relatively new mathematical theory of evidence, is contrasted with those approaches based on Bayesian probability models. The authors believe that their approach has some significant advantages, particularly its ability to represent and reason from bounded ignorance. 5 references.", "authors": ["John D. Lowrance", "Thomas D. Garvey"], "id": "beb266397808b817d6178f19bab4800fec522044", "title": "Evidential reasoning: a developing concept", "references": []}, {"date": "1966", "abstract": "A descriptive scheme for classes of pictures based on labeling techniques using parallel processing algorithms was proposed by the author some years ago. Since then much work has been done in applying this to bubble chamber pictures. The parallel processing simulator, originally written for an IBM 7094 system, has now been rewritten for a CDC 3600 system. This paper describes briefly the structure of syntactic descriptive models by considering their specific application to bubble chamber pictures. How the description generated in this phase can be embedded in a larger \u201cconversation\u201d program is explained by means of a certain specific example that has been worked out. A partial generative grammar for \u201chandwritten\u201d English letters is given, as are also a few computer-generated outputs using this grammar and the parallel processing simulator mentioned earlier.", "authors": ["R. Narasimhan"], "id": "fda841df3b0620c9322514d4434ca1dcccddd1f8", "title": "Syntax-directed interpretation of classes of pictures", "references": []}, {"date": "1969", "abstract": "A research pro ject applying a r t i f i c i a l i n t e l l igence techniques to the development of i n t e grated robot systems Is descr ibed. The experiment a l f a c i l i t y consists of an SDS-940 computer and associated programs c o n t r o l l i n g a wheeled vehic le that ca r r i es a TV camera and other sensors. The primary emphasis is on the development of a system of programs for processing sensory data from the veh ic le , fo r s to r i ng relevant in format ion about the environment, and for planning the sequence of motor act ions necessary to accomplish tasks in the environment. A t yp i ca l task performed by our present system requires the robot vehic le to r e arrange (by pushing) simple objects in i t s env i ronment .", "authors": ["Nils J. Nilsson"], "id": "c78a6bb3ff2ea09bd4a4cf971984f2ccd4e08e1a", "title": "A Mobile Automaton: An Application of Artificial Intelligence Techniques", "references": []}, {"date": "1977", "abstract": "This paper presents an overview of the selection process employed to choose a single Computer Family Architecture (CFA) to be used in a new Military Computer Family (MCF) intended for use in Army and Navy Systems. A joint Army/Navy Selection Committee studied the suitability of a number of architectures, and intensively evaluated three \"final candidate\" architectures, before selecting one, the PDP-11, for use with the MCF.", "authors": ["William E. Burr", "Aaron H. Coleman", "William R. Smith"], "id": "a9188a74a750ecd4217056fa0a5237c6b0d9ac65", "title": "Overview of the military computer family architecture selection", "references": []}, {"date": "1953", "abstract": "algebra 81; entities; 311, 45, 73: terms 21. 30. 76. 78. Set: also Attribute, Class, hame Abstraction: of attributes 76, 156; of classes 30, 76ff, 87, 94ff,. 104, 159; of functions 104; of relatrons 88; of universals 117ff; principle of 90,, 96ff; vacuous 95, 159 Accident 22, 158 ACKEXMANN, Wilhelm 9011, 12011 Actuality 3f Aggregate 114f. See alSo Class Algebra: abstract 81; of classes 87,. 92, 128; of numbers 18, 45; of relations 128 Alternation 84 Alternative denial Slf, 84,94 Ambiguity 32, 58, 67 Anla$tmal: geometry 81; philosophy Analyticity 20, 22f* and existence 161, 163; and modality 143, 159-153, 156f.; and postulation 35; and reductiomsm 41; and synonymy m\u2019, 3lf, 151; contrasted with truth 130, in artificial languages 32-37 138; Ancestor 115 Antinomy, see Contradiction, Paradox ABIST~TLE 22,81, 155f Arithmetic 18f, 45, 81, 92f, 127f Artificial language 32-37 Atomic sentence 23, 30, 166 Attribute 8-11, 18, 75, 108, 156ff; versa3 class 107f, 119, 122f, 15:3 haso?&runa 96 BI.AIZK, Max 15n Bmorr, Bernard 5On, 52n BI.ANMFIELD, Leonard 5On, 52n BOOLE, George 87, 92 Bound, least upper 127 Bound variable 86f, 102ff; in ontological commitment 6, 12ff, 103, 108ff, 113, 128; in stratification 91n; erstwhile schematic 113f, llSf, 121-124. restricted to elements 97 :lOO~ Greek 111. See also Quantificadon, qariable BROUWER, L. E. J. 14, 125n BUHLER, Karl 51 CANTOR, Georg 14,92n, 121f, 126f, 129 CARNAP, Rudolf 14, 23ff, 45f, 158n; Aufbau 39ff; on modality 144n, 152156; on semantical rules 33, 36; OD synonym 29n, 32 CASSIRER, E mst 61 hADWICK, J.A.166 t%ZC Zom of 89 CHUR& Alonao 14 104, 11.6; on modalitv\u2019l52ff, 156;\u2018on semantics 108, 132x( 135n, 142, 145n; his theorem 5,96n Classes: abstraction of 30, 76ff, 87, 94ff, 104, 159; abstractness of 114; algebra of 87, 92, 128; commitment to 45f, 113, 115, 122; determination of 89; existence of, in general 14, 18 114ff 125f; existence of, in speciai theollies 39.81.90.93-109. 113. 128: Axiom 35,8&T, 96f, IOOf; of infinity 89, 93; of reducibility 125n, 127 names of 30, 108, 113ff; &s&g of 71-77, 117-127; versus attributes BARCAN, Ruth 156 Collection 107, 114 122f, 157 Behaviorism 48 Combinator 104f Begi.nsEnde nonbeing lff, 7. Sets also Commitment, ontological lff, S-11,44f, Belief 142, 144, 148 to abstract entities lOf, 45, 73, Bentham, Jeremy 39, 42 78; 13Of; m logic and semantics 96, 112f, BERNAYS, Paul 15n, 9On, 97n 116; in mathematics 13, !9 103,122, BERRY? G. G. 134 127ff. See also Criterion, .dypostasis, Bicondrtional32, 84 Ontology Common sense 45 Bind, 84% Bound Completeness 19, 89, 96, 1X6, 131, 137", "authors": ["Richard Milton Martin", "Willard van Orman Quine"], "id": "d807145539dc1ebffc0a67182748ef5ce8c6aaab", "title": "From a Logical Point of View", "references": ["99383f03f6698eafe37cf0b646b0483fe651723b", "d169a71a085681efe4edf3521081fea95b20e780", "ba584e40e69f55bdeaad144e7ef3e6f92102b933", "a964c10db88ffadcc907aaa4277297cd57182085", "96b00a66f2f001ec0d5697f5d52311dffee7cffc", "3e865b8767daec19dceb3acdcda5b4e1e8cb52b1", "3172cb4b50f1f226e89747089eb98b83f8b17ab4"]}, {"date": "1963", "abstract": "Abstract : A formal theory is given concerning situations, causality and the possibility and effects of actions is given. The theory is intended to be used by the Advice Taker, a computer program that is to decide what to do by reasoning. Some simple examples are given of descriptions of situations and deductions that certain goals can be achieved.", "authors": ["John W. Mccarthy"], "id": "9010d8c1850605bbfed06f60be208b6639c95aff", "title": "Situations, Actions, and Causal Laws", "references": []}, {"date": "1976", "abstract": "Semantic Scholar extracted view of \"Computer-based medical consultations, MYCIN\" by Edward H. Shortliffe", "authors": ["Edward H. Shortliffe"], "id": "c8e38cee27196aebba894bba680e74f23bf9c560", "title": "Computer-based medical consultations, MYCIN", "references": []}, {"date": "1969", "abstract": "The language BCPL (Basic CPL) was originally developed as a compiler writing tool and as its name suggests it is closely related to CPL (Combined Programming Language) which was jointly developed at Cambridge and London Universities. BCPL adopted much of the syntactic richness of CPL and strived for the same high standard of linguistic elegance; however, in order to achieve the efficiency necessary for system programming its scale and complexity is far less than that of CPL. The most significant simplification is that BCPL has only one data type---the binary bit pattern---and this feature alone gives BCPL a characteristic flavour which is very different of that of CPL and most other current programming languages.", "authors": ["Martin Richards"], "id": "164ce547a4f3855840f4b8a8ff8212167a670be5", "title": "BCPL: a tool for compiler writing and system programming", "references": []}, {"date": "1968", "abstract": "In [2] Prior puts forward a tense logic, GHi, which is intended to axiomatise tense logic with time linear and rational; he also contemplates the tense logic with time linear and real. The purpose of this paper is to give completeness proofs for three axiom systems, GH1, GHlr, GHli, with respect to tense logic with time linear and rational, real, and integral, respectively.' In a fourth section I show that GHI and GHlr have the finite model property, but that GHli lacks it. GHI has the operators of the classical propositional calculus, together with operators P, H, F, G for 'It has been the case that', 'It has always been the case that', 'It will be the case that', 'It will always be the case that', respectively. G and H are primitive, the others being given by", "authors": ["R. A. Bull"], "id": "2d504759b4e1571075c85813d03304f0faf8e2fe", "title": "An Algebraic Study of Tense Logics with Linear Time", "references": []}, {"date": "1978", "abstract": "Packet radio (PR) is a technology that extends the application of packet switching which evolved for networks of point-to-point communication lines to the domain of broadcast radio. It offers a highly efficient way of using a multiple access radio channel with a potentially large number of mobile subscribers to support computer communication and to provide local distribution of information over a wide geographic area. We discuss the basic concepts of packet radio in this paper and present the recent technology and system advances in this field. Various aspects of spread spectrum transmission in the network environment are identified and our experience with a testbed network in the San Francisco Bay area is discussed.", "authors": ["R.E. Kahn", "S. Gronemeyer", "Jerry D. Burchfiel", "R. C. Kunzelman"], "id": "0f389b20703fd956cd7450f0e925bbb544677b69", "title": "Advances in packet radio technology", "references": ["95a2cd29167734c5dd295c620329358ad653e849", "73eff68aa35741f02c9d657564588595e0109bef", "42368f0bea8f9fea99e264e0b11c16f71c4ac53d", "fb9ced22df84ff07287101f55b09d7761a520458", "283286707c62b654909d7612510aa38f80373c7e", "2376c14623a3e1026cb50a3e0b738c89a42e109a"]}, {"date": "1986", "abstract": "Incidence Calculus is a technique for associating uncertainty values with logical sentences. These uncertainty values are called incidences and they are sets of points, which may be thought of as representing equivalence classes of situations, Tarskian models, or possible worlds. Incidence Calculus was originally introduced in [1].Incidence Calculus was designed to overcome various inherent problems with purely numeric mechanisms for uncertain reasoning [2]. In particular, incidences can represent the dependence between sentences, which numbers cannot, and hence Incidence Calculus can provide genuine, probabilistic reasoning.In this paper we prove soundness and completeness results for some algorithms introduced in [1] and hence satisfy some of the correctness criteria for Incidence Calculus. These algorithms can be used for probabilistic reasoning and to check the consistency of the subjective probabilities of sentences.", "authors": ["Alan Bundy"], "id": "1ebb0cfd4509b229d3d3739fe468616d4d11e0ba", "title": "Correctness criteria of some algorithms for uncertain reasoning using Incidence Calculus", "references": []}, {"date": "1971", "abstract": "We describe an operational program that locates objects in a television image and traces their edges. The program accommodates the television camera, maximizing dynamic range during acquisition and sensitivity during tracing, to obtain the most appropriate image for each phase. If the trace routine loses an edge, various heuristics diagnose the difficulty and tune both the camera and software to recover contrast at the point of difficulty. Experimental evidence of the effectiveness of accommodation is provided.", "authors": ["Karl K. Pingle", "Jay M. Tenenbaum"], "id": "9957429e56e0ac0a906b769476c0518732051261", "title": "An Accommodating Edge Follower", "references": ["10eae4058bd6a68600afc4a068e32eaf741f1e74", "0357413d2005fc1da140a3332f0a353354d4b99f"]}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"A Laboratory for Hand-Eye Research\" by Jay M. Tenenbaum et al.", "authors": ["Jay M. Tenenbaum", "Alan C. Kay", "Thomas O. Binford", "Gilbert Falk", "Jerome A. Feldman", "G. Grape", "R. Paul", "Karl K. Pingle", "Irwin Sobel", "Robert F. Sproull"], "id": "ae12520c730c4588f79f1751f04b9675fd10e3d9", "title": "A Laboratory for Hand-Eye Research", "references": []}, {"date": "1960", "abstract": "Some important features involved in the zero\u2010crossing waves of the signal generated by the human voice are described. The method of analyzing the vocal sound by a Sonagraph using the natural wave forms is compared with our new method using the zero\u2010crossing waves. Next, three kinds of devices are described which have been developed by the authors to analyze speech sounds. These include devices to analyze automatically the zero\u2010crossing intervals, to display the zero\u2010crossing waves in three\u2010dimensional form, and to make a visible pattern of the zero\u2010crossing waves. In this paper, as examples of an important application of these devices, the following are described: the results of analysis of vowels in the Japanese language, some characteristics of a number of groups of consonants (after the separation of the consonant and the vowel parts), and the discrimination of the nasal consonants, [m] and [n].", "authors": ["Toshiyuki Sakai", "Sei\u2010ichi Inoue"], "id": "5f87bb3dc34be8c90498b31c3bdbf43f539f1a8f", "title": "New Instruments and Methods for Speech Analysis", "references": []}, {"date": "1968", "abstract": "I The basic formal ideas of this interpretation were in a certain sense anticipated by E. W. Beth with his \"reduced logic\" in [3]. We are grateful to Professor Bas C. van Fraassen for bringing this to our attention. The idea has also been employed by P. T. Geach in his \"Quantification theory and the problem of identifying objects of reference\" in Acta Philosophica Fennica, 1963, and by Henry S. Leonard in his \"Essences, attributes, and predicates\" in the Proceedings of the American Philisophical Association, 1963-64. Further, H. Leblanc has studied the interpretation in a pair of forthcoming papers, \"A simplified strong completeness proof for QC=,\" and \"A simplified account of validity and implication for quantification logic.\"", "authors": ["J. Michael Dunn", "Nuel Belnap"], "id": "b1b61c2bf83a33531a6b8503c9963a48fd3df0cf", "title": "The substitution interpretation of the quantifiers", "references": ["670bd553cd1db22576b54c728d520c1b16f826c6"]}, {"date": "1968", "abstract": "The anthropomorphic terms of the title may suggest an interest in machines that look or act like men. To this extent it is misleading. Our interest is in extending the range of tasks to which machines can be applied to include those that, when performed by a human, require coordination between perceptual and motor processes. We attempt to suppress the egocentric idea that man performs these tasks in the best of all possible ways.", "authors": ["J. Michael McCarthy", "L. D. Earnest", "Raj Reddy", "Pierre J. Vicens"], "id": "c3576b28daab769c8ece29fed47e6f95f83f4777", "title": "A computer with hands, eyes, and ears", "references": []}, {"date": "1965", "abstract": "Semantic Scholar extracted view of \"Machine Perception of 3-Dimensional Solids\" by Lawrence G. Roberts", "authors": ["Lawrence G. Roberts"], "id": "e979c7bf95e924f71ed6697da3cb5bac3ea926b7", "title": "Machine Perception of 3-Dimensional Solids", "references": []}, {"date": "1968", "abstract": "Semantic Scholar extracted view of \"Computer control of a mechanical arm through visual input\" by Karl K. Pingle et al.", "authors": ["Karl K. Pingle", "Jonathan A. Singer", "William M. Wichman"], "id": "10eae4058bd6a68600afc4a068e32eaf741f1e74", "title": "Computer control of a mechanical arm through visual input", "references": []}, {"date": "1968", "abstract": "Abstract : Given a program, an algorithm will be described for constructing an expression, such that the program is valid (i.e., terminates and yields the right answer) if and only if the expression is inconsistent. Similar result for the equivalence problem of programs is given. These results suggest a new approach for proving the validity and the equivalence of programs. (Author)", "authors": ["Zohar Manna"], "id": "75ffaa9fd92e72c10c046f2cab543d46d3b25a35", "title": "FORMALIZATION OF PROPERTIES OF PROGRAMS", "references": []}, {"date": "1970", "abstract": "Abstract : The report developes a parametric model for a computer-controlled moveable camera on a pan-tilt head. The model expresses the transform relating object space to image space as a function of the control variables of the camera. We constructed a calibration system for measuring the model parameters which has a demonstrated accuracy more than adequate for our present needs. We have also identified the major source of error in model measurement to be undesired image motion and have developed means of measuring and compensating for some of it and eliminating other parts of it. The system can measure systematic image distortions if they become the major accuracy limitation. It has been shown how to generalize the model to handle small systematic errors due to aspects of pan-tilt head geometry not presently accounted for. The report demonstrates the model's application in stereo vision and have shown how it can be applied as a predictive device in locating objects of interest and centering them in an image. (Author)", "authors": ["Irwin Sobel"], "id": "18836cbf140d542022ec5f04805fa47d9d0772f9", "title": "Camera Models And Machine Perception", "references": ["bf47f891e889c39edd799eb1802a033bf7445412", "f8276f400d412a401986aabfec3b2ee19ced5113", "70e712cf863cd84963257b927bd57773cbcc0d11", "c78a6bb3ff2ea09bd4a4cf971984f2ccd4e08e1a", "7fa23280d121ea05e88157b268c476c6dbc6800f", "f9b3e5785bcf1743b3d7212122cc0f61ed3f49f3", "368c9d8c89036aee75bf4159ea3e4d5851d3c6c4", "203bc34514447f0228f368006eef2821945e7ac1"]}, {"date": "1969", "abstract": "By the modes of modality I do not mean the changing fashions that prevail or have prevailed in the study of modal logics, although I would be tempted to comment on them, too.1 I am referring to those modes or modifications which have given modal logic its name. In other words, I have in mind the variety of systems of modal logic and the variety of philosophically interesting interpretations which can often be given of them. The point of my paper is to recommend a specific method for the study of this variety, which to my mind constitutes a veritable embarrassment of riches. This embarrassment also affects my paper, I am afraid; the major part of it is a series of sketches for applications of my methods rather than a continuous argument.", "authors": ["Jaakko Hintikka"], "id": "349e996dcd41195d862d57b8ca230affdd1e344c", "title": "The Modes of Modality", "references": []}, {"date": "1950", "abstract": "The main purpose of this paper is to present a formal system P in which we enjoy a smooth-running technique and which countenances a universe of classes which is symmetrical as between large and small. More exactly, P is a system which differs from the inconsistent system of [ 1 ] only in the introduction of a rather natural new restrictive condition on the defining formulas of the elements (sets, membership-eligible classes). It will be proved that if the weaker system of [ 2 ] is consistent, then P is also consistent. After the discovery of paradoxes, it may be recalled, Russell and Zermelo in the same year proposed two different ways of safeguarding logic against contradictions (see [ 3 ], [ 4 ]). Since then various simplifications and refinements of these systems have been made. However, in the resulting systems of Zermelo set theory, generation of classes still tends to be laborious and uncertain; and in the systems of Russell's theory of types, complications in the matter of reduplication of classes and meaningfulness of formulas remain. In [ 2 ], Quine introduced a system which seems to be free from all these complications. But later it was found out that in it there appears to be an unavoidable difficulty connected with mathematical induction. Indeed, we encounter the curious situation that although we can prove in it the existence of a class V of all classes, and we can also prove particular existence theorems for each of infinitely many classes, nobody has so far contrived to prove in it that V is an infinite class or that there exists an infinite class at all.", "authors": ["Hao Wang"], "id": "99383f03f6698eafe37cf0b646b0483fe651723b", "title": "A Formal System of Logic", "references": []}, {"date": "1958", "abstract": "Machines would be more useful if they could learn to perform tasks for which they were not given precise methods. Difficulties that attend giving a machine this ability are discussed. It is proposed that the program of a stored-program computer be gradually improved by a learning procedure which tries many programs and chooses, from the instructions that may occupy a given location, the one most often associated with a successful result. An experimental test of this principle is described in detail. Preliminary results, which show limited success, are reported and interpreted. Further results and conclusions will appear in the second part of the paper.", "authors": ["Richard M. Friedberg"], "id": "46bed4c578e96e05fa3e5704620c4ffa0746d78f", "title": "A Learning Machine: Part I", "references": []}, {"date": "1948", "abstract": "There are logicians who maintain that modal logic violates Leibniz's principle that if x and y are identical, then y has every property of x . The alleged difficulty is illustrated in the following example due to Quine. 1 A. It is logically necessary that 9 is less than 10. B. 9 = the number of the planets. C. Therefore, it is logically necessary that the number of the planets is less than 10.", "authors": ["Arthur Francis Smullyan"], "id": "3e865b8767daec19dceb3acdcda5b4e1e8cb52b1", "title": "Modality and Description", "references": []}, {"date": "1949", "abstract": "Semantic Scholar extracted view of \"A Note on Truth\" by John F. Thomson", "authors": ["John F. Thomson"], "id": "96b00a66f2f001ec0d5697f5d52311dffee7cffc", "title": "A Note on Truth", "references": []}, {"date": "1949", "abstract": "Although several proofs have been published showing the completeness of the propositional calculus (cf. Quine (1)), for the first-order functional calculus only the original completeness proof of Godel (2) and a variant due to Hilbert and Bernays have appeared. Aside from novelty and the fact that it requires less formal development of the system from the axioms, the new method of proof which is the subject of this paper possesses two advantages. In the first place an important property of formal systems which is associated with completeness can now be generalized to systems containing a non-denumerable infinity of primitive symbols. While this is not of especial interest when formal systems are considered as logics \u2014i.e., as means for analyzing the structure of languages\u2014it leads to interesting applications in the field of abstract algebra. In the second place the proof suggests a new approach to the problem of completeness for functional calculi of higher order. Both of these matters will be taken up in future papers. The system with which we shall deal here will contain as primitive symbols and certain sets of symbols as follows: (i) propositional symbols (some of which may be classed as variables , others as constants ), and among which the symbol \u201c f \u201d above is to be included as a constant; (ii) for each number n = 1, 2, \u2026 a set of functional symbols of degree n (which again may be separated into variables and constants ); and (iii) individual symbols among which variables must be distinguished from constants . The set of variables must be infinite.", "authors": ["Leon Henkin"], "id": "3172cb4b50f1f226e89747089eb98b83f8b17ab4", "title": "The Completeness of the First-Order Functional Calculus", "references": ["ddd06f4501b9389883aae4fe22978be9e9fa28a5", "7e2b5d9e76b9e5d50b5602511e3341981be958e9"]}, {"date": "1944", "abstract": "One of the preeminent problems confronting logicians is that of constructing a system of logic which will be adequate for mathematics. By a system's being adequate for mathematics, we mean that all mathematical theorems in general use can be deduced within the system. Several distinct logical systems, all having this end in view, have been proposed. Among these perhaps the best known are the systems referred to as \u201cPrincipia Mathematica\u201d and \u201cset theory.\u201d In both of these systems (we refer to the revised and simplified versions) there is a nucleus of propositions which can be derived by using only the axioms and rules of the restricted predicate calculus. However, if anything like adequacy for mathematics is to be expected, additional primitives and axioms must be added to the restricted predicate calculus. It is in their treatment of the additional primitive e, denoting class or set membership, that the above-mentioned systems differ. In addition to these two, a third and a stronger system has been proposed by W. V. Quine in his paper New foundations for mathematical logic . It is with this system of Quine's that our work is concerned and of which we now give a brief description.", "authors": ["Theodore Hailperin"], "id": "a964c10db88ffadcc907aaa4277297cd57182085", "title": "A Set of Axioms for Logic", "references": []}, {"date": "1978", "abstract": "Measurements over a variety of urban and terrain conditions were made using a spread-spectrum waveform centered at 1370 MHz. Chip rates of 10 and 20 MHz were used, giving high time-delay resolution. The transmitter end of the measurement link was elevated and fixed while the receiver was mobile. A detailed analysis of the multipath structure was made for various terrain conditions. This included distributions of the number and spacing of individual multi-path components as a function of amplitude threshold and also distribution of total delay. Thresholds are referenced to both average signal level and the maximum level in each pulse interval. Some instances of spatial variation of the received signal are also presented.", "authors": ["D.L. Nielson"], "id": "fb9ced22df84ff07287101f55b09d7761a520458", "title": "Microwave propagation measurements for mobile digital radio application", "references": []}, {"date": "1972", "abstract": "Semantic Scholar extracted view of \"ARPA Network Control Center\" by Ralph Alter", "authors": ["Ralph Alter"], "id": "42368f0bea8f9fea99e264e0b11c16f71c4ac53d", "title": "ARPA Network Control Center", "references": []}, {"date": "1971", "abstract": "Because of the fundamental importance of edges as primitives of pictures, automatic edge finding is set as goal. A set of requirements which should be met by a local edge recognizer is formulated. Their main concerns are fast and reliable recognition in the presence of noise. A unique optimal solution for an edge operator results. The operator obtains the best fit of an ideal edge element to any empirically obtained edge element. Proof of this is given. A reliability assessment accompanies every recognition process.", "authors": ["Manfred H. Hueckel"], "id": "0357413d2005fc1da140a3332f0a353354d4b99f", "title": "An Operator Which Locates Edges in Digitized Pictures", "references": []}, {"date": "1975", "abstract": "The application of packet-switching techniques to radio channels has provided a solution to many computer-communications problems previously unsolved. For example, a packet radio network can readily be designed to provide area coverage at data rates fast enough to support interactive operations for thousands of users having a variety of terminals such as hand-held devices, TTY-like devices, display devices, computers, and unattended sensors. Since the interconnections are by radio, the users can be fixed or mobile, and the network can be easily moved. Furthermore, it can be readily established in remote or primitive areas where a wired network would be impossible, and total connectivity of users will be provided.", "authors": ["Stanley C. Fralick", "James C. Garrett"], "id": "2376c14623a3e1026cb50a3e0b738c89a42e109a", "title": "Technological considerations for packet radio networks", "references": ["bf4bfc4beabbb1a4daca16a0ab6fcfc9316a99de"]}, {"date": "2004", "abstract": "The subject of this paper is the foundations of modal logic. By founda tions, we generally mean the underlying assumptions, the underpinnings. There is a normative sense in which it has been claimed that modal logic is without foundation. Professor Quine, in Word and Object, suggests that it was conceived in sin: the sin of confusing use and mention. The original transgressors were Russell and Whitehead. Lewis followed suit and constructed a logic in which an operator corresponding to 'necessarily' operates on sentences whereas 'is necessary' ought to be viewed as a predicate of sentences. As Professor Quine reconstructs the history of the enterprise,1 the operational use of modalities promised only one advan tage : the possibility of quantifying into modal contexts. This several of us2 were enticed into doing. But the evils of the sentential calculus were found out in the functional calculus, and with it - to quote again from Word and Object - 'the varied sorrows of modality transpose'. I do not intend to claim that modal logic is wholly without sorrows, but only that they are not those which Professor Quine describes. I do claim that modal logic is worthy of defense, for it is useful in connection with many interesting and important questions such as the analysis of causa tion, entailment, obligation and belief statements, to name only a few. If we insist on equating formal logic with strongly extensional functional", "authors": ["Ruth Barcan Marcus"], "id": "670bd553cd1db22576b54c728d520c1b16f826c6", "title": "Modalities and intensional languages", "references": []}, {"date": "1978", "abstract": "Considerable advances in the modeling and measurements of packet-switched networks have been made since this concept emerged in the late sixties. In this paper, we first review the modeling techniques that are most frequently used to study these packet transport networks; for each technique we provide a brief introduction, a discussion of its capabilities and limitations, and one or more representative applications. Next we review the basic measurement tools, their capabilities, their limitation, and their applicability to and implementation in different networks, namely land based wire networks, satellite networks, and ground packet radio networks; we also show the importance of well-designed experiments in satisfying the many measurement goals. Finally we discuss briefly some open problems for future research.", "authors": ["F.A. Tobagi", "Mario Gerla", "R. E. Peebles", "E.G. Manning"], "id": "283286707c62b654909d7612510aa38f80373c7e", "title": "Modeling and measurement techniques in packet communication networks", "references": ["fd99b3ee449733002fc6dbe203c54d938bca9ed5", "da5b6c1d61ecd4d4f83ee7636f245028f7ea6faa", "2eea440182e0e9603948a82585c20ed2cd2138f8", "9edf551025fab81b7165783c8659bc4530d141fb", "c1e4a1a1b4f8d64bc52e4a50494c419807af453d", "c5435b6cf613c54c08c954df29525946e1e91e1b", "d01ae624b91580104982a31bb53c93a00efdf506", "8251af9e6a1c1dc266278b31fdcbb825085011c3", "d9a047eee56639e91abc8cabe963aef16b75a65e", "6260d7842e9d9b3baa6dc8d8bfbd11eab8ca4fca"]}, {"date": "1968", "abstract": "This report describes a computer method for predicting long-term median transmission loss over irregular terrain. The method is applicable for radio frequencies above 20 MHz and may be used either with detailed terrain profiles for actual paths or with profiles that are representative of median terrain characteristics for a given area. Estimates of variability in time and with location, and a method for computing service probability, are included.", "authors": ["Anita G. Longley", "Philip L. Rice"], "id": "73eff68aa35741f02c9d657564588595e0109bef", "title": "PREDICTION OF TROPOSPHERIC RADIO TRANSMISSION LOSS OVER IRREGULAR TERRAIN. A COMPUTER METHOD-1968", "references": []}, {"date": "1973", "abstract": "Imagine that two users require the use of a communication channel. The classical approach to satisfying this requirement is to provide a channel for their use so long as that need continues (and to charge them for the full cost of this channel). It has long been recognized that such allocation of scarce communication resources is extremely wasteful as witnessed by their low utilization (see for example the measurements of Jackson & Stubbs). Rather than provide channels on a user-pair basis, we much prefer to provide a single high-speed channel to a large number of users which can be shared in some fashion; this then allows us to take advantage of the powerful \"large number laws\" which state that with very high probability, the demand at any instant will be approximately equal to the sum of the average demands of that population. In this way the required channel capacity to support the user traffic may be considerably less than in the unshared case of dedicated channels. This approach has been used to great effect for many years now in a number of different contexts: for example, the use of graded channels in the telephone industry, the introduction of asynchronous time division multiplexing, and the packet-switching concepts introduced by Baran et al., Davies, and finally implemented in the ARPA network. The essential observation is that the full-time allocation of a fraction of the channel to each user is highly inefficient compared to the part-time use of the full capacity of the channel (this is precisely the notion of timesharing) We gain this efficient sharing when the traffic consists of rapid, but short bursts of data. The classical schemes of synchronous time division multiplexing and frequency division multiplexing are examples of the inefficient partitioning of channels.", "authors": ["Leonard Kleinrock", "Simon S. Lam"], "id": "95a2cd29167734c5dd295c620329358ad653e849", "title": "Packet-switching in a slotted satellite channel", "references": ["ffcd5523413b82d464340a722461b6e6d5d041d2", "fdfa2f44f91716b5475f2ff51a5c350234170e38"]}, {"date": "1950", "abstract": "Semantic Scholar extracted view of \"A Complete Theory of Natural, Rational, and Real Numbers\" by John R. Myhill", "authors": ["John R. Myhill"], "id": "d169a71a085681efe4edf3521081fea95b20e780", "title": "A Complete Theory of Natural, Rational, and Real Numbers", "references": []}, {"date": "1949", "abstract": "In this paper, we are concerned with the arithmetical definability of certain notions of integers and rationals in terms of other notions. The results derived will be applied to obtain a negative solution of corresponding decision problems. In Section 1, we show that addition of positive integers can be defined arithmetically in terms of multiplication and the unary operation of successor S (where Sa = a + 1). Also, it is shown that both addition and multiplication can be defined arithmetically in terms of successor and the relation of divisibility | (where x|y means x divides y ).", "authors": ["Julia Robinson"], "id": "ba584e40e69f55bdeaad144e7ef3e6f92102b933", "title": "Definability and Decision Problems in Arithmetic", "references": ["c3f7ad4b2af9d9888be85a8a376c61e7a612acc2", "4ec9c76a327e38958378a5a7be88639678d2edef", "2fcec1760208cc955cbcad4ec5c66d2f3189da55", "392746142b0b67726be2fa16611586f32bd95630", "626d07872e6e795dd4861908c58d4bed0fe6c98e", "60400c043b2624f9cfc2d8daa0f45f3c1d524de3"]}, {"date": "1967", "abstract": "Abstract : An introduction to LISP is given on an elementary level. Topics covered include the programming system, 240 exercises with solutions, debugging of LISP programs, and styles of programming. More advanced discussions are contained in the following articles: Techniques using LISP for automatically discovering interesting relations in data; Automation, using LISP, of inductive inference on sequences; Application of LISP to machine checking of mathematical proofs; METEOR: A LISP interpreter for string transformations; Notes on implementing LISP for the M-460 computer; LISP as the language for an incremental computer; The LISP system for the Q-2 computer; An auxiliary language for more natural expression -- the A-language. Some applications of the utilization of the LISP programming language are given in the appendices.", "authors": ["Paul W. Abrahams", "Edmund C. Berkeley", "Fischer Black", "Daniel G. Bobrow", "Thomas G. Evans"], "id": "dc668c1c80a7bc533ad7b20ead1f737e953f79f9", "title": "THE PROGRAMMING LANGUAGE LISP: ITS OPERATION AND APPLICATIONS,", "references": []}, {"date": "1969", "abstract": "Semantic Scholar extracted view of \"Automatic interpretation and classification of images\" by Antonio Grasselli", "authors": ["Antonio Grasselli"], "id": "368c9d8c89036aee75bf4159ea3e4d5851d3c6c4", "title": "Automatic interpretation and classification of images", "references": []}, {"date": "1971", "abstract": "Abstract : Theorems are given concerning the order (i.e., rate) of convergence of a successive interpolation process for finding simple zeros of a function or its derivatives, using only function evaluations. Special cases include the successive linear interpolation process for finding zeros, and a parabolic interpolation process for finding turning points. Results on interpolation and finite differences include weakening the hypotheses of a theorem of Ralston on the derivative of the error in Lagrangian interpolation. The theoretical results are applied to given algorithms for finding zeros or local minima of functions of one variable, in the presence of rounding errors. The algorithms are guaranteed to converge nearly as fast as would bisection or Fibonacci search, and in most practical cases convergence is superlinear, and much faster than for bisection or Fibonacci search.", "authors": ["Richard P. Brent"], "id": "203bc34514447f0228f368006eef2821945e7ac1", "title": "Algorithms for finding zeros and extrema of functions without calculating derivatives", "references": []}, {"date": "", "abstract": "Jakina da Whiteheadek eta Russellek logika eta matematika eraiki dutela ageriko zenbait proposizio axiomatzat hartuz, eta horietatik, zehatz azaldutako inferentzia printzipioetan oinarrituz, logikako eta matematikako teoremak guztiz modu formalean ondorioztatu dituztela (hau da, sinboloen esanahiaren erabilpen gehigarririk egin gabe). Horrelako jardunbideak galdera bat dakarkigu segituan burura, hasieran postulatutako axiomen eta inferentzia printzipioen sistema osoa ote den, hau da, proposizio logikomatematiko guztiak eratorri ahal izateko nahikoa den edo, akaso, egiazkoak diren eta sistema honetan ondorioztatu ezinak diren proposizioak otu dakizkigukeen. Kalkulu proposizionaleko formulen kasuan galderak baiezko erantzuna jaso du, hau da, erakutsi2 da, kalkulu proposizionaleko formula zuzen guztiak Principia Mathematican emandako axiometatik ondorioztatzen direla. Gauza bera egingo dugu hemen \u00abkalkulu funtzional mugatua\u00bb3 deitutako eremu zabalagoko formulentzat; hau da, ondokoa frogatuko dugu:", "authors": ["Kurt G\u00f6del"], "id": "7e2b5d9e76b9e5d50b5602511e3341981be958e9", "title": "Die Vollst\u00e4ndigkeit der Axiome des logischen Funktionenkalk\u00fcls", "references": []}, {"date": "1938", "abstract": "Semantic Scholar extracted view of \"Completeness of the Propositional Calculus\" by W. V. Quine", "authors": ["W. V. Quine"], "id": "ddd06f4501b9389883aae4fe22978be9e9fa28a5", "title": "Completeness of the Propositional Calculus", "references": []}, {"date": "1969", "abstract": "There is a large continuing project at Stanford Artificial Intelligence Laboratory aimed towards the development of a system capable of interesting perceptual-motor behavior. This paper presents a brief outline of the currently active efforts and suggests references for more detailed information. A more thorough discussion of the effort to organize a visual perception system is presented.", "authors": ["Jerome A. Feldman", "Gerald M. Feldman", "Gilbert Falk", "G. Grape", "J. Pearlman", "Irwin Sobel", "Jay M. Tenenbaum"], "id": "7fa23280d121ea05e88157b268c476c6dbc6800f", "title": "The Stanford Hand-Eye Project", "references": ["c0956c94191cebf50e3ec32e14897cbc6bde9119", "07f4f7c0e01282f4852cd1c13d97048c1c64106f", "f8276f400d412a401986aabfec3b2ee19ced5113", "c3576b28daab769c8ece29fed47e6f95f83f4777", "25a13868653f786ba00043be5b34cb12c0854976", "1374222f975da69b26b0c34d4fac6d139a065e4d", "06600e1ca9451d81e89d078a924184683ace9196", "313225cd9b6729c1a239a3af40d2dfeaf6946a52", "ab5387cf077f5b97c7dd08845c006e5c1ec89ff5"]}, {"date": "1966", "abstract": "A signature feeding mechanism having a shuttle plate and an associated vacuum gripper for feeding the bottom signature from a stack in a supply magazine into the bite of pinch rolls which advance successive signature pieces to a transport system for advance to a label applicator, or the like, and having a guard plate for bridging exposed portions of the gap between the leading edge of the shuttle plate and the signature receiving end of the associated transport conveyor floor so as to prevent possible damage to, or interference with the operation of, the feeding mechanism, by the presence of a foreign object in the gap, and possible injury to an operator through careless insertion of the fingers in the same.", "authors": ["James Jerry Gibson"], "id": "f9b3e5785bcf1743b3d7212122cc0f61ed3f49f3", "title": "The senses considered as perceptual systems", "references": []}, {"date": "1952", "abstract": "Let's read! We will often find out this sentence everywhere. When still being a kid, mom used to order us to always read, so did the teacher. Some books are fully read in a week and we need the obligation to support reading. What about now? Do you still love reading? Is reading only for you who have obligation? Absolutely not! We here offer you a new book enPDFd the perception of the visual world to read.", "authors": ["Ralph Hetherington"], "id": "bf47f891e889c39edd799eb1802a033bf7445412", "title": "The Perception of the Visual World", "references": []}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"An operator which locates edges in digitized pictures\" by H. Heuckel", "authors": ["H. Heuckel"], "id": "70e712cf863cd84963257b927bd57773cbcc0d11", "title": "An operator which locates edges in digitized pictures", "references": []}, {"date": "1969", "abstract": "Abstract : The paper describes the computer representation of scenes consisting of a number of simple three-dimensional objects. One method of representing such scenes is a space oriented representation where information about a region of space is accessed by its coordinates. Another approach is to access the information by object, where, by giving the object name, its description and position are returned. As the description of an object is lengthy, it is desirable to group similar objects. Groups of similar objects can be represented in terms of a common part and a number of individual parts. If it is necessary to simulate moving an object then only the individual information need be saved. (Author)", "authors": ["Richard P. Paul", "Gil Falk", "Jerome A. Feldman"], "id": "f8276f400d412a401986aabfec3b2ee19ced5113", "title": "THE COMPUTER REPRESENTATION OF SIMPLY DESCRIBED SCENES", "references": []}, {"date": "1972", "abstract": "Abstract : The goals of the research discussed in the report are: To create new models of time-shared computer systems which include important features commonly found in real systems; to insure that the formulations of, and solutions to, these models are relatively simple so that they may be used by designers and computer center managers; to compare the behavior of these models with the behavior of more complex systems through simulation studies and empirical performance investigations of operational computers; and to indicate some of the ways these models may be used to aid in the design, evaluation, and control of time-shared computers.", "authors": ["Jr. John William Mccredie"], "id": "4448051cadba1a198f1fe1d705780cb43c8ca302", "title": "Analytic models of time-shared computing systems: new results, validations, and uses", "references": []}, {"date": "1970", "abstract": "In this paper a computer network is defined to be a set of autonomous, independent computer systems, interconnected so as to permit interactive resource sharing between any pair of systems. An overview of the need for a computer network, the requirements of a computer communication system, a description of the properties of the communication system chosen, and the potential uses of such a network are described in this paper.", "authors": ["Lawrence G. Roberts", "Barry D. Wessler"], "id": "5444939ad7935c0302f414c136c0d4f09c202e67", "title": "Computer network development to achieve resource sharing", "references": []}, {"date": "1973", "abstract": "A computer network is being developed in France, under government sponsorship, to link about twenty heterogeneous computers located in universities, research and D.P. Centers. Goals are to set up a prototype network in order to foster experiment in various areas, such as: data communications, computer interaction, cooperative research, distributed data bases. The network is intended to be both, an object for research, and an operational tool.\n In order to speed up the implementation, standard equipment is used, and modifications to operating systems are minimized. Rather, the design effort bears on a carefully layered architecture, allowing for a gradual insertion of specialized protocols and services tailored to specific application and user classes.\n A particular objective, for which CYCLADES should be an operational tool, is to provide various departments of the French Administration with access to multiple data bases located in geographically distant areas.\n Host-host protocols, as well as error and flow control mechanisms are based on a simple message exchange procedure, on top of which various options may be built for the sake of efficiency, error recovery, or convenience. Depending on available computer resources, these options can be implemented as user software, system modules, or front end processor package. For each of them, network-wide interfaces are defined, to conserve consistency in human communications.\n CYCLADES uses a packet-switching sub-network, which is a transparent message carrier, completely independent of host-host conventions. While in many ways similar to ARPANET, it presents some distinctive differences in address and message handling, intended to facilitate interconnection with other networks. In particular, addresses can have variable formats, and messages are not delivered in sequence, so that they can flow out of the network through several gates toward an out-side target.\n Terminal concentrators are mini-hosts, and implement whatever services users or applications require, such as sequencing, error recovery, code translation, buffering, etc. Some specialized hosts may be installed to cater for specific services, such as mail, resource allocation, information retrieval, mass storage. A control center is also being installed and will be operated by the French PTT.", "authors": ["Louis Pouzin"], "id": "6260d7842e9d9b3baa6dc8d8bfbd11eab8ca4fca", "title": "Presentation and major design aspects of the CYCLADES computer network", "references": []}, {"date": "1950", "abstract": "This paper is concerned with the problem of constructing a computing routine or \u201cprogram\u201d for a modern general purpose computer which will enable it to play chess. Although perhaps of no practical importance, the question is of theoretical interest, and it is hoped that a satisfactory solution of this problem will act as a wedge in attacking other problems of a similar nature and of greater significance. Some possibilities in this direction are:- \n \n(1) \n \nMachines for designing filters, equalizers, etc. \n \n \n \n \n(2) \n \nMachines for designing relay and switching circuits. \n \n \n \n \n(3) \n \nMachines which will handle routing of telephone calls based on the individual circumstances rather than by fixed patterns. \n \n \n \n \n(4) \n \nMachines for performing symbolic (non-numerical) mathematical operations. \n \n \n \n \n(5) \n \nMachines capable of translating from one language to another. \n \n \n \n \n(6) \n \nMachines for making strategic decisions in simplified military operations. \n \n \n \n \n(7) \n \nMachines capable of orchestrating a melody. \n \n \n \n \n(8) \n \nMachines capable of logical deduction.", "authors": ["Claude E. Shannon"], "id": "1a6eacdbf4e881a91cf6b76a9f70f53ccc290ae1", "title": "Programming a computer for playing chess", "references": []}, {"date": "1969", "abstract": "Data and encoded voice signals differ in certain characteristics which require, if switching is the goal, slightly different approaches. Some of the characteristics of digitized speech and data are compared\n This paper describes an operating experimental system (three terminals) which switches messages of variable message length occurring at periodic and aperiodic rates, achieves fast access time, and accommodates bit rate inputs from teletypewriter (110 bits/sec) to disc transfer rates (4 \u00d7 10 6 bits/sec).", "authors": ["W. D. Farmer", "E. E. Newhall"], "id": "d9a047eee56639e91abc8cabe963aef16b75a65e", "title": "An experimental distributed switching system to handle bursty computer traffic", "references": []}, {"date": "1961", "abstract": "Foreword. Preface. Chapters: 1. Introduction. 2. Telephone Systems. 3. Probability Theory. 4. Traffic Characteristics. 5. Loss Systems. 6. Waiting Systems. 7. Limited Availability. 8. Link Systems. 9. Special Systems. 10. Exchange Problems. 11. Markovian Queues. Mathematical Appendix. References. Name Index. Subject Index.", "authors": ["R. A. Acton"], "id": "fdfa2f44f91716b5475f2ff51a5c350234170e38", "title": "Introduction to Congestion Theory in Telephone Systems", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"The ALOHA System-Another Alternative for Computer Communications\" by Norman M. Abramson", "authors": ["Norman M. Abramson"], "id": "ffcd5523413b82d464340a722461b6e6d5d041d2", "title": "The ALOHA System-Another Alternative for Computer Communications", "references": []}, {"date": "1935", "abstract": "Semantic Scholar extracted view of \"Der Wahrheitsbegriff in den formalisierten Sprachen\" by Alfred Tarski", "authors": ["Alfred Tarski"], "id": "626d07872e6e795dd4861908c58d4bed0fe6c98e", "title": "Der Wahrheitsbegriff in den formalisierten Sprachen", "references": []}, {"date": "1936", "abstract": "We shall say that a logic is \u201csimply consistent\u201d if there is no formula A such that both A and \u223c A are provable. \u201c\u03c9-consistent\u201d will be used in the sense of Godel. \u201cGeneral recursive\u201d and \u201cprimitive recursive\u201d will be used in the sense of Kleene, so that what Godel calls \u201crekursiv\u201d will be called \u201cprimitive recursive.\u201d By an \u201c Entscheidungsverfahren \u201d will be meant a general recursive function \u03d5 ( n ) such that, if n is the Godel number of a provable formula, \u03d5 ( n ) = 0 and, if n is not the Godel number of a provable formula, \u03d5 ( n ) = 1. In specifying that \u03d5 must be general recursive we are following Church in identifying \u201cgeneral recursiveness\u201d and \u201ceffective calculability.\u201d First, a modification is made in Godel's proofs of his theorems, Satz VI (Godel, p. 187\u2014this is the theorem which states that \u03c9 -consistency implies the existence of undecidable propositions) and Satz XI (Godel, p. 196\u2014this is the theorem which states that simple consistency implies that the formula which states simple consistency is not provable). The modifications of the proofs make these theorems hold for a much more general class of logics. Then, by sacrificing some generality, it is proved that simple consistency implies the existence of undecidable propositions (a strengthening of Godel's Satz VI and Kleene's Theorem XIII) and that simple consistency implies the non-existence of an Entscheidungsverfahren (a strengthening of the result in the last paragraph of Church).", "authors": ["J. Barkley Rosser"], "id": "392746142b0b67726be2fa16611586f32bd95630", "title": "Extensions of Some Theorems of G\u00f6del and Church", "references": []}, {"date": "1947", "abstract": "Semantic Scholar extracted view of \"Weak definitions of field\" by B. A. Bernstein", "authors": ["B. A. Bernstein"], "id": "2fcec1760208cc955cbcad4ec5c66d2f3189da55", "title": "Weak definitions of field", "references": []}, {"date": "", "abstract": "l n h \u00ab l t, Seile A. E in le i tung , , 129 \u00a7 l. Fragestellung und Behandlungsmethode . . . . . . . 129 \u00a7 2. Allgemeines , 131 B. U n \u00e4 r e und b i n \u00e4 r e F o r m e n . 134 \u00a7 3. Un\u00e4re Formen , 134 \u00a7 4. Darstellbarkeit der Null in K (p) durch tern\u00e4re Formen. 134 \u00a7 5. Darstellbarkeit in K (p) durch bin\u00e4re Formen , 135 \u00a7 6. Darstellbarkeit der Null in K (l) durch tern\u00e4re Formen 135 \u00a7 7. Darstellbarkeit in K(1) durch bin\u00e4re Formen 137. C. T e r n \u00e4 r e F o r m e n 138 \u00a7 8. Darstellbarkeit der Null in K (p) durch quatern\u00e4re Formen . . . 189 \u00a7 9. Darstellbarkeit in K(p) durch tern\u00e4re Formen 141 \u00a7 10. Darstellbarkeit der Null in .ff (1) durch quatern\u00e4re Formen 142 \u00a7 11. Darstellbarkeit in JT(1) durch tern\u00e4re Formen 143 D. n-\u00e4r e F o r m e n (n > 4) , 145 \u00a7 12. Darstellbarkeit in K (p) durch w-\u00e4re Formen (n >. 4) , 145 \u00a7 13. Darstellbarkeit in jfiT(l) durch >i-\u00e4re Formen (n \u0302 4) 146 \u00a7 14. Zusammenfassung 147", "authors": ["Helmut Hasse"], "id": "4ec9c76a327e38958378a5a7be88639678d2edef", "title": "\u00dcber die Darstellbarkeit von Zahlen durch quadratische Formen im K\u00f6rper der rationalen Zahlen.", "references": []}, {"date": "1936", "abstract": "Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission. The JSTOR Archive is a trusted digital repository providing for long-term preservation and access to leading academic journals and scholarly literature from around the world. The Archive is supported by libraries, scholarly societies, publishers, and foundations. It is an initiative of JSTOR, a not-for-profit organization with a mission to help the scholarly community take advantage of advances in technology. For more information regarding JSTOR, please contact support@jstor.org.", "authors": ["Alonzo Church"], "id": "60400c043b2624f9cfc2d8daa0f45f3c1d524de3", "title": "An Unsolvable Problem of Elementary Number Theory", "references": []}, {"date": "1931", "abstract": "A jack member onto a pair of which members a vehicle can be driven and then raised to a position inclined to the horizontal. The jack members are used more particularly for stowing vehicles in containers on a number of support frames. The jack members raise the vehicles so that they are supported at an angle to the horizontal with their wheels held in wheel support cradles attached to the frames. The jack members each comprises an approach ramp and a lifting ramp pivotally connected at one of their ends on which the vehicle is initially aligned and supported prior to being elevated at one end for inclined support on the vehicle frame.", "authors": ["Kurt G\u00f6del"], "id": "c3f7ad4b2af9d9888be85a8a376c61e7a612acc2", "title": "\u00dcber formal unentscheidbare S\u00e4tze der Principia Mathematica und verwandter Systeme I", "references": []}, {"date": "1968", "abstract": "We consider visual scenes composed by the optical image of a group of bodies. When such a scene is \"seen\" by a computer through a film spot scanner, image dissector, or similar device, it can be treated as a two-dimensional array of numbers, or as a function of two variables.", "authors": ["Adolfo Guzm\u00e1n-Arenas"], "id": "313225cd9b6729c1a239a3af40d2dfeaf6946a52", "title": "Decomposition of a visual scene into three-dimensional bodies", "references": []}, {"date": "1962", "abstract": "MH-1 is a motorized and sensitized servomanipulator operated by the TX-O computer at the Massachusetts Institute of Technology. It serves as an experimental vehicle to explore the feasibility of direct relations between a digital computer and the physical world with which this computer is concerned. Usually, a human interpreter stands between the computer and the physical world. Instead, the TX-O computer in the MH-1 system is programmed to perform by itself some of the functions normally assigned to the human intermediary; namely, to perceive the world, to appreciate it, and to determine a reasonable course of action after a goal has been specified for the hand. The data processing tools used are, rather than numerical operations on quantitative signals, pattern recognition and simulation of higher cognitive processes such as awareness and understanding. This paper describes some of the experiments performed with MH-1 and the mechanisms upon which the capabilities of MH-1 are based.", "authors": ["Heinrich A. Ernst"], "id": "06600e1ca9451d81e89d078a924184683ace9196", "title": "MH-1, a computer-operated mechanical hand", "references": []}, {"date": "1967", "abstract": "Abstract This computer program accepts expressions in natural language as on-line input. It searches each expression for syntactic and semantic patterns. When a pattern match is discovered an appropriate reply is typed out in natural language so that a continuing dialogue develops between person and program. The dialogue is restricted to the context of interpersonal relations such as occurs in a psychiatric interview. The program is an interpreter/supervisor written in SUBALGOL and runs on a 32K IBM 7090 connected via a direct-data device to a PDP-1 and a Philco console.", "authors": ["Kenneth Mark Colby", "Horace Enea"], "id": "25a13868653f786ba00043be5b34cb12c0854976", "title": "Heuristic methods for computer understanding of natural language in context-restricted on-line dialogues", "references": []}, {"date": "1969", "abstract": "In this paper w. present a general data structure for a semantic memory, and we give a definition of \"analogy\" between items of semantic information. We then construct an inductive process in which general laws are formulated and verified on the basis of observations of individual cases.", "authors": ["Joseph D. Becker"], "id": "07f4f7c0e01282f4852cd1c13d97048c1c64106f", "title": "The Modeling of Simple Analogic and Inductive Processes in a Semantic Memory System", "references": ["20eb54d6d0159bb257564e632727aed369c9fd1c"]}, {"date": "1967", "abstract": "A computer may gather a lot of information from its environment in an optical or graphical manner. A scene, as seen for instance from a TV camera or a picture, can be transformed into a symbolic description of points and lines or surfaces. This thesis describes several programs, written in the language CONVERT , for the analysis of such descriptions in order to recognize, differentiate and identify desired objects or classes or objects in the scene. Examples are given in each case. Although the recognition might be in terms of projections of 2-dim and 3-dim objects, we do not deal with stereoscopic information. One of our programs (Polybrick) identifies parallelepipeds in a scene which may contain hidden bodies and non-parallelepipedic objects. The program TD works mainly with 2-dimensional figures, although under certain conditions successfully identifies 3-dim objects. Overlapping objects are identified when they are transparent. A third program, DT, works with 3-dim and 2-dim objects, and does not identify objects which are not completely seen. Important restrictions and suppositions are: (a) the input is assumed perfect (noiseless), and in a symbolic format; (b) no perspective deformation is considered. A portion of this thesis is devoted to the study of models (symbolic representation) of the objects we want to identify; different schemes, some of them already in use, are discussed. Focusing our attention on the more general problem of identification of general objects when they substantially overlap, we propose some schemes for their recognition, and also analyze some problems that are met.", "authors": ["Adolfo Guzm\u00e1n-Arenas"], "id": "c0956c94191cebf50e3ec32e14897cbc6bde9119", "title": "SOME ASPECTS OF PATTERN RECOGNITION BY COMPUTER", "references": []}, {"date": "1963", "abstract": "Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering, 1963.", "authors": ["Lawrence G. Roberts"], "id": "ab5387cf077f5b97c7dd08845c006e5c1ec89ff5", "title": "Machine Perception of Three-Dimensional Solids", "references": []}, {"date": "1977", "abstract": "Abstract This paper presents a broad survey of packet-switched data network simulation experiments at the National Physical Laboratory during the years 1968\u20131976. Reference is made to several operating protocols, including the isarithmic method of flow control. The effects of various network enhancements and of several types of component failure are studied. Initial results of a study of hierarchical networks are reported.", "authors": ["Wyn L. Price"], "id": "8251af9e6a1c1dc266278b31fdcbb825085011c3", "title": "Data Network Simulation; Experiments at the National Physical Laboratory 1968-76", "references": []}, {"date": "1973", "abstract": "Abstract : The emphasis of this research is on the development of mathematical programming tools for the design of S/F communication networks. An analytical model for the system is first presented and discussed. The design variables (routing of messages, channel capacities, topology, etc.) are then defined and proper design criteria (delay, cost, thruput, etc.) are established and expressed in terms of the variables. Next, various design problems are defined and investigated; the most significant of them here follow: (1) Find the minimum cost channel capacity assignment, given the routing of the messages and the maximum admissible delay T; (2) Find the routing which minimizes the delay, given the channel capacities (and therefore the cost); (3) Find the routing and capacities assignment which minimizes the cost, given the maximum admissible delay T. (4) Find the topology, routing and capacities assignment which minimizes the cost, given the maximum admissible delay T. (Author Modified Abstract)", "authors": ["Mario Gerla"], "id": "d01ae624b91580104982a31bb53c93a00efdf506", "title": "The design of store-and-forward (s/f) networks for computer communications", "references": []}, {"date": "1976", "abstract": "The growth of computer networks has proven both the need for and the success of resource sharing technology. A new resource sharing technique, utilizing broadcast channels, has been under development as a Packet Radio system and will shortly undergo testing. In this paper, we consider that Packet Radio system, and examine the measurement tasks necessary to support such important measurement goals as the validation of mathematical models, the evaluation of system protocols and the detection of design flaws. We describe the data necessary to measure the many aspects of network behavior, the tools needed to gather this data and the means of collecting it at a central location; all in a fashion consistent with the system protocols and hardware constraints, and with minimal impact on the system operation itself.", "authors": ["Fouad A. Tobagi", "Stanley E. Lieberson", "Leonard Kleinrock"], "id": "9edf551025fab81b7165783c8659bc4530d141fb", "title": "On measurement facilities in packet radio systems", "references": []}, {"date": "1976", "abstract": "Modern multiprogrammed computing systems are dis t inguished by t h e i r a b i l i t y t o handle a number of jobs concurrent ly . When one job , r e s iden t i n such a system, performs an I/O, it r e l i n quishes i t s hold on the CPU, and another job takes over the CPU while t h e f i r s t awai ts completion o f t he I/O. Operations of this type r a i s e quest ions as t o j u s t how many jobs should be r e s iden t i n the computer, s ince it i s not a p r i o r i c l e a r j u s t how much degradation the jobs s u f f e r when t h e r e a r e many of them contending and i n t e r f e r i n g with each o t h e r i n obta in ing access t o the var ious processing devices of t h e computer. It is d i f f i c u l t t o judge the e f f e c t s of replacing one device by another with d i f f e r e n t speed, o r of adding o r sub t r ac t ing a device.", "authors": ["A Williams", "R. A. Bhandiwad"], "id": "c1e4a1a1b4f8d64bc52e4a50494c419807af453d", "title": "A generating function approach to queueing network analysis of multiprogrammed computers", "references": []}, {"date": "1967", "abstract": "The results contained herein pertain to the problem of determining the equilibrium distribution of customers in closed queuing systems composed of M interconnected stages of service. The number of customers, N, in a closed queuing system is fixed since customers pass repeatedly through the M stages with neither entrances nor exits permitted. At the ith stage there are ri parallel exponential servers all of which have the same mean service rate \u00b5i. When service is completed at stage i, a customer proceeds directly to stage j with probability pij. Such closed systems are shown to be stochastically equivalent to open systems in which the number of customers cannot exceed N. The equilibrium equations for the joint probability distribution of customers are solved by a separation of variables technique. In the limit of N \u00e2\u0086\u0092 \u221e it is found that the distribution of customers in the system is regulated by the stage or stages with the slowest effective service rate. Asymptotic expressions are given for the marginal distributions of customers in such systems. Then, an asymptotic analysis is carried out for systems with a large number of stages i.e., M \u00e2\u0089\u00ab 1 all of which have comparable effective service rates. Approximate expressions are obtained for the marginal probability distributions. The details of the analysis are illustrated by an example.", "authors": ["William J. Gordon", "Gordon F. Newell"], "id": "2eea440182e0e9603948a82585c20ed2cd2138f8", "title": "Closed Queuing Systems with Exponential Servers", "references": []}, {"date": "1975", "abstract": "{PG,1 Detergent-compatible antistatic compositions contain a combination of cationic antistatic agents, organic adjuncts, and water-soluble, alkaline salts as discrete particles. Preferred compositions also contain certain types of smectite clay to enhance fabric-softening properties.", "authors": ["Leonard Kleinrock", "Fouad A. Tobagi"], "id": "da5b6c1d61ecd4d4f83ee7636f245028f7ea6faa", "title": "Packet switching in radio channels: part I", "references": []}, {"date": "1968", "abstract": "Probably the most basic reason behind the absence of a general treatment of resource allocation in modern computer systems is an adequate model for program behavior. In this paper a new model, the \u201cworking set model,\u201d is developed. The working set of pages associated with a process, defined to be the collection of its most recently used pages, provides knowledge vital to the dynamic management of paged memories. \u201cProcess\u201d and \u201cworking set\u201d are shown to be manifestations of the same ongoing computational activity; then \u201cprocessor demand\u201d and \u201cmemory demand\u201d are defined; and resource allocation is formulated as the problem of balancing demands against available equipment.", "authors": ["Peter J. Denning"], "id": "7fc3a6862f746c9988d82c023d2b7aaf0e089168", "title": "The working set model for program behavior", "references": []}, {"date": "1975", "abstract": "We derive the joint equilibrium distribution of queue sizes in a network of queues containing N service centers and R classes of customers. The equilibrium state probabilities have the general form: P(S) - Cd(S) $f_1$($x_1$)$f_2$($x_2$)...$f_N$($x_N$) where S is the state of the system, $x_i$ is the configuration of customers at the ith service center, d(S) is a function of the state of the model, $f_i$ is a function that depends on the type of the ith service center, and C is a normalizing constant. We consider four types of service centers to model central processors, data channels, terminals, and routing delays. The queueing disciplines associated with these service centers include first-come-first-served, processor sharing, no queueing, and last-come-first-served. Each customer belongs to a single class of customers while awaiting or receiving service at a service center but may change classes and service centers according to fixed probabilities at the completion of a service request. For open networks we consider state dependent arrival processes. Closed networks are those with no arrivals. A network may be closed with respect to some classes of customers and open with respect to other classes of customers. At three of the four types of service centers, the service times of customers are governed by probability distributions having rational Laplace transforms, different classes of customers having different distributions. At first-come-first-served type service centers the service time distribution must be identical and exponential for all classes of customers. Many of the network results of Jackson on arrival and service rate dependencies, of Posner and Bernholtz on different classes of customers, and of Chandy on different types of service centers are combined and extended in this paper. The results become special cases of the model presented here. An example shows how different classes of customers can affect models of computer systems. Finally, we show that an equivalent model encompassing all of the results involves only classes of customers with identical exponentially distributed service times. All of the other structure of the first model can be absorbed into the fixed probabilities governing the change of class and change of service center of each class of customers.", "authors": ["Forest Baskett", "K. Mani Chandy", "Richard R. Muntz", "Fernando G. Palacios"], "id": "c5435b6cf613c54c08c954df29525946e1e91e1b", "title": "Open, Closed, and Mixed Networks of Queues with Different Classes of Customers", "references": []}, {"date": "1981", "abstract": "Self-timed logic provides a method for managing the complexity of asynchronous module connections; the correctness of a properly constructed self-timed system is independent of the speed of its components. In this paper we present a means of formally specifying self-timed systems and modules using temporal logic, an extension of ordinary logic to include an abstract notion of time. We show by example that temporal logic can describe Seitz\u2019s self-timed modules, giving detailed specifications for combinatory logic, and sketching the treatment of wires, align elements, feedback registers, pipelines and finite state machines. Temporal logic has an expressive power that makes it well suited to this task; it also provides a framework for proofs of the properties of self-timed systems.", "authors": ["Yonatan Malachi", "Susan S. Owicki"], "id": "f8c02120710b877d802114e360d723b63a5ea7e0", "title": "Temporal Specifications of Self-Timed Systems", "references": []}, {"date": "1973", "abstract": "This thesis describes an architecture for a parallel microcomputer system that permits a systematic and flexible approach to the emulation of a wide variety of complex sequential and parallel intermediate machine languages in a dynamically varying Processor-Memory-Switch (PKS) environment. This architecture is based on the view that complex emulators can be best structured in terms of a set of microprocessors that interact in a highly structure manner. These highly structured interaction patterns are defined through the concept of a virtual PMS environment. This concept embodies the capability for reconfiguring both the internal and the external environment of a microcomputer system: the number of internal working registers of each microprocessor; the structure of memory, e.g., its size and word length; and the number of microprocessors and functional units, and their interconnection and interaction patterns. The virtual PMS is implemented in the microcomputer architecture by adding a new global level of hardware control. A particular virtual PMS is dynamically defined by modifying the syntax (i.e., the number of data elements and their relationship) of the data structure for control used by this global hardware control level. The representational capabilities of this architecture have been examined through the microprogramming of an emulator for a sophisticated parallel machine language, Adams\u2019 Graph Machine Language. The emulator of this machine language has demonstrated the versatility and usefulness of the concept of a virtual PMS by requiring less than 600 64-bit microinstructions to be programmed, while at the same time being able to exploit fully the implicit parallelism of a graph machine program. In addition, the dynamic execution characteristics of this architecture have been studied through the use of a detailed simulator of a hardware organization for this microcomputer architecture. The simulator has been used to verify quantitatively that this organization permits parallel activity on the virtual PMS to be mapped without significant overhead onto the physical PMS. In particular, the simulation results indicate that where sufficient parallel activity exists, the addition of microprocessors to the PMS configuration will reduce in a linear way the time it takes to execute the computation. The simulation results have also indicated that the logical hardware design, with the appropriate PMS configuration, can efficiently handle sustained parallel activity, involving highly structured interaction patterns, of greater than sixteen microprocessors. . . . 111", "authors": ["Victor R. Lesser"], "id": "be2ccd19a3906c54172027d17cfc2c017c978572", "title": "Dynamic control structures and their use in emulation", "references": ["b44dbeb6e8dba0558c4c02b2786897d771d33d8f", "7fc3a6862f746c9988d82c023d2b7aaf0e089168", "96e741a8139bb6f06fa9903f830d5d4f23029487", "9193f366e993ed27c2605eceaa18f13b1439035e", "21678be84430f56942cf5172c281b1861b9ac7a0", "5198b80c4037a9ff9f84e6081c5567888b1ac0fb", "456ac66abf2d97b6cf8ebacd85107f69e7a97c08", "5ee22f744331bf35bc45c3538000790e9483b967", "0930f4d5637eb6f25e922f08ae09e03847a9404d", "fceac668a164d18128e8f1e06965b3c7ad5203fd"]}, {"date": "2009", "abstract": "Techniques of hierarchical specification and verification of hardware with temporal logic and Prolog are presented by example. Both hardware designs in gates and state-diagrams are translated into a relation between the present and the next state, which is represented in Prolog.1) Specifications are constructed by temporal logic that can express state sequences (e.g. timing diagrams) easily and also are translated into a relation between the present and the next state in Prolog.The verification method is based upon the temporal logic decision procedure in Ref. 2) and, referring to the relation tables between the present state and the next state, the verifier can reason in both directions\u2014forward and backward in temporal sequences. Prolog has very powerful pattern matching, and its automatic backtracking capabilities facilitate easy-to-write verifier programs.It is concluded that a total verification system handling various design levels can be constructed with temporal logic and Prolog.", "authors": ["Masahiro Fujita", "Hidehiko Tanaka", "Tohru Moto-Oka"], "id": "139036327823c07958686bb46b214ede5fa3928b", "title": "Temporal logic based hardware description and its verification with Prolog", "references": ["ca954e6b96da9e529b900aed0fcaab63d8e7dfc1", "d0d5aa9160b40107cdd8fc806b4ecf2e2dba7841", "dfcfc463e2b01b45e5d863445ea56805cad4110b", "bf38f9392762f95eacd64bce2c1399bbdd740d14"]}, {"date": "1982", "abstract": "The use of temporal logic for the specification of hardware modules is explored. Temporal logic is an extension of conventional logic. While traditional logic is useful for specifying combinational circuits, it is shown how the extensions of temporal logic apply to the specification of memory, as well as the safeness and liveness properties of active circuits representing processes. These ideas are demonstrated by the example of a self-timed arbiter. An implementation of the arbiter is also given, and its formal verification by a kind of reachability analysis is discussed. This verification approach is also useful for finding design errors, as demonstrated by an example.", "authors": ["Gregor von Bochmann"], "id": "2d206d2ee6f24c42ecb7e07005004650577e7966", "title": "Hardware Specification with Temporal Logic: An Example", "references": ["04edf2feffa2d2053e2d0ccc997cd51ba3b2b729"]}, {"date": "1969", "abstract": "Semantic Scholar extracted view of \"The teachable language comprehender\" by M. Ross Quillian", "authors": ["M. Ross Quillian"], "id": "20eb54d6d0159bb257564e632727aed369c9fd1c", "title": "The teachable language comprehender", "references": []}, {"date": "1984", "abstract": "Over the last few years, temporal logic has been investigated as a tool for reasoning about computer programs, digital circuits and message-passing systems. In the case of programs, the general feeling has been that temporal logic is an adjunct to existing languages. For example, one might use temporal logic to specify and prove properties about a program written in, say, CSP. This leads to the annoyance of having to simultaneously use two separate notations.", "authors": ["Ben C. Moszkowski"], "id": "f6279cd6ab20542b793044a1b4ffc45b506dc0e6", "title": "Executing temporal logic programs", "references": ["22800ac3f38d9e69ffa610091513338078f8c303", "30fa10a38a1c96ff7d63ed2357c6b575bae5facb", "d34bc3f7fe1a52ee72a530a28ce0f66d1ff2f57e", "d2661f91a7899bafa2a0d16dbda14bca5e6841f4", "29607edfbf48bd0d820f9d45854e19320c7520d1", "4ae93f8cae228353e07bf549c45b47ed622397eb", "f88fdf6f28232d7ada8a4a9c4f7b04bf13928a14", "a8b5ee34a375ac30289ecc166acc66dd02e3564e", "b62ed3d7b2d0fb0c8539a031ad743a2d749e95a9", "7874493d2b3e4acdb0e9fa7de5c84d7973a65db2"]}, {"date": "1970", "abstract": "The ARPA Network will provide store-and-forward communication paths between a set of computer centers distributed across the continental United States. The message handling tasks at each node in the network are performed by a special purpose Interface Message Processor (IMP) located at each computer center. The centers will be interconnected through the IMPs by fully duplex telephone lines, of typically 50 kilobit/sec capacity.", "authors": ["Howard Frank", "Ivan T. Frisch", "Wushow Chou"], "id": "fd99b3ee449733002fc6dbe203c54d938bca9ed5", "title": "Topological considerations in the design of the ARPA computer network", "references": []}, {"date": "1971", "abstract": "Procedures for generating proofs within a logical inference system can be applied to many information-retrieval and automatic problem-solving tasks. These applications require additional procedures for extracting information from the proofs when they are found. We present an extraction procedure for proofs generated by the resolution principle. The procedure uses a given proof to find solutions for existential quantifiers in the statement proved in terms of known quantities in the initial data. This procedure relies heavily on basic subfunctions in the resolution program, so that it requires relatively little additional programming. The correctness of the procedure is proved, and examples are given to illustrate how it operates and to show that it cannot be simplified at certain points without loss of generality.", "authors": ["David C. Luckham", "Nils J. Nilsson"], "id": "da6eb135261dc3b3f7df8ff741796688898b5cd4", "title": "Extracting Information from Resolution Proof Trees", "references": ["f516d9dd73b117eca287bd3a07c2f3d26523b765", "c8ed24d86755095c263a2f031752f817e668da3e", "577e96521d62b9ebb5fd67412a21b02e9cd67b90", "dd15a1958d5a7bfa45548eb101bef4186acf2299", "ffe701d533cffe8b81d1d2921b07a72e93648fa3", "d2109eba4f160755f0b9a7497b6b691c2fa2d5d8"]}, {"date": "1970", "abstract": "The Resolution procedure of J. A. Robinson is shown to remain a complete proof procedure when the refutations permitted are restricted so that clauses C and D and resolvent R of clauses C and D meet the following conditions: (1) C is the resolvent immediately preceding R in the refutation if any resolvent precedes R, (2) either D is a member of the given set S of clauses or D precedes C in the refutation and R subsumes an instance of C or R is the empty clause, and (3) R is not a tautology.", "authors": ["Donald W. Loveland"], "id": "b35aa4d90f7368fefaf05ca94f76edf03134eee7", "title": "A linear format for resolution", "references": ["f516d9dd73b117eca287bd3a07c2f3d26523b765", "9e15384995e5e8b0bd8ceb197804b37285a77c70", "05b44597834f6df07c1c1290fb33a979bdf99067"]}, {"date": "1972", "abstract": "USEFUL FACTS ABOUT SETS. SENTENTIAL LOGIC. FIRST-ORDER LOGIC. UNDECIDABILITY. SECOND-ORDER LOGIC.", "authors": ["Herbert B. Enderton"], "id": "66a302e05a154fb8a937e353c5ee75407c0b93c2", "title": "A mathematical introduction to logic", "references": []}, {"date": "1980", "abstract": "Abstract \u2018Non-monotonic\u2019 logical systems are logics in which the introduction of new axioms can invalidate old theorems. Such logics are very important in modeling the beliefs of active processes which, acting in the presence of incomplete information, must make and subsequently revise assumptions in light of new observations. We present the motivation and history of such logics. We develop model and proof theories, a proof procedure, and applications for one non-monotonic logic. In particular, we prove the completeness of the non-monotonic predicate calculus and the decidability of the non-monotonic sentential calculus. We also discuss characteristic properties of this logic and its relationship to stronger logics, logics of incomplete information, and truth maintenance systems.", "authors": ["Drew McDermott", "Jon Doyle"], "id": "0eaf153cf0e613845e44d4543a7e1c12916f8afe", "title": "Non-Monotonic Logic I", "references": ["b48d67c1e4e43f6ec2fdd3dc91a20a4efdc97ddc", "f08f699374a27cdbc2c1ecf050ae285b01bda723", "182c99e5bc08303fb84035dc23ce54033f4701ae", "76c880bd5bfa3c627a9497cd6f1ee0afa093e131", "d2109eba4f160755f0b9a7497b6b691c2fa2d5d8", "2255db1a8ada12287fb175f52805f4c5bac26873", "67a9db95296d2b2008d3c350690b67edd48c23b3", "07b82b58e1fd76540cf2217ed4537136855685d5", "68ff263250279e00572fc43a050fa348838f34a1", "c547e1f79e6039d05c5ae433a36612d7f8e4d3f5"]}, {"date": "1971", "abstract": "Abstract : A computer for artificial intelligence research is examined. The design is based on a large, straightforward primary memory facility (about 8 million 74 bit words). Access to the memory is via at least 16 ports which are hardware protected; there is dynamic assignment of the memory to the ports. The maximum port bandwidth is 8,600 million bits/sec. Processors for languages (e. g., LISP) and specialized terminals (e.g., video input/output) can be reliably connected to the system during its operation. The approach is evolutionary in that high performance processors, such as the Standford AI Processor, can be connected to the memory structure, giving an overall power of at least 100 times a PDP-10 (and 200 to 300 times a PDP-10 for list processing languages) for 10 processors -- although 20 processors can be attached.", "authors": ["Gregory Bruce Bell", "Peter Freeman", "Mario Barbacci", "Subhash Bhatia", "W. Broadley"], "id": "fceac668a164d18128e8f1e06965b3c7ad5203fd", "title": "C.ai: A Computing Environment for AI Research. Overview, PMS and Operating System Considerations", "references": []}, {"date": "1965", "abstract": "Seven kittens were used, and the various procedures of deprivation and subsequent studies are summarized in Table 1. In six animals the Iids of one eye were closed for the first 3 months of life. In the recovery period two of these kittens had the deprived eye opened. The other four had the deprived eye opened and the other (previously open) eye was closed. The seventh animal had both eyes closed for 3 months; the right eye was then opened. Recovery periods", "authors": ["Torsten N. Wiesel", "David H. Hubel"], "id": "3041e2f5116cab83b024d5a66dc85779ab082995", "title": "Extent of recovery from the effects of visual deprivation in kittens.", "references": ["cc099151146866673e340e3a23b3c0b5011c8cac", "75d37ea521e3fcff3706458e4aa6fcbe3fac446c", "34f989681ef0ed85c9f809e96118e1a360702527", "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7", "77734bcbfe2e939b340d870bf676775a879f747a"]}, {"date": "1977", "abstract": "In the previous chapter a theory of human problem solving was put forward with references to some of the evidence for its validity. The theory has been formalized and tested by incorporating it in programs for digital computers and studying the behavior of these programs when they are confronted with problem-solving tasks.", "authors": ["Herbert A. Simon"], "id": "3eed9d1d505e5ad021f8e3d77689750cc3c15013", "title": "Scientific Discovery and the Psychology of Problem Solving", "references": []}, {"date": "1959", "abstract": "A beginning has recently been made in recording single neurone activity from animals with chronically implanted electrodes (Hubel, 1957a; Gusel'nikov, 1957; Ricci, Doane & Jasper, 1957; Strumwasser, 1958). These methods eliminate anaesthetics, paralysing drugs, brain-stem lesions, and other acute experimental procedures. They make it possible to record electrical events in the higher central nervous system with the animal in a normal state, and to correlate these electrical events with such variables as waking state, attention, learning, and motor activity. The present paper describes a method for unit recording from the cortex of unanaesthetized, unrestrained cats, and presents some observations from the striate cortex. The objectives have been (1) to observe maintained unit activity under various conditions such as sleep and wakefulness, and (2) to find for each unit the natural stimuli which most effectively influence firing. Of 400 units observed, some 200 are presented here because of their common characteristics. Since there is reason to believe that the remainiing 200 units were afferent fibres from the lateral geniculate nucleus, these will be described in a separate paper. A preliminary account of some of this work has been given elsewhere (Hubel, 1958).", "authors": ["David H. Hubel"], "id": "5a0c4a618425328ba0bbb9737b02e2ac18f94646", "title": "Single unit activity in striate cortex of unrestrained cats.", "references": ["25a59237ac445e22d13c4089ec389771c8c5a843", "b446e3fb01026b4e5ad978812540f172405fff6f", "ec71953a116a869a063cf516fe5e57700fff6567", "f7453856d2f9bb80a53d347c348e15ffa0deb0d6", "b6ae85b11f1c6f30761830434844a73f2350befb", "663eb5b9acf0e9cc404728d98669012fbfcaa9c2", "83cf69db1a2897e0bf0a250cfe64a0fcf1835eb7"]}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Packet radio system network considerations\" by Howard Frank et al.", "authors": ["Howard Frank", "Israel Gitman", "Richard Van Slyke"], "id": "bf4bfc4beabbb1a4daca16a0ab6fcfc9316a99de", "title": "Packet radio system network considerations", "references": []}, {"date": "1963", "abstract": "THEIMPORTANCEOFNORMALSENSORYSTIMULATION inthedevelopment and maintenance of the nervous system is now generally recognized. In the visual system this problem has usually been approached by examining the effects of sensory deprivation on structure and behavior (see reviews by Hebb (12) and Riesen (28)). An obvious way of extending this work would be to examine electrophysiologically the functional effects of visual deprivation, but such experiments require some knowledge of normal function. During the last 10 years single-cell responses have been examined and receptive-field arrangements compared at several levels in the cat\u2019s visual pathway: in the retina (Zl), the lateral geniculate body (18), and the visual cortex (17, 19). This information provides the necessary background for a study of the immature and the stimulus-deprived visual system. The results of a physiological and anatomical study of the visual pathways in normal. and visually deprived kittens will be presented in a series of three papers. In the present paper we describe single-unit recordings in the optic tract and lateral geniculate body of kittens in which one eye had been deprived of vision, and an anatomical examination of the visual pathways in these animals. The second paper (20) will describe single-unit recordings in the striate cortex of newborn kittens. The final paper (32) will deal with responses of cells in the visual cortex of visually deprived animals.", "authors": ["Torsten N. Wiesel", "David H. Hubel"], "id": "75d37ea521e3fcff3706458e4aa6fcbe3fac446c", "title": "EFFECTS OF VISUAL DEPRIVATION ON MORPHOLOGY AND PHYSIOLOGY OF CELLS IN THE CATS LATERAL GENICULATE BODY.", "references": ["b446e3fb01026b4e5ad978812540f172405fff6f", "dd42483f405bfacb4aeee15c0befdad169a81c9b", "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7", "724945b672fb10d64654091f725cc66fe5a6298f", "401185a7ba67af1e01e5278932747e5fd777d3a6", "663eb5b9acf0e9cc404728d98669012fbfcaa9c2", "6f20e254e3993538c79e0ff2b9b8f198d3359cb3", "5a0c4a618425328ba0bbb9737b02e2ac18f94646", "34f989681ef0ed85c9f809e96118e1a360702527", "654315deba1019c7c3b7adde07bb49368cb76509"]}, {"date": "1971", "abstract": "The Stanford hand-eye system is implemented as several separate tasks, each executing under a timesharing executive. Development of a programming language (SAIL) and augmentation of the timesharing system were required to provide the necessary data sharing and control flow among the tasks. The SAIL language provides facilities for \"associative processing,\" and is extended to serve the data sharing and communication needs of the hand-eye system. Several user facilities are designed to aid running and debugging the system.", "authors": ["Jerome A. Feldman", "Robert F. Sproull"], "id": "70e69265cb050a5050e75ccb308e00b6d9571ab0", "title": "System Support for the Stanford Hand-Eye System", "references": ["c2e41b4445a5988f59136c62fc061958c088e731", "cf4570f4801181a2f8e1b9aac77c180d0206834e", "89729006f2b9358b46b8748b1958c7e8f0d6ffd8"]}, {"date": "1979", "abstract": "The growing complexity of machine designs and costs of engineering changes are increasing the demand for tools and methods to detect errors earlier in the hardware development cycle. Because of similar concerns in the development of software there has been a great deal of work on methods for proving that a program satisfies a given specification. This paper examines one such program verification technique, based on the notion of symbolic execution , and then explores its application to the problem of establishing the correct behavior of a piece of hardware.", "authors": ["J.A. Darringer"], "id": "bf38f9392762f95eacd64bce2c1399bbdd740d14", "title": "The Application of Program Verification to Hardware Verification", "references": []}, {"date": "1973", "abstract": "This paper describes the structure and operation of the Hearsay-I1speech understanding system by the use of a specific example illustrating the various stages of recognition. The system consists of a set of cooperating independent processes, each representing a source of knowledge. The knowledge is used either to predict what may appear in a given context or to verify hypotheses resulting from a prediction. The structure of the system is illustrated by considering its operation in a particular task situation: Voice-Chess. The representation and use of various sources of knowledge are outlined. Preliminary results of the reduction in search resulting from the use of various sources of knowledge are given.", "authors": ["Raj Reddy", "Lee D. Erman", "Richard D. Fennell", "Richard B. Neely"], "id": "04ffb20cbfa502d3d2611dcfe027cfa94b45a629", "title": "The Hearsay-I Speech Understanding System: An Example of the Recognition Process", "references": ["f027ce53a12f36f93897a2b5733549ca323c18d0", "62184a540db0f8c4712c7fe6c42272be3a7ed96e", "96a9cfa8a396fd8967b327dc675a8ce88f16a18f", "190f432914ca9f9925860139c88d9664787a5939", "6a69a48ababc4d2e1b6132d5a2fc1de365942c9e", "1ff661af7f909f8a8644a0b5d445216c357f8f76"]}, {"date": "1983", "abstract": "Abstract : In recent years, more and more computer scientists have been paying attention to temporal logic, since there are many properties of programs that can be described only be bringing the time parameter into consideration. But existing temporal logic languages, such as Lucid, in spite of their mathematical elegance, are still far from practical. I believe that a practical temporal-logic language, once it came into being, would have a wide spectrum of applications. XYZ/E is a temporal-logic language. Like other logic languages, it is a logic system as well as a programming language. But unlike them, it can express all conventional data structures and control structures, nondeterminate or concurrent programs, even programs with branching-time order. We find that the difficulties met in other logic languages often stem from the fact that they try to deal with these structures in a higher level. XYZ/E adopts another approach. We divide the language into two forms: the internal form and the external form. The former is lower level, while the latter is higher. Just as any logic system contains rules of abbreviation, so also in XYZ/E there are rules of abbreviation to transform the internal form into the external form, and vice versa. These two forms can be considered to be different representations of the same thing. We find that this approach can ameliorate many problems of formalization. Not only does XYZ/E have various practical applications, it also has a theoretical impact.", "authors": ["Chih-Sung Tang"], "id": "7874493d2b3e4acdb0e9fa7de5c84d7973a65db2", "title": "Toward a Unified Logic Basis for Programming Languages", "references": ["beb91dfc9e101903fc2b8fc171c52d5f1c74a583", "d2661f91a7899bafa2a0d16dbda14bca5e6841f4", "52e041d6735eb1dab39e80c142e4ad6b1b7d447f", "4a14f48ee3358a24914d78eb89c6a15bae271fe4", "b71885bb82053e53bff32e15aee6ab84489d5bf2", "f91890e983cae00dcc286e3961bf5a413884618f"]}, {"date": "1976", "abstract": "Two formal models for parallel computation are presented: an abstract conceptual model and a parallel-program model. The former model does not distinguish between control and data states. The latter model includes the capability for the representation of an infinite set of control states by allowing there to be arbitrarily many instruction pointers (or processes) executing the program. An induction principle is presented which treats the control and data state sets on the same ground. Through the use of \u201cplace variables,\u201d it is observed that certain correctness conditions can be expressed without enumeration of the set of all possible control states. Examples are presented in which the induction principle is used to demonstrate proofs of mutual exclusion. It is shown that assertions-oriented proof methods are special cases of the induction principle. A special case of the assertions method, which is called parallel place assertions, is shown to be incomplete. A formalization of \u201cdeadlock\u201d is then presented. The concept of a \u201cnorm\u201d is introduced, which yields an extension, to the deadlock problem, of Floyd's technique for proving termination. Also discussed is an extension of the program model which allows each process to have its own local variables and permits shared global variables. Correctness of certain forms of implementation is also discussed. An Appendix is included which relates this work to previous work on the satisfiability of certain logical formulas.", "authors": ["Robert M. Keller"], "id": "04edf2feffa2d2053e2d0ccc997cd51ba3b2b729", "title": "Formal verification of parallel programs", "references": ["efa53518898247c3d2aad15944036c062c2bc617", "fa1ba526c7f1fc8437c3b55c6798526c387073b9"]}, {"date": "1968", "abstract": "A refinement of the resolution method for mechanical theorem proving is presented. A resolvent <italic>C</italic> of clauses <italic>A</italic> and <italic>B</italic> is called a <italic>merge</italic> if literals from <italic>A</italic> and <italic>B</italic> merge together to form some literal of <italic>C</italic>. It is shown that the resolution method remains complete if it is required that two noninitial clauses which are not merges never be resolved with one another. It is also shown that this strategy can be combined with the set-of-support strategy.", "authors": ["Peter B. Andrews"], "id": "f516d9dd73b117eca287bd3a07c2f3d26523b765", "title": "Resolution With Merging", "references": []}, {"date": "1983", "abstract": "It is first proved that there are properties of sequences that are not expressible in temporal logic, even though they are easily expressible using, for instance, regular expressions. Then, it is shown how temporal logic can be extended to express any property definable by a right-linear grammar and hence a regular expression. Finally, a complete axiomatization and a decision procedure for the extended temporal logic are given and the complexity of the extended logic is examined.", "authors": ["Pierre Wolper"], "id": "dfcfc463e2b01b45e5d863445ea56805cad4110b", "title": "Temporal Logic Can Be More Expressive", "references": []}, {"date": "1983", "abstract": "We present an interval-based temporal logic that permits the rigorous specification of a variety of hardware components and facilitates describing properties such as correctness of implementation. Conceptual levels of circuit operation ranging from detailed quantitative timing and signal propagation up to functional behavior are integrated in a unified way. After giving some motivation for reasoning about hardware, we present the propositional and first-order syntax and semantics of the temporal logic. In addition we illustrate techniques for describing signal transitions as well as for formally specifying and comparing a number of delay models. Throughout the discussion, the formalism provides a means for examining such concepts as device equivalence and internal states.", "authors": ["Joseph Y. Halpern", "Zohar Manna", "Ben C. Moszkowski"], "id": "b62ed3d7b2d0fb0c8539a031ad743a2d749e95a9", "title": "A Hardware Semantics Based on Temporal Intervals", "references": ["2d206d2ee6f24c42ecb7e07005004650577e7966", "22800ac3f38d9e69ffa610091513338078f8c303", "c11023e86affcd7c7f7b358e026b4ad870ece157", "ccf50ab3faa69ec30d96b49158c1a6a4f2e8e2aa", "c954c8504cac9d9753255a7b4b986fdc0fddb7fd", "f8c02120710b877d802114e360d723b63a5ea7e0", "88a434c786e55bb3a72b9f09db33f465b4380373", "1f95ed618f677f4bf6062f896a2ba69d794c540f", "bf104ebfbd44924b6b7602e48b0a74e987baaca8", "116621c83f1bb7b331f769564e336b6b339ef2e5"]}, {"date": "1983", "abstract": "Predicate logic is a powerful and general descriptive formalism with a long history of development. However, since the logic''s underlying semantics have no notion of time, statements such as \"I increases by 2\" cannot be directly expressed. We discuss interval temporal logic (ITL), a formalism that augments standard predicate logic with operators for time-dependent concepts. Our earlier work used ITL to specify and reason about hardware. In this paper we show how ITL can also directly capture various control structures found in conventional programming languages. Constructs are given for treating assignment, iteration, sequential and parallel computations and scoping. The techniques used permit specification and reasoning about such algorithms as concurrent Quicksort. We compare ITL with the logic-based programming languages Lucid and Prolog.", "authors": ["Ben C. Moszkowski", "Zohar Manna"], "id": "f88fdf6f28232d7ada8a4a9c4f7b04bf13928a14", "title": "Reasoning in Interval Temporal Logic", "references": ["6eb3e06050e8f3287d3fd9def9e49e10eda80817", "64dfb6baddaed9705914a79a96c101562776b7ef", "66e10a19439d7ab65ae7d5f25c029489daf34759", "b62ed3d7b2d0fb0c8539a031ad743a2d749e95a9", "359eca57fe42d97cbb67f0b5591869abe5eb5421", "88a434c786e55bb3a72b9f09db33f465b4380373", "bb47920d8ad380b3718665abf3eeb99f9920e0b7", "3430d7929dc9c0c9278dca858e785ee3d89ce2b0", "4074d05de637ef892cddd08f547108c16ce5b6c9"]}, {"date": "1971", "abstract": "We describe a new problem solver called STRIPS that attempts to find a sequence of operators in a space of world models to transform a given initial world model into a model in which a given goal formula can be proven to be true. STRIPS represents a world model as an arbi trary collection of first-order predicate calculus formulas and is designed to work with models consisting of large numbers of formulas. It employs a resolution theorem prover to answer (juestions of particular models and uses means-ends analysis to guide it to the desired goal-satisfying model.", "authors": ["Richard Fikes", "Nils J. Nilsson"], "id": "c547e1f79e6039d05c5ae433a36612d7f8e4d3f5", "title": "STRIPS: A New Approach to the Application of Theorem Proving to Problem Solving", "references": ["1beae9ef432a57bb5ec0c43944a07182814ab443", "eb3a66f92106e467d1aabeb24c42b0e421b3c02f", "577e96521d62b9ebb5fd67412a21b02e9cd67b90", "b49bb7ecd2afd6461c78ff29536839b5ee45cd15", "4efdcacca99408c434f34c44ecd29198757a0ded", "62a7dc7a8774054ea11d32d3422b8234dcdca2d8", "6758ec88d7d0b2056a45b07d752a861badedb93a", "c25dc4608a45d29bc9498a019ca597093af213f0"]}, {"date": "1978", "abstract": "This paper is an introduction to the mechanization of a theory of reasoning. Currently formal systems are out of favor with the AI community. The aim of this paper is to explain how formal systems can be used in AI by explaining how traditional ideas of logic can be mechanized in a practical way. The paper presents several new ideas. Each of these is illustrated by giving simple examples of how this idea is mechanized in the reasoning system FOL. That is, this is not just theory but there is an existing running implementation of these ideas. In this paper: 1) we show how to mechanize the notion of model using the idea of a simulation structure and explain why this is particularly important to AI, 2) we show how to mechanize the notion of satisfaction, 3) we present a very general evaluator for first order expressions, which subsumes PROLOG and we propose as a natural way of thinking about logic programming, 4) we show how to formalize metatheory, 5) we describe reflection principles, which connect theories to their metatheories in a way new to AI, 6) we show how these ideas can be used to dynamically extend the strength of FOL by \"implementing\" subsidiary deduction rules, and how this in turn can be extended to provide a method of describing and proving theorems about heuristics for using these rules, 7) we discuss one notion of what it could mean for a computer to learn and give an example, 8) we describe a new kind of formal system that has the property that it can reason about its own properties, 9) we give examples of all of the above.", "authors": ["Richard W. Weyhrauch"], "id": "07b82b58e1fd76540cf2217ed4537136855685d5", "title": "Prolegomena to a theory of formal reasoning", "references": ["c5340982746f1aac55c1cc7d2c06b670f522f253", "236c58d8ba3e9e9d5ed8562084690090752bf5df", "f54a584f23c0940a6314c3782dd59b9893b8ae0a", "3c2eecf7fdcac147c06da1d53efe36a926530eee", "d8ce4b5489ef14e8878c869101e30432d057599c", "22fd3066b7e8051907309f6358a64638446c324f", "3750098165eec68d9ea680b52ef1338f1eaa0b7c"]}, {"date": "1976", "abstract": "We shall prove in this chapter that in a strong theory \u0413, some statements which naturally assert the consistency of \u0413 cannot be proved within \u0413. This famous result of Godel shows that our ordinary first-order languages have a severe limitation as far as any project for a thorough-going check on the consistency of mathematics is concerned. Historically, the theorem caused a major change of emphasis in foundational research away from a preoccupation with consistency proofs.", "authors": ["J. Donald Monk"], "id": "182c99e5bc08303fb84035dc23ce54033f4701ae", "title": "Unprovability of Consistency", "references": []}, {"date": "1959", "abstract": "Semantic Scholar extracted view of \"Analysis of receptive fields in the cat's retina.\" by Torsten N. Wiesel et al.", "authors": ["Torsten N. Wiesel", "Kenneth T. Brown"], "id": "b6ae85b11f1c6f30761830434844a73f2350befb", "title": "Analysis of receptive fields in the cat's retina.", "references": []}, {"date": "1977", "abstract": "Some extensions of a system for inferencing on partial information, which uses production rules, are described. Basically, the system consists of RULES, an active set of rules (a subset of potentially large set of rules), partially ordered by specificity, and FACTS, a small active set of facts (a subset of potentially large set of data base facts). The critical feature of the inference method is that only a partial match of the antecedent of a rule is needed. Some selected extensions of the system are presented. These concern some approaches to the problems of selecting from an ambiguous response and, more importantly, transforming or dynamic clustering of FACTS and RULES. This latter problem is important because partial match is defined over the sets RULES and FACTS, and unless these sets are reasonably small, partial match can be an unmanageable operation. Several issues concerning the use of this inference system in certain applications are also briefly discussed.", "authors": ["Aravind K. Joshi"], "id": "67a9db95296d2b2008d3c350690b67edd48c23b3", "title": "Some extensions of a system for inferencing on partial information", "references": []}, {"date": "2004", "abstract": "Zusammenfassung1.Die Reaktionen von 330 Neuronen des prim\u00e4ren optischen Cortex nach ipsi- oder kontralateraler Opticusreizung werden beschrieben (25 Katzen, enc\u00e9phale isol\u00e9). 211 dieser Neurone wurden gleichzeitig auf ihre Reaktionen bei Lichtreizen untersucht. Es wurde jeweils dasjenige Auge belichtet, dessen N. opticus nicht elektrisch gereizt wurde.2.Die Reaktionen der Neurone nach elektrischen Einzelreizen des N. opticus lassen sich in 4 Reaktionstypen gliedern: Typ 1a und b ohne reizgekoppelte Entladungen, Typ 2a und b Reaktionen mit kurzer Latenzzeit (1,7\u201312 msec), Typ 3 Reaktionen mit langer Latenzzeit (25\u2013120 msec), Typ 4 mit prim\u00e4rer Hemmung (Entladungspause nach dem Reiz).3.39% der registrierten Neurone zeigten nach einem Einzelreiz keine direkt reizgekoppelten Reaktionen (Typ 1). Zwei Untergruppen lie\u00dfen sich unterscheiden: Neurone des Typ 1a (28%) waren bei statistischer Auswertung auch nach zahlreichen Einzelreizen nicht beeinflu\u00dft, w\u00e4hrend Neurone des Typ 1b (11%) nach mehreren Reizen ihre Spontanaktivit\u00e4t diffus erh\u00f6hten.4.Neurone des Typ 2 (33%) reagierten nach 1,7\u201312 msec Latenzzeit mit einer kurzen Prim\u00e4raktivierung von 1\u20133 Entladungen. Darauf folgte eine Entladungspause von 50\u2013200 msec, an welche sich meist eine deutlich ausgepr\u00e4gte sekund\u00e4re Aktivierungsphase anschlo\u00df. Typ 2a zeigte kurze, konstante Latenzzeiten der ersten und zweiten Prim\u00e4rentladung zwischen 1,8 und 6 msec (90% von Typ 2). 10% der Neurone des Typ 2 hatten inkonstante Latenzzeiten zwischen 4 und 12 msec (Typ 2b).Bestand die Prim\u00e4raktivierung aus 2 Entladungen, so hatten diese meist verschiedene Reizschwellen. Einige Neurone mit einer Entladung in der Prim\u00e4raktivierung zeigten ein charakteristisches \u201eHin- und Herspringen\u201c der Latenzzeit, so da\u00df sich Maxima bei 2 Werten ergaben.5.Neurone des Typ 3 (19%) reagierten 25\u2013120 msec nach dem Reiz mit einer Aktivierungsphase wechselnder Dauer. Auffallend war die erhebliche Variationsbreite der Latenzzeiten beim gleichen Neuron unter konstanten Reizbedingungen.6.Neurone des Typ 4 (9%) zeigten von etwa 10 msec nach Reizbeginn bis 120\u2013300 msec nach dem Reiz eine Entladungspause (prim\u00e4re Hemmphase). An diese Entladungspause schlo\u00df sich meist eine Nachaktivierung an, in der die Entladungsfrequenz im Vergleich zu der Spontanaktivit\u00e4t erh\u00f6ht war.7.In 4 Versuchen wurde der ipsi- und kontralaterale N. opticus gereizt. Es wurden sowohl Neurone gefunden, welche von beiden N. optici nach dem gleichen Reaktionstyp aktiviert waren, als auch Neurone, welche nach verschiedenen Typen reagierten.8.Durch gleichzeitige Reizung in unspezifischen Thalamuskernen lie\u00df sich die Reaktion auf Opticusreize an den einzelnen Neuronen des optischen Cortex modifizieren. Dieser Befund entspricht der fr\u00fcher dargestellten Beeinflussung der Lichtreaktionen durch Thalamusreize (Creutzfeldt, Akimoto, Gr\u00fcsser).9.Etwa ein Drittel der Neurone blieb bei jeweils einer Reizart unbeeinflu\u03b2t. 34% konnten durch Lichtreize, 38% durch Opticus-Einzelreize nicht reizgekoppelt beeinflu\u00dft werden. 16% waren weder durch Lichtreize eines Auges noch durch Opticusreize des anderen Auges in ihrer Entladungsfolge zu ver\u00e4ndern.10.An 211 Neuronen wurde der Zusammenhang zwischen Opticusreiztypen und Lichtreaktionstypen untersucht und statistisch ausgewertet. Dabei ergaben sich folgende Kombinationen: 18% der Neurone waren durch Licht von einem Auge weder zu aktivieren noch zu hemmen (A-Typ), reagierten jedoch auf Opticusreizung des anderen Auges (Typ 2, 3, 4). 34% waren von einem Auge durch Lichtreize zu beeinflussen (Typ B, C, D, E,) jedoch nicht durch Opticusreizung vom anderen Auge (Typ 1). 32% waren sowohl von einem Auge durch Lichtreize (Typ B-, C-, D-, E-) als auch vom anderen Auge durch Opticusreize in ihrer Entladungsfrequenz zu modifizieren (Typ 2, 3, 4). 16% blieben durch beide Reize unbeeinflu\u00dfbar (Typ A und 1).11.Ipsi- und kontralaterale Opticusreize ergaben keine signifikant unterschiedenen H\u00e4ufigkeiten der Reaktionstypen im optischen Cortex.12.Beziehungen zwischen den Makrowellen des Cortex und den Neuronentladungen werden aufgezeigt. Die Latenzzeiten der Prim\u00e4raktivierung des Typ 2 gruppieren sich um die positiven Wellen 1\u20134. W\u00e4hrend der oberfl\u00e4chennegativen Welle 5, die von anderen Autoren als \u201eDendritenpotential\u201c interpretiert wurde, konnten keine reizbedingten Neuronentladungen registriert werden.Das Cortexpotential nach Opticusreizung kann als r\u00e4umlich-vektorielle Integralfunktion der elektrischen Felder um den Zellk\u00f6rper und die Dendriten erkl\u00e4rt werden.13.Es wird angenommen, da\u00df die Neurone des Typ 2 durch direkte spezifische Geniculatumafferenzen, die Neurone des Typ 3 \u00fcber unspezifische Thalamusafferenzen aktiviert werden. Die binoculare Erregung corticaler Neurone wird in der folgenden Arbeit dargestellt werden.", "authors": ["Anton Gr\u00fctzner", "Otto-Joachim Gr\u00fcsser", "Guenter Baumgartner"], "id": "f7453856d2f9bb80a53d347c348e15ffa0deb0d6", "title": "Reaktionen einzelner Neurone im optischen Cortex der Katze nach elektrischer Reizung des Nervus opticus", "references": []}, {"date": "1958", "abstract": "Semantic Scholar extracted view of \"Long-term recording' from single neurons in brain of unrestrained mammals.\" by Felix Strumwasser", "authors": ["Felix Strumwasser"], "id": "25a59237ac445e22d13c4089ec389771c8c5a843", "title": "Long-term recording' from single neurons in brain of unrestrained mammals.", "references": []}, {"date": "1963", "abstract": "Semantic Scholar extracted view of \"SINGLE-CELL RESPONSES IN STRIATE CORTEX OF KITTENS DEPRIVED OF VISION IN ONE EYE.\" by Torsten N. Wiesel et al.", "authors": ["Torsten N. Wiesel", "David H. Hubel"], "id": "cc099151146866673e340e3a23b3c0b5011c8cac", "title": "SINGLE-CELL RESPONSES IN STRIATE CORTEX OF KITTENS DEPRIVED OF VISION IN ONE EYE.", "references": ["6b4fe4aa4d66fecc7b2869569002714d91d0b3f7", "d7fb932bca642615fcbcf3f5d26b2c26666603d3", "fbb9508b12a752254296dd7fe8cec9cc0035edaa", "75d37ea521e3fcff3706458e4aa6fcbe3fac446c", "3923dd40a5dedf692ea59be150f21547474c5f33", "6f20e254e3993538c79e0ff2b9b8f198d3359cb3", "5a0c4a618425328ba0bbb9737b02e2ac18f94646", "34f989681ef0ed85c9f809e96118e1a360702527", "17b0743f7d37bfedc9494f630b509e0e6e87b786"]}, {"date": "1985", "abstract": "Abstract Establishing the correctness of complicated asynchronous circuit is in general quite difficult because of the high degree of nondeterminism that is inherent in such devices. Nevertheless, it is also very important in view of the cost involved in design and testing of circuits. We show how to give specifications for circuits in a branching time temporal logic and how to mechanically verify them using a simple and efficient model checker. We also show how to tackle a large and complex circuit by verifying it hierarchically.", "authors": ["Bhubaneswaru Mishra", "Edmund M. Clarke"], "id": "a8b5ee34a375ac30289ecc166acc66dd02e3564e", "title": "Hierarchical Verification of Asynchronous Circuits Using Temporal Logic", "references": []}, {"date": "1962", "abstract": "A speech recognition model is proposed in which the transformation from an input speech signal into a sequence of phonemes is carried out largely through an active or feedback process. In this process, patterns are generated internally in the analyzer according to an adaptable sequence of instructions until a best match with the input signal is obtained. Details of the process are given, and the areas where further research is needed are indicated.", "authors": ["Morris Halle", "Kenneth N. Stevens"], "id": "1ff661af7f909f8a8644a0b5d445216c357f8f76", "title": "Speech recognition: A model and a program for research", "references": []}, {"date": "1965", "abstract": "Publisher Summary The chapter discusses a semantical analysis of intuitionistic logic I. The chapter presents a semantical model theory for Heyting's intuitionist predicate logic and proves the completeness of that system relative to the modeling. The semantics for modal logic that is announced and developed together with the known mappings of intuitionistic logic into the modal system, S4, inspired the present semantics for intuitionist logic. It is important to develop the semantics of intuitionistic logic independently of that of S4; this procedure helps to obtain somewhat more information about intuitionistic logic, including the mapping into S4 as a consequence thereof. In addition to giving a simple decision procedure for Heyting's propositional calculus, the chapter presents the undecidability of monadic intuitionistic quantification theory. The proof is based on the semantics previously developed. Beth semantic tableaux for intuitionistic logic is developed in the chapter. The chapter describes consistency property: in a standard formalization of Heyting's predicate calculus, the axioms are all valid, and the rules preserve validity.", "authors": ["Saul Kripke"], "id": "b48d67c1e4e43f6ec2fdd3dc91a20a4efdc97ddc", "title": "Semantical Analysis of Intuitionistic Logic I", "references": []}, {"date": "1980", "abstract": "An implementation of translators between Ada and Pascal is described. The method used is to define subsets of each language between which there is a straightforward translation and to translate each source program to its respective sublanguage by program transformations. A common internal tree representation is used. The underlying organization of the translators is described, and some of the difficulties we have confronted and solves are discussed.", "authors": ["Paul F. Albrecht", "Philip E. Garrison", "Susan L. Graham", "Robert H. Hyerle", "Patricia Ip", "Bernd Krieg-Br\u00fcckner"], "id": "f91890e983cae00dcc286e3961bf5a413884618f", "title": "Source-to-source translation: Ada to Pascal and Pascal to Ada", "references": []}, {"date": "1975", "abstract": "A simple but general parallel programming language is considered. The semantics of programs is defined in a concise and natural way using relations. ''Verification conditions'' derived from the semantic definitions enable Floyd's method of proving correctness to be applied to the parallel programs. Proofs of properties of programs using the verification conditions are claimed to be more systematic versions of the informal arguments normally used to check parallel programs. A program simulating an elementary airline reservation system is given, and several properties of the program are demonstrated using the technique.", "authors": ["Edward A. Ashcroft"], "id": "efa53518898247c3d2aad15944036c062c2bc617", "title": "Proving Assertions about Parallel Programs", "references": []}, {"date": "1981", "abstract": "SummaryA temporal logic is defined which contains both linear and branching operators. The underlying model is the tree of all possible computations. The following metatheoretical results are proven: 1) an exponential decision procedure for satisfiability; 2) a finite model property; 3) the completeness of an axiomatization.", "authors": ["Mordechai Ben-Ari", "Amir Pnueli", "Zohar Manna"], "id": "b71885bb82053e53bff32e15aee6ab84489d5bf2", "title": "The temporal logic of branching time", "references": []}, {"date": "1962", "abstract": "Semantic Scholar extracted view of \"Towards a mathematical theory of computation\" by John McCarthy", "authors": ["John McCarthy"], "id": "4a14f48ee3358a24914d78eb89c6a15bae271fe4", "title": "Towards a mathematical theory of computation", "references": []}, {"date": "1970", "abstract": "Abstract : The paper describes a class of parallel programs and gives a formalization of certain properties of such programs in predicate calculus. Although the programs are syntactically simple, they do exhibit interaction between asynchronous parallel processes, which is the essential feature to be considered. The formalization can easily be extended to more complicated programs. Also presented is a method of simplifying parallel programs, i.e., constructing simpler equivalent programs, based on the 'independence' of statements in them. With these simplifications the formalization gives a practical method for proving properties of such programs. (Author)", "authors": ["Edward A. Ashcroft", "Zohar Manna"], "id": "fa1ba526c7f1fc8437c3b55c6798526c387073b9", "title": "Formalization of properties of parallel programs", "references": []}, {"date": "1981", "abstract": "Semantic Scholar extracted view of \"Verification of concurrent programs: the temporal framework\" by Zohar Manna et al.", "authors": ["Zohar Manna", "Amir Pnueli"], "id": "4074d05de637ef892cddd08f547108c16ce5b6c9", "title": "Verification of concurrent programs: the temporal framework", "references": []}, {"date": "1979", "abstract": "Abstract : An intelligent computer program must have both a representation of its knowledge, and a mechanism for manipulating that knowledge in a reasoning process. This thesis examines the problem of formalizing the expression and solution of reasoning problems in a machine manipulable form. It is particularly concerned with analyzing the interaction of the standard form of deductive steps with an observational analogy obtained by performing computation in a semantic model. This dissertation is centered on the world of retrograde analysis chess, a particularly rich domain for both observational tasks and long deductive sequences. A formalization is embodied in its axioms, and a major portion is devoted to both axiomatizing the rules of chess, and discussing and comparing the representational decisions involved in that axiomatization. Consideration was given not only to the necessity for these particular choices (and possible alternatives) but also the implications of these results for designers of representational systems for other domains. Using a reasoning system for first order logic, 'FOL', a detailed proof of the solution of a difficult retrograde chess puzzle was constructed. The close correspondence between this 'formal' solution to the problem, and an 'informal, descriptive' analysis a human might present was shown. The proof and axioms were then examined for their relevance to general epistemological formalisms.", "authors": ["Robert E. Filman"], "id": "3750098165eec68d9ea680b52ef1338f1eaa0b7c", "title": "The Interaction Of Observation And Inference", "references": []}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"Five Notes on the Application of Proof Theory to Computer Science.\" by Georg Kreisel", "authors": ["Georg Kreisel"], "id": "22fd3066b7e8051907309f6358a64638446c324f", "title": "Five Notes on the Application of Proof Theory to Computer Science.", "references": ["102765f4d74cc8e2de52c9fa26854887afb635f8"]}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"GPS: A Case Study in Generality and Problem Solving.\" by George W. Ernst et al.", "authors": ["George W. Ernst", "Allen Newell"], "id": "c25dc4608a45d29bc9498a019ca597093af213f0", "title": "GPS: A Case Study in Generality and Problem Solving.", "references": []}, {"date": "1974", "abstract": "A stroller having side frame sections which are adapted to fold longitudinally and also laterally toward and away from one another, with a flexible seat and a flexible seat back extending therebetween, and with preferably transparent side panels extending vertically from the seat back to the side sections of the frame, to retain a child between the side panels both in a downwardly retracted generally horizontal position of the seat back and in upper variously inclined positions thereof.", "authors": ["Robert A. Kowalski"], "id": "c5340982746f1aac55c1cc7d2c06b670f522f253", "title": "Predicate Logic as Programming Language", "references": []}, {"date": "1976", "abstract": "The meaning of many kinds of expressions in programming languages can be taken as elements of certain spaces of \u201cpartial\u201d objects. In this report these spaces are modeled in one universal domain ${\\bf P} \\omega $, the set of all subsets of the integers. This domain renders the connection of this semantic theory with the ordinary theory of number theoretic (especially general recursive) functions clear and straightforward.", "authors": ["Dana S. Scott"], "id": "359eca57fe42d97cbb67f0b5591869abe5eb5421", "title": "Data Types as Lattices", "references": ["54b052229eb9ae76546c7674e1137c9ce140ebc8", "67ca3ddd2c5067ce0f89bac7d88d50c360c3d59f", "427dd6f76ec119aa185f1e2ac82f040082e7d2ff", "b762d1701e32feeff7a60a66ca2d019bd1415c65", "2769c203102a875c10bc11affc161891472176d1", "9697b3978c639f9043acb29e4fd5b53aa9131fdd", "a5e82e5dba72ca4347597cb99ce6b9a1f9b9a812", "c679deecae18fb0e9a7c6f38ca09a534979cd421", "bd598056b1b945b1a0076f520de1aa1b3c655b09", "9f1fbbf387f2cfd6b01ce85f80e7a3a5fe5214a4"]}, {"date": "1969", "abstract": "This paper shows how a question-answering system can use first-order logic as its language and an automatic theorem prover, based upon the . Annual Review in Automatic Programming: International Tracts in. Google Books Result theorem-proving has concentrated on developing new inference systems. C. 1969b The application of theorem-proving to question\u2014answering systems. Symbolic Logic and Mechanical Theorem Proving Google Books Result deduction system, with an axiomatic application-domain theory, as the central. attachment mechanism, which allows the theorem prover to behave as if Artificial Intelligence Google Books Result ?QUESTION ANSWERING A question-answering system accepts information about some subject. The Application of Theorem Proving to Question Answering. niques, the application of theorem-proving techniques to new problem domains. the use of limited natural language input to a question-answering system. +. A. The application of theorem proving to question-answering systems. question-answering systems that use list-structured data bases and formal theorem-proving techniques to store facts, extract relevant data, and deduce logical . Deductive Question Answering Information Sciences Institute Finding Hypothetical Answers with a Resolution Theorem Prover. Intelligent Question-Answering Systems. formal theorem-proving techniques, the application of theorem-proving techniques to new problem domains, and the 10 Search Strategies for Theorem-Proving Department of Computing and question answering is modelled as a theorem proving task. To aid 2007 uses a machine learning method to develop the question and answer classifier Question Answering: From Partitions to Prolog* The application of theorem proving to question-answering systems. Claude Cordell. Green on Amazon.com. *FREE* shipping on qualifying offers. RESEARCH ON INTELLIGENT QUESTION-ANSWERING SYSTEMS to the Application of Theorem Proving to Problem Solving1. system that depended exclusively on formal theorem-proving methods to search for the only within a given world model to answer questions about it concerning which operators The use of theorem-proving techniques in question-answering. tics of questions in a simple question answering algorithm. The algorithm is sound, complete, and based on tableau theorem proving. The algorithm relies on a Automation of Reasoning: 2: Classical Papers on Computational. Google Books Result Using logical relevance for question answering Abstract. The Multiple ANSwer EXtraction system is a framework for interpreting a The use of theorem proving to implement question answering has received Theorem-proving by resolution as a basis for question-answering We expect to demonstrate the feasibili ty of question-answering systems that use both list-structure semantic models and formal theorem-proving techniques to . 11 Theorem-Proving by Resolution as a Basis for Question. 18 Jan 2006. In actual systems, theorem proving has long been used as a model for Applying the \u201clogical\u201d approach to question answering outlined above", "authors": ["C. Cordell Green"], "id": "3c2eecf7fdcac147c06da1d53efe36a926530eee", "title": "The Application of Theorem Proving to Question-Answering Systems", "references": []}, {"date": "1974", "abstract": "Abstract : This manual describes the use of the interactive proof checker FOL. FOL implements a version of the system of natural deduction described by Prawitz, augmented in the following ways: (1) it is a many-sorted first-order logic and a partial order over sorts may be declared: this reduces the size of formulas; (2) purely propositional deductions can be made in a single step; (3) the truth values of assertions involving numerical and LISP constants can be derived by computation; (4) there is a limited ability to make metamathematical arguments; and (5) there are many operational conveniences. The goal of FOL is to use formal proof techniques as practical tools for checking proofs in pure mathematics and proofs of the correctness of programs. It is also intended to be used as a research tool in modelling common-sense reasoning in the representation theory of artificial intelligence.", "authors": ["Richard W. Weyhrauch", "Arthur J. Thomas"], "id": "236c58d8ba3e9e9d5ed8562084690090752bf5df", "title": "FOL: A Proof Checker for First-order Logic", "references": []}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"The frame problem in problem-solving systems\" by Bertram Raphael", "authors": ["Bertram Raphael"], "id": "6758ec88d7d0b2056a45b07d752a861badedb93a", "title": "The frame problem in problem-solving systems", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"EFFECT OF TOTAL ABSENCE OF FUNCTION ON THE OPTIC SYSTEM OF RABBITS\" by Louis Sanford Goodman", "authors": ["Louis Sanford Goodman"], "id": "17b0743f7d37bfedc9494f630b509e0e6e87b786", "title": "EFFECT OF TOTAL ABSENCE OF FUNCTION ON THE OPTIC SYSTEM OF RABBITS", "references": []}, {"date": "1955", "abstract": "Semantic Scholar extracted view of \"Interocular transfer of learning in visually naive and experienced infant chimpanzees.\" by Kao Liang Chow et al.", "authors": ["Kao Liang Chow", "Henry W. Nissen"], "id": "fbb9508b12a752254296dd7fe8cec9cc0035edaa", "title": "Interocular transfer of learning in visually naive and experienced infant chimpanzees.", "references": []}, {"date": "1953", "abstract": "Semantic Scholar extracted view of \"Interocular transfer of habits learned monocularly in visually naive and visually experienced cats.\" by Austin H. Riesen et al.", "authors": ["Austin H. Riesen", "Martin I. Kurke", "Jeanne C. Mellinger"], "id": "3923dd40a5dedf692ea59be150f21547474c5f33", "title": "Interocular transfer of habits learned monocularly in visually naive and visually experienced cats.", "references": []}, {"date": "1971", "abstract": "Abstract : We describe PLANEXl, a plan executor for the Stanford Research Institute robot system. The problem-solving program STRIPS creates a plan consisting of a sequence of actions, and the PLANEXI program carries out the plan by executing the actions. PLANEXI is designed so that it executes only that portion of the plan necessary for completing the task, reexecutes any portion of the plan that has failed to achieve the desired results, and initiates replanning in situations where the plan can no longer be effective in completing the task. The scenario for an example plan execution is given.", "authors": ["Richard Fikes"], "id": "62a7dc7a8774054ea11d32d3422b8234dcdca2d8", "title": "Monitored Execution of Robot Plans Producted by STRIPS", "references": []}, {"date": "1977", "abstract": "A system of rules for transforming programs is described, with the programs in the form of recursion equations. An initially very simple, lucid, and hopefully correct program is transformed into a more efficient one by altering the recursion structure. Illustrative examples of program transformations are given, and a tentative implementation is described. Alternative structures for programs are shown, and a possible initial phase for an automatic or semiautomatic program-manipulation system is indicated.", "authors": ["Rod M. Burstall", "John Darlington"], "id": "f54a584f23c0940a6314c3782dd59b9893b8ae0a", "title": "A Transformation System for Developing Recursive Programs", "references": []}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"Towards automatic program synthesis\" by Zohar Manna et al.", "authors": ["Zohar Manna", "Richard J. Waldinger"], "id": "eb3a66f92106e467d1aabeb24c42b0e421b3c02f", "title": "Towards automatic program synthesis", "references": []}, {"date": "1971", "abstract": "An intelligent robot, operating in an external environment that cannot be fully modeled in the robot's software, must be able to monitor the success of its execution of a previously generated plan. This paper outlines a unifled i ormalism for describing and relating the various functions oi a robot operating in such an environment. After exploring the distinetion between the external world and the robot's internal model of it, and the distinction between actions that interact with the world and the robot's descriptions of those actions, we formalize the concepts of a plan and of its execution. Current developments at Stanford Research Institute, and the benchmark idea oi an ultimate rational robot, are both analyzed in this framework.", "authors": ["John H. Munson"], "id": "4efdcacca99408c434f34c44ecd29198757a0ded", "title": "Robot Planning, Execution, and Monitoring in an Uncertain Environment", "references": ["46bb9b198b3d2284a0d06b29d41c04dea0d0b243", "a45c86255ace7de61e92d42925eca9ca201c6368", "c25dc4608a45d29bc9498a019ca597093af213f0", "62a7dc7a8774054ea11d32d3422b8234dcdca2d8"]}, {"date": "1971", "abstract": "Feel lonely? What about reading books? Book is one of the greatest friends to accompany while in your lonely time. When you have no friends and activities somewhere and sometimes, reading book can be a great choice. This is not only for spending the time, it will increase the knowledge. Of course the b=benefits to take will relate to what kind of book that you are reading. And now, we will concern you to try reading problem solving methods in artificial intelligence as one of the reading material to finish quickly.", "authors": ["Nils J. Nilsson"], "id": "b49bb7ecd2afd6461c78ff29536839b5ee45cd15", "title": "Problem-solving methods in artificial intelligence", "references": []}, {"date": "1976", "abstract": "Lucid is both a programming language and a formal system for proving properties of Lucid programs. The programming language is unconventional in many ways, although programs are readily understood as using assignment statements and loops in a \u201cstructured\u201d fashion. Semantically, an assignment statement is really an equation between \u201chistories\u201d, and a whole program is simply an unordered set of such equations.From these equations, properties of the program can be derived by straightforward mathematical reasoning, using the Lucid formal system. The rules of this system are mainly those of first order logic, together with extra axioms and rules for the special Lucid functions.This paper formally describes the syntax and semantics of programs, and justifies the axioms and rules of the formal system.", "authors": ["Edward A. Ashcroft", "William W. Wadge"], "id": "66e10a19439d7ab65ae7d5f25c029489daf34759", "title": "Lucid - A Formal System for Writing and Proving Programs", "references": ["6eb3e06050e8f3287d3fd9def9e49e10eda80817", "bea7a030fa0e1e63605899d73743307e2b5a7336", "d5dd9f1f4ed9eb9db12c57c554b3589657fb6e18", "27f89af321ec6c641ecd15325d882b4827f5d45f"]}, {"date": "1965", "abstract": "Semantic Scholar extracted view of \"Natural Deduction: A Proof-Theoretical Study\" by Dag Prawitz", "authors": ["Dag Prawitz"], "id": "102765f4d74cc8e2de52c9fa26854887afb635f8", "title": "Natural Deduction: A Proof-Theoretical Study", "references": []}, {"date": "1969", "abstract": "This paper shows how an extension of the resolution proof procedure can be used to construct problem solutions. The extended proof procedure can solve problems involving state transformations. The paper explores several alternate problem representations and provides a discussion of solutions to sample problems including the \"Monkey and Bananas\" puzzle and the 'Tower of Hanoi\" puzzle. The paper exhibits solutions to these problems obtained by QA3, a computer program bused on these theorem-proving methods. In addition, the paper shows how QA3 can write simple computer programs and can solve practical problems for a simple robot.", "authors": ["C. Cordell Green"], "id": "1beae9ef432a57bb5ec0c43944a07182814ab443", "title": "Application of Theorem Proving to Problem Solving", "references": ["672fe922b933b188b167c6f50e433add2daffc3a", "c8ed24d86755095c263a2f031752f817e668da3e", "83f054294ba2726d02aa03e471da773c3383b146", "577e96521d62b9ebb5fd67412a21b02e9cd67b90", "eb8d64c0f34610597cc02d31ec10d5d6bcd6d39c", "dd15a1958d5a7bfa45548eb101bef4186acf2299", "d2109eba4f160755f0b9a7497b6b691c2fa2d5d8", "375c0850cd418002ee2be0fb2b6ec177573f950a", "bb84be1454b80de35032a1260b60384ccdf65489", "05b44597834f6df07c1c1290fb33a979bdf99067"]}, {"date": "1981", "abstract": "From the Publisher: \n\"First book-length exposition of the denotational (or `mathematical' or `functional') approach to the formal semantics of programming languages (in contrast to `operational' and `axiomatic' approaches). Treats various kinds of languages, beginning with the pure-lambda-calculus and progressing through languages with states, commands, jumps, and assignments. This somewhat discursive account is a valuable compilation of results not otherwise available in a single source.\" \n \n-- American Mathematical Monthly", "authors": ["Joseph E. Stoy"], "id": "64dfb6baddaed9705914a79a96c101562776b7ef", "title": "Denotational Semantics: The Scott-Strachey Approach to Programming Language Theory", "references": []}, {"date": "1977", "abstract": "Lucid is a formal system in which programs can be written and proofs of programs carried out. The proofs are particularly easy to follow and straightforward to produce because the statements in a Lucid program are simply axioms from which the proof proceeds by (almost) conventional logical reasoning, with the help of a few axioms and rules of inference for the special Lucid functions. As a programming language, Lucid is unconventional because, among other things, the order of statements is irrelevant and assignment statements are equations. Nevertheless, Lucid programs need not look much different than iterative programs in a conventional structured programming language using assignment and conditional statements and loops.", "authors": ["Edward A. Ashcroft", "William W. Wadge"], "id": "6eb3e06050e8f3287d3fd9def9e49e10eda80817", "title": "Lucid, a nonprocedural language with iteration", "references": ["f88915740132bb157b5e2a8239e61901a28e546d"]}, {"date": "1982", "abstract": "Abstract : The paper describes a logical notation for reasoning about digital circuits. The formalism provides a rigorous and natural basis for device specification as well as for proving properties such as correctness of implementation. Conceptual levels of circuit operation ranging from detailed quantitative timing and signal propagation up to functional behavior are integrated in a unified way. A temporal predicate calculus serves as the formal core of the notation, resulting in a versatile tool that has more descriptive power than any conventional hardware specification language. The logic has been applied to specifying and proving numerous properties of circuits ranging from delay elements up to the AM2901 ALU bit slice. Presentations of a delay model and a multiplication circuit illustrate various features of the notation.", "authors": ["Ben C. Moszkowski"], "id": "88a434c786e55bb3a72b9f09db33f465b4380373", "title": "A Temporal Logic for Multi-Level Reasoning About Hardware,", "references": ["2d206d2ee6f24c42ecb7e07005004650577e7966", "ccf50ab3faa69ec30d96b49158c1a6a4f2e8e2aa", "f8c02120710b877d802114e360d723b63a5ea7e0", "d24c890b7e0400828fd9d194331f5485c8d16d1a", "3ebe8ffa7e5cc5fa713c1e2faeb8db1f944d131e", "f84e74c58c2c68275a23645ae2e5913007767e45", "1f95ed618f677f4bf6062f896a2ba69d794c540f", "bf104ebfbd44924b6b7602e48b0a74e987baaca8", "116621c83f1bb7b331f769564e336b6b339ef2e5"]}, {"date": "1983", "abstract": "A method for specifying program modules in a concurrent program is described. It is based upon temporal logic, but uses new kinds of temporal assertions to make the specifications simpler and easier to understand. The semantics of the specifications is described informally, and a sequence of examples are given culminating in a specification of three modules comprising the alternating-bit communication protocol. A formal semantics is given in the appendix.", "authors": ["Leslie Lamport"], "id": "bf104ebfbd44924b6b7602e48b0a74e987baaca8", "title": "Specifying Concurrent Program Modules", "references": ["ccf50ab3faa69ec30d96b49158c1a6a4f2e8e2aa", "8f3e70744310495ae7d3e198126ae69bf5807365"]}, {"date": "1974", "abstract": "We define a semantics for the arithmetic part of PASCAL by giving it an interpretation in LCF, a language based on the typed $\\lambda$-calculus. Programs are represented in terms of their abstract syntax. We show sample proofs, using LCF, of some general properties of PASCAL and the correctness of some particular programs. A program implementing the McCarthy Airline reservation system is proved correct.", "authors": ["Luigia Carlucci Aiello", "Mario Aiello", "Richard W. Weyhrauch"], "id": "9f1fbbf387f2cfd6b01ce85f80e7a3a5fe5214a4", "title": "The semantics of PASCAL in LCF.", "references": []}, {"date": "1978", "abstract": "This paper gives the technical details and proofs for the notion of approximate reduction introduced in an earlier paper. The main theorem asserts that every lambda expression determines a set of approximate normal forms of which it is the limit in the lambda calculus models discovered by Scott in 1969. The proof of this theorem rests on the introduction of a notion of type assignments for the lambda calculus corresponding to the projections present in Scott\u2019s models; the proof is then achieved by a series of lemmas providing connections between the type-free lambda calculus and calculations with these type assignments.As motivation for these semantic properties, we derive also some relations between the computational behavior of lambda expressions and their approximate normal forms, and we establish a syntactic analogue of the general considerations motivating the continuity of functions in Scott\u2019s lattice theoretic approach.", "authors": ["Christopher P. Wadsworth"], "id": "bd598056b1b945b1a0076f520de1aa1b3c655b09", "title": "Approximate Reduction and Lambda Calculus Models", "references": ["d7c3c95d5b9c3c4fd8b07863d072d59804f2506a"]}, {"date": "1969", "abstract": "Abstract : Recursive definitions are considered which consist of Algol-like conditional expressions. By specifying a computation rule for evaluating such recursive definition, it determines a partial function. However, for different computation rules, the same recursive definition may determine different partial functions. Two types of computation rules are distinguished: sequential and parallel. The purpose of the paper is to formalize properties (such as termination, correctness and equivalence) of these partial functions by means of the satisfiability or validity of certain formulas in partial function logic.", "authors": ["Zohar Manna", "John McCarthy"], "id": "a5e82e5dba72ca4347597cb99ce6b9a1f9b9a812", "title": "PROPERTIES OF PROGRAMS AND PARTIAL FUNCTION LOGIC", "references": []}, {"date": "1974", "abstract": "We define a measure of the generality of the control structure of a program schema. This imposes a partial ordering on program schemas, and leads to a concept of the \u201cdifficulty\u201d of a programming problem. In this sense there exists a \u201chardest\u201d flowchart program, recursive program etc. Some earlier proofs can also be simplified and/or clarified by this approach.", "authors": ["Ashok K. Chandra"], "id": "b762d1701e32feeff7a60a66ca2d019bd1415c65", "title": "Degrees of translatability and canonical forms in program schemas: Part I", "references": []}, {"date": "1971", "abstract": "This is the introductory paper in a series devoted to a general algebraic theory of \u201crecursive definitions\u201d and \u201crecursive languages\u201d. In this paper we present the fundamental concepts and theorems concerning the basic structure (basic syntax), the semantics and the combination and manipulation of \u201crecursive definitions\u201d and the closure properties of \u201crecursive languages\u201d. The development is carried out within the framework of category theory and lattice theory. To illustrate the generality of the approach and our results we show how they apply directly to the specific examples of \u201crecursive languages\u201d of (generalized) context-free grammars, Turing machines, and flowcharts.", "authors": ["Eric G. Wagner"], "id": "54b052229eb9ae76546c7674e1137c9ce140ebc8", "title": "An algebraic theory of recursive definitions and recursive languages", "references": []}, {"date": "1964", "abstract": "Abstract : The author describes Microalgol, a trivial subset of ALGOL, by means of an interpreter. The notions of abstract syntax and of state of the computation permit a compact description of both syntax and semantics. The author advocates an extension of this technique as a general way of describing programming languages.", "authors": ["John McCarthy"], "id": "9697b3978c639f9043acb29e4fd5b53aa9131fdd", "title": "A Formal Description of a Subset of Algol", "references": []}, {"date": "1972", "abstract": "by Sara B. Quinn In mathematics, one often tries to classify some collection of objects up to isomor-phism. In mathematical logic, we can explore the complexity of that classification. A structure consists of a universe and an interpretation of a language, where the language has symbols representing constants, operations, and relations. We consider only structures whose universe is a subset of \u03c9, and we define a class as a collection of structures all with the same language and closed under isomorphism. One way that the complexity of the classification problem can be explored is by looking at the index set for a computable structure. We consider indices for computable structures, and write A e where \u03d5 e = \u03c7 D(A). The index set for A is the set of all indices for computable isomorphic copies of A. We write I(A) = {e : A e \u223c = A}. In the present work, the relationship between the complexity of the index set for a structure and the complexity of a sentence describing the structure (called a Scott sentence) is explored. We find an example of a structure for which there is not a match between the complexity of the index set and the complexity of a Scott sentence. We also examine the possible complexities of an index set, and give results characterizing when a particular class will have an index set that is m-complete at a certain complexity level. Another idea explored in the present work is that of comparing the complexity of the classification problem for various classes of structures, using the notion of a Turing computable embedding. Definition. A Turing computable embedding of K into K is an operator \u03a6 = \u03d5 e such that 1. for each A \u2208 K, there exists B \u2208 K such that \u03d5 D(A) e = \u03c7 D(B) , and 2. if A, A \u2208 K correspond, respectively, to B, B \u2208 K , then A \u223c = A if and only if B \u223c = B. The ordering of classes of structures that arises from this embedding allows us to compare the complexity of the classification problem for those classes. In the present work, we give characterizations for the classes of structures that embed into the class of equivalence structures, as well as into the class of reduced Abelian p-groups of various lengths.", "authors": ["Denis J. Kfoury"], "id": "67ca3ddd2c5067ce0f89bac7d88d50c360c3d59f", "title": "Comparing Algebraic Structures up to Algorithmic Equivalence", "references": ["55c858475e30d1c3f8739a937ec805ac3a908c76", "d4d1d912704b09a699f8af9b5c2507f1958f4e1e", "b6829b69831f783cca4cfc18d6e50ec6a5996eaa", "ce15618da8025c217c4bf6dcd9db157229b943f7", "1be9dcfb94cbecd7abf1e45d7b4783f041ad33d9", "e272c33167aef37436ce6df64b428ebbdbefa742", "29e083f96c2dae37aba7c20ca240c29c3482649f"]}, {"date": "1976", "abstract": "This paper deals with logics of programs. The objective is to formalize a notion of program description, and to give both plausible (semantic) and effective (syntactic) criteria for the notion of truth of a description. A novel feature of this treatment is the development of the mathematics underlying Floyd-Hoare axiom systems independently of such systems. Other directions that such research might take are also considered. This paper grew out of, and is intended to be usable as, class notes [27] for an introductory semantics course. The three sections of the paper are: 1. A framework for the logic of programs. Programs and their partial correctness theories are treated as binary relations on states and formulae respectively. Truth-values are assigned to partial correctness assertions in a plausible (Tarskian) but not directly usable way. 2. Particular Programs. Effective criteria for truth are established for some programs using the Tarskian criteria as a benchmark. This leads directly to a sound, complete, effective axiom system for the theories of these programs. The difficulties involved in finding such effective criteria for other programs are explored. The reader's attention is drawn to Theorems 4, 16, 18 and 22-24, as worthy of mention even out of the context in which they now appear. 3. Variations and extensions of the framework. Alternatives to binary relations for both programs and theories are speculated on, and their possible roles in semantics are considered. We discuss a hierarchy of varieties of programs and the importance of this hierarchy to the issues of definability and describability. Modal logic is considered as a first-order alternative to Floyd-Hoare logic. We give an appropriate axiom system which is complete for loop-free programs and also puts conventional predicate calculus in a different light by lumping quantifiers with non-logical assignments rather than treating them as logical concepts. Proofs of all theorems are relegated to an appendix.", "authors": ["Vaughan R. Pratt"], "id": "c954c8504cac9d9753255a7b4b986fdc0fddb7fd", "title": "Semantical consideration on floyo-hoare logic", "references": ["16c762445f11fa2020994918dc4f93e76264df17", "93cc8cb9c34a57a0d6edcf29039f57a5a0dd23c4", "f55f8866e18663886c8d283297404284f8714843", "cee5d4d123d6d289a14d41baffa73723dcd3e9e7", "9cf159a7e67143dcda48637391f96ee0d6f7ab36", "5d8056e326d4199d157a17fbeee97a7349d2824c", "201d7231a2493225c23c9bd148fce3e408da1d6b", "f110e72d78c9dc557c1a4ace23d474200eed0715", "a02c5e0608a8c0466fcb2536740e2add77507e18", "567cbc604dc30b1ae9ce2708128cd5e382363d01"]}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"Problem-Solving Methods in AI\" by Nils J. Nilsson", "authors": ["Nils J. Nilsson"], "id": "46bb9b198b3d2284a0d06b29d41c04dea0d0b243", "title": "Problem-Solving Methods in AI", "references": []}, {"date": "1971", "abstract": "In this paper, the fuzzy set ZZadeh (1965)] is viewed as a multivalued logic with a continuum of truth values in the interval Z0, 1]. The concepts of inconsistency, validity, prime implicant and prime implicate are extended to fuzzy logic and various properties of these notions in the context of fuzzy logic are established. It is proved that a formula is valid (inconsistent) in fuzzy logic iff it is valid (inconsistent) in two-valued logic. An algorithm that generates fuzzy prime implicants (implicates) is introduced. A proof of the completeness of this algorithm is also given.", "authors": ["Richard C. T. Lee", "Chin-Liang Chang"], "id": "a45c86255ace7de61e92d42925eca9ca201c6368", "title": "Some Properties of Fuzzy Logic", "references": ["92c1316871b83d4adfa9461c589be81d7a88b595", "107dd7e51dee6d83a0f4782333d708c788cbfc77", "f93d7109d1f1e4b4a5c70a5aa10d74bdbab50ec8", "ee787c6090c333eb83786ea48e3e1b1ae6499662", "1d029e10d24cd18755e14508ec3fd2703e282e0a"]}, {"date": "1980", "abstract": "Programs that implement computer communications protocols can exhibit extremely complicated behavior, and neither informal reasoning nor testing is reliable enough to establish their correctness. In this paper we discuss the application of program verification techniques to protocols. This approach is more reliable than informal reasoning, but has the advantage over formal reasoning based on finite-state models that the complexity of the proof does not grow unmanageably as the size of the program increases. Certain tools of concurrent program verification that are especially useful for protocols are presented: history variables that record sequences of input and output values, temporal logic for expressing properties that must hold in a future system state (such as eventual receipt of a message), and module specification and composition rules. The use of these techniques is illustrated by verifying a simple data transfer protocol from the literature.", "authors": ["Brent Hailpern", "Susan S. Owicki"], "id": "ccf50ab3faa69ec30d96b49158c1a6a4f2e8e2aa", "title": "Verifying network protocols using temporal logic", "references": ["9adc51fc971a91c6a67f66fd8c49bc6ca44118fc", "9324137651eb90c96c93e537093a3c17e1a3f797", "676259739cd8747d298833db80bf1dad8041fd1c", "beb91dfc9e101903fc2b8fc171c52d5f1c74a583", "d2661f91a7899bafa2a0d16dbda14bca5e6841f4", "9d29c46c431ccb5edce50bc1c23c3c15c47d4a39", "7141329f5af52934b725846763d6bada3fda2161", "c3fcf116688ffc322b6c78915e1896c7daeab6b0", "bba8a9a829d0f4c10af20df980a7b1ea7f6a61bd", "edfa95c61f1c3eae906bd259e9c401bf06ccf521"]}, {"date": "1974", "abstract": "Semantic Scholar extracted view of \"Introduction to Mathematical Theory of Computation\" by Zohar Manna", "authors": ["Zohar Manna"], "id": "d5dd9f1f4ed9eb9db12c57c554b3589657fb6e18", "title": "Introduction to Mathematical Theory of Computation", "references": []}, {"date": "1981", "abstract": "Semantic Scholar extracted view of \"Logic of Programs\" by Erwin Engeler", "authors": ["Erwin Engeler"], "id": "52e041d6735eb1dab39e80c142e4ad6b1b7d447f", "title": "Logic of Programs", "references": []}, {"date": "1974", "abstract": "Semantic Scholar extracted view of \"Program Proving as Hand Simulation with a Little Induction\" by Rod M. Burstall", "authors": ["Rod M. Burstall"], "id": "27f89af321ec6c641ecd15325d882b4827f5d45f", "title": "Program Proving as Hand Simulation with a Little Induction", "references": []}, {"date": "1973", "abstract": "LCF is a deductive system for computable functions proposed by D. Scott in 1969 in an unpublished memorandum. The purpose of the present paper is to demonstrate the soundness of the system with respect to certain models, which are partially ordered domains of continuous functions. This demonstration was supplied by Scott in his memorandum; the present paper is merely intended to make this work more accessible.", "authors": ["Robin Milner"], "id": "bea7a030fa0e1e63605899d73743307e2b5a7336", "title": "Models of LCF.", "references": []}, {"date": "1980", "abstract": "We define a process logic PL that subsumes Pratt's process logic, Parikh's SOAPL, Nishimura's process logic, and Pnueli's Temporal Logic in expressiveness. The language of PL is an extension of the language of Propositional Dynamic Logic (PDL). We give a deductive system for PL which includes the Segerberg axioms for PDL and prove that it is complete. We also show that PL is decidable.", "authors": ["David Harel", "Dexter Kozen", "Rohit Parikh"], "id": "c11023e86affcd7c7f7b358e026b4ad870ece157", "title": "Process Logic: Expressiveness, Decidability, Completeness", "references": []}, {"date": "1981", "abstract": "Semantic Scholar extracted view of \"Temporal Logic Specification of Distributed Systems\" by Richard L. Schwartz et al.", "authors": ["Richard L. Schwartz", "P. M. Melliar-Smith"], "id": "8f3e70744310495ae7d3e198126ae69bf5807365", "title": "Temporal Logic Specification of Distributed Systems", "references": []}, {"date": "1979", "abstract": "We explore the general framework of Modal Logic and its applicability to program reasoning. We relate the basic concepts of Modal Logic to the programming environment: the concept of \"world\" corresponds to a program state, and the concept of \"accessibility relation\" corresponds to the relation of derivability between states during execution. Thus we adopt the Temporal interpretation of Modal Logic. The variety of program properties expressible within the modal formalism is demonstrated.", "authors": ["Zohar Manna", "Amir Pnueli"], "id": "d24c890b7e0400828fd9d194331f5485c8d16d1a", "title": "The Modal Logic of Programs", "references": []}, {"date": "1976", "abstract": "An approach to the development of correct microprograms is to use the methodologies that have been beneficial in the generation of correct user programs, i. e., structured programming, high-level languages (HLL's), and formal program verification using Floyd's inductive assertion method. This paper presents a system that combines these techniques to simplify the design and implementation of correct microprograms for a real microprogrammable computer. It gives some statistics which support our emphasis on generation as well as correctness and some preliminary results on the use of our system.", "authors": ["David A. Patterson"], "id": "f84e74c58c2c68275a23645ae2e5913007767e45", "title": "Strum: Structured Microprogram Development System for Correct Firmware", "references": ["c50eec60013132f139b589f84fb77487ecafc680", "09661a6bb7578979e42c75d6ce382baba64d4981", "950d442bdd212a7a4512baa4b3882cf4491ee974", "543e5c863e8e083e0371fc89cd6f18c375fdaa40", "b27c300fe6bb23035a897abdb471cd3eae7c5038", "8f2ac14bd3694b96e71bb26bd622711bfcfaf460", "7803a0a99a313403519c25a8374c067219168f21", "6023269ef7c857619f8d10e59430f42497891a67", "424d0cf0319e3fe68354c74694b3dfaf18c2ed1d", "e91ee3bf78f4c1e0e7ac22261d8e0930400750a1"]}, {"date": "1973", "abstract": "Semantic Scholar extracted view of \"A preliminary theory for parallel programs\" by Gilles Kahn", "authors": ["Gilles Kahn"], "id": "f88915740132bb157b5e2a8239e61901a28e546d", "title": "A preliminary theory for parallel programs", "references": []}, {"date": "1969", "abstract": "A formal model of a problem is developed and its relationship to the General Problem Solver (GPS) is discussed. Before GPS can work on a problem it must be given differences, a difference-ordering, and a table of connections, in addition to the specifications of a problem. Formal definitions of this additional information are given, and sufficient conditions for the success of GPS are derived. These conditions point out the utility of differences and a difference-ordering that yield a \u201ctriangular\u201d table of connections. Several different formulations of the Tower of Hanoi are given to illustrate the formal concepts. The use of subproblems in narrowing search is discussed.", "authors": ["George W. Ernst"], "id": "bb84be1454b80de35032a1260b60384ccdf65489", "title": "Sufficient Conditions for the Success of GPS", "references": []}, {"date": "1969", "abstract": "Massachusetts Institute of Technology, Alfred P. Sloan School of Management. Thesis. 1969. Ph.D.", "authors": ["James H. Morris"], "id": "d7c3c95d5b9c3c4fd8b07863d072d59804f2506a", "title": "Lambda-calculus models of programming languages.", "references": ["d8ce4b5489ef14e8878c869101e30432d057599c", "f82211423d9cbf034d87598a93d9b4cae147ef34", "9aed59ed036b5715706ac44ba53eb20eff0911ed", "83f054294ba2726d02aa03e471da773c3383b146", "27dd189065bd8847a8ec8f27553282df67a42d3e", "f07bc62734f02066f0d081d462398fb9ebfde3c4", "8c2b7fc9bd3186b455fd674732833035bce8aa5d", "cee5d4d123d6d289a14d41baffa73723dcd3e9e7", "9c6e618fe404c84ecb2fcca1dba505e040460f51", "d4efc5c9aa688473beebf65f0937b91b12dd4d60"]}, {"date": "1969", "abstract": "This paper is concerned with the relationship between the correctness of programs and the satisfiability (or unsatisfiability) of certain formulas of the first-order predicate calculus. Results on the equivalence of programs are also included.", "authors": ["Zohar Manna"], "id": "375c0850cd418002ee2be0fb2b6ec177573f950a", "title": "The Correctness of Programs", "references": ["c43ebe201e2015d84cf44d8c17438ddbeddf3af9", "bc9e164cae0db0cdb7b830120a83d750a37a3608", "47eed66353713f7a723c71ae1f6aa73383653f2c"]}, {"date": "1963", "abstract": "This report describes some experiments in constructing a compiler that makes use of heuristic problem~solving techniques such as those incorporated in the General Problem Solver (GPS) [1]. The experiments were aimed at the dual objectives of throwing light on some of the problems of constructing more powerful programming languages and compilers, and of testing whether the task of writing a computer program can be regarded as a \"problem\" in the sense in which that term is used in GPS. The present paper is concerned primarily with the second objective--with analyzing some of the problem-solving processes that are involved in writing computer programs. At the present stage of their development, no claims will be made for the heuristic programming procedures described here as practical approaches to the construction of compilers. Their interest lies in what they teach us about the nature of the programming task.", "authors": ["Herbert A. Simon"], "id": "eb8d64c0f34610597cc02d31ec10d5d6bcd6d39c", "title": "Experiments with a Heuristic Compiler", "references": []}, {"date": "1937", "abstract": "Semantic Scholar extracted view of \"Languages with expressions of infinite length\" by Olaf Helmer", "authors": ["Olaf Helmer"], "id": "29e083f96c2dae37aba7c20ca240c29c3482649f", "title": "Languages with expressions of infinite length", "references": []}, {"date": "1972", "abstract": "From the Publisher: \nDefining his subject as making the art of verifying computer programs (debugging) into a science, the author addresses both practical and theoretical aspects of the process. A self-contained treatment, it includes selected concepts of computability theory and mathematical logic, and each chapter concludes with bibliographic remarks, references, and problems. This book is a classic text on sequential program verification; it has been widely translated from the original Hebrew and is much in demand among graduate students in the field of computer science (it may also be used as an undergraduate text for advanced classes). Unabridged republication of the edition published by McGraw-Hill, New York, 1974. 77 Figures.", "authors": ["Zohar Manna"], "id": "567cbc604dc30b1ae9ce2708128cd5e382363d01", "title": "Mathematical Theory of Computation", "references": []}, {"date": "2004", "abstract": "SummaryA weak logic of programs is a formal system in which statements that mean \u201cthe program halts\u201d cannot be expressed. In order to prove termination, we would usually have to use a stronger logical system. In this paper we show how we can prove termination of both iterative and recursive programs within a weak logic by augmenting the programs with counters and adding bound assertions on the counters as loop invariants and entry conditions. Thus, most of the existing verifiers which are based on a weak logic of programs can be used to prove termination of programs without any modification. We give examples of proofs of termination and of upper bounds on computation time that were obtained using a Pascal program verifier. The use of the method to locate causes of non-termination is also illustrated.", "authors": ["David C. Luckham", "Norihisa Suzuki"], "id": "a02c5e0608a8c0466fcb2536740e2add77507e18", "title": "Proof of termination within a weak logic of programs", "references": ["6321686427c86b87e1071497ffd633b71aad6fb6", "56e1d5cb852b342646a25e0d09b6362f3ece0f7f", "2cf7ae6adfb101ca984b1988bd6bb0be4f9b739f", "e93035a3107ce04321b0e22abaeb56e293fccb58", "244485ba5b8a0fcb678af91dec91d12d13f0f1d3", "ebbec3500ddff64f04a46b0a9a3d48b6e92ba2b5", "cf16b4cfcd611a7483bddecade8d34df1fb70b71", "5d8056e326d4199d157a17fbeee97a7349d2824c", "bf15ce3d1575d124527496cb249dc1249eee0acb", "d31243fa8cad1c432495bf883b65034862304781"]}, {"date": "1970", "abstract": "The resolution principle is an inference rule for quantifier-free first-order predicate calculus. In the past, the completeness theorems for resolution and its refinements have been stated and proved for finite sets of clauses. It is easy (by G\u00f6del's Compactness Theorem) and of practical interest to extend them to countable sets, thus allowing schemata representing denumerably many axioms. In addition, some theorems similar to Craig's Interpolation Theorem are proved for deduction by resolution. In propositional calculus, the theorem proved is stronger, whereas in predicate calculus the theorems proved are in some ways stronger and in some ways weaker than Craig's theorem. These interpolation theorems suggest procedures which could be embodied in computer programs for automatic proof finding and consequence finding.", "authors": ["James R. Slagle"], "id": "1d029e10d24cd18755e14508ec3fd2703e282e0a", "title": "Interpolation Theorems for Resolution in Lower Predicate Calculus", "references": ["7a36d6f67916a37bd9d1076be19319fb42df175c", "9beb5df44f0d98fe676710b3e8b5bca78c593ca6"]}, {"date": "1977", "abstract": "Verification of communication protocols usually involves two parts: a state-machine analysis of the control structure and proving some assertions about the semantic content of the protocol' s actions. The two parts are traditionally, treated separately. This paper suggests that the two approaches are not independent but rather complementary. It intro duces a unified model for protocols (and generally cooperating distant subsystems) encompassing both aspects. The method is demonstrated on three different descriptions of the same protocol, each with a different tradeoff between state machine and programming aspects. Verification of partial and full correctness is carried out in terms of the three descriptions.", "authors": ["Gregor von Bochmann", "Jan Gecsei"], "id": "9d29c46c431ccb5edce50bc1c23c3c15c47d4a39", "title": "A Unified Method for the Specification and Verification of Protocols", "references": ["f2a3b972c0f80c3ab5c817a749fc0562fe263e8b"]}, {"date": "1980", "abstract": "Pnueli [15] has recently introduced the idea of using temporal logic [18] as the logical basis for proving correctness properties of concurrent programs. This has permitted an elegant unifying formulation of previous proof methods. In this paper, we attempt to clarify the logical foundations of the application of temporal logic to concurrent programs. In doing so, we will also clarify the relation between concurrency and nondeterminism, and identify some problems for further research.In this paper, we consider logics containing the temporal operators \"henceforth\" (or \"always\") and \"eventually\" (or \"sometime\"). We define the semantics of such a temporal logic in terms of an underlying model that abstracts the fundamental concepts common to almost all the models of computation which have been used. We are concerned mainly with the semantics of temporal logic, and will not discuss in any detail the actual rules for deducing theorems.We will describe two different temporal logics for reasoning about a computational model. The same formulas appear in both logics, but they are interpreted differently. The two interpretations correspond to two different ways of viewing time: as a continually branching set of possibilities, or as a single linear sequence of actual events. The temporal concepts of \"sometime\" and \"not never\" (\"not always not\") are equivalent in the theory of linear time, but not in the theory of branching time -- hence, our title. We will argue that the logic of linear time is better for reasoning about concurrent programs, and the logic of branching time is better for reasoning about nondeterministic programs.The logic of linear time was used by Pnueli in [15], while the logic of branching time seems to be the one used by most computer scientists for reasoning about temporal concepts. We have found this to cause some confusion among our colleagues, so one of our goals has been to clarify the formal foundations of Pnueli's work.The following section gives an intuitive discussion of temporal logic, and Section 3 formally defines the semantics of the two temporal logics. In Section 4, we prove that the two temporal logics are not equivalent, and discuss their differences. Section 5 discusses the problems of validity and completeness for the temporal logics. In Section 6, we show that there are some important properties of the computational model that cannot be expressed with the temporal operators \"henceforth\" and \"eventually\", and define more general operators.", "authors": ["Leslie Lamport"], "id": "edfa95c61f1c3eae906bd259e9c401bf06ccf521", "title": "", "references": ["623605bd5e083c9abf45d3a497d54fd02144a31e"]}, {"date": "1978", "abstract": "Shared abstract data types, such as queues and buffers, are useful tools for building well-structured concurrent programs. This paper presents a method for specifying shared types in a way that simplifies concurrent program verification. The specifications describe the operations of the shared type in terms of their effect on variables of the process invoking the operation. This makes it possible to verify the processes independently, reducing the complexity of the proof. The key to defining such specifications is the concept of a private variable: a variable which is part of a shared object but belongs to just one process. Shared types can be implemented using an extended form of monitors; proof rules are given for verifying that a monitor correctly implements its specifications. Finally, it is shown how concurrent programs can be verified using the specifications of their shared types. The specification and proof techniques are illustrated with a number of examples involving a shared bounded buffer.", "authors": ["Susan S. Owicki"], "id": "676259739cd8747d298833db80bf1dad8041fd1c", "title": "Specifications and Proofs for Abstract Data Types in Concurrent Programs", "references": ["ace9601f004e847958d45a466ff62033c85a3ac3", "a097fc73c5d7467dc425a2879558c9c592b75b69", "33689b52b67bb8822720f0a54cdcac89b00873d9", "559b4b532ee8dd9fffc6c24f3f578cbd3c867b2c", "1548c821b925cee264dd58a6a193bc31ceb62502", "499b5ad66088959a5d929e91ffc598abfcc982f9", "5d8056e326d4199d157a17fbeee97a7349d2824c", "b48edff22909c9cfb43fb65cc7fbe18b376241ac", "7d06bf84338e89456f609896de4e41f61086d98e", "b52566a7f643837ad4d383c800fcdcd653b80fc4"]}, {"date": "1971", "abstract": "As late as 1967, a prominent researcher reported to his organization that he believed a successful higher-level microprogramming language seemed unlikely. At the same time, other members of the same organization were describing what they termed \"A Microprogram Compiler\". Meanwhile, other hardware and software designers, equally oblivious of each other, were generating useful and powerful higher-level languages to assist them in their work. As the reader will see, the stage had been set for the development of a higher-level, machine-independent language to be used for the task of writing microprograms.", "authors": ["Richard H. Eckhouse"], "id": "e91ee3bf78f4c1e0e7ac22261d8e0930400750a1", "title": "A high-level microprogramming language (MPL)", "references": []}, {"date": "1979", "abstract": "Concurrency in Gypsy is based on a unique, formal approach to specifying and proving systems of concurrent processes. The specification and proof methods are designed so that proofs of individual processes are totally independent, even when operating concurrently. These methods can be applied both to terminating and non-terminating processes, and the proof methods are well suited to automated verification aids. The basic principles of these methods and their interaction with the design of Gypsy are described.", "authors": ["Donald I. Good", "Richard M. Cohen", "James Keeton-Williams"], "id": "7141329f5af52934b725846763d6bada3fda2161", "title": "Principles of proving concurrent programs in Gypsy", "references": []}, {"date": "1982", "abstract": "Concurrent processes can exhibit extremely complicated behavior, and neither informal reasoning nor testing is reliable enough to establish their correctness. In this thesis, we develop a new technique for the verification of parallel programs. The technique is stated in terms of axioms and inference rules, and it is used to prove safety and liveness properties of parallel programs. Safety properties are assertions that must be satisfied by the system state at all times; they are analogous to partial correctness. Liveness properties refer to events that will occur in the future, such as program termination or the eventual receipt of a message. In addition to the formal proof rules, we present several heuristics to aid in the preparation of correctness proofs. \nWe model a parallel program as a set of interacting modules (processes and monitors), and we exploit this modularity in the verification process. First we prove properties of the low-level modules directly from their code. We then combine the specifications of the low-level modules to prove properties of higher-level modules, without again referring to the code. Eventually, we prove properties of the entire program. \nWe discuss the application of this verification technique to two classes of parallel programs: network protocols and resource allocators. Most previous approaches to verifying network protocols have been based upon reachability arguments for finite-state models of the protocols. Only protocols of limited complexity can be verified using the finite-state model, because of the combinatorial explosion of the state space as the complexity of the protocol increases. In contrast, our approach allows us to abstract information from the details of the implementation, so that the proof need not grow unmanageably as the protocol size increases. \nThe discussion of resource allocation centers around Hoare's structured paging system, which is a complex hierarchical program. With this example, we demonstrate that many of the techniques used in program verification can be used for specification as well. \nThe thesis also describes a number of tools that have been useful in proving concurrent programs. Two of the most important are history variables and temporal logic. We employ history variables to record the interaction between the modules that constitute a program. Temporal logic serves as a convenient notation for stating and proving liveness properties.", "authors": ["Brent Hailpern"], "id": "9324137651eb90c96c93e537093a3c17e1a3f797", "title": "Verifying Concurrent Processes Using Temporal Logic", "references": []}, {"date": "1982", "abstract": "A liveness property asserts that program execution eventually reaches some desirable state. While termination has been studied extensively, many other liveness properties are important for concurrent programs. A formal proof method, based on temporal logic, for deriving liveness properties is presented. It allows a rigorous formulation of simple informal arguments. How to reason with temporal logic and how to use safety (invariance) properties in proving liveness is shown. The method is illustrated using, first, a simple programming language without synchronization primitives, then one with semaphores. However, it is applicable to any programming language.", "authors": ["Susan S. Owicki", "Leslie Lamport"], "id": "bba8a9a829d0f4c10af20df980a7b1ea7f6a61bd", "title": "Proving Liveness Properties of Concurrent Programs", "references": ["d2661f91a7899bafa2a0d16dbda14bca5e6841f4", "f7ccdf117c6729d5ec11b5306c48ac7e89bb23ec"]}, {"date": "1977", "abstract": "A unified approach to program verification is suggested, which applies to both sequential and parallel programs. The main proof method suggested is that of temporal reasoning in which the time dependence of events is the basic concept. Two formal systems are presented for providing a basis for temporal reasoning. One forms a formalization of the method of intermittent assertions, while the other is an adaptation of the tense logic system Kb, and is particularly suitable for reasoning about concurrent programs.", "authors": ["Amir Pnueli"], "id": "c3fcf116688ffc322b6c78915e1896c7daeab6b0", "title": "The temporal logic of programs", "references": ["2cf7ae6adfb101ca984b1988bd6bb0be4f9b739f", "33689b52b67bb8822720f0a54cdcac89b00873d9", "2a41593d839323ddcd8597a22e4df6e398d4f98c", "cbddd72855580c2ad8ca52f477edd0385a3b78cf", "1e96e18ddad4f90d3f641109d3ef0d95cbcc44a9", "f628d62d2e7b8aaed47a3f8e28d7fd7ca3fe1047", "626ff57d5ea189f16b9c15e1a37fb17844867d3e", "b6cea23b4a07b66e3122252f47f9cc715b469cf9", "efa53518898247c3d2aad15944036c062c2bc617", "fa1ba526c7f1fc8437c3b55c6798526c387073b9"]}, {"date": "1973", "abstract": "This paper is concerned with developing automatic methods for the testing of correctness and equivalence of straight line segments of microcode. A model, which accounts for a composite of features common to a large variety of contemporary machines is first presented. Correctness and equivalence of straight line microprograms are then defined. The testing techniques are then developed.", "authors": ["C. V. Ramamoorthy", "K. S. Shankar"], "id": "8f2ac14bd3694b96e71bb26bd622711bfcfaf460", "title": "Correctness and equivalence of straight line microprograms", "references": []}, {"date": "1967", "abstract": "An experimental processing system for the algorithmic ~anguage EULER has been implemented in microprogramming on an IBM System/360 Model 30 using a second Read-Only Storage unit. The system consists of a mlcroprogrammed compiler and a microprogrammed String Language Interpreter, and of an I/O control program written in 360 machine language. The system is described and results are given in terms of microprogram and main storage space required and compiler and interpreter performance obtained. The role of microprogramming is stressed, which opens a new dimension in the processing of interpretive code. The structure and content of a higher level language can be matched by an appropriate interpretive language which can be executed efficiently by microprograms on existing computer hardware.", "authors": ["Helmut Weber"], "id": "7803a0a99a313403519c25a8374c067219168f21", "title": "A microprogrammed implementation of EULER on IBM system/360 model 30", "references": []}, {"date": "1966", "abstract": "In this section the algorithmic language EULER is described first informally and then formally by its syntax and semantics. An attempt has been made to generalize and extend some of the concepts of ALGOL, thus creating a language which is simpler and yet more flexible than ALGOL 60. A second objective in developing this language was to show that a useful programming language which can be processed with reasonable efficiency can be defined in rigorous formality.", "authors": ["Niklaus Wirth", "Helmut Weber"], "id": "6023269ef7c857619f8d10e59430f42497891a67", "title": "EULER: a generalization of ALGOL, and its formal definition: Part II", "references": []}, {"date": "1971", "abstract": "With increased use of microprogramming in present computer systems, the need arises to automate the checking and optimization of microcode. This paper reviews the optimization objectives, characterizes microprogranis, and discusses the machine structure. A translator from machine code to microcode appears feasible and is described. Some compiler techniques are reviewed and adapted to improve microprograms by studying operational interaction. Microprogram characteristics permit additional methods to reduce the computation effort. The techniques are integrated into a scheme implemented for optimizing a simulated machine.", "authors": ["Richard L. Kleir", "C. V. Ramamoorthy"], "id": "424d0cf0319e3fe68354c74694b3dfaf18c2ed1d", "title": "Optimization Strategies for Microprograms", "references": ["4c004e924b239800a6f249f85e96f00b80617d5f", "ed7fabd99ecb6069ae19b5efe99c00baab6904a3", "35bd93848bcc4b6e2f0bee9326651e1df0090d40", "d0032d121eb78d06acf98a313fced11dd3f3fb38", "d593f7584854d61e32ce345fb88cf192ef5f60b0", "e46faa52ff86efb77b35497699d0910fed41dcde", "59ca6576188bfbd30c139af9d642f54fddda1c8f", "ef6ec2f8b3b15b7fddbab372158bc8001ff3a302", "597c20e184da8f265bcd5e4a5a1dbe2be32d9283", "15428e9f83e0f3aaa61fe57f4cb663450c4e7bc0"]}, {"date": "1968", "abstract": "Abstract : The thesis contains two parts which are self-contained units. In Part 1 we present several results on the relation between the problem of termination and equivalence of programs and abstract programs, and the first order predicate calculus. Part 2 is concerned with the relation between the termination of interpreted graphs, and properties of well-ordered sets and graph theory. (Author)", "authors": ["Zohar Manna"], "id": "47eed66353713f7a723c71ae1f6aa73383653f2c", "title": "Termination of algorithms", "references": []}, {"date": "1974", "abstract": "Detection of concurrently executable microoperations is an important consideration for effective horizontal microprogramming. Since it is highly machine-dependent and requires knowledge of highly intricate features of a machine, only limited effort has been made so far to derive an algorithm for detection of microprogram parallelism to enable optimization of horizontal microprograms.\n In this paper, problems involved in optimization of horizontal microprograms are described. Existing techniques are applied to detect microprogram parallelism in a sequential source microprogram to allow subsequent optimization of the microprogram in a horizontal format. An algorithm for optimization of horizontal microprograms is derived. An upper bound for microprogram execution time is determined. The results derived in this paper are very general so that they are applicable to any machine with a horizontal microinstruction format (including a vertical format which is a special case of a horizontal format). An example is given to illustrate the algorithm.", "authors": ["Masahiro Tsuchiya", "Mario J. Gonzalez"], "id": "b27c300fe6bb23035a897abdb471cd3eae7c5038", "title": "An approach to optimization of horizontal microprograms", "references": []}, {"date": "1968", "abstract": "A Comparative Study of Programming LanguagesBy Bryan Higman. (Macdonald Computer Monographs.) Pp. v + 164. (Macdonald: London, 1967.) 45s.", "authors": ["E. B. Spratt"], "id": "d4efc5c9aa688473beebf65f0937b91b12dd4d60", "title": "Programming Linguistics", "references": []}, {"date": "1967", "abstract": "Semantic Scholar extracted view of \"A completeness theorem and a computer program for finding theorems derivable from given axioms\" by Char-Tung Lee", "authors": ["Char-Tung Lee"], "id": "9beb5df44f0d98fe676710b3e8b5bca78c593ca6", "title": "A completeness theorem and a computer program for finding theorems derivable from given axioms", "references": []}, {"date": "1969", "abstract": "This paper is concerned with the relationship of the termination problem for programs and abstract programs to the validity of certain formulas in the first-order predicate calculus. By exploiting this relationship, subclasses of abstract programs for which the termination problem is decidable can be isolated. Moreover, known proof procedures for the first-order predicate calculus (e.g. resolution) can be applied to prove the termination of both programs and abstract programs. The correctness and equivalence problems of abstract programs are shown to be reducible to the termination problem.", "authors": ["Zohar Manna"], "id": "bc9e164cae0db0cdb7b830120a83d750a37a3608", "title": "Properties of Programs and the First-Order Predicate Calculus", "references": []}, {"date": "1957", "abstract": "Semantic Scholar extracted view of \"Linear Reasoning. A New Form of the Herbrand-Gentzen Theorem\" by William Craig", "authors": ["William Craig"], "id": "7a36d6f67916a37bd9d1076be19319fb42df175c", "title": "Linear Reasoning. A New Form of the Herbrand-Gentzen Theorem", "references": []}, {"date": "1978", "abstract": "We describe a formal theory of the total correctness of parallel programs, including such heretofore theoretically incomplete properties as safety from deadlock and starvation. We present a consistent and complete set of proof rules for the total correctness of parallel programs expressed in nondeterministic form. The proof of consistency and completeness is novel in that we show that the weakest preconditions for each correctness criterion are actually fixed-points (least or greatest) of continuous functions over the complete lattice of total predicates. We have obtained proof rule schemata which can universally be applied to least or greatest fixed points of continuous functions. Therefore, our proof rules are a priori consistent and complete once it is shown that certain weakest preconditions are extremum fixed-points. The relationship between true parallelism and nondeterminism is also discussed.", "authors": ["Lawrence Flon", "Norihisa Suzuki"], "id": "623605bd5e083c9abf45d3a497d54fd02144a31e", "title": "Consistent and complete proof rules for the total correctness of parallel programs", "references": ["9e2fbe8f42e7fc5b7cb6a784137b830fcebc6345", "6210de8384c5d5aa2fcc940d8ebc3e3f6f9f927d", "5ada347492332825d38c7f7cfcc760842476ee52", "631a6083389feb859cba144df8401d27c5c089a6", "04edf2feffa2d2053e2d0ccc997cd51ba3b2b729", "4ee419815cfb15f2088fa1733c71906f3fbcec37", "1cc8d0f37ba80b1dabb6ed3f7028e6746f79dd1a", "de35370eaabebeb777c3e7ffc468712bea3bfc83"]}, {"date": "1973", "abstract": "This paper describes a theorem prover that embodies knowledge about programming constructs, such as numbers, arrays, lists, and expressions. The program can reason about these concepts and is used as part of a program verification system that uses the Floyd-Naur explication of program semantics. It is implemented in the QA4 language; the QA4 system allows many bits of strategic knowledge, each expressed as a small program, to be coordinated so that a program stands forward when it is relevant to the problem at hand. The language allows clear, concise representation of this sort of knowledge. The QA4 system also has special facilities for dealing with commutative functions, ordering relations, and equivalence relations; these features are heavily used in this deductive system. The program interrogates the user and asks his advice in the course of a proof. Verifications have been found for Hoare's FIND program, a real-number division algorithm, and some sort programs, as well as for many simpler algorithms. Additional theorems have been proved about a pattern matcher and a version of Robinson's unification algorithm.", "authors": ["Richard J. Waldinger", "Karl N. Levitt"], "id": "d31243fa8cad1c432495bf883b65034862304781", "title": "Reasoning about programs", "references": ["5a19db7d827f623143e92bca8c082de5c014d59c"]}, {"date": "1975", "abstract": "So-called \u201cguarded commands\u201d are introduced as a building block for alternative and repetitive constructs that allow non-deterministic program components for which at least the activity evoked, but possibly even the final state, is not necessarily uniquely determined by the initial state. For the formal derivation of programs expressed in terms of these constructs, a calculus will be shown.", "authors": ["Edsger W. Dijkstra"], "id": "cf16b4cfcd611a7483bddecade8d34df1fb70b71", "title": "Guarded commands, non-determinacy and a calculus for the derivation of programs", "references": []}, {"date": "1978", "abstract": "A deep look at contemporary operating systems finds data flow principles of value to computer networks large and small.", "authors": ["Peter J. Denning"], "id": "b48edff22909c9cfb43fb65cc7fbe18b376241ac", "title": "Operating Systems Principles for Data Flow Networks", "references": ["449fc430a40029a22777e72974fb85ae579798d4", "29bc5d481e54d8a8e3641d15aa49ef1dff27f5e8", "a02ef36214954043aacaa8c0468db84211bae798", "98454b3b6db6a1eca9a9addb00af483e7dba248c", "e8459f1a9e01327a56e45176fc9c0ec8296393ba", "88b88cce902b1bf663d62e6629938b94f2c7e1b9", "6f50450244f1f91afab9ce876f69ffa4d0450714", "d1deecbd75cfc7e44b01d314ea72556ab4b924c2", "4721b915d14351cb5663163f0202c880b2536d44", "3e7dd4ce45d8db958c9adaa9a287ab439fdf37ab"]}, {"date": "1975", "abstract": "The main purposes in writing this paper are to discuss the importance of formal specifications and to survey a number of promising specification techniques. The role of formal specifications both in proofs of program correctness, and in programming methodologies leading to programs which are correct by construction, is explained. Some criteria are established for evaluating the practical potential of specification techniques. The importance of providing specifications at the right level of abstraction is discussed, and a particularly interesting class of specification techniques, those used to construct specifications of data abstractions, is identified. A number of specification techniques for describing data abstractions are surveyed and evaluated with respect to the criteria. Finally, directions for future research are indicated.", "authors": ["Barbara Liskov", "Stephen N. Zilles"], "id": "b52566a7f643837ad4d383c800fcdcd653b80fc4", "title": "Specification techniques for data abstractions", "references": []}, {"date": "1978", "abstract": "A data abstraction can be naturally specified using algebraic axioms. The virtue of these axioms is that they permit a representation-independent formal specification of a data type. An example is given which shows how to employ algebraic axioms at successive levels of implementation. The major thrust of the paper is twofold. First, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type. Second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms. This implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished.", "authors": ["John V. Guttag", "Ellis Horowitz", "David R. Musser"], "id": "499b5ad66088959a5d929e91ffc598abfcc982f9", "title": "Abstract data types and software validation", "references": ["1f966eaa651fb021be225a8c56c9121759059001", "09661a6bb7578979e42c75d6ce382baba64d4981", "ebbec3500ddff64f04a46b0a9a3d48b6e92ba2b5", "3596e5121dd74f173400cd38eea4bef68a2b475d", "7d06bf84338e89456f609896de4e41f61086d98e"]}, {"date": "1971", "abstract": "Progress of Cybernetics. Vol. 1: Main Papers, The Meaning of Cybernetics, Neuro- and Biocybernetics. Edited by J. Rose. (Proceedings of the First International Congress of Cybernetics, London, 1969.) Pp. xiv + 521. (Gordon and Breach: London and New York, September 1970.) $24.50; \u00a310.00.", "authors": ["J Young"], "id": "b6cea23b4a07b66e3122252f47f9cc715b469cf9", "title": "Machine Intelligence", "references": []}, {"date": "1977", "abstract": "Semantic Scholar extracted view of \"The temporal logic of programs\" by A. Pneuli", "authors": ["A. Pneuli"], "id": "f7ccdf117c6729d5ec11b5306c48ac7e89bb23ec", "title": "The temporal logic of programs", "references": []}, {"date": "1977", "abstract": "A new logic for reasoning about programs is proposed here, and its metamathematics is investigated. No new primitive notions are needed for the logic beyond those used in elementary programming and mathematics, yet the combination of these notions is remarkably powerful. The logic includes a programming language, designed with Michael O'Donnell, for program verification. It forms the core of the PL/CV verifier at Cornell. This study belongs to the discipline of Algorithmic Logic as conceived by Engeler.\n The logic is related to Park's mu-calculus based on functions instead of relations. The monadic quantifier free subset is developed in the style of the system in J.W. deBakker's Recursive Procedures. But this logic is substantially different from either of these. It is intended to be a practical programming logic in the spirit of E. Dijkstra's calculus.\n This paper proves the completeness and decidability of the monadic (iterative) programming logic. It discusses the polyadic logic and the programming language briefly, and considers general models for the iterative programming logic. The polyadic logic is shown to be incomplete with respect to standard models, but complete with respect to general models.", "authors": ["Robert L. Constable"], "id": "626ff57d5ea189f16b9c15e1a37fb17844867d3e", "title": "On the theory of programming logics", "references": []}, {"date": "1977", "abstract": "The intermittent assertion technique of Burstall can be formulated and made rigorous in the formal-system/programming-language Lucid, in a very straightforward way. This reinforces the contention that Lucid is a framework within which many sorts of proofs of program properties may be expressed. This paper includes three proofs, all of which are the Lucid versions of intermittent assertion proofs found in the literature.", "authors": ["Edward A. Ashcroft", "William W. Wadge"], "id": "1e96e18ddad4f90d3f641109d3ef0d95cbcc44a9", "title": "Intermittent Assertion Proofs in Lucid", "references": ["7a36165ece98987792e56d625f32625c73525557", "6eb3e06050e8f3287d3fd9def9e49e10eda80817", "27f89af321ec6c641ecd15325d882b4827f5d45f", "66e10a19439d7ab65ae7d5f25c029489daf34759", "3ce1df838461ea6247b2a0e33d62456c9edac051", "ab99c14cdab69a960a8346ed952eda3e6332f240"]}, {"date": "1966", "abstract": "Bfichi (1962) has given a decision procedure for a system of logic known as \" the Sequential Calculus,\" by showing that each well formed formula of the system is equivalent to a fornmla that, roughly speaking, says something about the infinite input history of a finite automaton. In so doing he managed to answer an open question that was of concern to pure logicians, some of whom had no interest in the theory of automata. Muller (1963) came upon quite similar concepts in studying a problem in asynchronous switching theory. The problem was to describe the behavior of an asynchronous circuit tha t does not reach any stability condition when starting from a certain state and subject to a certain input condition. Many different things can happen, since there is no control over how fast various parts of the circuit react with respect to each other. Since at no time during the presence of that input condition will the circuit reach a terminal condition, it will be possible to describe the total set of possibilities in an ideal fashioll only if an infinite amount of time is assumed for tha t input condition. Neither Biichi's Sequential Calculus nor ~Iuller's problem of asynchronous circuitry will be described further here. I t is interesting tha t two such apparently divergent areas of inquiry should give rise to the same problem, namely, that of describing the infinite history of finite automata. I t is this problem to which the remainder of this paper will address itself. I t will be recalled that a well known basic theorem in the theory of", "authors": ["Robert McNaughton"], "id": "f628d62d2e7b8aaed47a3f8e28d7fd7ca3fe1047", "title": "Testing and Generating Infinite Sequences by a Finite Automaton", "references": ["a496212ca3444e1e14b0668b82e2459d02dc275a"]}, {"date": "1969", "abstract": "The term microprogramming was first coined by Professor M. V. Wilkes of the Cambridge Universi ty Mathemat ica l Laboratory in 1951 [1, 2]. His thesis was tha t one can envision the control portion of a computer as effecting a number of registerto-register transfers of information, some in sequence and some in parallel, in order to carry out the execution of a single machine instruction. Each of these steps can itself be thought of as the execution of an instruction for some machine (whose existence is unknown to the programmer) . The steps used to effect a single instruction in the user machine can be thought of as const]tuting a program, usually called a microprogram. Microprograms can also be used for other necessary operations which are in some sense invisible to the programmer, for example, fetching the next instruction or computing effective addresses. These concepts are spelled out more completely in the next section of this paper. By the mid 1960s it became both possible and practicable to build computers in which the control is driven explicitly by microprograms [3-8]. These programs generally reside in a device, distinct from the", "authors": ["Robert F. Rosin"], "id": "597c20e184da8f265bcd5e4a5a1dbe2be32d9283", "title": "Contemporary Concepts of Microprogramming and Emulation", "references": []}, {"date": "1970", "abstract": "Dynamic microprogramnming (i. e., utilizing a READ/ WRITE microstorage) allows the structure of a computer to be altered to suit a problem at hand and results in major efficiencies (an order of magnitude) in running time for nonarithmetic programs (e. g., compilers).", "authors": ["Robert W. Cook", "Michael J. Flynn"], "id": "e46faa52ff86efb77b35497699d0910fed41dcde", "title": "System Design of a Dynamic Microprocessor", "references": []}, {"date": "1977", "abstract": "<italic>We introduce a fundamental propositional logical system for describing correctness, termination and equivalence of programs. We define a formal syntax and semantics for the <underline>propositional modal logic of programs</underline> and give several consequences of the definition. Principal conclusions are that deciding satisfiability requires time</italic> d<supscrpt>n/log n</supscrpt><italic>for some</italic> d > 1 <italic>and that satisfiability, even in an extended system, can be decided in nondeterministic time</italic> c<supscrpt>n</supscrpt><italic>for some</italic> c. <italic>We provide applications of the decision procedure to regular expressions, Ianov schemes, and classical systems of modal logic.</italic>", "authors": ["Michael J. Fischer", "Richard E. Ladner"], "id": "cbddd72855580c2ad8ca52f477edd0385a3b78cf", "title": "Propositional modal logic of programs", "references": []}, {"date": "1972", "abstract": "A method of analyzing directed graphs by establishing a particular ordering for the nodes is presented, and properties of the ordered graph are derived. Both the method and the resultant order are interesting graph-t heoretically, and also practically: the method is quite efficient, and the results of the analysis are part icularly useful to an object code optimizer for programs. The paper includes examples of interesting graphs, and a conjecture about a refinement of the method.", "authors": ["C. P. Earnest", "K. G. Balke", "J. Anderson"], "id": "ef6ec2f8b3b15b7fddbab372158bc8001ff3a302", "title": "Analysis of Graphs by Ordering of Nodes", "references": ["a98f508786064d07b4aa6c4d7482e97fe9bf4a11", "513dbf8b19e9cf0425fb2102e181acb5f5ff3ba2"]}, {"date": "1969", "abstract": "The literature is surveyed beginning with the first paper published in 1951. At that time microprogrammmg was proposed primarily as a means for designing the control umt of an otherwme conventmnal digital computer, although the possible use of a read/write control memory was noted. The survey reveals the way in which interest has successively developed in the following aspects of the subject: stored logic, the apphcatmn of m]croprogrammmg to the design of a range of computers, emulatmn, m]croprogrammmg m support of software, and read/write control memories. The bibliography includes 55 papers.", "authors": ["Maurice V. Wilkes"], "id": "d593f7584854d61e32ce345fb88cf192ef5f60b0", "title": "The Growth of Interest in Microprogramming: A Literature Survey", "references": []}, {"date": "1969", "abstract": "Methods of analyzing the control flow and data flow of programs during compilation are applied to transforming the program to improve object time efficiency. Dominance relationships, indicating which statements are necessarily executed before others, are used to do global common expression elimination and loop identification. Implementation of these and other optimizations in OS/360 FORTRAN H are described.", "authors": ["Edward S. Lowry", "C. W. Medlock"], "id": "15428e9f83e0f3aaa61fe57f4cb663450c4e7bc0", "title": "Object code optimization", "references": []}, {"date": "1976", "abstract": "Data types are an important design tool because they allow freedom of abstraction. Thus, they are useful for constructing large software systems, including operating systems. It is shown that when dealing with problems of concurrency, the use of path expressions, which are associated with data, makes the task of verification simpler than when the synchronization conditions are associated with programs.", "authors": ["Lawrence Flon", "A. Nico Habermann"], "id": "de35370eaabebeb777c3e7ffc468712bea3bfc83", "title": "Towards the construction of verifiable software systems", "references": []}, {"date": "1977", "abstract": "Abstract : This paper presents the weakest preconditions which describe weak correctness, blocking, deadlock, and starvation for nondeterministic programs. A procedure for converting parallel programs to nondeterministic programs is described, and the correctness of various example parallel programs is treated in this manner. Among these are a busy-wait mutual exclusion scheme, and the problem of the Five Dining Philosophers.", "authors": ["Lawrence Flon", "Norihisa Suzuki"], "id": "1cc8d0f37ba80b1dabb6ed3f7028e6746f79dd1a", "title": "Nondeterminism and the Correctness of Parallel Programs", "references": []}, {"date": "1977", "abstract": "Abstract : This thesis applies and extends mathematical program verification to systems programs. The design methodology is based upon the use of abstract data types and the construction and verification of both specifications and implementations for them. The abstract data type is a means of modularization which encapsulates the representation of a data structure and the algorithms which operate directly upon it. The specification technique appeals to various mathematical structures (e.g. sets and sequences) to describe an abstract state for objects of a given type. The correctness of the formal specifications is cast in terms of the proof of certain invariant properties of the abstract state. An axiomatic proof rule is given to formulate the theorems necessary for proving the invariance of predicates across formal specifications. The applicability of the methodology to operating systems is explored. It is found that a hierarchical decomposition is most amenable to verification, and that the implementation language used is a function of that hierarchy. The example of a process dispatcher module of a hypothetical operating system is used to illustrate the process of design, specification, implementation, and verification using the methodology. Various properties are proven of the abstract specifications, including one representation of the concept of fair service. Programs are then written for the specifications and their correctness is verified.", "authors": ["Lawrence Flon"], "id": "4ee419815cfb15f2088fa1733c71906f3fbcec37", "title": "On the design and verification of operating systems.", "references": []}, {"date": "1972", "abstract": "Formalization of a well-defined synchronization mechanism can be used to prove that concurrently running processes of a system communicate correctly. This is demonstrated for a system consisting of many sending processes which deposit messages in a buffer and many receiving processes which remove messages from that buffer. The formal description makes it very easy to prove that the buffer will neither overflow nor underflow, that senders and receivers will never operate on the same message frame in the buffer nor will they run into a deadlock.", "authors": ["A. Nico Habermann"], "id": "631a6083389feb859cba144df8401d27c5c089a6", "title": "Synchronization of communicating processes", "references": ["3fea018ca5e6fecf60a90c2612391f9805c86c15"]}, {"date": "1975", "abstract": "Semantic Scholar extracted view of \"Abstract data-types as initial algebras and correctness of data representations\" by Joseph A. Goguen", "authors": ["Joseph A. Goguen"], "id": "1f966eaa651fb021be225a8c56c9121759059001", "title": "Abstract data-types as initial algebras and correctness of data representations", "references": []}, {"date": "1977", "abstract": "CLU is a new programming language designed to support the use of abstractions in program construction. Work in programming methodology has led to the realization that three kinds of abstractions, procedural, control, and especially data abstractions, are useful in the programming process. Of these, only the procedural abstraction is supported well by conventional languages, through the procedure or subroutine. CLU provides, in addition to procedures, novel linguistic mechanisms that support the use of data and control abstractions. This paper provides an introduction to the abstraction mechanisms in CLU. By means of programming examples, we illustrate the utility of the three kinds of abstractions in program construction and show how CLU programs may be written to use and implement abstractions. We also discuss the CLU library, which permits incremental program development with complete type-checking performed at compile-time.", "authors": ["Barbara Liskov", "Alan Snyder", "Russell R. Atkinson", "Craig Schaffert"], "id": "3596e5121dd74f173400cd38eea4bef68a2b475d", "title": "Abstraction mechanisms in CLU", "references": []}, {"date": "1976", "abstract": "Semantic Scholar extracted view of \"A Discipline of Programming\" by Edsger W. Dijkstra", "authors": ["Edsger W. Dijkstra"], "id": "5ada347492332825d38c7f7cfcc760842476ee52", "title": "A Discipline of Programming", "references": []}, {"date": "1976", "abstract": "This paper develops four related architectural principles which can guide the construction of error-tolerant operating systems. The fundamental principle, system closure, specifies that no action is permissible unless explicitly authorized. The capability based machine is the most efficient known embodiment of this principle: it allows efficient small access domains, multiple domain processes without a privileged mode of operation, and user and system descriptor information protected by the same mechanism. System closure implies a second principle, resource control, that prevents processes from exchanging information via residual values left in physical resource units. These two principles enable a third, decision verification by failure-independent processes. These principles enable prompt error detection and cost-effective recovery. Implementations of these principles are given for process management, interrupts and traps, store access through capabilities, protected procedure entry, and tagged architecture.", "authors": ["Peter J. Denning"], "id": "a02ef36214954043aacaa8c0468db84211bae798", "title": "Fault Tolerant Operating Systems", "references": []}, {"date": "1973", "abstract": "Semantic Scholar extracted view of \"An introduction to modal logic, 2e \u00e9d., 1 vol\" by Gwennan E. Hughes et al.", "authors": ["Gwennan E. Hughes", "John V. Cresswell"], "id": "ab99c14cdab69a960a8346ed952eda3e6332f240", "title": "An introduction to modal logic, 2e \u00e9d., 1 vol", "references": []}, {"date": "1978", "abstract": "In this paper how a pure denotative (nonprocedural) language based on the lambda calculus can be provided with purely denotative analogs of the various constructs - such as while loops, blocks, case statements and the like - of conventional imperative structured programming languages. They can be simulated quite adequately using only two simple tools: phrases, which are compound expressions not unlike blocks, and pronouns, special variables not unlike keywords between which certain relationships are 'understood' to hold.", "authors": ["William W. Wadge"], "id": "3ce1df838461ea6247b2a0e33d62456c9edac051", "title": "Programming Constructs for Nonprocedural Languages", "references": ["6eb3e06050e8f3287d3fd9def9e49e10eda80817", "3fdae4603265209ddf420cfaa9cbd0286c567c6c", "27dd189065bd8847a8ec8f27553282df67a42d3e", "d24b2214c2eaaddc6ac644c7e334093d2926fb03"]}, {"date": "1974", "abstract": "A computer of unusual architecture is described that achieves highly parallel operation through use of a data-flow program representation. The machine is especially suited for signal processing computations such as waveform generation, modulation, and filtering, in which a group of operations must be performed once for each sample of the signals being processed. The difficulties of processor switching and memory/processor interconnection arising in attempts to adapt Von Neuman type computers for parallel operation are avoided by an organization in which sections of the machine communicate through transmission of information packets, and delays in packet transmission do not compromise effective utilization of the hardware. The design concept is especially suited to implementation using asynchronous logic and large-scale integrated circuits. Application of the concepts to generalized data-flow program languages is under study.", "authors": ["Jack B. Dennis", "David P. Misunas"], "id": "4721b915d14351cb5663163f0202c880b2536d44", "title": "A computer architecture for highly parallel signal processing", "references": []}, {"date": "1951", "abstract": "Abstract : To what kinds of events can a McCulloch-Pitts nerve net respond by firing a certain neuron? More generally, to what kinds of events can any finite automaton respond by assuming one of certain states? This memorandum is devoted to an elementary exposition of the problems and of results obtained on it during investigations in August 1951.", "authors": ["Stephen Cole Kleene"], "id": "a496212ca3444e1e14b0668b82e2459d02dc275a", "title": "Representation of Events in Nerve Nets and Finite Automata", "references": []}, {"date": "1977", "abstract": "This paper describes the fundamentals and some of the details of task communication in DEMOS, the operating system for the CRAY-1 computer being developed at the Los Alamos Scientific Laboratory. The communication mechanism is a message system with several novel features. Messages are sent from one task to another over links. Links are the primary protected objects in the system; they provide both message paths and optional data sharing between tasks. They can be used to represent other objects with capability-like access controls. Links point to the tasks that created them. A task that creates a link determines its contents and possibly restricts its use. A link may be passed from one task to another along with a message sent over some other link subject to the restrictions imposed by the creator of the link being passed. The link based message and data sharing system is an attractive alternative to the semaphore or monitor type of shared variable based operating system on machines with only very simple memory protection mechanisms or on machines connected together in a network.", "authors": ["Forest Baskett", "John H. Howard", "John T. Montague"], "id": "d1deecbd75cfc7e44b01d314ea72556ab4b924c2", "title": "Task communication in DEMOS", "references": []}, {"date": "1976", "abstract": "This paper explores a technique for proving the correctness and termination of programs simultaneously. This approach, which we call the intermittent-assertion method, involves documenting the program with assertions that must be true at some time when control is passing through the corresponding point, but that need not be true every time. The method, introduced by Knuth and further developed by Burstall, promises to provide a valuable complement to the more conventional methods.\n We first introduce and illustrate the technique with a number of examples. We then show that a correctness proof using the invariant assertion method can always be expressed using intermittent assertions instead, but argue that the reverse is not always the case. The method can also be used just to prove termination, and any proof of termination using the conventional well-founded sets approach can be rephrased as a proof using intermittent assertions. Finally, we show how the method can be applied to prove the validity of program transformations and the correctness of continuously operating programs.\n This research was supported in part by the Advanced Research Projects Agency of the Department of Defense under Contract DAHC15-73-C-0435, by the National Science Foundation under Grant GJ-36146, by the Office of Naval Research under Contract N00014-75-C-0816, and by a grant from the United States-Israel Binational Science Foundation (BSF), Jerusalem, Israel.", "authors": ["Zohar Manna", "Richard J. Waldinger"], "id": "7a36165ece98987792e56d625f32625c73525557", "title": "Is \u201csometime\u201d sometimes better than \u201calways\u201d?: Intermittent assertions in proving program correctness", "references": ["c43ebe201e2015d84cf44d8c17438ddbeddf3af9", "27f89af321ec6c641ecd15325d882b4827f5d45f"]}, {"date": "1975", "abstract": "This paper describes the capability based protection mechanisms provided by the Hydra Operating System Kernel. These mechanisms support the construction of user-defined protected subsystems, including file and directory subsystems, which do not therefore need to be supplied directly by Hydra. In addition, we discuss a number of well known protection problems, including Mutual Suspicion, Confinement and Revocation, and we present the mechanisms that Hydra supplies in order to solve them.", "authors": ["Ellis S. Cohen", "David Jefferson"], "id": "3e7dd4ce45d8db958c9adaa9a287ab439fdf37ab", "title": "Protection in the Hydra Operating System", "references": []}, {"date": "1965", "abstract": "A fact confirmed by experience is that, presented with any program of a certain minimal complexity, an experienced programmer can always improve the program, make it shorter, faster or reduce the amount of temporary storage it needs. Since it may take a person anywhere front minutes to weeks to perform such an improvement, and since experienced programmers are rare, the very practical question arises whether such improvements could be made automatically by a computer program. The question is not whether this could be done in pr inciple-one can always resort to exhaustive procedures like examining all possible programs which fit, into a given memory-bu t whether it is practically feasible. A person certainly does not resort to exhaustive proeedures in simplifying a program. But his way of proceeding is based mainly on a feature which one cannot, at present, hope to incorporate into a simplifying program. He heavily relies on his understanding of the program: He knows the meaning of the variables (or memory locations) occurring, where they are used and where they are \"dead.\" I te knows the structure of data (e.g. that a certain region of memmT contains a square matrix). And finally, he has a large lore of mathematical identities at his disposal which he can readily use to transform expressions. We make no a t tempt here to automate this kind of program simplification, and we restrict ourselves to those simplifications which depend on the form of the program only, i.e. can be detected and proved to lead to an equivalent", "authors": ["J\u00fcrg Nievergelt"], "id": "59ca6576188bfbd30c139af9d642f54fddda1c8f", "title": "On the automatic simplification of computer programs", "references": []}, {"date": "1973", "abstract": "This paper describes a graphical programming language based on the concept of pure data flow sequencing of computations. Programs in this language are constructed through function definition and composition, and are based on the primitive notions of iteration, recursion, conditional expression, data replication, aggregation and selection, and the usual arithmetic and logical operations. Various useful programming devices such as the DO loop and, surprisingly, the memory cell are defined in terms of these primitives. Programs in this language are determinate in operation unless indeterminism is explicitly introduced. The utility of this language for designing and implementing operating systems is discussed.", "authors": ["Paul R. Kosinski"], "id": "6f50450244f1f91afab9ce876f69ffa4d0450714", "title": "A data flow language for operating systems programming", "references": []}, {"date": "1966", "abstract": "Discrete sequential systems, such as sampled data systems, discrete Markov processes, linear shift register generators, computer programs, sequential code generators, and prefixed comma-free codes, can be represented and studied in a uniform manner by directed graphs and their generating functions. In this paper the properties of the generating functions are examined from certain connectivity considerations of these graphs.", "authors": ["C. V. Ramamoorthy"], "id": "513dbf8b19e9cf0425fb2102e181acb5f5ff3ba2", "title": "Analysis of Graphs by Connectivity Considerations", "references": []}, {"date": "1975", "abstract": "An overview is presented of queueing theoretic modeling and analysis of interactive computing systems. While the emphasis is on recent advances, an introduction is given which briefly trices the historical development of this area to put current research in perspective. Models, methods of analysis, and applications to real systems are described. An extensive bibliography provides the references for a more in-depth study.", "authors": ["R. A. Muntz"], "id": "88b88cce902b1bf663d62e6629938b94f2c7e1b9", "title": "Analytic modeling of interactive systems", "references": ["0c070dbb7fe2d05485e6b97ef993ec397413087f", "156243b2ae3024a3f1babc0c6b36e6d840a5396f", "77dad2be1c11d646ae106908387349b9391199c9", "52fdb95c28218852330380b26ab68906b754bc46", "c5435b6cf613c54c08c954df29525946e1e91e1b", "72f340d4f8b3598bc7688ff7e98c37d5b50c9c4b", "52a9521fcf01a1558daad19975e8b39a21bd9d3d", "0ca7f5346a1b4eddc0da29744669fe1239e78aca", "cc33117c20b0d04b71af1357bed5fddf1362184b", "5aa487e695bc5387af4d4dfc3a91eb38a3c62e54"]}, {"date": "1963", "abstract": "Semantic Scholar extracted view of \"The Theory of Graphs and Its Applications.\" by F. Harary et al.", "authors": ["F. Harary", "Claude Berge"], "id": "a98f508786064d07b4aa6c4d7482e97fe9bf4a11", "title": "The Theory of Graphs and Its Applications.", "references": []}, {"date": "1975", "abstract": "This thesis presents an axiomatic method for proving certain correctness properties of parallel programs. Axioms and inference rules for partial correctness are given for two parallel programming languages: The General Parallel Language and the Restricted Parallel Language. The General Language is flexible enough to represent most standard synchronizers (e.g. semaphores, events), so that programs using these synchronizers may be verified using the GPL deductive system. However, proofs for GPL programs are in general quite complex. The Restricted Language reduces this complexity by requiring shared variables to be protected by critical sections, so that only one process at a time has access to them. This discipline does not significantly reduce the power of the language, and it greatly simplifies the process of program verification. Although the axioms and inference rules are primarily intended for proofs of partial correctness, there are a number of other important properties of parallel programs. We give some practical techniques which use information obtained from a partial correctness proof to derive other correctness properties, including program termination, mutual exclusion, and freedom from deadlock. A number of examples of such proofs are given. Finally, the axioms and inference rules are shown to be consistent and complete (in a special sense) with respect to an interpretive model of parallel execution. Thus the deductive system gives an accurate description of program execution and is powerful enough to yield a proof of any true partial correctness formula.", "authors": ["Susan S. Owicki"], "id": "6210de8384c5d5aa2fcc940d8ebc3e3f6f9f927d", "title": "Axiomatic Proof Techniques for Parallel Programs", "references": []}, {"date": "1977", "abstract": "This paper presents the architecture of a highly concurrent multiprocessor which runs programs expressed in data flow notation. Sequencing of data flow instruction execution depends only on the availability of operands required by instructions. Because data flow instructions have no side effects, unrelated instructions can be executed concurrently without interference if each has its required operands. The data flow multiprocessor is hierarchically constructed as a network of simple modules. All module interactions are asynchronous. The principal working elements of the machine are a set of activation processors, each of which performs the execution of one invocation of a data flow procedure held in a local memory within the processor. A pipeline of logical units within each processor executes several concurrently active instructions. All data flow operations are performed within single processors except procedure calls, which cause the creation of new activations in other processors, and operations on large data structures, which are performed by structure controller modules using values stored in a central memory. Concurrency within a data flow procedure provides a processor with something to do while a slow operation is being processed. The behavior of the machine has been specified by a formal description language and has been shown to correctly implement the data flow language. The principal advantages of the data flow multiprocessor over conventional designs are reduced complexity of the processor-memory connection, greater use of pipelining, and a simpler representation and implementation of concurrent activity.", "authors": ["James E. Rumbaugh"], "id": "29bc5d481e54d8a8e3641d15aa49ef1dff27f5e8", "title": "A Data Flow Multiprocessor", "references": []}, {"date": "1970", "abstract": "This paper describes the philosophy and structure of a multi-programming system that can be extended with a hierarchy of operating systems to suit diverse requirements of program scheduling and resource allocation. The system nucleus simulates an environment in which program execution and input/output are handled uniformly as parallel, cooperating processes. A fundamental set of primitives allows the dynamic creation and control of a hierarchy of processes as well as the communication among them.", "authors": ["Per Brinch Hansen"], "id": "e8459f1a9e01327a56e45176fc9c0ec8296393ba", "title": "The nucleus of a multiprogramming system", "references": []}, {"date": "1974", "abstract": "In recent years there has been a great deal of discussion concerned with the advantages of a top-down structured style of programming and the question of whether or not to banish the goto control statement. The most compelling arguments for the retention of goto have referred to common programming situations where the absence of goto causes some awkwardness in the programming task or loss of efficiency for the running program. We propose a new control statement which alleviates many of these known difficulties while maintaining the philosophy of structured control. The new statement is well-matched to top-down programming and in some respects allows program modifications to be performed more easily.", "authors": ["Charles T. Zahn"], "id": "d24b2214c2eaaddc6ac644c7e334093d2926fb03", "title": "A control statement for natural top-down structured programming", "references": []}, {"date": "1978", "abstract": "An architecture for a highly modular, recursively structured class of machines is presented. DDM1 is an instance of such a machine structure, and is capable of executing machine language programs which are data driven nets. These nets may represent arbitrary amounts of concurrency as well as arbitrary amounts of pipelining. DDM1 is a fully distributed multiprocessing system composed of completely asynchronous modules. The architecture allows for limitless physical extensibility without necessitating special programming or special hardware to support individual machines of widely varying sizes. DDM1 is capable of automatically and dynamically allocating concurrent tasks to the available physical resources. The essential characteristics of the highly parallel, pipelined machine language are also described along with its method for execution on DDM1.", "authors": ["A. L. Davis"], "id": "449fc430a40029a22777e72974fb85ae579798d4", "title": "The architecture and system method of DDM1: A recursively structured Data Driven Machine", "references": []}, {"date": "1971", "abstract": "The common features of third generation operating systems are surveyed from a general view, with emphasis on the common abstractions that constitute at least the basis for a \u201ctheory\u201d of operating systems. Properties of specific systems are not discussed except where examples are useful. The technical aspects of issues and concepts are stressed, the nontechnical aspects mentioned only briefly. A perfunctory knowledge of third generation systems is presumed.", "authors": ["Peter J. Denning"], "id": "98454b3b6db6a1eca9a9addb00af483e7dba248c", "title": "Third Generation Computer Systems", "references": []}, {"date": "1975", "abstract": "The authors investigate the strong verification of programs using the concept of predicate transformer introduced by Dijkstra (1974). They show that every do-while program has a loop invariant that is both necessary and sufficient proving strong verification. This loop invariant is shown to be the least fixpoint of a recursive function mapping predicates to predicates that is defined by the program and the postcondition.", "authors": ["Sanat K. Basu", "Raymond T. Yeh"], "id": "9e2fbe8f42e7fc5b7cb6a784137b830fcebc6345", "title": "Strong verification of programs", "references": ["9a9e7645cf325745abb85210e498a647b3512843", "85bb63adc85a812d61f845813b792033c40e214f", "6d6aacdcc42b968c6d1317dc1502d8f9f7da4309", "791269bb32735512f13938cbef74ca97713fb7fc", "5d8056e326d4199d157a17fbeee97a7349d2824c", "9ef033c165f242d090e911017beb620a307b279e", "75ffaa9fd92e72c10c046f2cab543d46d3b25a35", "567cbc604dc30b1ae9ce2708128cd5e382363d01", "12ef8d15690b612d36e30f124a2c5e7511298800"]}, {"date": "1974", "abstract": "For serious students of structured programming, and also for language designers, Knuth's \"Structured Programming with go to Statements\" is probably the paper to read. It is by far the most complete description of structured programming of all the selections in this book. Even though it originally was published in late 1974, Knuth's ideas have not aged a bit. \n \nThe title, as Knuth acknowledges, is somewhat of a misnomer: \" . . . I should confess that the title of this article was chosen primarily to generate attention.\" Indeed, he is not arguing that structured programming always should be carried out with goto statements; instead, as he says, \"what I am really doing is striving for a reasonably well-balanced viewpoint about the proper role of go to statements.\" Like a number of other authors whose views are presented in this book, Knuth seems deeply concerned that the average programmer will over-react to the \"no goto\" philosophy. Reflecting that concern, the first few pages of Knuth's paper give an indication of the fanaticism of its time --- a fanaticism that has largely died away. \n \nFor history buffs, Knuth has carefully documented the origins of structured programming. He claims that the programming style now referred to as \"structured programming\" actually was first practiced by D.V. Schorre in 1960, and that was followed by publication of the work of Peter Naur in 1963 and of George Forsythe in 1964 -- all of this before the better-known works of Dijkstra, and of Bom and Jacopini. \n \nThere are three major themes in Knuth's paper, and avast number of examples to illustrate each one, One theme --- a familiar one, if you've read many of the other papers in this book is that a program needs to be rewritten several times before it can beconsidered a truly good program. As Knuth says, \" . . . I learned again that I should always keep looking for improvements, even When I have a satisfactory program.\" Indeed, one gets the impression that Knuth views this as the major virtue of structured programming: The requirement to eliminate gotos forces the programmer to rewrite and thus think more about what he was trying to accomplish with his program. \n \nA second theme --- one that you won't find in any of the other papers - is that efficiency is important in some programming applications, and that the programmer needs the freedom to use gotos to optimize critical portions of a program. Actually, Martin Hopkins makes the same point in \"A Case for the GOTO\" [Paper 9], but he does so much less forcefully than Knuth. Knuth recognizes the danger of overemphasizing optimization; as he says, \"premature optimization is the root of all evil.\" And, he does state that only three percent of the code in a typical program ever needs to be optimized; but for that critical three percent, he demonstrates that structured code often is twenty to thirty percent less efficient than equivalent code with gotos. Unfortunately, the point is repeated so many times, with so many examples, that the average reader is likely to conclude that Knuth is obsessed with efficiency. However, given the number of authors exhorting programmers to ignore efficiency altogether, it is probably very healthy to have someone argue for a careful consideration of program efficiency. \n \nKnuth's third main theme is that structured programming could be made much more practical with the addition of a few language constructs. In addition to constructs like LEAVE and BREAK, Knuth argues for a \"situation\" construct patterned after a proposal originally made by C.T. Zahn. \n \nIn keeping with the heretical title of his paper, Knuth introduces a fourth theme: There are times when the programmer should put gotos into his code, rather than take them out. For example, gotos can be used to convert recursion to iteration; or to implement coroutines; or to eliminate Boolean variables by branching into common code. In this context, Knuth suggests the following strategy: First, write the program in a structured manner to convince yourself that it is correct; then, transform it into an efficient program, possibly by introducing some goto statements; and, finally, leave the original structured code behind as documentation, so that subsequent readers can understand how the transformation took place. Whether the average programmer would go through these steps in an orderly, formal way is something worth pondering. My own suspicion is that it won't work, but I'm often branded a skeptic. \n \nTo conclude: It's probably best not to read Knuth's paper in one sitting, for you almost surely will become mentally fatigued by the eight major examples, each of which is rewritten three or four (or more!) times in an ALGOL-like pseudocode. Read the paper piece by piece, and do read the code --- you'll learn a lot from it!", "authors": ["Donald E. Knuth"], "id": "3fdae4603265209ddf420cfaa9cbd0286c567c6c", "title": "Structured Programming with go to Statements", "references": ["ed930c69cdc66f983c5abfd041ce9fef3565c08b", "cd52c6a8690781f294115c554bfab8bf3f461487", "cb5eebdd166a6cb0d1e2de65fb70ea1c913ef00d", "244485ba5b8a0fcb678af91dec91d12d13f0f1d3", "b3d2c98c190a3b54bda9ce3ab670549d25579e6e", "f110e72d78c9dc557c1a4ace23d474200eed0715", "5a19db7d827f623143e92bca8c082de5c014d59c"]}, {"date": "1972", "abstract": "Sandia Laboratories has established a consulting service designed to improve the overall efficiency of Fortran production programs in use by the scientific community. Since this service was inaugurated in May, 1971, approximately 40 jobs have been reviewed for optimization with varying degrees of success. In several instances it was not possible to improve the operating efficiency of a given program. However, in most cases it has been possible to achieve worthwhile reductions in the use of critical machine resources (primary attention is directed at use of central processor time and allocation of central memory). Average reduction of CPU usage is approximately 30% with one recorded instance of a 97% reduction.This paper addresses the problems encountered in launching and promoting a program optimization service in an open shop environment among users with widely varying levels of programming to indicate the method used to isolate and correct the sections of code that preempt the vital computer resources.", "authors": ["Sidney B. Gasser"], "id": "35bd93848bcc4b6e2f0bee9326651e1df0090d40", "title": "Program optimization", "references": []}, {"date": "1970", "abstract": "The user microprogrammable computer as the fourth generation computer is investigated from the user's point of view. In the first part of the paper microprogramming and its concept as well as the problems and requirements incurring its use in various applications are discussed. The current status of the microprogrammed computer is also studied to indicate the differences of philosophy in microprogramming. A number of suggestions are made for the design of fourth generation user-microprogrammable computers.", "authors": ["C. V. Ramamoorthy", "Masahiro Tsuchiya"], "id": "4c004e924b239800a6f249f85e96f00b80617d5f", "title": "A study of user-microprogrammable computers", "references": []}, {"date": "1969", "abstract": "State-of-the-art advances---in particular, anticipated advances generated by LSI---have given fresh impetus to research in the area of parallel processing. The motives for parallel processing include the following:\n 1. Real-time urgency. Parallel processing can increase the speed of computation beyond the limit imposed by technological limitations.\n 2. Reduction of turnaround time of high priority jobs.\n 3. Reduction of memory and time requirements for \"housekeeping\" chores. The simultaneous but properly interlocked operations of reading inputs into memory and error checking and editing can reduce the need for large intermediate storages or costly transfers between members in a storage hierarchy.\n 4. An increase in simultaneous service to many users. In the field of the computer utility, for example, periods of peak demand are difficult to predict. The availability of spare processors enables an installation to minimize the effects of these peak periods. In addition, in the event of a system failure, faster computational speeds permit service to be provided to more users before the failure occurs.\n 5. Improved performance in a uniprocessor multi-programmed environment. Even in a uniprocessor environment, parallel processable segments of high priority jobs can be overlapped so that when one segment is waiting for I/O, the processor can be computing its companion segment. Thus an overall speed up in execution is achieved.", "authors": ["C. V. Ramamoorthy", "Mario J. Gonzalez"], "id": "ed7fabd99ecb6069ae19b5efe99c00baab6904a3", "title": "A survey of techniques for recognizing parallel processable streams in computer programs", "references": []}, {"date": "1975", "abstract": "The paper describes a new programming language for structured programming of computer operating systems. It extends the sequential programming language Pascal with concurrent programming tools called processes and monitors. Part I explains these concepts informally by means of pictures illustrating a hierarchical design of a simple spooling system. Part II uses the same example to introduce the language notation. The main contribution of Concurrent Pascal is to extend the monitor concept with an explicit hierarchy of access rights to shared data structures that can be stated in the program text and checked by a compiler.", "authors": ["Per Brinch Hansen"], "id": "1548c821b925cee264dd58a6a193bc31ceb62502", "title": "THE PROGRAMMING LANGUAGE CONCURRENT PASCAL PER BRINCH HANSEN", "references": ["f1d5f15fdd8db9af0f3a523dd00f4f31ef14c965", "2de8bb082dba785d6a5501116d89097d396f03cc", "e8459f1a9e01327a56e45176fc9c0ec8296393ba", "ff568f642a545c7d62df73e7798e3d008b509ea5", "6faf40fa84ceda43ecb586f4bfe54ad549c7e072", "59668eba7441f29dc42719651bf5ebd12c902975", "365f1c3b6634112a6966d15bee88b5ea7d2e5808", "4044d246faecb78d4cadc25b6c2f83fa062174a2", "c917651f961bbbc314f45d9b4f3c710f11a36bea"]}, {"date": "1967", "abstract": "This paper describes the methods employed in the floating-point area of the System/360 Model 91 to exploit the existence of multiple execution units. Basic to these techniques is a simple common data busing and register tagging scheme which permits simultaneous execution of independent instructions while preserving the essential precedences inherent in the instruction stream. The common data bus improves performance by efficiently utilizing the execution units without requiring specially optimized code. Instead, the hardware, by 'looking ahead' about eight instructions. automatically optimizes the program execution on a local basis. \n \nThe application of these techniques is not limited to floating-point arithmetic or System/360 architecture. It may be used in almost any computer having multiple execution units and one or more 'accumulators.' Both of the execution units, as well as the associated storage buffers, multiple accumulators and input /output buses, are extensively checked.", "authors": ["R. M. Tomasulo"], "id": "d0032d121eb78d06acf98a313fced11dd3f3fb38", "title": "An efficient algorithm for exploiting multiple arithmetic units", "references": ["82e3bc6a6a9a4772f07ec06b4a69c81c1165c6d4"]}, {"date": "1976", "abstract": "An axiomatic method for proving a number of properties of parallel programs is presented. Hoare has given a set of axioms for partial correctness, but they are not strong enough in most cases. This paper defines a more powerful deductive system which is in some sense complete for partial correctness. A crucial axiom provides for the use of auxiliary variables, which are added to a parallel program as an aid to proving it correct. The information in a partial correctness proof can be used to prove such properties as mutual exclusion, freedom from deadlock, and program termination. Techniques for verifying these properties are presented and illustrated by application to the dining philosophers problem.", "authors": ["Susan S. Owicki", "David Gries"], "id": "33689b52b67bb8822720f0a54cdcac89b00873d9", "title": "Verifying properties of parallel programs: an axiomatic approach", "references": ["4044d246faecb78d4cadc25b6c2f83fa062174a2", "5d8056e326d4199d157a17fbeee97a7349d2824c"]}, {"date": "1971", "abstract": "This study suggests a method for performance evaluation in a multiprogramming environment. The joint effect of two major cost elements - the processing time and the memory space - have been investigated. This involves two issues; 1) optimal memory loading and sizing, and 2) central site timing analysis. The latter has been approached mathematically with a cyclic queuing model; the memory space utilization was studied with a linear optimization model. Varying the values of the major cost carrying factors, through iteration, the authors investigate marginal utilities and search for the concept of an optimally balanced system.", "authors": ["Sant R. Arora", "Antonio Gallo"], "id": "cc33117c20b0d04b71af1357bed5fddf1362184b", "title": "The optimal organization of multiprogrammed multi-level memory", "references": []}, {"date": "1977", "abstract": "The inductive assertion method is generalized to permit formal, machine-verifiable proofs of correctness for multiprocess programs. Individual processes are represented by ordinary flowcharts, and no special synchronization mechanisms are assumed, so the method can be applied to a large class of multiprocess programs. A correctness proof can be designed together with the program by a hierarchical process of stepwise refinement, making the method practical for larger programs. The resulting proofs tend to be natural formalizations of the informal proofs that are now used.", "authors": ["Leslie Lamport"], "id": "2a41593d839323ddcd8597a22e4df6e398d4f98c", "title": "Proving the Correctness of Multiprocess Programs", "references": ["7748ee206fbdf2d6769f9b645abb6a9004a49888", "559b4b532ee8dd9fffc6c24f3f578cbd3c867b2c", "9e710b50eb6ca202fa3f330cdc4bfa6cfd7adc9e", "6210de8384c5d5aa2fcc940d8ebc3e3f6f9f927d", "114108b350866595cebc14a96a8cb6f7aede7c93", "631a6083389feb859cba144df8401d27c5c089a6", "04edf2feffa2d2053e2d0ccc997cd51ba3b2b729", "a9784da374ca7f0ca091618e602f81831181d9f5", "efa53518898247c3d2aad15944036c062c2bc617", "fa1ba526c7f1fc8437c3b55c6798526c387073b9"]}, {"date": "2004", "abstract": "SummaryA language for parallel programming, with a primitive construct for synchronization and mutual exclusion, is presented. Hoare's deductive system for proving partial correctness of sequential programs is extended to include the parallelism described by the language. The proof method lends insight into how one should understand and present parallel programs. Examples are given using several of the standard problems in the literature. Methods for proving termination and the absence of deadlock are also given.", "authors": ["Susan S. Owicki", "David Gries"], "id": "559b4b532ee8dd9fffc6c24f3f578cbd3c867b2c", "title": "An axiomatic proof technique for parallel programs I", "references": ["4c88e871290bbf08bd2b71412e4430a4c89350dd", "35f1fa0934849171ab651025e6c71980fedd4818", "7fa540f88688a22436749db28c4940c559e885d5", "682fbc4a30842faa97adb7c0563a4571033a6c70", "ea548fbbdfe46a37dd0edaac4a8d116343b7c39e", "f4a00bdc978e316a85509d9b872dfc996c3da1cd", "cf4570f4801181a2f8e1b9aac77c180d0206834e", "6210de8384c5d5aa2fcc940d8ebc3e3f6f9f927d", "efa53518898247c3d2aad15944036c062c2bc617", "fa1ba526c7f1fc8437c3b55c6798526c387073b9"]}, {"date": "1972", "abstract": "Abstract : An annotated bibliography of all reports published under these contracts, with supplementary remarks. (Author)", "authors": ["Walter F. Freiberger"], "id": "0ca7f5346a1b4eddc0da29744669fe1239e78aca", "title": "Statistical Computer Performance Evaluation", "references": []}, {"date": "1974", "abstract": "Mathematical models based on queueing theory are widely used in the analysis of computer system performance. As in the case of other engineering disciplines, these models never correspond exactly to the real systems they are intended to represent. However, if the associated error terms are sufficiently small the models can still serve as valuable tools for estimating performance levels in specific cases and for studying the factors which influence overall system behavior. In this paper we examine some error terms which arise when the familiar M/G/1 queueing model is used to predict expected response times and queue lengths in systems which contain only a finite number of sources.", "authors": ["Jeffrey P. Buzen", "P. S. Goldberg"], "id": "52a9521fcf01a1558daad19975e8b39a21bd9d3d", "title": "Guidelines for the use of infinite source queueing models in the analysis of computer system performance", "references": []}, {"date": "1961", "abstract": "In many problems of economic theory we need to use aggregates. The general Walrasian system and its more modern dynamic extensions are relatively barren of results for macroeconomics and economic policy. Hence, in our desire to deal with such questions we use highly aggregated systems by sheer necessity, often without having much more than the same necessity as our justification. Perhaps the most important result to date for justifying aggregation under certain circumstances is the Lange-Hicks1 condition, about which we shall say more later.", "authors": ["Herbert A. Simon"], "id": "5aa487e695bc5387af4d4dfc3a91eb38a3c62e54", "title": "Aggregation of Variables in Dynamic Systems", "references": []}, {"date": "1974", "abstract": "Queueing network models are well suited for analyzing certain resource allocation problems associated with operating system design. An example of such a problem is the selection of the level of multiprogramming in virtual memory systems. If the number of programs actively competing for main memory is allowed to reach too high a value, trashing will occur and performance will be seriously degraded. On the other hand, performance may also suffer if the level of multiprogramming drops too low since system resources can become seriously under utilized in this case. Thus it is important for virtual memory systems to maintain optimal or near optimal levels of multiprogramming at all times.\n This paper presents an analytic model of computer system behavior which can be used to study multiprogramming optimization in virtual memory systems. The model, which explicitly represents the numerous interactions which occur as the level of multiprogramming varies, is used to numerically generate performance curves for representative sets of parameters. A simplified model consisting of a CPU and a single backing store device is then used to derive an approximate expression for the optimal level of multiprogramming. The simplified model is also used to examine the transient behavior of such systems.\n The mathematical model we present is based on some simplifying assumptions; in particular all programs executing in the system are supposed to be statistically identical. In this respect the model we present must be considered to be a theoretical explanation of a phenomenon (thrashing) observed in certain operating systems rather than an exact representation of reality. Certain assumptions of the mathematical model are relaxed in a simulation model where distribution functions of service times at the secondary memory and input-output devices are arbitrary; by comparison with the theoretical results we see that CPU utilization and throughput are not very sensitive to the specific forms of these distributions and that the usual exponential assumptions yield quite satisfactory results. The simulation model is also programmed to contain overhead. Again we observe that the mathematical model's predictions are in fair agreement with the useful CPU utilization predicted by the simulation experiments.", "authors": ["A. Brandwain", "Jeffrey P. Buzen", "Erol Gelenbe", "Dominique Potier"], "id": "72f340d4f8b3598bc7688ff7e98c37d5b50c9c4b", "title": "A model of performance for virtual memory systems", "references": []}, {"date": "1974", "abstract": "The design of the storage component is essential to the achieving of a good overall cost-performance balance in a computing system. \n \nA method is presented for quickly assessing many of the technological and structural possibilities that exist today for designing storage hierarchies. \n \nThe evaluation is based on a cycling queuing model of the computer system and its programming environment, which are taken into account by miss ratio curves.", "authors": ["Jan Gecsei", "Joseph A. Lukes"], "id": "52fdb95c28218852330380b26ab68906b754bc46", "title": "A Model for the Evaluation of Storage Hierarchies", "references": ["c07e33daa59d559accf32968f18aa84ca88640b0", "a968f4cb46a4743f981034322b091f6ad45cc77a", "2eea440182e0e9603948a82585c20ed2cd2138f8"]}, {"date": "1971", "abstract": "Models are developed to describe delays and backlogs at remote terminals polled in turn by a single computer. The effects modeled include transmission delays caused by line noise, and the number and types of terminals (passive input, and active or two-way response). Use is made of the diffusion approximation to state variables, the latter being especially relevant when the system is heavily loaded. A limited amount of mathematical and stimulationgenerated evidence attests to the adequacy of this approximation. KEY W O R D S A N D PHRASES: computer systems, line noise, diffusion, queues, terminals, buffers, probability, simulation, delays CR CATEGORIES : 6.20 1. Problem Statement The speed with which modern digital computers operate now makes possible the servicing of many, perhaps remotely located, terminals. Nevertheless, as rapidly as processing may occur, there are system features that tend to cause the occurrence of work backlogs at remote stations (e.g. stacks of cards awaiting transmission). Among these are the following: (i) The finite rate at which information may be read into local buffers, where it is then ready for transmission. (ii) The finite rate at which transmission of local buffer contents to the central computer system may occur. In fact, the t ime required to complete the transmission of the local buffer contents may be strongly affected by randomly occurring bursts of electronic noise caused by switching, weather, etc. Error-contaminated transmissions are signalled by pari ty check and are repeated until parities at sending and receiving stations agree. We explicitly model this process. (iii) The number of remote terminals tha t are to be serviced by the central computer system. (iv) The rate at which information (e.g. stacks of cards) is brought to the remote terminals. Typically, the lat ter will have random characteristics, so tha t even if the long-run transmission capabili ty of the system is entirely adequate, backup and delays occasionally occur. I t is the purpose of this paper to analyze the nature of the remote terminal backlogs mentioned, and to describe the likely delays in the initiation of processing. Some readers will recognize tha t we are here dealing with a complex queueing problem tha t is quite difficult to s tudy under the usual, perhaps now classical, * Present address: Naval Postgraduate School, Monterey, Ca. Research supported in part by National Science Foundation Grant GP8824 at Carnegie-Mellon University. Journal of the Association for Comput ing Machinery, Vol. 18, No. 3, J u l y 1971, pp. 405-415. 406 D O N A L D P. G A V E R assumptions (cf. Riordan [6]). Instead, we shall proceed approximately, using a probability model that is especially appropriate when demand rates are relatively large compared to transmission rates. For the kind of system outlined here it is often necessary to simulate system histories in order to obtain useful information-particularly, for example, when arrival rates at remote terminals are not stationary in time. Simulation has the virtue of being applicable in principle to a system of any complexity. Nevertheless, as practitioners know, it is often quite time-consuming to debug extremely complex programs. Moreover, the cost of repeatedly running complex programs in order first to debug and then to obtain valid estimates of, say, the expected backlog at a terminal can be prohibitive. I t is thus of value to have at hand flexible approximate methods that may serve as a check on-or even as a replacement for--straightforward faithful simulations. 2. Diffusion as a Backlog Model: An Introduction In order to explain our basic model for the backlog (number of cards present, for example) we consider the queue at a moment when it is large, as it will often be when demand rate nearly equals transmission rate and random fluctuations are present. Formally, let Q(t) = queue size at time t and (2.1) AQ(r) = the change in queue size over (t, t + r ) , i.e. = Q( t + r) Q ( t ) . Thus t might represent a specific moment or epoch, such as 9:00 h.~., and r is perhaps 10 minutes, so AQ(r) represents the net increase in backlog that occurs between t and t + r. In fact, AQ(r) = A ( r ) D(T), (2.2) where A ( r ) represents the number of arrivals in the time duration r, and D(r) the corresponding departures. Now transmission rate is relatively rapid, being perhaps at an average rate of about one or more cards per second, with inevitable variations owing to factor (ii) above. There are theoretical reasons, given subsequently, for assuming that D will typically vary independently and nearly normally (in a Gaussian manner) over disjoint r intervals of several minutes duration. Likewise, arrivals may be expected to behave in a similar manner; it is quite reasonable to assume that arrivals and departures-transmissions--are in this case statistically independent. Finally, then, changes in the backlog are, at least locally, approximately normally distributed: PIAQ(r) < x} .-~ 1 f_x-,,/~(,~) _ ~ ~ exp [-1\u20442 z 2] dz. (2.3) Thus we can conceive of the backlog, viewed on an appropriate time scale, as being the sum of normally distributed increments. The only complication is tha t the backlog is constrained to be positive, so that when Q(t) = 0 it is reflected up by the next arrival and continues as before. Journal of the Association for Computing Machinery, Vol. 18, No. 3, Ju ly 1971 Analysis of Remote Terminal Backlogs under Heavy Dema~d Conditions 407 Now it is well known tha t the normal distribution F ( x , t) l f~ -\"t)/~(t~) (27r)-t exp [-1\u20442 z 2] dz (2.4) satisfies the diffusion or heat equation: OF OF 2 02F (2.5) Ot u ~ + ~ Ox2 According to the previous discussion F describes the probabil i ty that the backlog Q does not exceed x provided the boundary at zero has not intervened. If, however, g < 0, then tim queue will occasionally empty ; to account for this effect we impose the boundary condition F(x, t) = 0 for x < 0 and solve (2.5). Actually we consider here only the steady-state solution tha t exists when tt < 0, i.e. when the expected number of departures per unit t ime tha t occur when a backlog exists exceeds the expected number of arrivals: u ~ (E[A(T)] E[D(T)])/T < 0. (2.6) In the long run (actually in a mat te r of one-quarter hour or less for the present application) OF/Ot = 0 for practical purposes. Familiar characteristic equation calculations then show that the distribution approaches F(x) = lira F(x, t) = A + B exp [(2#/o'2)x]. (2.7) t ~ Now F must be a distribution, so A = 1 and B = 1 . The distribution of Q is thus approximately exponential with tile parameters = lim T-'(E[A(T)] -E[D(T)]), t o ~ 2 a = limt_~ T-l (Var [A(r)] + Var [D(~-)]). (2.8) Although the above analysis is heuristic, it may be shown rigorously tha t the exponential (2.7) distribution is approached as ~ ---~ 0 through negative values; see for example Iglehart [4], Kingman [5], and Gaver [3]. For further illustrations of the adequacy of the approximation the reader is referred to the Appendix. In what follows we s tudy some special cases, and identify the parameters # and a 2. Case I. Arrivals are Poisson, with rate h. Each arrival brings in a bunch of cards of random size G; the bunch sizes are independent from arrival to arrival. That is, the total number of card arrivals is compound Poisson (see Feller [2]) with ~--'E[A (~-)] = hE[G], T -a Var [A(T)] = XE[G2]. (2.9) (a) Departures may be at constant, deterministic, rate r. In this case it may be seen that if ~ increases, the number of arrivals per unit t ime approaches the normal form, and so do the increments Q. We have = hE[G] r ~nd (2.10) 2 ~E[G 2] (b) Perhaps departures may be at exponential intervals of mean 1/r. Such is Journal of the Association for Computing Machinery, Vol. 18, No. 3, July 1971 4 0 8 DONALD P. GAVER approximately the case, for example, if retransmission for reason of line noise [see (ii) above] becomes important . In this case, when the queue is full, the output is approximately Poisson: E[D(T)] = r r , Var [D(T)] = rr . (2.11) Thus # = XE[G] r, 2 a = XE[G 2] + r. These parameters then determine the long-run behavior of the queue length, provided # < 0 but very close to zero. We see, for example, tha t if r > XE[G], then Case I ( a ) : E[Q] = XE[G2]/2(r XE[G]) ; (2.12) Case I ( b ) : E[Q] = (XE[G 2] + r ) / 2 ( r XE[G]). (2.13) Furthermore, the long-run distribution of Q is approximately exponential (2.7) with mean (2.12) or (2.13). Case I I . Arrivals occur at identically distributed time intervals o~i, where oa, o~2, \" \" , an , \" ' \" are independent. Furthermore, departures also take place in the same manner at intervals ~1, 62, . \" , ~ , \" \" ; the lat ter assumption is valid where the system is usually fully occupied; Q > 0. Furthermore, the a and 5 sequences are independent. Then renewal theory (cf. Feller [2, p. 359]) shows that on an appropriate t ime scale the number of arrivals and departures in t ime r are approximately normally distributed with (arrivals) E[A(7\")] = T/E[a], Var [A(r)] = r Var [o~]/E3[o~]. (2.14) Corresponding formulas hold true for departures. Therefore we put tt = 1 / E [ a ] l/E[61, 2 = Var [ a ] / (E [a ] ) 3 + Var [~]/(E[~]) ~ (2.15) and use the diffusion approximation. If we are interested in examining the system at intervals long with respect to E[a] and E[5], then we anticipate tha t the solution of the diffusion equation quite adequately describes the backlog at t ime t. Case I I I . This is the same as Case I I , except tha t \" t ime\" is now customer number, in arrival order. The increment to the total waiting t ime tha t occurs between two customers is, when the queue is long, distributed like ~ a. Thus u = E [ ~ ] E[a] and (2.16) 2 = Var [8] + Var [c~] and the diffusion equation now describes the probabil i ty density of the waiting t ime at the time of arrival (or departure, for on this t ime scale they are indistinguishable) of the nth customer. 3. Diffusion Mod", "authors": ["Donald P. Gaver"], "id": "77dad2be1c11d646ae106908387349b9391199c9", "title": "Analysis of Remote Terminal Backlogs under Heavy Demand Conditions", "references": ["3e32d1e728fa3288d123c998981b5dde96ed2745", "6ec07c88834fdda46332a081ded0747af629d5c1"]}, {"date": "1973", "abstract": "This paper (based on a keynote address presented at the SIGACT/SIGPLAN Symposium on Principles of Programming Languages, Boston, October 1-3, 1973) presents the view that a programming language is a tool which should assist the programmer in the most difficult aspects of his art, namely program design, documentation, and debugging. It discusses the objective criteria for evaluating a language design, and illustrates them by application to language features of both high level languages and machine code programming. It concludes with an annotated reading list, recommended for all intending language designers.", "authors": ["C. A. R. Hoare"], "id": "b3d2c98c190a3b54bda9ce3ab670549d25579e6e", "title": "Hints on programming language design.", "references": []}, {"date": "1976", "abstract": "SummaryHere we give methods of mechanically converting programs that are easy to understand into more efficient ones, converting recursion equations using high level operations into lower level flowchart programs.The main transformations involved are (i) recursion removal (ii) eliminating common subexpressions and combining loops (iii) replacing procedure calls by their bodies (iv) introducing assignments which overwrite list cells no longer in use (compiletime garbage collection).", "authors": ["John Darlington", "Rod M. Burstall"], "id": "cb5eebdd166a6cb0d1e2de65fb70ea1c913ef00d", "title": "A system which automatically improves programs", "references": ["1c1216e2986966d0f84bccc49aff42edb9fb59da", "f606c5c942bf2cbe5ec18e7e36fc48fce1dd24a9", "96d6eff34c4f97ef4323068125a545bbb0becaff", "04f6109159b80a3f09b6ae72d269b8cb97d61dad", "a93df07591911a203b6438530b84c4f81ba3e85b", "8f10eb4e5d74c772a9a5a9695e027512327e7c4a", "ec981d6a325ed395cc9e9e18c99b35664d0da28e"]}, {"date": "1973", "abstract": "Semantic Scholar extracted view of \"A simple axiomatic basis for programming language constructs\" by Einar Dahl", "authors": ["Einar Dahl"], "id": "12ef8d15690b612d36e30f124a2c5e7511298800", "title": "A simple axiomatic basis for programming language constructs", "references": []}, {"date": "1974", "abstract": "Semantic Scholar extracted view of \"A Programming Methodology for Operating System Design\" by Per Brinch Hansen", "authors": ["Per Brinch Hansen"], "id": "c917651f961bbbc314f45d9b4f3c710f11a36bea", "title": "A Programming Methodology for Operating System Design", "references": []}, {"date": "1975", "abstract": "Given a `DO WHILE' program P and a function F on a domain D, the authors investigate the problem of proving (or disproving) if P computes F over D. It is shown that if P satisfies certain natural constraints (well behaved), then there is a loop assertion independent of the structure of the loop body, that is both necessary and sufficient for proving the hypothesis. These results are extended to classes of loop programs which are not well behaved and to FOR loops. The sufficiency of Hoare's DO WHILE axiom for well-behaved loop programs is shown. Applications of these ideas to the problem of mechanical generation of assertions is discussed.", "authors": ["Sanat K. Basu", "Jayadev Misra"], "id": "791269bb32735512f13938cbef74ca97713fb7fc", "title": "Proving loop programs", "references": ["09661a6bb7578979e42c75d6ce382baba64d4981"]}, {"date": "1972", "abstract": "Following the fixpoint theory of Scott, the semantics of computer programs are defined in terms of the least fixpoints of recursive programs. This allows not only the justification of all existing verification techniques, but also their extension to the handling, in a uniform manner of various properties of computer programs, including correctness, termination, and equivalence.", "authors": ["Zohar Manna", "Jean Vuillemin"], "id": "9ef033c165f242d090e911017beb620a307b279e", "title": "Fixpoint approach to the theory of computation", "references": ["5796e9c28f35dbd35258cfb5bb4dd2b73d7b6832"]}, {"date": "1972", "abstract": "This paper discusses the expression of algorithms by flowcharts, and in particular by flowcharts without explicit go-to's (D-charts). For this purpose we introduce a machine independent definition of algorithm which is broader than usual. Our conclusion is that D- charts are in one technical sense more restrictive than general flowcharts, but not if one allows the introduction of additional variables which represent a history of control flow. The term \"algorithm\" is used in many different ways. Sometimes we speak of an algorithm as a process in the abstract, without reference to a particular computer. It is in this sense, for example, that we speak of the \"radix exchange sort algorithm,\" or the \"simplex algorithm.\" Often we identify an algorithm with a particular se- quence of instructions for a particular computer. In this paper we shall present a new definition of algorithm which emphasizes the sequence of commands associated with a particular \"input.\" We then define the notion \"expression\" of algorithms by general flowcharts and flowcharts without explicit go-to's (D-charts). Some theorems are given which exhibit some of the rela- tionships between algorithms, flowcharts, and D-charts.", "authors": ["John L. Bruno", "Kenneth Steiglitz"], "id": "cd52c6a8690781f294115c554bfab8bf3f461487", "title": "The Expression of Algorithms by Charts", "references": ["44f4110824897f8ec257855d16b4c6778b78d3d2", "572527f774793778cc6a4bbfbcacf144890ffbff"]}, {"date": "1967", "abstract": "The principal requirement for the Model 91 floating-point execution unit was that it be designed to support the instructionissuing rate of the processor. The chosen solution was to develop separate, instruction-oriented algorithms for the add, multiply, and divide functions. Linked together by the floating-point instruction unit, the multiple execution units provide concurrent instruction execution at the burst rate of one instruction per cycle.", "authors": ["Swan Friteof Anderson", "J. G. Earle", "Robert E Goldschmidt", "Don Michaels Powers"], "id": "82e3bc6a6a9a4772f07ec06b4a69c81c1165c6d4", "title": "The IBM system/360 model 91: floating-point execution unit", "references": ["66c3661f86ca40fed69e683288b2106635fa5f37", "3912f421669e7beca3d8597ab409324ef516a4e0"]}, {"date": "1976", "abstract": "Abstract For many purposes, asynchronous parallel programs may be viewed as sequential but nondeterministic programs. The direct translation to nondeterministic sequential form leads to a combinatorial explosion of program size before correctness proofs can even begin. The Church-Rosser approach to correctness of asynchronous parallel programs is a flexible way to divide a correctness proof into several lemmas, no one of which requires both deep reasoning and explicit enumeration of all the control states required in the nondeterministic sequential form of the program. The approach is stated and justified abstractly, demonstrated in detail for a simple example program, and compared with other approaches to the correctness of parallel programs. The abstract formulation is independent of the model of parallelism in the example and can also be applied to nondeterminism not derived from asynchronous parallelism. We conclude with a survey of prospects for computer assisted proofs structured by the Church-Rosser approach.", "authors": ["Barry K. Rosen"], "id": "35f1fa0934849171ab651025e6c71980fedd4818", "title": "Correctness of Parallel Programs: The Church-Rosser Approach", "references": []}, {"date": "1973", "abstract": "This paper describes the evolution of language features for multiprogramming from event queues and semaphores to critical regions and monitors. It suggests \nthat the choice of language concepts should be guided by two simple principles: First, it should be possible to understand a concurrent program in time-independent \nterms by an effort proportional to its size; secondly, it should be possible to state assumptions about invariant relationships among program components and have these assumptions checked automatically. The central \nproblems of multiprogramming are illustrated by annotated algorithms written in a well-structured programming language.", "authors": ["Per Brinch Hansen"], "id": "4044d246faecb78d4cadc25b6c2f83fa062174a2", "title": "Concurrent Programming Concepts", "references": []}, {"date": "1974", "abstract": "Semantic Scholar extracted view of \"Monitors: an operating system structuring concept\" by C. Robin Home", "authors": ["C. Robin Home"], "id": "365f1c3b6634112a6966d15bee88b5ea7d2e5808", "title": "Monitors: an operating system structuring concept", "references": []}, {"date": "1975", "abstract": "When proving that a system of processes has a given property it is often convenient to assume that a routine is uninterruptible, i.e. that the routine cannot be interleaved with the rest of the system. Here sufficient conditions are obtained to show that the assumption that a routine is uninterruptible can be relaxed and still preserve basic properties such as halting and determinacy. Thus correctness proofs of a system of processes can often be greatly simplified. This technique - called reduction - is viewed as the replacement of an interruptible routine by an uninterruptible one.", "authors": ["Richard J. Lipton"], "id": "f4a00bdc978e316a85509d9b872dfc996c3da1cd", "title": "Reduction: a new method of proving properties of systems of processes", "references": []}, {"date": "1973", "abstract": "Semantic Scholar extracted view of \"Operating System Principles\" by Per Brinch Hansen", "authors": ["Per Brinch Hansen"], "id": "59668eba7441f29dc42719651bf5ebd12c902975", "title": "Operating System Principles", "references": []}, {"date": "2004", "abstract": "SummaryThis paper presents a comparative study of different methods for formal description of programming languages. These methods have been applied to a simple but realistically usable programming language; the more abstract definitions have been proved to be consistent relative to the more concrete ones.", "authors": ["C. A. R. Hoare", "Peter E. Lauer"], "id": "4c88e871290bbf08bd2b71412e4430a4c89350dd", "title": "Consistent and complementary formal theories of the semantics of programming languages", "references": ["2cf7ae6adfb101ca984b1988bd6bb0be4f9b739f", "4f2bb3bae07df4061f35c0322de5081694f0afe9", "7db45adfec35a4b958b185fe979e65b5fcca9bcc", "2769c203102a875c10bc11affc161891472176d1", "755170dd54c6013cce03e6c4efb81566151f86a6", "375c0850cd418002ee2be0fb2b6ec177573f950a", "9bb05141b7276340c277007f8b6b99100bb03116", "b3fe91923c1356a8c6dee2c725dd11fcbfeae903"]}, {"date": "2004", "abstract": "SummaryProof methods adequate for a wide range of computer programs have been given in [1\u20136]. This paper develops a method suitable for programs which incorporate coroutines. The implementation of coroutines described follows closely that given in SIMULA [7, 8], a language in which such features may be used to great advantage. Proof rules for establishing the correctness of coroutines are given and the method is illustrated by the proof of a useful program for histogram compilation.", "authors": ["Maurice Clint"], "id": "ea548fbbdfe46a37dd0edaac4a8d116343b7c39e", "title": "Program proving: Coroutines", "references": ["7133cc278ed48ca1f6466a91eddb0c82c7b744a8"]}, {"date": "1973", "abstract": "The results are presented of an analysis of a probabilistic model of a multiprogrammed computer system with a two-level storage system in which there is sequential dependency of accesses between the devices. Expressions are obtained for the long-run probability that both the CPU and each of the storage devices are busy. Some numerical results are given which quantify the gains in CPU utilization obtainable by multiprogramming in the presence of this type of storage system.", "authors": ["Gerald S. Shedler"], "id": "c07e33daa59d559accf32968f18aa84ca88640b0", "title": "A queuing model of a multiprogrammed computer with a two-level storage system", "references": []}, {"date": "1968", "abstract": "Semantic Scholar extracted view of \"Diffusion approximations in applied probability\" by J. L. Iglehart", "authors": ["J. L. Iglehart"], "id": "3e32d1e728fa3288d123c998981b5dde96ed2745", "title": "Diffusion approximations in applied probability", "references": []}, {"date": "1975", "abstract": "Semantic Scholar extracted view of \"Application of program transformation to program synthesis\" by John Darlington", "authors": ["John Darlington"], "id": "1c1216e2986966d0f84bccc49aff42edb9fb59da", "title": "Application of program transformation to program synthesis", "references": []}, {"date": "1973", "abstract": "Methods are presented for computing the equilibrium distribution of customers in closed queueing networks with exponential servers. Expressions for various marginal distributions are also derived. The computational algorithms are based on two-dimensional iterative techniques which are highly efficient and quite simple to implement. Implementation considerations such as storage allocation strategies and order of evaluation are examined in some detail.", "authors": ["Jeffrey P. Buzen"], "id": "a968f4cb46a4743f981034322b091f6ad45cc77a", "title": "Computational algorithms for closed queueing networks with exponential servers", "references": []}, {"date": "1972", "abstract": "Most algorithms for compiling arithmetic expressions regard the operators + and \u2014 as being essentially the same: in the ALGOL Report they are given the same precedence, in an Operator Precedence context they have the same precedence relations. They have, though, some important differences: minus is neither commutative {a \u2014 b # b \u2014 a), nor associative (a \u2014 (b \u2014 c) # (a \u2014 b) \u2014 c). Similar differences apply between * and /. In the field theory sense, there are two binary operators + and *; the 'reciprocal' operators \u2014 and / are unary. The expression a \u2014 b is a convenient shorthand for a H\u2014 b, and ajb a shorthand for a*/b. (Note that this automatically gives the ALGOL interpretation to a/b/c.) These two binary operators are both commutative and associative so that terms of an expression can be reordered at will, as can the factors of a term. Consider first the terms of an expression. For example, using the symbol = to show the equivalence between short and long-hand, and ^ to indicate transformation:", "authors": ["Jeffrey S. Rohl", "J. A. Linn"], "id": "a93df07591911a203b6438530b84c4f81ba3e85b", "title": "A note on compiling arithmetic expressions", "references": ["9094a856e89dd31c8e22f4c023cf217081858b7e", "88decc8d12fbc6dbe8081178a514333dc9e872a3", "a48b6b2fe62b35df3c6e611a2ad8768e30e7ffd5"]}, {"date": "1965", "abstract": "Semantic Scholar extracted view of \"The heavy traffic approximation in the theory of queues\" by J. F. C. Kingman", "authors": ["J. F. C. Kingman"], "id": "6ec07c88834fdda46332a081ded0747af629d5c1", "title": "The heavy traffic approximation in the theory of queues", "references": []}, {"date": "1966", "abstract": "From the combination of knowledge and actions, someone can improve their skill and ability. It will lead them to live and work much better. This is why, the students, workers, or even employers should have reading habit for books. Any book will give certain knowledge to take all benefits. This is what this compiling techniques tells you. It will add more knowledge of you to life and work better. Try it and prove it.", "authors": ["Thomas E. Cheatham"], "id": "04f6109159b80a3f09b6ae72d269b8cb97d61dad", "title": "Compiling techniques", "references": []}, {"date": "1973", "abstract": "This paper presents general methods for studying the problems of translatability between classes of schemes and equivalence of schemes in a given class. There are four methods: applying the theory of formal languages, programming, measuring the complexity of a computation, and ''cutting and pasting''. These methods are used to answer several questions of translatability and equivalence for classes of program schemes, program schemes augmented with counters, and recursively defined schemes. In particular, it is shown that (i) the quasirational recursion schemes are translatable into strongly equivalent program schemes, (ii) monadic recursion schemes are translatable into strongly equivalent program schemes with two counters, (iii) there is a monadic recursion scheme not strongly equivalent to any program scheme with one counter.", "authors": ["Stephen J. Garland", "David C. Luckham"], "id": "ec981d6a325ed395cc9e9e18c99b35664d0da28e", "title": "Program Schemes, Recursion Schemes, and Formal Languages", "references": ["e92a9984035938b2a97c6e2891889bc2247bcfd0"]}, {"date": "1966", "abstract": "Semantic Scholar extracted view of \"Turing machines and languages with only two formation rules\" by Corrado Bohm", "authors": ["Corrado Bohm"], "id": "44f4110824897f8ec257855d16b4c6778b78d3d2", "title": "Turing machines and languages with only two formation rules", "references": []}, {"date": "1966", "abstract": "A constructive approach to the question of proofs of algorithms is to consider proofs that an object resulting from the execution of an algorithm possesses certain static characteristics. It is shown by an elementary example how this possibility may be used to prove the correctness of an algorithm written in ALGOL 60. The stepping stone of the approach is what is called General Snapshots, i.e. expressions of static conditions existing whenever the execution of the algorithm reaches particular points. General Snapshots are further shown to be useful for constructing algorithms.", "authors": ["Peter Naur"], "id": "5796e9c28f35dbd35258cfb5bb4dd2b73d7b6832", "title": "Proof of algorithms by general snapshots", "references": []}, {"date": "1972", "abstract": "Semantic Scholar extracted view of \"A semantic approach to automatic program improvement\" by John Darlington", "authors": ["John Darlington"], "id": "96d6eff34c4f97ef4323068125a545bbb0becaff", "title": "A semantic approach to automatic program improvement", "references": []}, {"date": "1967", "abstract": "The full switching-speed potential of high performance transistors is difficult to realize in a current switch configuration because of the instability which exists when many circuits are interconnected in a system. With a phase compensating network in the emitter current source, howeveirt, has been possiblet o design a stable circuit using 1 Gc/sec transistors. Design techniques and engineering aspects of the circuit which result in a 5-nsec in-the-environment propagation delaya re described. Particular attention is given to the dc design, stability analysis, switching performance, evaluation and specification of the circuit.", "authors": ["Robert Sechler", "Adolf Strube", "John R. Turnbull"], "id": "3912f421669e7beca3d8597ab409324ef516a4e0", "title": "ASLT circuit design", "references": []}, {"date": "1966", "abstract": "In the first part of the paper, flow diagrams are introduced to represent inter ah mappings of a set into itself. Although not every diagram is decomposable into a finite numbm of given base diagrams, this becomes hue at a semantical level due to a suitable extension of the given set and of the basic mappings defined in it. Two normalization methods of flow diagrams are given. The first has |hree base diagrams; the second, only two. In the second part of the paper, the second method is applied to 'lhe theory of Turing machines. With every Turing maching provided with a two-way half-tape, ihere is associated a similar machine, doing essentially 'lhe same job, but working on a tape obtained from the first one by interspersing alternate blank squares. The new machine belongs to the family, elsewhere introduced, generated by composition and iteration from the two machines X and R. That family is a proper subfamily of the whole family of Turing machines.", "authors": ["Corrado B\u00f6hm", "Giuseppe Jacopini"], "id": "572527f774793778cc6a4bbfbcacf144890ffbff", "title": "Flow diagrams, turing machines and languages with only two formation rules", "references": ["7bbdbbab17fd0fbaa69704fb32c0eba8893f9e26"]}, {"date": "1975", "abstract": "The paper describes a system of rules for transforming programs, the programs being in the form of recursion equations. The idea is to start with a very simple, lucid and hopefully correct program, then to transform it into a more efficient one by altering the recursion structure. Illustrative examples of program transformations are given, and a tentative implementation is described. We hope to throw some light on the alternative structures for programs, also to indicate a possible initial phase for an automatic or semi-automatic program manipulation system.", "authors": ["Rod M. Burstall", "John Darlington"], "id": "f606c5c942bf2cbe5ec18e7e36fc48fce1dd24a9", "title": "Some transformations for developing recursive programs", "references": []}, {"date": "1964", "abstract": "The architecture* of the newly announced IBM System/360 features four innovations: 1. An approach to storage which permits and exploits very large capacities, hierarchies of speeds, readonly storage for microprogram control, flexible storage protection, and simple program relocation. 2. An input/output system offering new degrees of concurrent operation, compatible channel operation, data rates approaching 5,000,000 characters/second, integrated design of hardware and software, a new low-cost, multiple-channel package sharing main-frame hardware, new provisions for device status information, and a standard channel interface between central processing unit and input/output devices. 3. A truly general-purpose machine organization offering new supervisory facilities, powerful logical processing operations, and a wide variety of data formats. 4. Strict upward and downward machine-language compatibility over a line of six models having a performance range factor of 50. This paper discusses in detail the objectives of the design and the rationale for the main features of the architecture. Emphasis is given to the problems raised by the need for compatibility among central processing units of various size and by the conflicting demands of commercial, scientific, real-time, and logical information processing. A tabular summary of the architecture is shown in the Appendices.", "authors": ["Gene M. Amdahl", "Gerrit A. Blaauw", "Frederick P. Brooks"], "id": "66c3661f86ca40fed69e683288b2106635fa5f37", "title": "Architecture of the IBM System/360", "references": ["05a48e3b2f59dbb2417e0ec5fd993dfd0aab7c40"]}, {"date": "2004", "abstract": "SummaryThe Set of Interacting Procedures (SIP) model, a graph model suitable for representing interacting computations, is presented. It includes n\u00f3tation for creating and destroying processes, permits well-defined critical sections, and facilitates examining the logical properties of the modeled procedures. The essential parts of a theory of correctness of interacting processes are informally presented. These include a definition of correctness with respect to an assertion, a non-interference condition which justifies the use of the SIP model, and a set of sufficient conditions for correctness. To illustrate the use of the model and theory, a solution to the reader-writer problem is presented and modeled. An assertion expressing the correctness of the solution is formulated, and the SIP model of the solution is shown to be correct with respect to the assertion. Finally, the non-interference condition shows that the conclusions about the model also apply to the solution itself.", "authors": ["Glen E. Newton"], "id": "7fa540f88688a22436749db28c4940c559e885d5", "title": "Proving properties of interacting processes", "references": ["43aa639d8df2285bd805531889d1d5f49563c17d", "a9784da374ca7f0ca091618e602f81831181d9f5", "394ef0577482d22d0d86d52f0d10a7c313caace2", "932bbc2425cea91f50d1ff0af936e89d0e973ebe"]}, {"date": "1970", "abstract": "For casting a concrete inside corner wall structure, the corner is defined by a corner member to extend lengthwise of the corner with the adjacent edges of a pair of form panels that are disposed generally perpendicular to each other being butted against the outer edges of the corner member and a control unit is interconnected with the form panels and the angle member to retain this relationship between the panels and angle member incident pouring and hardening of the concrete. The control unit has actuator means associated therewith operable to move the panel edges into abutment with the outer edges of the angle member prior to concrete pouring and to strip both the form panels and the angle member from the hardened concrete structure.", "authors": ["Peter E. Lauer"], "id": "9bb05141b7276340c277007f8b6b99100bb03116", "title": "Formal Definition of ALGOL 60 (notice only)", "references": []}, {"date": "1955", "abstract": "undefined", "authors": [], "id": "7133cc278ed48ca1f6466a91eddb0c82c7b744a8", "title": "The Queen's University of Belfast", "references": []}, {"date": "1965", "abstract": "Semantic Scholar extracted view of \"A Correspondence Between ALGOL 60 and Church's Lambda- Notation: Part I*\" by E Lohse et al.", "authors": ["E Lohse", "Robert V. Smith"], "id": "755170dd54c6013cce03e6c4efb81566151f86a6", "title": "A Correspondence Between ALGOL 60 and Church's Lambda- Notation: Part I*", "references": []}, {"date": "1964", "abstract": "Semantic Scholar extracted view of \"A formal description of Algol 60\" by P. J. Landin", "authors": ["P. J. Landin"], "id": "7db45adfec35a4b958b185fe979e65b5fcca9bcc", "title": "A formal description of Algol 60", "references": []}, {"date": "1964", "abstract": "Oil-soluble detergent-dispersant Group I and Group II metal or lead salts of substantially saturated hydrocarbyl-substituted ethylsulfonic acids, processes for preparing these salts, lubricating oil additive concentrates and lubricating oil compositions containing them are disclosed.", "authors": ["Christopher Strachey"], "id": "4f2bb3bae07df4061f35c0322de5081694f0afe9", "title": "Towards a formal semantics", "references": []}, {"date": "1975", "abstract": "As an example of cooperation between sequential processes with very little mutual interference despite frequent manipulations of a large shared data space, a technique is developed which allows nearly all of the activity needed for garbage detection and collection to be performed by an additional processor operating concurrently with the processor devoted to the computation proper. Exclusion and synchronization constraints have been kept as weak as could be achieved; the severe complexities engendered by doing so are illustrated.", "authors": ["Edsger W. Dijkstra", "Leslie Lamport", "Alain J. Martin", "Carel S. Scholten", "Elisabeth F. M. Steffens"], "id": "114108b350866595cebc14a96a8cb6f7aede7c93", "title": "On-the-fly garbage collection: an exercise in cooperation", "references": []}, {"date": "1974", "abstract": "This paper presents a method of proving properties of the implementation of levels of abstraction in a hierarchically structured Operating System. For any two adjacent levels L (low) and H (high), the processes of level L are seen as (virtual) processors realizing the processes of level H. A Scheduling program is assumed to implement the realization of level H by level L. Means of defining properties of this realization are studied, together with a formal method of proving these properties. The proofs are carried out on the basis of two kinds of hypotheses: (1) assumptions about the behaviour of the processors, and (2) properties of the scheduling program. The hierarchy of the system is taken into account to proceed step by step to the proofs of the realization of the last level and, consequently, of the whole system. Advantages and inconveniences of the method are discussed.", "authors": ["Gerald Belpaire", "J. R. Wilmotte"], "id": "9e710b50eb6ca202fa3f330cdc4bfa6cfd7adc9e", "title": "Correctness of realizations of levels of abstraction in operating systems", "references": []}, {"date": "1967", "abstract": "As we have demonstrated in another paper in this issue (see page 345), the Compiler Compiler has proven itself to be a very useful tool in writing compilers. As we came to use it for more general data-processing tasks, the lack of any comprehensive arithmetic facilities became a limitation. Rather than add these to the Compiler Compiler, we chose to add the phrase structure features of the Compiler Compiler to an algebraic compiler. This is Atlas Autocode (AA), which has been described elsewhere (Brooker, Rohl and Clark, 1966), but which can be thought of as a dialect of ALGOL. So far we have implemented a minimum of facilities, and we may be forced to add others in the light of experience. It is the broad outlines, however, which are of interest. In this description we assume a knowledge of the concepts discussed in \"Experience with the Compiler Compiler\" (see page 345).", "authors": ["R. A. Brooker", "D. Morris", "Jeffrey S. Rohl"], "id": "a48b6b2fe62b35df3c6e611a2ad8768e30e7ffd5", "title": "Compiler Compiler Facilities in Atlas Autocode", "references": []}, {"date": "1970", "abstract": "Semantic Scholar extracted view of \"A System Program Generator\" by D. Morris et al.", "authors": ["D. Morris", "I. R. Wilson", "P. C. Capon"], "id": "88decc8d12fbc6dbe8081178a514333dc9e872a3", "title": "A System Program Generator", "references": []}, {"date": "1972", "abstract": "Semantic Scholar extracted view of \"Hierarchical Program Structures\" by Tony Hoare", "authors": ["Tony Hoare"], "id": "6faf40fa84ceda43ecb586f4bfe54ad549c7e072", "title": "Hierarchical Program Structures", "references": []}, {"date": "1968", "abstract": "Semantic Scholar extracted view of \"A system design proposal\" by Tom Kilburn et al.", "authors": ["Tom Kilburn", "D. Morris", "Jeffrey S. Rohl", "Frank H. Sumner"], "id": "9094a856e89dd31c8e22f4c023cf217081858b7e", "title": "A system design proposal", "references": []}, {"date": "1972", "abstract": "The purpose of this paper is to establish the applicability of program-proving techniques to the verification of operating systems, control programs and synchronization programs. All the illustrative examples to be presented use Dijkstra's P and V operations for controlling the synchronization of competing processes. However, the techniques discussed are applicable to any set of such control primitives. A major portion of the paper is devoted to the proof of correctness of two programs devised by Courtois et al. that control the sequencing of \"readers\" and \"writers\" requesting the use of a common device.", "authors": ["Karl N. Levitt"], "id": "7748ee206fbdf2d6769f9b645abb6a9004a49888", "title": "The application of program-proving techniques to the verification of synchronization processes", "references": []}, {"date": "1959", "abstract": "Finite automata are considered in this paper as instruments for classifying finite tapes. Each one-tape automaton defines a set of tapes, a two-tape automaton defines a set of pairs of tapes, et cetera. The structure of the defined sets is studied. Various generalizations of the notion of an automaton are introduced and their relation to the classical automata is determined. Some decision problems concerning automata are shown to be solvable by effective algorithms; others turn out to be unsolvable by algorithms.", "authors": ["Michael O. Rabin", "Dana S. Scott"], "id": "e92a9984035938b2a97c6e2891889bc2247bcfd0", "title": "Finite Automata and Their Decision Problems", "references": ["a496212ca3444e1e14b0668b82e2459d02dc275a", "921617b2c3e166f5efddf77aad055dea73da7dc8", "5e391bd0bd05d6fd6b9ec2e82d7bc95feecee335", "a6021735e8de4f32010c6313396432d99bbe2440", "5c406be5ae6f2935b6653f101f02332186cb50d9", "ed56719962b555b7c37dc9ab3e9920ba9a21397c"]}, {"date": "1966", "abstract": "Semantic Scholar extracted view of \"Review: Corrado Bohm, On a Family of Turing Machines and the Related Programming Language\" by Martin Davis", "authors": ["Martin Davis"], "id": "7bbdbbab17fd0fbaa69704fb32c0eba8893f9e26", "title": "Review: Corrado Bohm, On a Family of Turing Machines and the Related Programming Language", "references": []}, {"date": "1975", "abstract": "Semantic Scholar extracted view of \"Guarded commands, non-determinancy and a calculus for the derivation of programs\" by Edsger W. Dijkstra", "authors": ["Edsger W. Dijkstra"], "id": "85bb63adc85a812d61f845813b792033c40e214f", "title": "Guarded commands, non-determinancy and a calculus for the derivation of programs", "references": []}, {"date": "1972", "abstract": "This paper presents a proposal for structured representation of multiprogramming in a high level language. The notation used explicitly associates a data structure shared by concurrent processes with operations defined on it. This clarifies the meaning of programs and permits a large class of time-dependent errors to be caught at compile time. A combination of critical regions and event variables enables the programmer to control scheduling of resources among competing processes to any degree desired. These concepts are sufficiently safe to use not only within operating systems but also within user programs.", "authors": ["Per Brinch Hansen"], "id": "2de8bb082dba785d6a5501116d89097d396f03cc", "title": "Structured multiprogramming", "references": []}, {"date": "1974", "abstract": "Semantic Scholar extracted view of \"A more mechanical approach to program verification\" by Irene Greif et al.", "authors": ["Irene Greif", "Richard J. Waldinger"], "id": "9a9e7645cf325745abb85210e498a647b3512843", "title": "A more mechanical approach to program verification", "references": []}, {"date": "2004", "abstract": "The solutions offered by Hansen [4] to the second of the \"Readers and Writers\" problems which were originally posed in [t] call for a few comments. I t was stated in [t] that \"once a writer is ready to write he performs his 'wr i te ' as soon as possible\". In both of Hansen's solutions, readers (incoming and outgoing) and writers (incoming and outgoing) must pass through a common \"critical region\". Writers and readers may thus have to wait together before being admitted, in unpredictable order, into this critical region. If all the outgoing writers and any number of incoming readers are admit ted before any one waiting incoming writer, the variable counting the number of active writers becomes zero, and all these incoming readers may be granted reading while the writers are still waiting. Under congested conditions the writers might wait forever. This situation, although not likely to occur frequently, is carefully avoided in the solution proposed in [t ] by taking care that every reader and every first writer passes a certain semaphore on which there may never be more than one process waiting at a time. Perhaps our solution \"seems to be unnecessarily complicated\" to Hansen because he has ignored this aspect of the problem. His \"correctness criteria\" are not violated by his solution but they are not equivalent to those of the original problem statement. Moreover, we are not convinced that his first solution is simpler than the one we had previously published: if we count a \"critical section\" as a semaphore, Hansen's first solution has the same number of semaphores but two additional variables; Hansen's solution has doubled the number of assignment statements and it contains four loops while our solution has none. Let us add, however, the following information. If writers are required (as in the original problem) to have exclusive access to the resource, the variable counting the number of \"scheduled\" writers takes only the values 0 or 1. By trying to show \" t h a t the mutual exclusion of writers is completely irrelevant to the readers\", Hansen fails to mention that, with a few minor changes, the additional critical region w becomes superfluous in his first solution. The touted symmetry is in fact unconvincing. I t seems to have been the insistence on a symmetric solution to an asymmetric problem which prevented a really satisfactory solution from being found. Finally, Hansen's second solution reintroduces a \"busy form of wait ing\" which has been held to be undesirable E2]. Allowing a process to wait for an", "authors": ["Pierre-Jacques Courtois", "F. Heymans", "David Lorge Parnas"], "id": "394ef0577482d22d0d86d52f0d10a7c313caace2", "title": "Comments on \u201cA comparison of two synchronizing concepts by P.B. Hansen\u201d", "references": ["b4e374f2d889f9cef9b31f2b02ad225e9c0a9bed", "cd46e66972b056719a988e5c25440ab796c3f2f2"]}, {"date": "1975", "abstract": "One person's perspectives of program verification and its relation to some aspects of reliable software are presented. The main verification method of inductive assertions is illustrated with several variations of one detailed example; a second example shows a surprisingly simple inductive assertion proof of an iterative tree traversal example. Briefly discussed also are the implicit assumptions of most verifications, proving termination, the creating of assertions, and languages in which to write assertions. An abstract overview is given of existing program verification systems together with a sample list of verified programs. A short bibliography is included.", "authors": ["Ralph L. London"], "id": "6d6aacdcc42b968c6d1317dc1502d8f9f7da4309", "title": "A view of program verification", "references": ["2cf7ae6adfb101ca984b1988bd6bb0be4f9b739f", "44484fdf6fcb0ae811362afdaa86841537d85b2a", "bac9f6742d85e74cbf28413dc7ad70fca59d4436", "7d6974b8b61da3dfd32b2c4f16ac3920eb934402", "5796e9c28f35dbd35258cfb5bb4dd2b73d7b6832", "c282549889226ad9ab6cd867a5d7d6dbae58089c", "5d8056e326d4199d157a17fbeee97a7349d2824c", "c77a34f6e3d738e944e93972a292ad4f39c5e794"]}, {"date": "1987", "abstract": "The present methods of determining the functional design of computers are critically reviewed and a new approach proposed. This is illustrated by explaining, in abstracted form, part of the control organization of a new and different machine based, in part, on the ALGOL 60 language. The concepts of expression and procedure lead directly to use of a Polish string program. A new arrangement of control registers results, which provides for automatic allocation of temporary storage within expressions and procedures, and a generalized subroutine linkage. The simplicity and power of these notions suggests that there is much room for improvement in present machines and that more attention should be given to control functions in new designs.(1961).", "authors": ["Robert S. Barton"], "id": "05a48e3b2f59dbb2417e0ec5fd993dfd0aab7c40", "title": "A New Approach to the Functional Design of a Digital Computer", "references": ["4c61310dbfcd29ed74c3a8b9703cee4bdf4c1791", "859c0cf9447ae4633913183031656cd200c84c13", "4b4f7cd6e795f404a544cf60cf587c0876395b27"]}, {"date": "1971", "abstract": "This paper considers some of the issues tha t arise when messages or jobs inbound to a computer facility are buffered prior to being processed. Models are developed tha t describe (a) the results of blocking a single memory unit for the use of diverse messages, (b) the occupancy behavior of a buffer tha t is tied to a single message source, and (c) the occupancy of a buffer dynamical ly shared among many independent sources.", "authors": ["Donald P. Gaver", "Peter A. W. Lewis"], "id": "156243b2ae3024a3f1babc0c6b36e6d840a5396f", "title": "Probability Models for Buffer Storage Allocation Problems", "references": ["b1f61c792ea7bd233892dfd0252f4721d901857a", "65ba8fd8ef9c2a70cee99d2e5cab9302d0307a1e"]}, {"date": "1973", "abstract": "This paper presents results of an approximation study of cyclic queueing phenomena that occur in multiprogrammed computer systems. Based on Wald\u2019s identity and using ideas of diffusion, the objective is to develop convenient and nearly explicit formulas relating processor utilization in such systems to simple program parameters and the level of multiprogramming. Some numerical results to indicate the quality of the proposed approximation are given.", "authors": ["Donald P. Gaver", "Gerald S. Shedler"], "id": "0c070dbb7fe2d05485e6b97ef993ec397413087f", "title": "Approximate Models for Processor Utilization in Multiprogrammed Computer Systems", "references": []}, {"date": "2004", "abstract": "SummaryThe concept of machine extension is a commonly used technique for implementing complex software: sets of object classes and operations on these objects are defined and used, often in a layered fashion, to construct the system. This paper addresses the adaptation of this technique to automatic programming. It discusses how such sets of data structures may be precisely specified, presents an axiomatization of a programming language suitable for machine verification, and shows how programs which realize these data structures may be proved correct. A range of data type classes is treated\u2014including arrays, records, and pointers. Some new verification rules are presented to handle programs which use assignments and structured objects.", "authors": ["Jay M. Spitzen", "Ben Wegbreit"], "id": "ace9601f004e847958d45a466ff62033c85a3ac3", "title": "The verification and synthesis of data structures", "references": ["6321686427c86b87e1071497ffd633b71aad6fb6", "94d9cbfc474cb6ce961de45a06233b2853ca6724", "c5340982746f1aac55c1cc7d2c06b670f522f253", "ed930c69cdc66f983c5abfd041ce9fef3565c08b", "2bb1c27799a1b6a22bb8b92cbc5bf66d1411a36b", "5a061b1cab0f241d6f7226f6c0b12e931cabd90f", "3c2eecf7fdcac147c06da1d53efe36a926530eee", "bf15ce3d1575d124527496cb249dc1249eee0acb"]}, {"date": "1975", "abstract": "The design of multiple computer systems and computer networks poses some interesting new problems and shows some older problems in the light of a new context. We are interested in the problems which are related to the design of communication protocols and error recovery procedures. In the design of computer systems, error recovery procedures have often not received very much attention, although they are essential for the reliable operation of most systems. In the case of multiple computer systems, appropriate error recovery procedures can be used to obtain a very much increased availability of the system services. This is also an advantage of computer networks. Whereas single computer systems can sometimes function to a certain extent without elaborate error recovery procedures, data communication protocols, on the other hand, normally have to contain explicit error recovery procedures, since the underlying communication line is normally not very reliable. Therefore, approaches that have been used for the design of protocols in computer networks could also be useful in the design of local multiple computer systems for obtaining higher reliability and availability.", "authors": ["Gregor von Bochmann"], "id": "f2a3b972c0f80c3ab5c817a749fc0562fe263e8b", "title": "Communication protocols and error recovery procedures", "references": []}, {"date": "1976", "abstract": "The semantics of a simple parallel programming language is presented in two ways: deductively, by a set of Hoare-like axioms and inference rules, and operationally, by means of an interpreter. It is shown that the deductive system is consistent with the interpreter. It would be desirable to show that the deductive system is also complete with respect to the interpreter, but this is impossible since the programming language contains the natural numbers. Instead it is proved that the deductive system is complete relative to a complete proof system for the natural numbers; this result is similar to Cook's relative completeness for sequential programs.\n The deductive semantics given here is an extension of an incomplete deductive system proposed by Hoare. The key difference is an additional inference rule which provides for the introduction of auxiliary variables in a program to be verified.", "authors": ["Susan S. Owicki"], "id": "a097fc73c5d7467dc425a2879558c9c592b75b69", "title": "A consistent and complete deductive system for the verification of parallel programs", "references": []}, {"date": "1975", "abstract": "Methods for verifying programs written in a higher level programming language are devised and implemented. The system can verify programs written in a subset of PASCAL, which may have data structures such as ARRAY, POINTER, and RECORD and control structures such as WHILE, REPEAT, FOR, PROCEDURE, FUNCTION and COROUTINE. The process of creation of verification conditions is an extension of the work done by lgarashi, London and Luckham (1972) which is based on the deductive theory by Hoare (1969). Verification conditions are proved using specialized simplification and proof techniques, which consist of an arithmetic simplifier, equality replacement rules, fast algorithm for simplifying formulas using propositional truth value evaluation, and a depth first proof search process. The basis of deduction mechanism used in this prover is Gentzen-type formal system. Several sorting programs including Floyd's TREESORT3 and Hoare's FIND are verified. It is shown that the resulting array is not only ordered but also a permutation of the input array\n This research was supported in part by the Advanced Research Projects Agency of the Office of the Secretary of Defence under contract DAHC 15-73-C-0435.", "authors": ["Norihisa Suzuki"], "id": "e93035a3107ce04321b0e22abaeb56e293fccb58", "title": "Verifying programs by algebraic and logical reduction", "references": ["273c7c39279e19861e306f4d96efda9362ec4cf3", "34d9635a5dc5d38f7c4712fb249dcbe966d312c1"]}, {"date": "1974", "abstract": "The paper investigates methods for applying an on-line interactive verification system designed to prove properties of PASCAL programs. The methodology is intended to provide techniques for developing a debugged and verified version starting from a program, that (a) is possibly unfinished in some respects, (b) may not satisfy the given specifications, e.g., may contain bugs, (c) may have incomplete documentation, (d) may be written in non-standard ways, e.g., may depend on user-defined data structures. The methodology involves (i) interactive application of a verification condition generator, an algebraic simplifier and a theorem-prover; (ii) techniques for describlng data structures, type constraints, and properties of programs and subprograms (i.e. lower level procedures); (iii) the use of (abstract) data types in structuring programs and proofs. Within each unit (i.e. segment of a problem), the interactive use is aimed at reduclng verification conditions to manageable proportions so that the non-trivial factors may be analysed. Analysis of verification conditions attempts to localize errors in the program logic, to extend assertions inside the program, to spotlight additional assumptions on program subfunctions (beyond those already specified by the programmer), and to generate appropriate lemmas that allow a verification to be completed. Methods for structuring correctness proofs are discussed that are similar to those of \"structured programming\". A detailed case study of a pattern matching algorithm illustrating the various aspects of the methodology (including the role played by the user) is given.", "authors": ["Friedrich W. von Henke", "David C. Luckham"], "id": "56e1d5cb852b342646a25e0d09b6362f3ece0f7f", "title": "Automatic program verification III: a methodology for verifying programs.", "references": ["2cf7ae6adfb101ca984b1988bd6bb0be4f9b739f", "c2bdd82fd3b74abe6564b7267e99f976d4956f0f", "bac9f6742d85e74cbf28413dc7ad70fca59d4436", "ed930c69cdc66f983c5abfd041ce9fef3565c08b", "d8ce4b5489ef14e8878c869101e30432d057599c", "7b9b83987be4369161938d00eb31e7e197c40aa3", "ea548fbbdfe46a37dd0edaac4a8d116343b7c39e"]}, {"date": "1965", "abstract": "A method is provided for treating waste water containing solids of difficultly-soluble compounds of at least one heavy metal, the method comprising the steps of adding an amount of ferrous ions to said waste water corresponding on a molar basis to about 2 to 100 times the total molar amount of heavy metal present in said waste water, adding an alkaline substance to said waste water in an amount corresponding to about 0.9 to 1.2 equivalent of free acid present in said waste water, and stirring the waste water without aeration at a temperature of at least about 40 DEG C., whereby insoluble ferrite crystals are precipitated having said heavy metal incorporated therein.", "authors": ["Peter J. Landin"], "id": "9aed59ed036b5715706ac44ba53eb20eff0911ed", "title": "Correspondence between ALGOL 60 and Church's Lambda-notation: part I", "references": []}, {"date": "1967", "abstract": "This dissertation presents a descriptive notation for data structures which is embedded in a programming language in such a way that the resulting language behaves as a synthetic tool for describing data and processes in a number of application areas. A series of examples including formulae, lists, flow charts, Algol text, files, matrices, organic molecules and complex variables is presented to explore the use of this tool. In addition, a small formal treatment is given dealing with the equivalence of evaluators and their data structures.", "authors": ["Thomas A. Standish"], "id": "8c2b7fc9bd3186b455fd674732833035bce8aa5d", "title": "A Data Definition Facility for Programming Languages", "references": ["71a372e5f528e0d55a64cfdf398da60c6e572d67", "44e11083e4b15bfa1a09c9df7a5e6f7eefeaec01", "94d69ebef4df1fc18cb95e87a14c6d25c2dc92ea", "83f054294ba2726d02aa03e471da773c3383b146", "d075466245c0a58a6c2c98198ae2c6d937b0af11", "bfaf9f138b54a6e8f1093078672ce0f8368bc280", "2769c203102a875c10bc11affc161891472176d1", "b443e18512181514b19363cd54dd3309c70be20e", "e52924efad15d1c3cd464e1e27e031dbfe722a11", "d97d69e68057bf43c844c799c9132937b2efcd98"]}, {"date": "1970", "abstract": "Semantic Scholar extracted view of \"Toward a man-machine system for proving program correctness\" by Donald I. Good", "authors": ["Donald I. Good"], "id": "932bbc2425cea91f50d1ff0af936e89d0e973ebe", "title": "Toward a man-machine system for proving program correctness", "references": []}, {"date": "1958", "abstract": "Let R be a nonempty set, let N consist of all non-negative rational integers, and denote by RN the set of all functions on N to R. If R is a ring, a map M: R\"\u2014>P^ is linear if M(rxfx+r2f2)=rx(Mfx) +r2(Mf2) for rx, r2 in R, fx, f2 in RN. For a finite commutative ring with unit we determine which linear transformations M: RN\u2014+RN can be realized by finite automata. More precisely, let A, B he finite nonempty sets. A map M: AN\u2014>BN is an automaton transformation if there exists a finite set Q, maps Mq: A X\u00a3>\u2014><2, Mb: A XQ-*B, elements h in B, q in Q such that corresponding to each/ in AN there exists an h in QN satisfying", "authors": ["Anil Nerode"], "id": "ed56719962b555b7c37dc9ab3e9920ba9a21397c", "title": "Linear automaton transformations", "references": ["a496212ca3444e1e14b0668b82e2459d02dc275a"]}, {"date": "1956", "abstract": "Semantic Scholar extracted view of \"The logic of automata\" by Arthur W. Burks et al.", "authors": ["Arthur W. Burks", "Hao Wang"], "id": "5e391bd0bd05d6fd6b9ec2e82d7bc95feecee335", "title": "The logic of automata", "references": []}, {"date": "1956", "abstract": "Semantic Scholar extracted view of \"Gedanken-Experiments on Sequential Machines\" by Edward F. Moore", "authors": ["Edward F. Moore"], "id": "a6021735e8de4f32010c6313396432d99bbe2440", "title": "Gedanken-Experiments on Sequential Machines", "references": []}, {"date": "1959", "abstract": "Rabin has proved 1,2 that two-way finite automata, which are allowed to move in both directions along their input tape, are equivalent to one-way automata as far as the classification of input tapes is concerned. Rabin's proof is rather complicated and consists in giving a method for the successive elimination of loops in the motion of the machine. The purposeo f this note is to give a short, direct proof of the result.", "authors": ["John C. Shepherdson"], "id": "921617b2c3e166f5efddf77aad055dea73da7dc8", "title": "The Reduction of Two-Way Automata to One-Way Automata", "references": []}, {"date": "1968", "abstract": "Basel has been designed as the base language component for an extensible language facility called Elf. Elf will have several components: one for syntactic extension, one for the definition of communications with a given kind of environment, and some others. All these components will be able to perform modifications, additions, and deletions to Basel.", "authors": ["P. Jorrand"], "id": "f82211423d9cbf034d87598a93d9b4cae147ef34", "title": "Some aspects of Basel, the base language for an extensible language facility", "references": []}, {"date": "1946", "abstract": "By a string on a, 6 we mean a row of a's and 6's such as baabbbab. I t may involve only a, or 6, or be null. If, for example, gi, g2, gz represent strings baby aa, b respectively, string g2gigigzg2 on gi, g2, gz will represent, in obvious fashion, the string aababbabbaa on a, 6. By the correspondence decision problem we mean the problem of determining for an arbitrary finite set (gu g{), (g2, g2), \u2022 \u2022 \u2022 , (gM, gi) of pairs of corresponding non-null strings on a, b whether there is a solution in w, iu ii, \u2022 \u2022 \u2022 , in of equation", "authors": ["Emil L. Post"], "id": "5c406be5ae6f2935b6653f101f02332186cb50d9", "title": "A variant of a recursively unsolvable problem", "references": ["6fd197ec4e9495c1940c4c1023d56e721aa00944", "abefc3047e2c6025121a096d42bc1616f0c9911b", "60400c043b2624f9cfc2d8daa0f45f3c1d524de3"]}, {"date": "1975", "abstract": "Certifying an entire operating system to be reliable is too large a task to be practicable. Instead, we are designing a Security Kernel which will provide information security. The kernel's job is to monitor information flow in order to prevent compromise of security.\n Sound design is encouraged by using a technique called Structured Specification, in which successively more detailed models of the Security Kernel are developed. The initial model, M0, is an abstract description which formalizes governmental security applied to computer systems.\n Subsequent levels of modeling provide increasingly more detail, and gradually the models begin to resemble a particular system (Multics in this case). The second model, M1, defines a tree-structured file system, and an interagent communication system while M2 adds details concerning segmentation in a dynamic environment.\n It is intended that the final level of modeling will specify the primitive commands for the kernel of a Multics-like system and will enumerate precisely those assertions which must be proved about the implementation in order to establish correctness.", "authors": ["Kenneth G. Walter", "Samuel I. Schaen", "William F. Ogden", "William C. Rounds", "D. G. Shumway", "Doris Schaeffer", "K. Biba", "Franklyn T. Bradshaw", "Stanley R. Ames", "Janet M. Gilligan"], "id": "543e5c863e8e083e0371fc89cd6f18c375fdaa40", "title": "Structured specification of a Security Kernel", "references": []}, {"date": "1960", "abstract": "The evaluation of a formula of propositional calculus is considerably simplified if this formula is written in the parenthesis-free notation of the Warsaw School, [1]. The Warsaw notation may be formulated in the following way: There are symbols for operations, e.g.: N for negation; K for conjunction; A for disjunction; E for equivalence; C for implication; and symbols p, q, r, 8, t for variables. A variable is a formula. A formula preceded by the symbol N is a formula. Two juxtaposed formulas preceded by any one of the symbols, K, A, E, C are a formula. Evaluation of such a formula is done in the following way: Each of the variable symbols p, q, r,... has a value 0 or 1. The operation symbol acts on the value of the one or two formulas governed by it giving the value of the compound formula. It was remarked in 1950 by H. Angstl, [21, that a mechanical evaluation of a formula, written in Warsaw notation without brackets, can be done in the following easy way: Each of the variable symbols is represented by a box with one output, the negation by a box with one input and one output, and the other operation symbols by a box with two inputs and one output. The meaning of a formula in the Warsaw notation is given by Angstl's rule: The first input of each operation symbol is to be connected with the output of the next following symbol, either variable or operation. The symbol N excepted, the second input of each operation symbol is to be connected with the first remaining free output of a symbol going from left to right. This may be demonstrated by an example, which uses Stanislaus present capacity of eleven symbols: the tautology of transitivity of the implication [(p -+ q) & (q -+ r)] -+ (p -r), written in Warsaw notation", "authors": ["Friedrich L. Bauer"], "id": "4b4f7cd6e795f404a544cf60cf587c0876395b27", "title": "The formula-controlled logical computer \u201cStanislaus\u201d", "references": ["4c61310dbfcd29ed74c3a8b9703cee4bdf4c1791"]}, {"date": "1964", "abstract": "A computational procedure is derived analytically to evaluate the input/output buffer storage requirements in a data exchange. \n \nValidity of the analysis is substantiated by comparing--in a typical instance--the analytical results with those obtained by simulation.", "authors": ["Ints Delgalvis", "Gerald A. Davison"], "id": "b1f61c792ea7bd233892dfd0252f4721d901857a", "title": "Storage Requirements for a Data Exchange", "references": []}, {"date": "1960", "abstract": "Semantic Scholar extracted view of \"Report on the algorithmic language ALGOL 60\" by John W. Backus et al.", "authors": ["John W. Backus", "Friedrich L. Bauer", "Julien Green", "C. Katz", "John McCarthy", "Peter Naur", "Alan J. Perlis", "Heinz Rutishauser", "Klaus Samelson", "Bernard Vauquois", "J. H. Wegstein", "Adriaan van Wijngaarden", "Michael Woodger"], "id": "859c0cf9447ae4633913183031656cd200c84c13", "title": "Report on the algorithmic language ALGOL 60", "references": []}, {"date": "1973", "abstract": "Semantic Scholar extracted view of \"An interactive system for the verification of computer programs\" by Bernard Elspas et al.", "authors": ["Bernard Elspas", "Karl N. Levitt", "Richard J. Waldinger"], "id": "7b9b83987be4369161938d00eb31e7e197c40aa3", "title": "An interactive system for the verification of computer programs", "references": []}, {"date": "1971", "abstract": "An elementary outline of the theorem-proving approach to automatic program synthesis is given, without dwelling on technical details. The method is illustrated by the automatic construction of both recursive and iterative programs operating on natural numbers, lists, and trees.\nIn order to construct a program satisfying certain specifications, a theorem induced by those specifications is proved, and the desired program is extracted from the proof. The same technique is applied to transform recursively defined functions into iterative programs, frequently with a major gain in efficiency.\nIt is emphasized that in order to construct a program with loops or with recursion, the principle of mathematical induction must be applied. The relation between the version of the induction rule used and the form of the program constructed is explored in some detail.", "authors": ["Zohar Manna", "Richard J. Waldinger"], "id": "94d9cbfc474cb6ce961de45a06233b2853ca6724", "title": "Toward automatic program synthesis", "references": []}, {"date": "2004", "abstract": "SummaryA powerful method of simplifying the proofs of program correctness is suggested; and some new light is shed on the problem of functions with side-effects.", "authors": ["C. A. R. Hoare"], "id": "2bb1c27799a1b6a22bb8b92cbc5bf66d1411a36b", "title": "Proof of correctness of data representations", "references": ["7387b90ea3b2b367ba69d9e1594cc62c800abddf", "9ad3cac3bee4e8dfab3b3440da2b4070bb455a36", "5a70187cf84a450f2691a124c3bb94ab43aadcd7", "1d2e601af32ed6ce00406e4d24e156bffaf87c70", "f38381e73224670f0cfc9b8b9e0f751938ce5a40", "8cb908733160a00b45d8ad8e459649753781891d"]}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"A Program Verifier\" by James C. King", "authors": ["James C. King"], "id": "34d9635a5dc5d38f7c4712fb249dcbe966d312c1", "title": "A Program Verifier", "references": []}, {"date": "1966", "abstract": "Formula ALGOL is an extension of ALGOL 60 incorporating formula manipulation and list processing. The extension is ac1 complished by adding two new types of data structures: formulas and list structures with an appropriate set of processes to manipulate them. The control structure of ALGOL 60 is inherited without change. Algorithms may construct formulas and list structures at run time; in fact, ALGOL 60 is contained within the language as a subset. Operations are available which alter or combine formulas and list structures, and which access arbitrary subexpressions. Formulas may be evaluated, substituting numerical or logical values for occurrences of variables contained within. They may be subjected to substitution processes causing the replacement of occurrences of variables by designated formulas. They may be subjected to transformations defined by sets of rules akin to Markov algorithms. Predicates are available to determine precisely the structure and composition of any formula or list structure constructable, and mechanisms are provided to extract subexpressions of a formula or sublists of a list provided its structure is known. Numerical, logical and formula values may be stored as elements in list structures, and retrieval mechanisms exist to select them for use as constituents in other processes. Description lists composed of attributes with associated lists of values may be attached to formulas and to list structures and may be associated with identifiers naming variables. Processes exist for retrieving value lists and for creating, altering and deleting attribute-value list pairs. Pushdown stacks of arbitrary depth are available for the storage of all types of data structures, and generators are provided in the form of new types of for statements which assign to controlled variables the elements of list structures for use in an arbitrary process. Several standard procedures create names at run-time, provide the current size of the available space list, differentiate a formula with respect to a given variable, erase list structures and so on. Finally, both arrays and procedures may be defined having formulas or list structures as values. The system is in operation on the CDC G-21 at Carnegie Institute of Technology.", "authors": ["Alan J. Perlis", "Renato Iturriaga", "Thomas A. Standish"], "id": "71a372e5f528e0d55a64cfdf398da60c6e572d67", "title": "A definition of forumla ALGOL", "references": []}, {"date": "1972", "abstract": "A special purpose theorem prover for establishing the validity of expressions over integer variables was developed as part of a program verifier. It is built around a powerful system for manipulating and simplifying integer expressions.", "authors": ["James C. King", "Robert W. Floyd"], "id": "c2bdd82fd3b74abe6564b7267e99f976d4956f0f", "title": "An Interpretation-Oriented Theorem Prover over Integers", "references": []}, {"date": "1965", "abstract": "Abstract : Programmers developing systems of the complexity required in artificial intelligence research are frequently hindered by the rigid programming languages available and the time-consuming task of implementing new languages. AMOS (for associative memory organizing system) provides a flexible means to structure data and experiment with the syntactic forms of program statements while lessening the implementation bottleneck. AMOS is a syntax-directed compiler used to define languages for constructing a variety of data organizations of which Fortran-like arrays and IPL-like list structues are special cases. This research explores the use of syntactic descriptions which are not Backus Normal Form grammars and provides means for defining two-demensional languages as well as the usual linear type. In order to facilitate implementation, the system may be conveniently imbedded in any monitor system of common design; AMOS operations are manipulations within high-speed storage only. (Author)", "authors": ["Robert K. Lindsay", "Terrence W. Pratt", "Kenneth M. Shavor"], "id": "d97d69e68057bf43c844c799c9132937b2efcd98", "title": "AN EXPERIMENTAL SYNTAX-DIRECTED DATA STRUCTURE LANGUAGE,", "references": []}, {"date": "1963", "abstract": "To meet the need for improved documentation of written computer programs, a simple system for effective communication is presented, which has shown great promise. The programmer describes his program in a simple format, and the computer prepares flow charts and other cross-referenced listings from this input. The description can be kept up-to-date easily, and the final output clearly explains the original program. The system has also proved to be a valuable debugging and coding aid.", "authors": ["Donald E. Knuth"], "id": "bfaf9f138b54a6e8f1093078672ce0f8368bc280", "title": "Computer-drawn flowcharts", "references": []}, {"date": "1966", "abstract": "Bell Telephone Laboratories' Low-Level Linked List Language <italic>L</italic><supscrpt>6</supscrpt> (pronounced \u201c<italic>L</italic>-six\u201d) is a new programming language for list structure manipulations. It contains many of the facilities which underlie such list processors as IPL, LISP, COMIT and SNOBOL, but permits the user to get much closer to machine code in order to write faster-running programs, to use storage more efficiently and to build a wider variety of linked data structures.", "authors": ["Kenneth C. Knowlton"], "id": "e52924efad15d1c3cd464e1e27e031dbfe722a11", "title": "A programmer's description of L6", "references": ["30901b8eb11da71262fd343114efcb42c5c486fa"]}, {"date": "1943", "abstract": "Semantic Scholar extracted view of \"Formal Reductions of the General Combinatorial Decision Problem\" by Emil L. Post", "authors": ["Emil L. Post"], "id": "6fd197ec4e9495c1940c4c1023d56e721aa00944", "title": "Formal Reductions of the General Combinatorial Decision Problem", "references": []}, {"date": "1964", "abstract": "Abstract : This dissertation presents a number of results attained in a study of the formalization of certain properties of computer-oriented languages. The presentation of the results in the thesis is based on the structure of this program, called the compiler-compiler. Although there are several sections devoted to theoretical questions, these are set off from the main development. A more detailed introduction to the paper is given and some of the philosophical questions raised by formalizing semantics are considered. A formal syntax language used in the compiler-compiler is discussed; relationships are established between this formalization of syntax and others appearing in the literature. A complete discussion of the Formal Semantic Language is given further. The two formal systems were combined to form the basis for a useful computer technique. The final chapter contains a discussion of the strengths and weaknesses of our system as well as several suggestions for future research. The appendices include a record of the development of a translator for one small language from a formal definition of the language to examples of resultant machine code.", "authors": ["Jerome A. Feldman"], "id": "94d69ebef4df1fc18cb95e87a14c6d25c2dc92ea", "title": "A FORMAL SEMANTICS FOR COMPUTER-ORIENTED LANGUAGES", "references": ["8e412b65ead1d45cf2a8200ed3632eea12e048a4", "dafabc60fe64f5fea4d20d464d453c262d2649b4", "28dcfd519a2058d5deef6c9e64a1d4d5721dba45", "79fcb6fa7106e2a1d78654ce958377b8f64a5156", "e5d8b62af26b9240989ddb567c11e710eb42e331"]}, {"date": "1959", "abstract": "This paper gives a summary of the syntax and interpretation rules of the proposed international algebraic language put forward by the Zurich ACM-GAMM Conference, followed by a formal , complete presentation of the same information. Notations are presented for numbers, numerical variables, Boolean variables , relations, n-dimensional arrays, functions, operators and algebraic expressions. Means are provided in the language for the assignment of values to variables, conditional execution of statements , iterative procedures, formation of compound statements from sequences of statements, definition of new statements for arbitrary procedures, and the re-use and alteration of program segments. The proposed language is intended to provide convenient and concise means for expressing virtually all procedures of numerical computation while employing relatively few syntactical rules and types of statement. La syl1taxe et la semantique de langage algebraic international propose par la Conference de Zurich (ACM et GAMM). L'autcur caracterise brievement la syntaxe et les regles d'inter-pretation du langage algebrique international propose a la Conference de Zurich (ACM-GAMM) puis en donne un expose formel et complet. II indique les notations utilisees pour designer les nombres, les variables numeriques ou booIeennes, les relations, les agencements pluri-dimensionnels, les fonctions, les operateurs et les expressions algebriques. Ce langage permet d'exprimer difierentes operations: affectation de valeurs aux variables, execution conditionnelle des expressions, procedes iteratifs, formation d'expressions complexes a partir d'une suite d'expressions elementaires, definition de nouvelles expressions pour des operations arbitraires, reemploi et modification de certaines parties du programme. Le lang age envisage est conyu pour permettre d'exprimer la quasi totalite des procedes de calcul numerique de maniere com-mode et concise a l'aide d'un nombre relativement restreint de regles de syntaxe et d'expressions-types.", "authors": ["John W. Backus"], "id": "d075466245c0a58a6c2c98198ae2c6d937b0af11", "title": "The syntax and semantics of the proposed international algebraic language of the Zurich ACM-GAMM Conference", "references": ["52219ea02e97f867892435f023215d31cdcc5fa2"]}, {"date": "1961", "abstract": "The algebraic command languages (ALGOL, IT, FORTRAN, UNICODE), although useful in preparing numerical algorithms, have not in the author's opinion proven themselves useful for symbol manipulation algorithms, particularly compilers. List processors, in fact, have been designed primarily to fill this gap. Analogously, the traditional flowchart serves well as a descriptive language for numerical algorithms, but does not lend itself to description of symbol manipulation algorithms in such a way that the intent of the process is clear. I t will be the purpose of this paper to present a more suitable notation for description of compilers and other complicated symbol manipulation algorithms. The algorithms used in formula translation consist principally of the following elements: (1) A set of linguistic transformations upon the input string, together with conditions determining the applicability of each transformation. (2) A set of actions, such as the generation of machine language coding, associated with each transformation. (3) A rule for transfering the attention of the translator from one portion of the input string to another. The notation presented here greatly simplifies the representation of the first and third elements. For illustrative purposes, a compilation process for a small subset of ALGOL is described below. The subset consists of assignment statements constructed from identifiers, the five binary arithmetic operators ( T, \u00d7 , / , 4 , ), the two unary arithmetic operators (-{-, --), the replacement operator ( : = ) , parentheses, and the library functions of one variable (sin, exp, sqrt, etc.). The assignment statement Z to be translated is initially taken in the augmented form ~ A2~ ~ , where the characters ~and ~ serve as termination symbols and a is a pointer which indicates the portion of the statement where the translator's attention is currently focused. The following productions and the associated generation rules respectively decompose the original statement in accordance with its structure, and simultaneously create coding to implement the statement. Coding will be represented by ALGOL statements with at most one operator, to avoid reference to particular computers.", "authors": ["Robert W. Floyd"], "id": "44e11083e4b15bfa1a09c9df7a5e6f7eefeaec01", "title": "A Descriptive Language for Symbol Manipulation", "references": []}, {"date": "1973", "abstract": "Semantic Scholar extracted view of \"Proving theorems about lisp problems\" by Robert S. Boyer et al.", "authors": ["Robert S. Boyer", "James A. Moore"], "id": "273c7c39279e19861e306f4d96efda9362ec4cf3", "title": "Proving theorems about lisp problems", "references": []}, {"date": "1943", "abstract": "Semantic Scholar extracted view of \"Review: Emil L. Post, Formal Reductions of the General Combinatorial Decision Problem\" by Alonzo Church", "authors": ["Alonzo Church"], "id": "abefc3047e2c6025121a096d42bc1616f0c9911b", "title": "Review: Emil L. Post, Formal Reductions of the General Combinatorial Decision Problem", "references": []}, {"date": "1954", "abstract": "Semantic Scholar extracted view of \"An analysis of a logical machine using parenthesis-free notation\" by Arthur W. Burks et al.", "authors": ["Arthur W. Burks", "Don W. Warren", "Jesse B. Wright"], "id": "4c61310dbfcd29ed74c3a8b9703cee4bdf4c1791", "title": "An analysis of a logical machine using parenthesis-free notation", "references": []}, {"date": "1973", "abstract": "Semantic Scholar extracted view of \"A verified program-verifier.\" by Larry C. Ragland", "authors": ["Larry C. Ragland"], "id": "c77a34f6e3d738e944e93972a292ad4f39c5e794", "title": "A verified program-verifier.", "references": []}, {"date": "1972", "abstract": "Semantic Scholar extracted view of \"Proof of a structured program: 'the sieve of Eratosthenes'\" by C. A. R. Hoare", "authors": ["C. A. R. Hoare"], "id": "7d6974b8b61da3dfd32b2c4f16ac3920eb934402", "title": "Proof of a structured program: 'the sieve of Eratosthenes'", "references": []}, {"date": "1974", "abstract": "Semantic Scholar extracted view of \"Proving that computer programs terminate cleanly.\" by Richard L. Sites", "authors": ["Richard L. Sites"], "id": "c282549889226ad9ab6cd867a5d7d6dbae58089c", "title": "Proving that computer programs terminate cleanly.", "references": []}, {"date": "1971", "abstract": "A simulation relation between programs is defined which is quasi-ordering. Mutual simulation is then an equivalence relation, and by dividing out by it we abstract from a program such details as how the sequencing is controlled and how data is represented. The equivalence classes are approxiamtions to the algorithms which are realized, or expressed, by their member programs. A technique is given and illustrated for proving simulation and equivalence of programs; there is an analogy with Floyd''s technique for proving correctness of programs. Finally, necessary and sufficient conditions for simulation are given.", "authors": ["Robin Milner"], "id": "8cb908733160a00b45d8ad8e459649753781891d", "title": "An Algebraic Definition of Simulation Between Programs", "references": ["375c0850cd418002ee2be0fb2b6ec177573f950a", "1b433c7acad4a5c246383c16bc97832982a734b1"]}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"The Development of Programs by Stepwise Refinement\" by Niklaus Wirth", "authors": ["Niklaus Wirth"], "id": "5a70187cf84a450f2691a124c3bb94ab43aadcd7", "title": "The Development of Programs by Stepwise Refinement", "references": []}, {"date": "1969", "abstract": "Semantic Scholar extracted view of \"An axiomatic approach for computer programming\" by C. A. R. Hoare", "authors": ["C. A. R. Hoare"], "id": "1d2e601af32ed6ce00406e4d24e156bffaf87c70", "title": "An axiomatic approach for computer programming", "references": []}, {"date": "1968", "abstract": "As an alternative to methods by which the correctness of given programs can be established a posteriori, this paper proposes to control the process of program generation such as to produce a priori correct programs. An example is treated to show the form that such a control might then take. This example comes from the field of parallel programming; the way in which it is treated is representative of the way in which a whole multiprogramming system has actually been constructed.", "authors": ["Edsger W. Dijkstra"], "id": "f38381e73224670f0cfc9b8b9e0f751938ce5a40", "title": "A constructive approach to the problem of program correctness", "references": []}, {"date": "1964", "abstract": "SNOBOL is a programming language for the manipulation of strings of symbols. A statement in the SNOBOL language consists of a rule that operates on symbolically named strings. The basic operations are string formation, pattern matching and replacement. Facilities for integer arithmetic, indirect referencing, and input-output are included. In the design of the language, emphasis has been placed on a format that is simple and intuitive. SNOBOL has been implemented for the IBM 7090.", "authors": ["David J. Farber", "Ralph E. Griswold", "I. P. Polonsky"], "id": "30901b8eb11da71262fd343114efcb42c5c486fa", "title": "SNOBOL , A String Manipulation Language", "references": []}, {"date": "1950", "abstract": "Office hours: MWF, immediately after class or early afternoon (time TBA). We will cover the mathematical foundations of probability theory. The basic terminology and concepts of probability theory include: random experiments, sample or outcome spaces (discrete and continuous case), events and their algebra, probability measures, conditional probability A First Course in Probability (8th ed.) by S. Ross. This is a lively text that covers the basic ideas of probability theory including those needed in statistics. Theoretical concepts are introduced via interesting concrete examples. In 394 I will begin my lectures with the basics of probability theory in Chapter 2. However, your first assignment is to review Chapter 1, which treats elementary counting methods. They are used in applications in Chapter 2. I expect to cover Chapters 2-5 plus portions of 6 and 7. You are encouraged to read ahead. In lectures I will not be able to cover every topic and example in Ross, and conversely, I may cover some topics/examples in lectures that are not treated in Ross. You will be responsible for all material in my lectures, assigned reading, and homework, including supplementary handouts if any.", "authors": ["Feller William"], "id": "65ba8fd8ef9c2a70cee99d2e5cab9302d0307a1e", "title": "An Introduction To Probability Theory And Its Applications", "references": []}, {"date": "1970", "abstract": "These notes have the status of \"Letters written to myself\": I wrote them down because, without doing so, I found myself repeating the same arguments over and over again. When reading what I had written, I was not always too satisfied. For one thing, I felt that they suffered from a marked verbosity. Yet I do not try to condense them (now), firstly because that would introduce another delay and I would like to \"think on\", secondly because earlier experiences have made me afraid of being misunderstood: many a programmer tends to see his (sometimes rather specific) difficulties as the core of the subject and as a result there are widely divergent opinions as to what programming is really about. I hope that, despite its defects, you will enjoy at least parts of it. If these notes prove to be a source of inspiration or to give you a new appreciation of the programmer's trade, some of my goals will have been reached. Prior to their publication in book form, the \"Notes on Structured Programming\" have been distributed privately. The interest then shown in them, for which I would like to express my gratitude here, has been one of the main incentives to supplement them with some additional material and to make them available to a wider public. In particular I would like to thank Bob Floyd, Ralph London and Mike Woodger for their encouraging comments and Peter Naur for the criticism he expressed. Finally I would like to express my gratitude to Mrs. E. L. Dijkstra-Tucker for her kind assistance in my struggles with the English language.", "authors": ["Edsger W. Dijkstra"], "id": "9ad3cac3bee4e8dfab3b3440da2b4070bb455a36", "title": "Notes on structured programming", "references": []}, {"date": "1973", "abstract": "Semantic Scholar extracted view of \"Nucleus---a language for provable programs\" by Donald I. Good et al.", "authors": ["Donald I. Good", "Larry C. Ragland"], "id": "bac9f6742d85e74cbf28413dc7ad70fca59d4436", "title": "Nucleus---a language for provable programs", "references": []}, {"date": "1972", "abstract": "Semantic Scholar extracted view of \"Notes on Data Structuring\" by Tony Hoare", "authors": ["Tony Hoare"], "id": "7387b90ea3b2b367ba69d9e1594cc62c800abddf", "title": "Notes on Data Structuring", "references": []}, {"date": "1974", "abstract": "Abstract : This report discusses the programming language tools needed to support the expression of 'well-structured' programs. In particular it deals with the tools needed to express abstractions and their realizations; to this end it introduces the concept of a 'form' to subsume the notions of type (mode), macro, procedure, generator, and coercion. An extendedexample is given together with the sketch of a proof of the example. The proof is included to support the contention that formal verification is substantially simplified when the abstractions and their realization are retained in the program text.", "authors": ["William A. Wulf"], "id": "44484fdf6fcb0ae811362afdaa86841537d85b2a", "title": "ALPHARD: Toward a Language to Support Structured Programs", "references": ["f90111945b1c643feb4c7eebd182ba54fd87fba8", "7387b90ea3b2b367ba69d9e1594cc62c800abddf"]}, {"date": "1964", "abstract": "The complexity of the tasks performed by computer systems has been expanding rapidly throughout the short history of these machines, but only in recent years has the basic feature of sequential control of operations been seriously violated. The large computing system of the future will have multiple processing capabilities and will be operated in a shared manner in order to obtain the potential efficiencies and expansions of application areas which are possible in such a system. A shared computer system will be heavily dependent on real time interactions with people and other machines. The effectiveness of such expansion in the application areas of shared systems depends upon advances in both hardward and software structures, and upon the feedback between them. In order to solve the language problems of such a system, it is not sufficient to try to find a more convenient language to describe conventional program structures. Brown has indicated the need for a new concept of programming in the environment and has discussed many of the necessary features including the ability to leave sequencing control in the hands of the system.", "authors": ["D. R. Fitzwater", "Earl J. Schweppe"], "id": "b4e374f2d889f9cef9b31f2b02ad225e9c0a9bed", "title": "Consequent procedures in conventional computers", "references": []}, {"date": "1958", "abstract": "Editor's Notes: Although this method is not novel, it has been printed here to summarize for the benefit of a new generation of computer personnel. I t should be noted that : 1) This method seems advantageous if only a few significant figures are required. Otherwise the normal method, Log-Multiply-Antilog, is more desirable and faster in particular for higher order roots. These subroutines are normally required for other purposes anyway and space is not lost. 2) One immediately notices many tricky ways of coding this method for a computer, via looping and the use of tables or converting instructions. Note that, as one proceeds, the contribution of the left-hand term becomes proportionately large enough such tha t it alone might be used within accuracy limits after a certain number of digits are developed. 3) Although the author states that this method used more memory space than other routines, it seems tha t the converse could well be true if advantage were taken of higher order differences in building up the subtrahend. This appears to be a natural method for a 256 memory machine, if it had good indexing and looping features. Remember that An(X ~) = a constant n[", "authors": ["Alan J. Perlis", "Klaus Samelson"], "id": "52219ea02e97f867892435f023215d31cdcc5fa2", "title": "Preliminary report: international algebraic language", "references": []}, {"date": "1957", "abstract": "Semantic Scholar extracted view of \"Introduction to mathematical logic: Vol. I, by Alonzo Church. 376 pages, 6 \u00d7 9 in. Princeton, Princeton University Press, 1956. Price, $7.50\" by Haskell B. Curry", "authors": ["Haskell B. Curry"], "id": "79fcb6fa7106e2a1d78654ce958377b8f64a5156", "title": "Introduction to mathematical logic: Vol. I, by Alonzo Church. 376 pages, 6 \u00d7 9 in. Princeton, Princeton University Press, 1956. Price, $7.50", "references": []}, {"date": "2002", "abstract": "This chapter is intended for all those who expect that in their future activities they will become seriously involved in the problems that arise in either the design or the more advanced applications of digital information processing equipment; they are further intended for all those who are just interested in information processing.", "authors": ["Edsger W. Dijkstra"], "id": "cd46e66972b056719a988e5c25440ab796c3f2f2", "title": "Cooperating sequential processes", "references": []}, {"date": "1963", "abstract": "In recent years formal languages have become a subject of wide interest--from a theoretical point of view in connection with symbolic logic and automata theory, from a practical point of view as input languages for information processing systems. In both cases some processing of these languages enters into the question, generally as a translation into another formal language. Some examples illustrate the possible variation of the circumstances with regard to the origin and meaning of formM languages and to their syntactical structure: languages for describing theorems, sequential circuits, differential equations systems or numerical algorithms. However, actual processors constructed for languages whose syntax can be described in the same metalanguage, e.g. Baekus notation [2], have shown remarkable similarities. In our own experience an important general principle in processing formal languages was the cellar principle introduced in 1957 by F. L. Bauer and K. Samelson [4, 5] in the design of a formal language-controlled computer. This principle has been described in [19, 20]. The essential features are as follows: The incoming information is analyzed sequentially, the meaning of each symbol already being established insofar as it can be determined from previous history. Corresponding information is stored into a state pushdown store, the \"cellar.\" In this way the momentary top levels of the cellar always reflect previous histo~T to the extent necessary to analyze the next incoming symbol. This principle proved to be applicable in translating programs from the algorithmic language ALGOL [3, 17] into machine code (pilot ALGOL 58 translator of the ZMMD group for the Zuse Z22, ERMETH and PERM computer) or into a computer-oriented, macro instruction language (logical plans of the ALCOR group in 1.959 and 1960). The principle was found and used independently elsewhere. Processors for other formal languages have been built, using a similar technique [7, 18].", "authors": ["J\u00fcrgen Eickel", "Manfred Paul", "Friedrich L. Bauer", "Klaus Samelson"], "id": "28dcfd519a2058d5deef6c9e64a1d4d5721dba45", "title": "A syntax controlled generator of formal language processors", "references": []}, {"date": "1970", "abstract": "Semantic Scholar extracted view of \"A formal notion of simulation between programs\" by Robin Milner", "authors": ["Robin Milner"], "id": "1b433c7acad4a5c246383c16bc97832982a734b1", "title": "A formal notion of simulation between programs", "references": []}, {"date": "1975", "abstract": "It is shown how the Lisp iterative primitives PROG, SETQ, GO, and RETURN may be introduced into the Boyer-Moore method for automatically verifying Pure Lisp programs. This is done by extending some of the previously described heuristics for dealing with recursive functions. The resulting verification procedure uses structural induction to handle both recursion and iteration. The procedure does not actually distinguish between the two and they may be mixed arbitrarily. For example, since properties are stated in terms of user-defined functions, the theorem prover will prove recursively specified properties of iterative functions. Like its predecessor, the procedure does not require user-supplied inductive assertions for the iterative programs.", "authors": ["J. Strother Moore"], "id": "950d442bdd212a7a4512baa4b3882cf4491ee974", "title": "Introducing iteration into the Pure Lisp theorem prover", "references": ["fa53b77026cc5c361b02ad8e6cc209d6c4a880df", "09661a6bb7578979e42c75d6ce382baba64d4981", "b3fe91923c1356a8c6dee2c725dd11fcbfeae903", "cb5eebdd166a6cb0d1e2de65fb70ea1c913ef00d"]}, {"date": "1964", "abstract": "Semantic Scholar extracted view of \"An ALGOL 60 Compiler\" by Arthur Evans", "authors": ["Arthur Evans"], "id": "e5d8b62af26b9240989ddb567c11e710eb42e331", "title": "An ALGOL 60 Compiler", "references": []}, {"date": "1963", "abstract": "Semantic Scholar extracted view of \"Formal properties of grammars\" by Noam Chomsky", "authors": ["Noam Chomsky"], "id": "8e412b65ead1d45cf2a8200ed3632eea12e048a4", "title": "Formal properties of grammars", "references": []}, {"date": "1959", "abstract": "A grammar can be regarded as a device that enumerates the sentences of a language. We study a sequence of restrictions that limit grammars first to Turing machines, then to two types of system from which a phrase structure description of the generated language can be drawn, and finally to finite state Markov sources (finite automata). These restrictions are shown to be increasingly heavy in the sense that the languages that can be generated by grammars meeting a given restriction constitute a proper subset of those that can be generated by grammars meeting the preceding restriction. Various formulations of phrase structure description are considered, and the source of their excess generative power over finite state sources is investigated in greater detail.", "authors": ["Noam Chomsky"], "id": "dafabc60fe64f5fea4d20d464d453c262d2649b4", "title": "On Certain Formal Properties of Grammars", "references": ["0725438da27f7f83334cae357bd4415cf93d2344", "b449b779203f4392211e6db98720d46eecc0e4bd", "ed71ebe0eee4f88f095247c8b62ba1d3b217a68d", "0f2d98c02fbcac128c9aa925964eeb0ddfae8840", "1c2c6558d2b7be61d1f4910d7ff438ffbda52890"]}, {"date": "1975", "abstract": "We describe some simple heuristics combining evaluation and mathematical induction which we have implemented in a program that automatically proves a wide variety of theorems about recursive LISP functions. The method the program uses to generate induction formulas is described at length. The theorems proved by the program include that REVERSE is its own inverse and that a particular SORT program is correct. Appendix B contains a list of the theorems proved by the program.", "authors": ["Robert S. Boyer", "J. Strother Moore"], "id": "09661a6bb7578979e42c75d6ce382baba64d4981", "title": "Proving Theorems about LISP Functions", "references": []}, {"date": "1976", "abstract": "A protocol for use at the host-host level of computer networks is presented. The data transfer aspects of the protocol, rather than the connection establishment aspects, are considered. The protocol is designed to operate correctly in an environment where packets may be lost, duplicated and/or re-ordered in transit. The protocol is conventional; it uses a positive acknowledgment/retransmission on timeout system, and both transmitter and receiver maintain a window . The behaviour of the stations using the protocol is defined by means of a high level programming language, under the assumption that sequence numbers may increase indefinitely. Working directly from this definition, assertions which reflect important characteristics of the protocol are derived and verified. Partial verification of the protocol consists of showing that if the protocol progresses then it will progress correctly; however it is not shown that the protocol will progress. The assertions are then used as a basis for a demonstration that cyclic sequence numbers could be used in the protocol without ambiguity.", "authors": ["Vic Stenning"], "id": "9adc51fc971a91c6a67f66fd8c49bc6ca44118fc", "title": "A Data Transfer Protocol", "references": []}, {"date": "1971", "abstract": "Digital computing systems have traditionally been described as being composed of the five basic units: input, output, memory, arithmetic/logic, and control (see Figure 1). Machine instructions and data communicated among these units (as indicated by the solid lines in the figure) are generally well-known and understood. The control signals (as indicated by dashed lines in the figure), are generally less well-known and understood except by the system designer. These control signals generated in the control unit determine the information flow and timing of the system.", "authors": ["Earl W. Reigel", "U. Faber", "D. A. Fisher"], "id": "c50eec60013132f139b589f84fb77487ecafc680", "title": "The interpreter: a microprogrammable building block system", "references": []}, {"date": "1969", "abstract": "Fuzzy logic deals with propositions which may be ascribed values between falsehood and truth (0 and 1) subjectively in either a continuous or a discrete fashion. This is in contrast to ordinary logic (two-valued or k-valued logic) in which a given proposition is ascribed values objectively using either deterministic or probabilistic approaches.", "authors": ["Peter N. Marinos"], "id": "f93d7109d1f1e4b4a5c70a5aa10d74bdbab50ec8", "title": "Fuzzy Logic and its Application to Switching Systems", "references": []}, {"date": "1970", "abstract": "This paper describes an algorithm which will generate all the prime implicants of a Boolean function. The algorithm is different from those previously given in the literature, and in many cases it is more efficient. It is proved that the algorithm will find all the prime implicants. The algorithm may possibly generate some nonprime implicants. However, using frequency orderings on literals, the experiments with the algorithm show that it usually generates very few ( possibly none) nonprime implicants. Furthermore, the algorithm may be used to find the minimal sums of a Boolean function. The algorithm is implemented by a computer program in the LISP language.", "authors": ["James R. Slagle", "Chin-Liang Chang", "Richard C. T. Lee"], "id": "92c1316871b83d4adfa9461c589be81d7a88b595", "title": "A New Algorithm for Generating Prime Implicants", "references": []}, {"date": "1971", "abstract": "Some of the papers presented in this book already have been widely circulated; others were published in well-known journals, like IBM Systems Journal but largely were ignored when they first appeared; and then there are the obscure papers like this one by Ashcroft and Manna, which was presented at the 1971 IFIP Conference in Ljubljana, Yugoslavia. It's not that the ideas in the paper are obscure -- it's just that very few people in the mainstream EDP community attended the Conference, and precious few copies of the conference proceedings ever found their way into American libraries. It is, however, a paper that many people over the years have wanted to read, particularly since it deals with a subject also mentioned by Knuth (\"Structured Programming with go to State, ments\" [see Paper 20]), Wulf (\"A Case Against the GOTO\" [Paper 8]), and Bom and Jacopini (\"Flow Diagrams, Turing Machines and Languages with Only Two Formation Rules\" [Paper 2]). \n \nThe subject of the Ashcroft and Manna paper is the translation of unstructured programs into equivalent structured programs. Although Wulf's paper sets forth a more practical, step-by-step mechanism for such translations, Ashcroft and Manna give an extremely detailed, extremely theoretical presentation, providing an important addition to the work of Bom and Jacopini --- but it's definitely for the more serious students of computer science. \n \nThe larger issue of \"restructuring\" is ignored by Ashcroft and Manna, and, to a large extent, by everyone else. The issue began as a theoretical question: Could any program be written as a structured program? Bom and Jacopini answered the question in the affirmative by demonstrating that any arbitrary program could be translated ir/to an equivalent structured program. Wulf, Knuth, and Ashcroft and Manna shifted the emphasis of the question slightly: Could one translate an existing program into a structured program that still would have the same topology as the original program? To most people, the mere question suggested heresy, perverting the very idea of writing structured programs! Rather than writing bad code and then cleaning it up, they argued, we should begin by writing good code in the first place. \n \nBut the larger issue of restructuring does exist. The vast mountains of unstructured code, which already were written before structured programming came along, clearly can't be thrown away. Members of the average EDP organization have to live with their code, for better or worse, for a period of ten years or more before they can afford to discard it. Do Ashcroft and Manna have the solution for these people? Can we take existing unstructured code and translate it into more maintainable, structured code? And, more important, can we do it mechanically? \n \nIn principle, we can. Indeed, the Ashcroft-Manna algorithm has been built into so-called structuring engines such as the one described by Guy de Balbine in \"Better Manpower Utilization Using Automatic Restructuring.\" But there are questions that still have not been completely answered: For example, can one really trust such an automatic translation process? What if the original unstructured program worked because of its use of syntactically illegal COBOL statements --- not in spite of, but because of illegal statements that the compiler ignored, or for which it produced mysterious object code that accidentally produced the right result! \n \nAnother somewhat ironic situation could occur that would hamper the success of an automatic translation process: After living with a program for ten years, a veteran maintenance programmer may have become intimately familiar with the rat's-nest unstructured logic, and a mechanical translation of the program into a structured form actually might be less understandable! Of course, it is unlikely that a structuring engine could improve anything but the control structures (and perhaps the formatting, if a PRETTYPRINT function is included); the data-names still might be so cryptic that nobody would be able to understand the program. Moreover, the program might be part of a larger system suffering from all the problems of, say, pathological connections or global data areas. \n \nSo, it is not entirely clear that the world really wants mechanical structuring algorithms. However, since situations do exist in which the capability might be useful, it is a very good idea to be familiar with the kind of translation mechanisms that Ashcroft and Manna present.", "authors": ["Edward A. Ashcroft", "Zohar Manna"], "id": "f110e72d78c9dc557c1a4ace23d474200eed0715", "title": "The Translation of 'Go To' Programs to 'While' Programs", "references": []}, {"date": "1968", "abstract": "Semantic Scholar extracted view of \"The simula 67 common base language; norwegian computing center\" by Ole-Johan Dahl et al.", "authors": ["Ole-Johan Dahl", "Bjorn Myhrhaug", "Use Nygaard"], "id": "f90111945b1c643feb4c7eebd182ba54fd87fba8", "title": "The simula 67 common base language; norwegian computing center", "references": []}, {"date": "1951", "abstract": "In this paper we shall use a logic with truth values ranging over all the real numbers x such that 0 \u2266 x \u2266 1.1 will be \u201ccomplete truth\u201d and 0 will be \u201ccomplete falsity.\u201d The primitive sentential connectives are \u2018\u2283\u2019 and \u2018\u223c\u2019; other connectives are \u2018 \u2228 \u2019 and \u2018\u00b7\u2019. Assume that \u2018 p \u2019 and \u2018 q \u2019 are sentential variables, whose truth values are respectively x and y . Then 1.1. \u2018 p \u2283 q \u2019 has the value min(1 \u2212 x + y , 1), 1.2. \u2018\u223c p \u2019 has the value 1 \u2212 x , 1.3. \u2018 p \u2228 q \u2019 has the value max( x, y ), and 1.4. \u2018 p\u00b7q \u2019 has the value min ( x, y ). \u2018 \u2228 \u2019 and \u2018\u00b7\u2019 can be defined as follows: It is the purpose of this paper to prove a theorem which will be stated in the next section. The following symbolism and convention will be used throughout the paper: S is a logical formula. \u03bd ( S ) is the value of S . \u2018 p \u2019, \u2018 pi 1 , \u2019 p 2 , \u2026, \u2018 q \u2019, are sentential variables. \u03bd ( p ) = x and \u03bd ( x 1 ) = x 1 , etc. \u03bd ( S ) = \u03c3 and \u03bd ( S 1 ) = \u03c3 1 , etc. If S contains the sentential variables \u2018 p 1 \u2019, \u2018 p 2 \u2019, \u2026, then we write for S , S ( p 1 , P 2 , \u2026). Also \u03bd { S ( p 1 , p 2 , \u2026)) = \u03c3 ( x 1 , x 2 , \u2026). A logical formula is defined in the usual manner. 1. A sentential variable is a logical formula; 2. if S is a logical formula then \u00b7 S is a logical formula; and 3. if S and S \u2032 are logical formulae then ( S \u2283 S \u2032) is a logical formula.", "authors": ["Robert McNaughton"], "id": "107dd7e51dee6d83a0f4782333d708c788cbfc77", "title": "A Theorem About Infinite-Valued Sentential Logic", "references": []}, {"date": "1958", "abstract": "This paper is an attempt at developing a theory of algebraic systems that would correspond in a natural fashion to the No-valued propositional calculus(2). For want of a better name, we shall call these algebraic systems MV-algebras where MV is supposed to suggest many-valued logics. It is known that the classical two-valued logic gives rise to the study of Boolean algebras and, as can be expected, every Boolean algebra will be an MValgebra whereas the converse does not hold. However, many results for Boolean algebras can be appropriately carried over to MV-algebras, although in some cases the proofs become more subtle and delicate. The motivation behind the present study is to find a proof of the completeness of the Novalued logic by using some algebraic results concerning MV-algebras; more specifically, it is known that the completeness of the two-valued logic is a consequence of the Boolean prime ideal theorem and we wish to exploit just some such corresponding result for MV-algebras(3). It will be seen that our effort in duplicating this result is only partially successful. In the first four sections of this paper we present various theorems concerning both the arithmetic in MV-algebras and the structure of these algebras. In the last section we give some applications of our results to the study of completeness of NO-valued logic and some related topics. We point out here that the treatment of MV-algebras as given here is not meant to be complete and exhaustive. 1. Axioms of MV-algebras and some elementary consequences. An MV", "authors": ["Charles C. Chang"], "id": "ee787c6090c333eb83786ea48e3e1b1ae6499662", "title": "ALGEBRAIC ANALYSIS OF MANY VALUED LOGICS(", "references": ["4e2374294ac9a5961cdca1f1164cc6ee218ad258", "c4d52202de4d7279ff0fdf8b1e9ed09143401aa2", "8eb404d1b5d73e78fe0c272e42c42a52811e02cc"]}, {"date": "1944", "abstract": "X is true if, and only if, p. [...] we shall call a definition of truth \u201cadequate\u201d if all these equivalences follow from it. [...] The definition of truth which was outlined above [...] implies all equivalences of the form (T). In this connnection it is important to notice that the conditions for the material adequacy of the definition determine uniquely the extension of the term \u201ctrue.\u201d Therefore, every definition of truth which is materially adequate would necessarily be equivalent to that actually constructed. The semantic conception of truth gives us, so to speak, no possibility of choice between variaous non-equivalent definitions of this notion..", "authors": ["Alfred Tarski", "C. I. Lewis", "Nelson Goodman"], "id": "f55f8866e18663886c8d283297404284f8714843", "title": "The Semantic Conception of Truth: and the Foundations of Semantics", "references": ["28ed587d038422e962f011f1592b708f95f7c5c7", "69e141df8620d16d7886782eb5170c7318d8f21c", "8fe96fbc0c9ced8908ed9a544980f35f9b33a218"]}, {"date": "1969", "abstract": "Central concerns of the book are related theories of recursively enumerable sets, of degree of un-solvability and turing degrees in particular. A second group of topics has to do with generalizations of recursion theory. The third topics group mentioned is subrecursive computability and subrecursive hierarchies", "authors": ["Jr. Hartley Rogers"], "id": "cee5d4d123d6d289a14d41baffa73723dcd3e9e7", "title": "Theory of Recursive Functions and Effective Computability", "references": []}, {"date": "1976", "abstract": "While \"Dijkstra flow-chart schemes\" (built out of assignment statement schemes by means of composition, IF\u2014THEN and WHILEDO) are simple and perspicuous, they lack the descriptive power of flow-chart schemes (provided additional \"variables\" are not permitted). On the other hand, the analogous multiexit composition binary alternation-conditional iteration (CACI) schemes introduced below, which are virtually as simple and perspicuous as Dijkstra schemes, describe exactly the same computational processes as flow-chart schemes (without the aid of additional variables).", "authors": ["Calvin C. Elgot"], "id": "93cc8cb9c34a57a0d6edcf29039f57a5a0dd23c4", "title": "Structured Programming With and Without go to Statements", "references": ["85ee42037d6e1d369fbf65f52661e72c10ddd485", "cd52c6a8690781f294115c554bfab8bf3f461487", "df0e586636f6ce2bffef6d6717fbcf2550ad43dd", "7c939e99eb66e4f102da4f34a4e466f099b93142", "572527f774793778cc6a4bbfbcacf144890ffbff", "f5fe755968857e001f3e7029efbfc1ba8f1dd083", "08081efb05fd98bd9c6637228236dd70306c089e", "0b981e6d93b203353fcc98b782447db2cf8c5cb4", "3fdae4603265209ddf420cfaa9cbd0286c567c6c", "f110e72d78c9dc557c1a4ace23d474200eed0715"]}, {"date": "1954", "abstract": "Semantic Scholar extracted view of \"Infinite Abelian groups\" by Irving Kaplansky", "authors": ["Irving Kaplansky"], "id": "1be9dcfb94cbecd7abf1e45d7b4783f041ad33d9", "title": "Infinite Abelian groups", "references": []}, {"date": "1970", "abstract": "A high momentum industrial gas burner designed to create a high velocity which, in turn, is capable of creating high wind circulation that can be maintained during burner turndown. The various chambers of the burner are specially designed, so that fluid pressure within the burner is less than atmospheric pressure, or the fluid pressure within the heating chamber of a furnace wherein the burner is used to heat, for example, air.", "authors": ["Robert L. Vaught", "Lars Svenonius", "Erwin Engeler", "Gebhard Fukrken"], "id": "e272c33167aef37436ce6df64b428ebbdbefa742", "title": "Denumerable Models of Complete Theories", "references": ["68c00c14e0832e2f1d3a945dc3a27effc4de6ba4", "5805e4838505299c059fb937bc6bf99501e72355", "11eb86e0e9f44c8c53fca959aba2f272fa445049", "27e21381178340e476d5ca1ba98b282684ea583f", "686812651628808e11f6fe661e681bc2a572c7da", "087ee9eb460c2d2c5135e1bef73442a99a6829cf", "3172cb4b50f1f226e89747089eb98b83f8b17ab4"]}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"Model theory for infinitary logic\" by H. Jerome Keisler", "authors": ["H. Jerome Keisler"], "id": "ce15618da8025c217c4bf6dcd9db157229b943f7", "title": "Model theory for infinitary logic", "references": []}, {"date": "1957", "abstract": "This paper defines a formal relation among sentences, by virtue of which one sentence structure may be called a transform of another sentence structure (e. g. the active and the passive, or in a different way question and answer). The relation is based on comparing the individual co-occurrences of morphemes. By investigating the individual co-occurrences (\u00a7 1.2; \u00a7 2) we can characterize the distribution of certain classes which may not be definable in ordinary linguistic terms (e. g. pronouns, \u00a7 2.6). More important, we can then proceed to define transformation (\u00a7 1.3), based on two structures having the same set of individual co-occurrences. This relation yields unique analyses of certain structures and distinctions which could not be analyzed in ordinary linguistic terms (\u00a7 3). It replaces a large part of the complexities of constituent analysis and sentence structure, at the cost of adding a level to grammatical analysis. It also has various analytic and practical applications (\u00a7 5.7), and can enter into a more algebraic analysis of language structure (\u00a7 5.2, 4, 6) than is natural for the usual classificatory linguistics. A list of English transformations is given in \u00a7 4. The main argument can be followed in \u00a7 1.11 (Co-Occurrence Defined), \u00a7 1.2 (Constructional Status), \u00a7 1.3 (Transformation Defined), \u00a7 2.9 (Summary of Constructions), \u00a7 3.9 (Summary of Sentence Sequences), \u00a75 (The Place of Transformations in Linguistic Structure).1", "authors": ["Zellig S. Harris"], "id": "0725438da27f7f83334cae357bd4415cf93d2344", "title": "Co-Occurrence and Transformation in Linguistic Structure", "references": []}, {"date": "1965", "abstract": "Abstract : Contents: Methodological preliminaries: Generative grammars as theories of linguistic competence; theory of performance; organization of a generative grammar; justification of grammars; formal and substantive grammars; descriptive and explanatory theories; evaluation procedures; linguistic theory and language learning; generative capacity and its linguistic relevance Categories and relations in syntactic theory: Scope of the base; aspects of deep structure; illustrative fragment of the base component; types of base rules Deep structures and grammatical transformations Residual problems: Boundaries of syntax and semantics; structure of the lexicon", "authors": ["Noam Chomsky"], "id": "16c762445f11fa2020994918dc4f93e76264df17", "title": "Aspects of the Theory of Syntax", "references": []}, {"date": "1952", "abstract": "This paper offers an example of how connected discourse can be formally analyzed in such a way as to reveal something of its structure. The method used here was described in a previous paper, \u2018Discourse Analysis\u2019, Lg. 28 (1952), 1\u201330. It consists essentially of the following steps: given a particular text, we collect those linguistic elements (morphemes or sequences of morphemes) which have identical environments within a sentence, and we call these equivalent to each other; thus, if we find the sentences A F and B Fin our text, we write A=B and say that A is equivalent to B or that both are in the same equivalence class. We further collect those linguistic elements which have equivalent (rather than identical) environments, and we call these also equivalent to each other; if we find the sentences A F and B E, and if A=B (because B F occurs too), then F is secondarily equivalent to E, and we write F=E. (Note that in the sentence A F, A is the environment of F, and Fis the environment of A.) This operation enables us to collect many or all of the linguistic elements or sections of any particular text into a few equivalence classes. For example, if our text consists of the sentences1 A F: B E: C G: B F: M E: A G: N E: N G: M H, we set up two classes: one class to include A, B (because of A F and B F), C (because of A G and C G), M, and N (because of B E and M E and N E); the other class to include F, E (because of B F and B E), G (because of A F and A G), and H (because of M E and M H).2", "authors": ["Zellig S. Harris"], "id": "0f2d98c02fbcac128c9aa925964eeb0ddfae8840", "title": "Discourse Analysis: A Sample Text", "references": []}, {"date": "1958", "abstract": "A finite state language is a finite or infinite set of strings (sentences) of symbols (words) generated by a finite set of rules (the grammar), where each rule specifies the state of the system in which it can be applied, the symbol which is generated, and the state of the system after the rule is applied. A number of equivalent descriptions of finite state languages are explored. A simple structural characterization theorem for finite state languages is established, based on the cyclical structure of the grammar. It is shown that the complement of any finite state language formed on a given vocabulary of symbols is also a finite state language, and that the union of any two finite state languages formed on a given vocabulary is a finite state language; i.e., the set of all finite state languages that can be formed on a given vocabulary is a Boolean algebra. Procedures for calculating the number of grammatical strings of any given length are also described.", "authors": ["Noam Chomsky", "George A. Miller"], "id": "1c2c6558d2b7be61d1f4910d7ff438ffbda52890", "title": "Finite State Languages", "references": ["7d3d377325d6e7183159129b2a3635869685eb10"]}, {"date": "1974", "abstract": "Semantic Scholar extracted view of \"Invariant sets in topology and logic\" by Robert L. Vaught", "authors": ["Robert L. Vaught"], "id": "b6829b69831f783cca4cfc18d6e50ec6a5996eaa", "title": "Invariant sets in topology and logic", "references": []}, {"date": "1965", "abstract": "Semantic Scholar extracted view of \"An interpolation theorem for denumerably long formulas\" by E. G. K. L\u00f3pez-Escobar", "authors": ["E. G. K. L\u00f3pez-Escobar"], "id": "55c858475e30d1c3f8739a937ec805ac3a908c76", "title": "An interpolation theorem for denumerably long formulas", "references": []}, {"date": "1959", "abstract": "Semantic Scholar extracted view of \"The First Order Properties of Products of Algebraic Systems\" by Solomon Feferman et al.", "authors": ["Solomon Feferman", "Robert L. Vaught"], "id": "d4d1d912704b09a699f8af9b5c2507f1958f4e1e", "title": "The First Order Properties of Products of Algebraic Systems", "references": []}, {"date": "1954", "abstract": "Semantic Scholar extracted view of \"Contributions to the theory of models. III\" by Alfred Tarski", "authors": ["Alfred Tarski"], "id": "8eb404d1b5d73e78fe0c272e42c42a52811e02cc", "title": "Contributions to the theory of models. III", "references": []}, {"date": "1944", "abstract": "Introduction. Recent developments of symbolic logic have considerable importance for mathematics both with respect to its philosophy and practice. That mathematicians generally are oblivious to the importance of this work of G\u00f6del, Church, Turing, Kleene, Rosser and others as it affects the subject of their own interest is in part due to the forbidding, diverse and alien formalisms in which this work is embodied. Yet, without such formalism, this pioneering work would lose most of its cogency. But apart from the question of importance, these formalisms bring to mathematics a new and precise mathematical concept, that of the general recursive function of H\u00ebrbrand-G\u00f6delKleene, or its proved equivalents in the developments of Church and Turing. It is the purpose of this lecture to demonstrate by example that this concept admits of development into a mathematical theory much as the group concept has been developed into a theory of groups. Moreover, that stripped of its formalism, such a theory admits of an intuitive development which can be followed, if not indeed pursued, by a mathematician, layman though he be in this formal field. It is this intuitive development of a very limited portion of a sub-theory of the hoped for general theory that we present in this lecture. We must emphasize that, with a few exceptions explicitly so noted, we have obtained formal proofs of all the consequently mathematical theorems here developed informally. Yet the real mathematics involved must lie in the informal development. For in every instance the informal \"proof\" was first obtained; and once gotten, transforming it into the formal proof turned out to be a routine chore. We shall not here reproduce the formal definition of recursive function of positive integers. A simple example of such a function is an", "authors": ["Emil L. Post"], "id": "ed71ebe0eee4f88f095247c8b62ba1d3b217a68d", "title": "Recursively enumerable sets of positive integers and their decision problems", "references": ["c689742c12623e71b52d3a20c612c2c8f65a1408", "ab7790485f26ce65f9d83dd700c43e49058bdd2b", "8fe96fbc0c9ced8908ed9a544980f35f9b33a218", "e13482a70ce7201089bdbea89f2d805f3177cb58", "392746142b0b67726be2fa16611586f32bd95630", "60400c043b2624f9cfc2d8daa0f45f3c1d524de3"]}, {"date": "1958", "abstract": "Only for you today! Discover your favourite computability and unsolvability book right here by downloading and getting the soft file of the book. This is not your time to traditionally go to the book stores to buy a book. Here, varieties of book collections are available to download. One of them is this computability and unsolvability as your preferred book. Getting this book b on-line in this site can be realized now by visiting the link page to download. It will be easy. Why should be here?", "authors": ["Martin D. Davis"], "id": "b449b779203f4392211e6db98720d46eecc0e4bd", "title": "Computability and Unsolvability", "references": []}, {"date": "1972", "abstract": "The es tab l i shment of lower bounds on the number of comparisons necessary to solve various combinator ia l problems is considered. Some of the new results are : (a) given two finite sets of real numbers , A and B, where n = max ( I A I , I B I ), O(n. log n) comparisons are required to determine if A = B, even when comparisons are allowed between l inear functions of the numbers ; and (b) the maximum of a set of n real numbers cannot be computed in fewer than n 1 comparisons if comparisons of only l inear funct ions of the numbers are permitted, but the maximum can be computed in Ilog2n] comparisons if comparisons are allowed between exponential funct ions of the numbers ,", "authors": ["Edward M. Reingold"], "id": "201d7231a2493225c23c9bd148fce3e408da1d6b", "title": "On the Optimality of Some Set Algorithms", "references": []}, {"date": "1939", "abstract": "It is my intention in this paper to add somewhat to the observations already made in my earlier publications on the existence of undecidable statements in systems of logic possessing rules of inference of a \"non-finitary\" (\"non-constructive\") character (??1-4).' I also wish to emphasize once more the part played by the concept of truth in relation to problems of this nature (??5-8). At the end of this paper I shall give a result which was not touched upon in my earlier publications. It seems to be of interest for the reason (among others) that it is an example of a result obtained by a fruitful combination of the method of constructing undecidable statements (due to K. Godel) with the results obtained in the theory of truth.", "authors": ["Alfred Tarski"], "id": "8fe96fbc0c9ced8908ed9a544980f35f9b33a218", "title": "On Undecidable Statements in Enlarged Systems of Logic and the Concept of Truth", "references": []}, {"date": "1941", "abstract": "Semantic Scholar extracted view of \"Symbolic Logic. (Scientific Books: Introduction to Logic)\" by Alfred Tarski", "authors": ["Alfred Tarski"], "id": "69e141df8620d16d7886782eb5170c7318d8f21c", "title": "Symbolic Logic. (Scientific Books: Introduction to Logic)", "references": []}, {"date": "1964", "abstract": "Ianov has defined a formal abstraction of the notion of program which represents the sequential and control properties of a program but suppresses the details of the operations. For these schemata he defines a notion corresponding to computation and defines equivalence of schemata in terms of it. He then gives a decision procedure for equivalence of schemata, and a deductive formalism for generating schemata equivalent to a given one. The present paper is intended, first as an exposition of Ianov's results and simplification of his method, and second to point out certain generalizations and extensions of it. We define a somewhat generalized version of the notion of schema, in a language similar to that used in finite automata theory, and present a simple algorithm for the equivalence problem solved by Ianov. We also point out that the same problem for an extended notion of schema, considered rather briefly by Ianov, is just the equivalence problem for finite automata, which has been solved, although the decision procedure is rather long for practical use. A simple procedure for generating all schemata equivalent to a given schema is also indicated.", "authors": ["Joseph D. Rutledge"], "id": "0b981e6d93b203353fcc98b782447db2cf8c5cb4", "title": "On Ianov's Program Schemata", "references": []}, {"date": "1972", "abstract": "The structure of , programs can often be described by a technique called \u201cinterval analysis\u201d on their flow graphs. Here, we characterize the set of flow graphs that can be analyzed in this way in terms of two very simple transformations on graphs. We then give a necessary and sufficient condition for analyzability and apply it to \u201cgoto-less programs,\u201d showing that they all meet the criterion.", "authors": ["Matthew S. Hecht", "Jeffrey D. Ullman"], "id": "f5fe755968857e001f3e7029efbfc1ba8f1dd083", "title": "Flow Graph Reducibility", "references": []}, {"date": "1972", "abstract": "As is evident from some of the other material reprinted in this book, much of the early discussion about structured programming and the related techniques was conducted by academic people and was published in scholarly journals, thus escaping the attention of the average software professional. In the rare instances in which structured programming was brought to the attention of the realworld programmer, the subject usually was greeted with loud hoots: \"Bah! Humbug! What do those academic types know about real programming? By God, they should have to write a payroll system under a tight deadline with XYZ's version of COBOL. Then they'd stop yapping!\" \n \nIt's precisely because of this traditionally academic association that Terry Baker's article in the January 1972 IBM Systems Journal was so important. IBM's name, for the first time, was associated with top-down design, structured programming, and the related disciplines. Granted, IBM Systems Journal is not as widely read as Datamation or Computerworld, but it attracts more readers than the proceedings of the IFIP Conferences. The article served to call popular attention to the new techniques, and, as happened with virtual memory and several other technological developments, caused a substantial number of people in the field to believe that IBM invented structured programming! Who invented structured programming clearly is debatable, but IBM's role in popularizing it is indisputable. \n \nMany of Baker's topics deserve mention, either because of their initial impact, or because of their long-term implications. The first noteworthy concept is the major topic of the paper: the Chief Programmer Team. Baker's description of the team is a good one, and is illustrated with a real case study. The fact that the concept has been expanded and refined in later works, such as Fred Brooks's The Mythical Man-Month (Reading, Mass.: Addison-Wesley, 1975), should not detract from its worth. Nor should the realization, some seven years after the article's publication, that the Chief Programmer Team concept probably will never work in an ordinary EDP organization, for the following reasons: There are precious few chief programmers. Those that do exist are very expensive, and are not interested in working on small computers and mundane applications. In short, the Chief Programmer Team concept probably will work only in companies that are in the EDP business to make a profit. In most other companies, data processing is regarded as a necessary evil, and programmers (chief and indians alike) are tolerated with the greatest reluctance. \n \nSimilarly, the concept of the program librarian, discussed at length by Baker, is less popular today than when it was first introduced. The concept of a program library is an important one, and its use has indeed been accepted, but the idea of hiring a human being to create, maintain, and control the library is becoming increasingly less popular. \n \nMost of the other structured concepts discussed by Baker still are used widely. Interestingly, Baker mentions top-down testing, but seems to attach relatively little importance to it, whereas I think it is one of the most important structured techniques. Baker also mentions structured programming, of course, and refers to Bom and Jacopini as the source of the idea; what's interesting is his emphasis on the formatting of structured code, and his effort to show how structured programming works with real languages such as PL/I and assembler. \n \nThe other major significance of Baker's paper relates to the so-called New York Times project. For several years following publication of the paper, programmers quoted Baker's figures as proof that structured programming increases productivity by a factor of two or five, depending on your viewpoint. Statistics from a companion paper, \"System Quality Through Structured Programming\" [Paper 11], have been used by many EDP professionals to prove that structured programming leads to more reliable software. Indeed, the productivity and reliability figures of the New York Times project are impressive, but one has to wonder whether the Hawthorne Effect was a factor: Were the programmers more productive because they knew they were working on a special project? Or were they more productive simply because they were extraordinarily gifted programmers? Was the success of the New York Times project the result of the people or was it because of the particular organization of the people ? \n \nQuestions like these still are being debated, and they will continue to be debated for several years to come. Significantly, Baker's paper first brought the questions to everyone's attention.", "authors": ["F. Terry Baker"], "id": "08081efb05fd98bd9c6637228236dd70306c089e", "title": "Chief Programmer Team Management of Production Programming", "references": []}, {"date": "1960", "abstract": "Semantic Scholar extracted view of \"Homogeneous Universal Relational Systems.\" by Bjarni J\u00f3nsson", "authors": ["Bjarni J\u00f3nsson"], "id": "087ee9eb460c2d2c5135e1bef73442a99a6829cf", "title": "Homogeneous Universal Relational Systems.", "references": []}, {"date": "1956", "abstract": "Semantic Scholar extracted view of \"On an application of semi groups methods to some problems in coding\" by Marcel Paul Sch\u00fctzenberger", "authors": ["Marcel Paul Sch\u00fctzenberger"], "id": "7d3d377325d6e7183159129b2a3635869685eb10", "title": "On an application of semi groups methods to some problems in coding", "references": []}, {"date": "1973", "abstract": "We investigate various control structures to understand their computational complexity and limitations. It is generally felt that GOTO-less programs constructed from the classical primitives are very restrictive; structured programming languages like BLISS, however, incorporating Repeat-Exit constructs appear to ease this sense of restrictiveness. In this paper we analyze this construct. We answer a conjecture of Knuth and Floyd as a special case of the general theory. We also investigate a general Top-Down Programming construct, which we call the TDn-construct.\n We structurally characterize the class of GOTO-less programs. We also generalize such an analysis and solve an open problem of B\u00f6hm and Jacopini.", "authors": ["S. Rao Kosaraju"], "id": "7c939e99eb66e4f102da4f34a4e466f099b93142", "title": "Analysis of structured programs", "references": []}, {"date": "1954", "abstract": "Semantic Scholar extracted view of \"On the categoricity in power of elementary deductive systems and some related problems\" by Jerzy \u0141o\u015b", "authors": ["Jerzy \u0141o\u015b"], "id": "11eb86e0e9f44c8c53fca959aba2f272fa445049", "title": "On the categoricity in power of elementary deductive systems and some related problems", "references": []}, {"date": "1956", "abstract": "Semantic Scholar extracted view of \"A Result on Consistency and its Application to theTheory of Definition\" by Abraham Robinson", "authors": ["Abraham Robinson"], "id": "27e21381178340e476d5ca1ba98b282684ea583f", "title": "A Result on Consistency and its Application to theTheory of Definition", "references": []}, {"date": "1954", "abstract": "Semantic Scholar extracted view of \"Applications of the L\u00f6wenheim\u2013Skolem\u2013Tarski Theorem to Problems of Completeness and Decidability\" by Robert L. Vaught", "authors": ["Robert L. Vaught"], "id": "68c00c14e0832e2f1d3a945dc3a27effc4de6ba4", "title": "Applications of the L\u00f6wenheim\u2013Skolem\u2013Tarski Theorem to Problems of Completeness and Decidability", "references": []}, {"date": "1951", "abstract": "By a decision method for a class K of sentence (or other expressions) is meant a method by means of which, given any sentence \u03b8, one can always decide in a finite number of steps whether \u03b8 is in K; by a decision problem for a class K we mean the problem of finding a decision method for K. A decision method must be like a recipe, which tells one what to do at each steps so that no intelligence is required to follow it; and the method can be applied by anyone so long as he is able to read and follow directions.", "authors": ["Alfred Tarski"], "id": "686812651628808e11f6fe661e681bc2a572c7da", "title": "A decision method for elementary algebra and geometry", "references": []}, {"date": "1931", "abstract": "Semantic Scholar extracted view of \"Principia mathematica\" by Carnap", "authors": ["Carnap"], "id": "28ed587d038422e962f011f1592b708f95f7c5c7", "title": "Principia mathematica", "references": []}, {"date": "1976", "abstract": "In ''Monadic Computation and Iterative Algebraic Theories'' by Calvin C. Elgot,the notion ''iterative theory'' (more fully, ''ideal theory closed under conditional iteration'') is introduced and applied to the study of computational processes. The main point of the present paper is to show the existence (in a constructive sense) of free iterative theories. The main complication is the fact that in an iterative theory I the ''iteration'' operation is not defined for all elements of I. Were it not for this complication, the existence of free iterative theories would follow from general algebraic considerations (extended to many-sorted algebras). Actually we sketch two proofs of the existence of free iterative theories. One argument follows as much as possible general algebraic lines and is given a linguistic flavor in order to emphasize the concreteness of the ideas involved. The second argument depends upon ''normal descriptions'': a morphism in the free iterative theory being an equivalence class of normal descriptions.", "authors": ["Stephen L. Bloom", "Calvin C. Elgot"], "id": "df0e586636f6ce2bffef6d6717fbcf2550ad43dd", "title": "The Existence and Construction of Free Iterative Theories", "references": []}, {"date": "1937", "abstract": "In an earlier paperf we have developed an abstract theory of Boolean algebras and their representations by algebras of classes. We now relate this theory to the study of general topology. The first part of our discussion is devoted to showing that the theory of Boolean rings is mathematically equivalent to the theory of locally-bicompact totally-disconnected topological spaces. In R we have already prepared the way for a topological treatment of the perfect representation of an arbitrary Boolean ring. Continuing in this way, we find that the perfect representation is converted by the introduction of a suitable topology into a space of the indicated type. We have no difficulty in inverting this result, proving that every locally-bicompact totally-disconnected topological space arises by the same procedure from a suitable Boolean ring.' It is thus convenient to call the spaces corresponding in this manner to Boolean rings, Boolean spaces. The algebraic properties of Boolean rings can, of course, be correlated in detail with the topological properties of the corresponding Boolean spaces. A simple instance of the correlation is the theorem that the Boolean rings with unit are characterized as those for which the corresponding Boolean spaces are bicompact. A familiar example of a bicompact Boolean space is the Cantor discontinuum or ternary set, which we discuss at the close of Chapter I. Having established this direct connection between Boolean rings and topology, we proceed in the second part of the discussion to considerations of a yet more general nature. We propose the problem of representing an arbitrary TVspace by means of maps in bicompact Boolean spaces. Our solution of this problem embodies an explicit construction of such maps, which we shall now describe briefly. In a given TVspace dt, the open sets and the nowhere dense sets generate a Boolean ring, with 9\u00ce as unit, which characterizes the topological structure of 9\u00ce. Those subrings which contain 9\u00ce and which are so large that the interiors of their member sets constitute bases for 9\u00ee, also char-", "authors": ["Marshall Harvey Stone"], "id": "5805e4838505299c059fb937bc6bf99501e72355", "title": "Applications of the theory of Boolean rings to general topology", "references": []}, {"date": "1939", "abstract": "In this address, no attempt to deal with any problem of symbolic technique will be made, for even if a problem of this kind should be found that was suitable to the present occasion, I should probably lack sufficient competence in the handling of symbols to deal with it satisfactorily. It occurred to me, however, that it might be interesting to consider the nature of symbols in general, to point out certain characteristics peculiar to the symbols used by logicians and mathematicians, and to say something concerning the relations of these symbols to the knowledge of Nature. Although developments in the science of logic are not dependent on such an inquiry, it yet provides us with a perspective on the nature and importance of that science which we cannot gain so long as we attend only to the problems arising inside its field. The symbolic relation . Our attempt to gain this perspective may well begin with the trite remark that nothing is intrinsically a symbol, but that anything is a symbol if and only if it symbolizes. Moreover, the relation called symbolizing is not a dyadic but rather a tetradic relation. That is, in order for something A to be a symbol of something B , there must be in addition C , a mind trained in a special way, and D , a certain manner in which that mind is occupied at the time. For although we do say, for instance, that a mark consisting of a little cross is the symbol of addition, the fact is of course that at times when that mark is not present to a mind, it does not symbolize addition or anything else. Moreover, even when it is present to a mind, it does not symbolize addition unless that mind has been trained in a certain manner; for obviously such a mark does not symbolize addition to the mind of a Hottentot or other wholly illiterate person.", "authors": ["C. J. Ducasse"], "id": "e13482a70ce7201089bdbea89f2d805f3177cb58", "title": "Symbols, Signs, and Signals", "references": []}, {"date": "1967", "abstract": "Authors Teichroew and Lubin [CACM 9, 10 (Oct. 66)] deserve credit for their excellent paper on Simulation Languages. As a member of \"the other camp,\" which is concerned with \"con-ftinuous systems simulation languages,\" I am particularly grateful lor the insight gained from the analysis of discrete event simu-ators. The authors included a brief discussion of \"continuous-change simulation languages\" and gave reference to the appropriate literature on the subject. I should like here to add some thoughts on this topic, within the framework of the subject paper. First, let me comment that the Simulation Software Committee of the Simulation Councils, Inc. (an AFIPS member) was formed in 1965 for the express purpose of preparing language standards for the class of simulation languages it has chosen to call \"continuous system simulation language\" (CSSL). As noted by Teichroew and Lubin, there have been many such programs developed since the first one in 1957-the count is at least 23. The committee expects to publish the completed standard this spring. It is customary in casual discussion to distin~fish between the two classes of languages by use of the terms \"continuous\" and \"discrete\" simulations. While it is true that these words characterize the typical models represented in the two kinds of languages , I conclude from these authors that such a distinction is not fundamental to the structure of the language, given appropriate programming or \"activity subroutines.\" I suspect that CSL can approximate continuous simulation, and that a present-day CSSL certainly can represent discrete behavior. The distinction that is fundamental is characterized by these excerpts: CSSL: the system simulation consists of \"a continuous flow of information or material counted in the aggregate rather than individual items.\" \"Discrete\" Simulators: \"items flow through the system.\" \"This type of sinmlation consists.., in keeping track of where individual items are,\" (italics mine) It is possible with CSSL to represent flow of discrete items through a system, as well as queueing and actions that are conditional upon the size of the queue. However, the flow of items must be homogeneous: individual items cannot be distinguished; core space is not required for all items of a queue, only the current size of the queue is retained. The authors have taken care in clarifying the terminology of the languages analyzed. Moreover, they have suggested a basic set of terms, in the legends of the tables. Looking at these from a different point of \u2026", "authors": ["David C. Cooper"], "id": "85ee42037d6e1d369fbf65f52661e72c10ddd485", "title": "B\u00f6hm and Jacopini's reduction of flow charts", "references": []}, {"date": "1943", "abstract": "In this paper we shall be concerned with questions regarding decision problems for certain classes of sentences (without quantifiers) of various kinds of algebra. In the first section we shall establish a result of a general nature, which enables one, in a number of cases, to reduce a decision problem of the type considered to a somewhat simpler problem. In the second section we formulate a rather broad sufficient condition for the existence of a decision method for sentences without quantifiers; and in the last section we show that this condition holds of lattices.", "authors": ["J. C. C. McKinsey"], "id": "c689742c12623e71b52d3a20c612c2c8f65a1408", "title": "The Decision Problem for Some Classes of Sentences Without Quantifiers", "references": []}, {"date": "1964", "abstract": "The process by which a computer can learn is demonstrated by asking it to solve increasingly more difficult versions of the Tower of Hanoi puzzle. Ultimately, the system may learn how to generalize in a particular problem domain Present uses of computers, valuable as they may be, are far from the ultimate in what might be accomplished. One of the reasons is that the solution of even well-defined problems, for which goals and rules are precisely known, can be extremely difficult to program (e.g., chess-playing programs). But intellectual capacities of machines might be extended by means of an adaptive system to handle increasingly complex and varied tasks.", "authors": ["Aiko M. Hormann"], "id": "672fe922b933b188b167c6f50e433add2daffc3a", "title": "How a computer system can learn", "references": []}, {"date": "1948", "abstract": "Semantic Scholar extracted view of \"Lattice Theory Revised Edition\" by Garrett Birkhoff", "authors": ["Garrett Birkhoff"], "id": "c4d52202de4d7279ff0fdf8b1e9ed09143401aa2", "title": "Lattice Theory Revised Edition", "references": []}, {"date": "1958", "abstract": "Semantic Scholar extracted view of \"FRAGMENTS OF MANY-VALUED STATEMENT CALCULI\" by Alan Rose et al.", "authors": ["Alan Rose", "J. Barkley Rosser"], "id": "4e2374294ac9a5961cdca1f1164cc6ee218ad258", "title": "FRAGMENTS OF MANY-VALUED STATEMENT CALCULI", "references": ["107dd7e51dee6d83a0f4782333d708c788cbfc77", "bfcd62b8887306333fdfe9f4c409381cfd475329", "eb40339b13500645ee627337d2f4ea0663c8f0b0"]}, {"date": "1980", "abstract": "The use of the temporal logic formalism for program reasoning is reviewed. Several aspects of responsiveness and fairness are analyzed, leading to the need for an additional temporal operator: the 'until' operator -U. Some general questions involving the 'until' operator are then discussed. It is shown that with the addition of this operator the temporal language becomes expressively complete. Then, two deductive systems DX and DUX are proved to be complete for the languages without and with the new operator respectively.", "authors": ["Dov M. Gabbay", "Amir Pnueli", "Saharon Shelah", "Jonathan Stavi"], "id": "beb91dfc9e101903fc2b8fc171c52d5f1c74a583", "title": "On the temporal analysis of fairness", "references": []}, {"date": "1972", "abstract": "A chess program has been developed which plays good chess (for a program) using a very simple structure. It is based on a brute force search of the move tree with no forward pruning, using material as the only terminal evaluation function, and using a limited positional analysis at the top level for a tiebreak between moves which are materially equal. Because of the transparent structure, this program is proposed as a technological benchmark for chess programs which will continue to improve as computer technology increases.", "authors": ["James J. Gillogly"], "id": "190f432914ca9f9925860139c88d9664787a5939", "title": "The Technology Chess Program", "references": []}, {"date": "1965", "abstract": "Semantic Scholar extracted view of \"On the Motor Theory of Speech Perception\" by Peter B. Denes", "authors": ["Peter B. Denes"], "id": "6a69a48ababc4d2e1b6132d5a2fc1de365942c9e", "title": "On the Motor Theory of Speech Perception", "references": []}, {"date": "1937", "abstract": "1. Computing machines. 2. Definitions. Automatic machines. Computing machines. Circle and circle-free numbers. Computable sequences and numbers. 3. Examples of computing machines. 4. Abbreviated tables Further examples. 5. Enumeration of computable sequences. 6. The universal computing machine. 7. Detailed description of the universal machine. 8. Application of the diagonal process. Pagina 1 di 38 On computable numbers, with an application to the Entscheidungsproblem A. M. ...", "authors": ["Alan M. Turing"], "id": "ab7790485f26ce65f9d83dd700c43e49058bdd2b", "title": "On computable numbers, with an application to the Entscheidungsproblem", "references": ["8dea163a553333ac0b28963fd50efc509bfa789a", "2c0010fc66890cfa8280de25de99734d46ac34e1", "e53637d308345ee4d442509b4d3f20a66f2fb4a4", "b6b8523c4e6ab63373fdf5ca584b02e404b14893", "ee8c779e7823814a5f1746d883ca77b26671b617", "2e842fcd9b30aaf88378f395cb49bf54d962b9b4", "947efa54175145240c1a33f62321f23db43571e7", "60400c043b2624f9cfc2d8daa0f45f3c1d524de3", "db6f721b9dbf8cc7d7bec0569bc5d78e3f7f3d99", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "1972", "abstract": "Abstract : The report is a collection of Working Papers in Speech Recognition on the following topics: Organization of the HEARSAY II speech understanding system; The DRAGON system -- an overview; Parameter-independent machine segmentation and labeling; A new time-domain analysis of fricatives and stop consonants; Sub-lexical levels in the HEARSAY II speech understanding system; Inference and use of simple predictive grammars; Real-time linear predictive coding of speech on the SPS-41 microprogrammed triple-processor system; A 16-bit A-D-A conversion system for high-fidelity audio research.", "authors": ["D. Raj Reddy", "Lee D. Erman", "Richard B. Neely"], "id": "96a9cfa8a396fd8967b327dc675a8ce88f16a18f", "title": "Working Papers in Speech Recognition - II,", "references": []}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"Speech Recognition: Prospects of the Seventies\" by Raj Reddy", "authors": ["Raj Reddy"], "id": "62184a540db0f8c4712c7fe6c42272be3a7ed96e", "title": "Speech Recognition: Prospects of the Seventies", "references": []}, {"date": "1937", "abstract": "* Accepted for publication by Karl S. Lashley of the Editorial Board, and received in the Editorial Office on January 11, 1937. 1 The writer is greatly indebted to Professor K. S. Lashley for aid in planning and carrying out these experiments and in the writing of this report.", "authors": ["Donald Olding Hebb"], "id": "654315deba1019c7c3b7adde07bb49368cb76509", "title": "The Innate Organization of Visual Activity: I. Perception of Figures by Rats Reared in Total Darkness", "references": []}, {"date": "1960", "abstract": "Semantic Scholar extracted view of \"4. EFFECTS OF STIMULUS DEPRIVATION ON THE DEVELOPMENT AND ATROPHY OF THE VISUAL SENSORY SYSTEM\" by Austin H. Riesen", "authors": ["Austin H. Riesen"], "id": "724945b672fb10d64654091f725cc66fe5a6298f", "title": "4. EFFECTS OF STIMULUS DEPRIVATION ON THE DEVELOPMENT AND ATROPHY OF THE VISUAL SENSORY SYSTEM", "references": []}, {"date": "1970", "abstract": "The paper discusses some basic refinements of the Resolution Principle which are intended to improve the speed and efficiency of theorem-proving programs based on this rule of inference. It is proved that two of the refinements preserve the logical complete\u00adness of the proof procedure when used separately, but not when used in conjunction. The results of some preliminary experiments with the refinements are given.", "authors": ["David Luckham"], "id": "ffe701d533cffe8b81d1d2921b07a72e93648fa3", "title": "Refinement theorems in resolution theory", "references": []}, {"date": "1965", "abstract": "One of the major problems in mechanical theorem proving is the generation of a plethora of redundant and irrelevant information. To use computers effectively for obtaining proofs, it is necessary to find strategies which will materially impede the generation of irrelevant inferences. One strategy wilich achieves this end is the set of support strategy. With any such strategy two questions of primary interest are that of its efficiency and that of its logical completeness. Evidence of the efficiency of this strategy is presented, and a theorem giving sufficient conditions for its logical completeness is proved.", "authors": ["Larry Wos", "George A. Robinson", "Daniel F. Carson"], "id": "05b44597834f6df07c1c1290fb33a979bdf99067", "title": "Efficiency and Completeness of the Set of Support Strategy in Theorem Proving", "references": []}, {"date": "1961", "abstract": "Abstract Retinas of cats, hooded rats, and chimpanzees reared in total darkness from birth were compared with those from controls matched for age and reared in light with normal day and night variation. Microphotometric measurement of azure B binding was used to estimate ribonucleic acid (RNA) concentration in individual ganglion cells. Relative protein levels were judged visually after staining with fast green or the Millon reaction. There was a marked lowering of cytoplasmic and nucleolar RNA levels in neurons from the multipolar, bipolar, and receptor cell layers of retinas from dark-reared cats. Protein levels were also appreciably lowered in these subjects, and a significant reduction of mean nucleolar volume and in cytoplasmic cross-sectional area of ganglion cells was found. The inner plexiform layer was consistently thinner in retinas from cats reared in darkness, although there were no apparent differences in thickness of cell body layers, nor in frequencies of the several cell types within any one particular layer. Comparable but less extensive data are presented for hooded rats. Markedly depressed RNA and protein levels were also found in retinal ganglion cells of two dark-reared chimpanzees, associated with degeneration of the ganglion cell layer. The data suggest that adequate light stimulation is a major variable controlling the development of normal ribonucleoprotein levels in cells of the mammalian retina.", "authors": ["Ellen M. Rasch", "H. Swift", "Austin H. Riesen", "Kao Liang Chow"], "id": "dd42483f405bfacb4aeee15c0befdad169a81c9b", "title": "Altered structure and composition of retinal cells in darkreared mammals.", "references": []}, {"date": "1968", "abstract": "A multiprogramming system is described in which all activities are divided over a number of sequential processes. These sequential processes are placed at various hierarchical levels, in each of which one or more independent abstractions have been implemented. The hierarchical structure proved to be vital for the verification of the logical soundness of the design and the correctness of its implementation.", "authors": ["Edsger W. Dijkstra"], "id": "cf4570f4801181a2f8e1b9aac77c180d0206834e", "title": "The structure of the \u201cTHE\u201d-multiprogramming system", "references": []}, {"date": "1969", "abstract": "This paper Describes a program, called \"PROW\", which writes programs PROW accepts the specification of the program in the language of predicate calculus, decides the algorithm for the program and then produces a LISP program which is an implementation of the algorithm. Since the construction of the algorithm is obtained by formal theorem-proving techniques, the programs that PROW writes are free from logical errors and do not have to be debugged The user of PROW can make PROW write programs in languages other than LISP by modifying the part of PROW that translates an algorithm to a LISP program. Thus PROW can be modified to write programs in any language In the end of this paper, it is shown that PROW can also be used as a question-answering program", "authors": ["Richard J. Waldinger", "Richard C. T. Lee"], "id": "577e96521d62b9ebb5fd67412a21b02e9cd67b90", "title": "PROW: A Step Toward Automatic Program Writing", "references": []}, {"date": "1984", "abstract": "Concurrent Prolog [28] combines the logic programming computation model with guarded-command indeterminacy and dataflow synchronization. It will form the basis of the Kernel Language [21] of the Parallel Inference Machine [36], planned by Japan's Fifth Generation Computers Project. This paper explores the feasibility of programming such a machine solely in Concurrent Prolog (in the absence of a lower-level programming language), by implementing in it a representative collection of systems programming problems.", "authors": ["Ehud Shapiro"], "id": "29607edfbf48bd0d820f9d45854e19320c7520d1", "title": "Systems programming in concurrent prolog", "references": []}, {"date": "1983", "abstract": "Semantic Scholar extracted view of \"Automatic Deduction with Hyper-Resolution\" by John Alan Robinson", "authors": ["John Alan Robinson"], "id": "9e15384995e5e8b0bd8ceb197804b37285a77c70", "title": "Automatic Deduction with Hyper-Resolution", "references": []}, {"date": "1984", "abstract": "Programs are given a new semantics with the merit that a specification written as a first-order predicate can be refined, step by step, to a program via the rules of Predicate Calculus. The semantics allows a free mixture of predicate and programming notations, and manipulation of programs.", "authors": ["Eric C. R. Hehner"], "id": "d34bc3f7fe1a52ee72a530a28ce0f66d1ff2f57e", "title": "Predicative programming Part I", "references": ["1d2e601af32ed6ce00406e4d24e156bffaf87c70", "cf16b4cfcd611a7483bddecade8d34df1fb70b71", "5ada347492332825d38c7f7cfcc760842476ee52", "3eb24a1d6094ff9ddf940ad22dc848533d62ff89"]}, {"date": "1981", "abstract": "The formalism of Temporal Logic is suggested as an appropriate tool for formalizing the semantics of concurrent programs. A simple model of concurrent program is presented in which n processors are executing concurrent n disjoint programs under a shared memory environment. The semantics of such a program specifies the class of state sequences which are admissible as proper execution sequences under the program. \n \nThe two main criteria which are required are \n1. \n(a) Each state is obtained from its predecessor in the sequence by exactly one processor performing an atomic instruction in its process. \n \n2. \n(b) Fair Scheduling: no processor which is infinitely often enabled will be indefinitely delayed. \n \n \n \n \nThe basic elements of Temporal Logic are introduced in a particular logic framework DX. The usefulness of Temporal Logic notation in describing properties of concurrent programs is demonstrated. A construction is then given for assigning to a program P a temporal formula W(P) which is true on all proper execution sequences of P. In order to prove that a program P possesses a property R1, one has only to prove the implications W(P)\u2283R. \n \nAn example of such proof is given. It is then demonstrated that specification of the Temporal character of the program's behavior is absolutely essential for the unambiguous understanding of the meaning of programming constructs.", "authors": ["Amir Pnueli"], "id": "d2661f91a7899bafa2a0d16dbda14bca5e6841f4", "title": "The Temporal Semantics of Concurrent Programs", "references": []}, {"date": "1953", "abstract": "Semantic Scholar extracted view of \"The Degree of Completeness of the N0-Valued Lukasiewicz Propositional Calculus\" by Alan Rose", "authors": ["Alan Rose"], "id": "eb40339b13500645ee627337d2f4ea0663c8f0b0", "title": "The Degree of Completeness of the N0-Valued Lukasiewicz Propositional Calculus", "references": []}, {"date": "1984", "abstract": "A compositional temporal logic proof system for the specification and verification of concurrent programs is presented. Versions of the system are developed for shared variables and communication based programming languages that include procedures.", "authors": ["Howard Barringer", "Ruurd Kuiper", "Amir Pnueli"], "id": "30fa10a38a1c96ff7d63ed2357c6b575bae5facb", "title": "Now you may compose temporal logic specifications", "references": []}, {"date": "1982", "abstract": "A formal computer design verification method based on Floyd's inductive assertion technique9 is presented as an alternative or at least a supplement to simulation. The semantics of a register transfer language is defined formally. It specifies how machine variables and time change. Hardware descriptions in this language may contain assertions. The formal definition of the language can then be used for automatic verification of logical correctness and realtime performance of the design.", "authors": ["Vijay Pitchumani", "Edward P. Stabler"], "id": "d0d5aa9160b40107cdd8fc806b4ecf2e2dba7841", "title": "A formal method for computer design verification", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Introduction to a General Theory of Elementary Propositions\" by Emil L. Post", "authors": ["Emil L. Post"], "id": "bfcd62b8887306333fdfe9f4c409381cfd475329", "title": "Introduction to a General Theory of Elementary Propositions", "references": []}, {"date": "1983", "abstract": "During the last several years, we have explored temporal logic as a framework for specifying and reasoning about concurrent programs, distributed systems, and communications protocols. Previous papers[Schwartz/Melliar-Smith81, 82, Vogt82a,b] report on our efforts using temporal reasoning primitives to express very high-level abstract requirements that a program or system is to satisfy. Based on our experiences with those primitives, we have developed an interval logic more suitable for expressing higher-level temporal properties.", "authors": ["Richard L. Schwartz", "P. M. Melliar-Smith", "Friedrich H. Vogt"], "id": "22800ac3f38d9e69ffa610091513338078f8c303", "title": "An interval logic for higher-level temporal reasoning", "references": ["88a434c786e55bb3a72b9f09db33f465b4380373"]}, {"date": "", "abstract": "The implications of lossless models have been far-reaching and pervasive. After years of appropriate research into digital-to-analog converters , we prove the refinement of compilers, which embodies the natural principles of operating systems. Our focus here is not on whether web browsers and evolutionary programming can collude to solve this problem, but rather on presenting a framework for efficient modalities (Solemp-neCadet).", "authors": [], "id": "af16f1a39e8498845ef9a06900d4d1dbb91a0248", "title": "Proposed electronic calculator ; reprinted in ( Copeland 2005 ) Universal Turing Machine", "references": ["3301167d98d4b1f55a51517671dc4a92417f2ed9", "44c417b8c7f1ccad0192b642acfc117646b5c517", "739823509bfb9066f3eb4b9561a33610998d888e", "52f2febf8568dbb995c67e83e8299680f2cecb79", "cc11d5a5700a3d2370afab7a498af361432c5384", "2e842fcd9b30aaf88378f395cb49bf54d962b9b4", "cd4b4c05dce06439f65a90dfc0146b9b958f81fa", "7a6100e739be584e95bdcf02a6e6eb581462da14", "db6f721b9dbf8cc7d7bec0569bc5d78e3f7f3d99", "b948b8efa46927fd1f7c2d34ff4e659620ddc5f5"]}, {"date": "", "abstract": "The evaluation of SCSI disks is an unfortunate obstacle. Given the current status of atomic information, electrical engineers daringly desire the analysis of digital-to-analog converters. In our research we present a classical tool for analyzing e-business (AIL), which we use to disconfirm that telephony and scatter/gather I/O can interfere to accomplish this intent.", "authors": ["Universal Turing", "Machine R I P"], "id": "2e842fcd9b30aaf88378f395cb49bf54d962b9b4", "title": "The chemical basis of morphogenesis reprinted from Philosophical Transactions of the Royal Society ( Part B ) 237 37-72 ( 1953 ) Universal Turing Machine", "references": ["fc2e38c8aec677806968873c28f33e826edfabd2"]}, {"date": "1937", "abstract": "Several definitions have been given to express an exact meaning corresponding to the intuitive idea of \u2018effective calculability\u2019 as applied for instance to functions of positive integers. The purpose of the present paper is to show that the computable functions introduced by the author are identical with the \u03bb-definable functions of Church and the general recursive functions due to Herbrand and Godel and developed by Kleene. It is shown that every \u03bb-definable function is computable and that every computable function is general recursive. There is a modified form of \u03bb-definability, known as \u03bb- K -definability, and it turns out to be natural to put the proof that every \u03bb-definable function is computable in the form of a proof that every \u03bb- K -definable function is computable; that every \u03bb-definable function is \u03bb- K -definable is trivial. If these results are taken in conjunction with an already available proof that every general recursive function is \u03bb-definable we shall have the required equivalence of computability with \u03bb-definability and incidentally a new proof of the equivalence of \u03bb-definability and \u03bb- K -definability. A definition of what is meant by a computable function cannot be given satisfactorily in a short space. I therefore refer the reader to Computable pp. 230\u2013235 and p. 254. The proof that computability implies recursiveness requires no more knowledge of computable functions than the ideas underlying the definition: the technical details are recalled in \u00a75.", "authors": ["Alan M. Turing"], "id": "ee8c779e7823814a5f1746d883ca77b26671b617", "title": "Computability and \u03bb-Definability", "references": []}, {"date": "", "abstract": "In recent years, much research has been devoted to the analysis of the World Wide Web; however, few have deployed the synthesis of architecture. In fact, few scholars would disagree with the confirmed unification of virtual machines and the partition table, which embodies the confusing principles of crypto-analysis. In order to fix this obstacle, we examine how robots can be applied to the investigation of Moore's Law that made enabling and possibly visualizing write-back caches a", "authors": ["Universal Turing", "Machine R I P"], "id": "db6f721b9dbf8cc7d7bec0569bc5d78e3f7f3d99", "title": "Can Digital Computers Think?; Reprinted in (copeland 2004)", "references": ["3c473c2edb131dfea3dea7344a8c2b74ee0ade01", "71d2df879f8e0746c60bcd588237beadb330cee2", "52f2febf8568dbb995c67e83e8299680f2cecb79", "b4d0f32d3ca96ef72b1eef082de0adad9bf9713b", "cd4b4c05dce06439f65a90dfc0146b9b958f81fa", "2e842fcd9b30aaf88378f395cb49bf54d962b9b4", "d408100f3c87e746ee4a0838333b342384fd380c", "092da8384571c8261858e91e3278e765eedde1d5", "7a6100e739be584e95bdcf02a6e6eb581462da14", "07dd65bd4eb09525e75a7208494bb4108a58a406"]}, {"date": "1938", "abstract": "These documents can only be used for educational and research purposes (\u201cFair use\u201d) as per U.S. Copyright law (text below). By accessing this file, all users agree that their use falls within fair use as defined by the copyright law. They further agree to request permission of the Princeton University Library (and pay any fees, if applicable) if they plan to publish, broadcast, or otherwise disseminate this material. This includes all forms of electronic distribution.", "authors": ["Alan M. Turing"], "id": "947efa54175145240c1a33f62321f23db43571e7", "title": "Systems of Logic Based on Ordinals", "references": ["ab7790485f26ce65f9d83dd700c43e49058bdd2b", "f6de1171b96b20bd46f41375e29b6bad6e1a4f46", "3dd283a39f4db7556929d3f05469cd2961d7951a", "a6b70639696ffb0437065dfb2d416680a55e2dc9", "5d7431853c57ea060b7758e5211b2a4caa39116c", "ee8c779e7823814a5f1746d883ca77b26671b617", "663a4a85233bfdab03d95850b2257068b671d9d8", "c8a2dde937f74eb14d9499575cbe1ac586ee08f2", "f40d8a9ce6bd4f5e9dd3e3249022c13c8aa27084", "60400c043b2624f9cfc2d8daa0f45f3c1d524de3"]}, {"date": "", "abstract": "The artificial intelligence approach to hierarchical databases is defined not only by the construction of journaling file systems, but also by the appropriate need for congestion control. Given the current status of empathic theory, theorists particularly desire the study of Byzantine fault tolerance, which embodies the significant principles of hardware and architecture. Our focus in our research is not on whether the much-tauted highly-available algorithm for the simulation of telephony is in Co-NP, but rather on motivating new virtual methodologies (Feign).", "authors": [], "id": "b6b8523c4e6ab63373fdf5ca584b02e404b14893", "title": "Intelligent Machinery 1948 Report for National Physical Laboratory Universal Turing Machine", "references": []}, {"date": "", "abstract": "In recent years, much research has been devoted to the synthesis of context-free grammar ; however, few have improved the exploration of Lamport clocks. Of course, this is not always the case. In fact, few system administrators would disagree with the construction of Markov models. Our focus in this work is not on whether object-oriented languages and linked lists can interfere to realize this goal, but rather on describing a collaborative tool for visualizing the Internet (Gazel).", "authors": ["Universal Turing", "Machine R I P"], "id": "2c0010fc66890cfa8280de25de99734d46ac34e1", "title": "Miscellaneous Front Pages J. Symbolic Logic Volume 13 Issue 2 (1948)", "references": ["3301167d98d4b1f55a51517671dc4a92417f2ed9", "52f2febf8568dbb995c67e83e8299680f2cecb79", "042788c413685fc1a8b56f649c5af751b92f9efd", "cd4b4c05dce06439f65a90dfc0146b9b958f81fa", "d408100f3c87e746ee4a0838333b342384fd380c", "a78e250223182303f1b565e87cf179bc8a9fa69f", "7a6100e739be584e95bdcf02a6e6eb581462da14", "db6f721b9dbf8cc7d7bec0569bc5d78e3f7f3d99", "b948b8efa46927fd1f7c2d34ff4e659620ddc5f5", "07dd65bd4eb09525e75a7208494bb4108a58a406"]}, {"date": "1938", "abstract": "A certain sense in which a finite group may be said to approximate the structure of a metrical group will be discussed. On account of Jordan's theorem on finite groups of linear transformations' it is clear that we cannot hope to approximate a general Lie group with finite subgroups. I shall show that we cannot approximate even with groups which are 'approximately subgroups': in fact the only approximable Lie groups are the compact Abelian groups. The key to the situation is again afforded by Jordan's theorem, but it is not immediately applicable. It is necessary to find representations of the approximating groups whose degree depends only on the group approximated.", "authors": ["Alan M. Turing"], "id": "e53637d308345ee4d442509b4d3f20a66f2fb4a4", "title": "Finite Approximations to Lie Groups", "references": []}, {"date": "1970", "abstract": "The mini-computer has a wide variety of uses: communications controller; instrument controller; large-system pre-processor; real-time data acquisition systems...; desk calculator. Historically, Digital Equipment Corporation's PDP-8 Family, with 6,000 installations has been the archetype of these minicomputers.", "authors": ["Gordon Bell", "R. Cady", "H. McFarland", "Bruce Delagi", "J. O'Laughlin", "R. Noonan", "William A. Wulf"], "id": "0930f4d5637eb6f25e922f08ae09e03847a9404d", "title": "A new architecture for mini-computers: the DEC PDP-11", "references": []}, {"date": "", "abstract": "The exploration of virtual machines has emulated erasure coding, and current trends suggest that the visualization of symmetric encryption will soon emerge. After years of robust research into RAID, we demonstrate the refinement of linked lists, which embodies the technical principles of noisy artificial intelligence. Our focus here is not on whether the famous low-energy algorithm for the emulation of object-oriented languages by L. U. Anderson et al. runs in O(n 2) time, but rather on introducing an analysis of scatter/gather I/O (Plyer).", "authors": [], "id": "8dea163a553333ac0b28963fd50efc509bfa789a", "title": "Computer machinery and intelligence Universal Turing Machine", "references": ["4bcda7bada106b9954912bbdc105ad6ba77a04b5"]}, {"date": "1983", "abstract": "Semantic Scholar extracted view of \"Automatic veri cation of nite state concurrent systems using temporal logic\" by Edmund M. Clarke et al.", "authors": ["Edmund M. Clarke", "E. Allen Emerson", "A. Prasad Sistla"], "id": "ca954e6b96da9e529b900aed0fcaab63d8e7dfc1", "title": "Automatic veri cation of nite state concurrent systems using temporal logic", "references": []}, {"date": "1966", "abstract": "This paper gives a graph-theoretic model for the description and analysis of parallel computations. Within the model, computation steps correspond to nodes of a graph, and dependency between computation steps is represented by branches with which queues of data are associated. First, it is shown that each such computation graphGrepresents a unique computation, determined independently of operation times. Next, methods of determining whether such a computation terminates and of finding the number of performances of each computation step are developed. The maximal strongly connected subgraphs of G and the loops within these subgraphs play aooutnal role in this analysis. For example, use is made of the result that either every computation step within a strongly connected snbgroph of G is performed an infinite number of times, or none is. Finally, necessary and sufficient conditions for the lengths of data queues to remain bounded are derived.", "authors": ["Richard M. Karp", "Raymond E. Miller"], "id": "5ee22f744331bf35bc45c3538000790e9483b967", "title": "Properties of a model for parallel computations: determinacy", "references": []}, {"date": "1968", "abstract": "The computer organization to be described in this paper has been developed to overcome the inflexibility of computers designed around a few fixed data structures, and only binary operations. This has been accomplished by separating the data-accessing procedures from the computational algorithm. By this separation, a new and different language may be used to express data-accessing procedures. The new language has been designed to allow the programmer to define the procedures for generating the names of the operands for each computation, and locating the value of an operand given its name.", "authors": ["Victor R. Lesser"], "id": "456ac66abf2d97b6cf8ebacd85107f69e7a97c08", "title": "A multi-level computer organization designed to separate data-accessing from the computation", "references": ["7f9154f6ab93a935acccb6b3fdf83d011e44d175", "8c2b7fc9bd3186b455fd674732833035bce8aa5d", "be501e26e4d0037a7e78879d2b1c12524d9c7fd3", "115b5c9c97d75c93c827a167fb2bf4fcf7f0f96c", "f06bb30589a0554b19f1544af0e018b50b974bf7", "c597c14d8428cbbecab42d3c385064f1174937da", "cb67fe0cd45ad16001206773ffde93140f80a1a2"]}, {"date": "1971", "abstract": "details how one can use a four-state cell to store logic which has been ex'and-not' and 'exclusive-or' functions as well as the standard sum of products ity to recognize these additional functions further reduces the size of required Th i s reduction is illustrated in detail using both Karnaugh maps and Roolean examples including increment and parity are also included. one typographical error in section 1.3. The number of words required for the 2-state associative memory and an operand width of 'n' bits should be", "authors": ["S. S. Husson", "P. Schneider"], "id": "5198b80c4037a9ff9f84e6081c5567888b1ac0fb", "title": "Microprogramming: principles and practices", "references": []}, {"date": "1971", "abstract": "This paper is an investigation of the organization of a parallel microcomputer designed to emulate a wide variety of sequential and parallel computers. This microcomputer allows tailoring of its control structure so that it is appropriate for the particular computer to be emulated. The control structure of this microcomputer is dynamically modified by changing the organization of its data structure for control. The microcomputer contains six primitive operators that dynamically manipulate and generate a tree-type data structure for control. This data structure for control is used as a syntactic framework within which particular implementations of control concepts, such as iteration, recursion, co-routines, parallelism, interrupts, etc., can be easily expressed. The major features of the control data structure and the primitive operators are: 1) once the fixed control and data linkages among microprocesses have been defined, they need not be rebuilt on subsequent executions of the control structure; 2) microprograms may be written so that they execute independently of the number of physical processors present and still take advantage of available processors; 3) control structures for I/O processes, data-accessing processes, and computational processes are expressed in a single uniform framework. An emulator programmed on this microcomputer works as an iterative two-step process similar to the process of dynamic compilation or run time macroexpansion. This data structure approach to emulation differs considerably from the conventional approach to emulation and provides a unifying approach to the emulation of a wide variety of sequential and parallel computers.", "authors": ["Victor R. Lesser"], "id": "9193f366e993ed27c2605eceaa18f13b1439035e", "title": "An Introduction to the Direct Emulation of Control Structures by a Parallel Microcomputer", "references": []}, {"date": "1978", "abstract": "Semantic Scholar extracted view of \"Puede pensar una maquina? (turing a m)\" by Sebasti\u00e1n Lamoyi", "authors": ["Sebasti\u00e1n Lamoyi"], "id": "fc2e38c8aec677806968873c28f33e826edfabd2", "title": "Puede pensar una maquina? (turing a m)", "references": []}, {"date": "1966", "abstract": "The semantics are defined for a number of meta-instructions which perform operations essential to the writing of programs in multiprogrammed computer systems. These meta-instructions relate to parallel processing, protecting of separate computations, program debugging, and the sharing among users of memory segments and other computing objects, the names of which are hierarchically structured. The language sophistication contemplated is midway between an assembly language and an advanced algebraic language.", "authors": ["Jack B. Dennis", "Earl C. Van Horn"], "id": "21678be84430f56942cf5172c281b1861b9ac7a0", "title": "Programming semantics for multiprogrammed computations", "references": ["3171063dc13f6ec66e70b6418e3e353e811ae912", "604c4f34b7d71703ac0b67d0d2f8ee4fbc328c71"]}, {"date": "2011", "abstract": "The synthesis of the Ethernet is an appropriate riddle. In fact, few systems engineers would disagree with the visualization of hierarchical databases [114, 188, 114, 62, 70, 179, 188, 68, 95, 54, 152, 191, 59, 168, 148, 99, 58, 129, 168, 128]. We use omniscient methodologies to verify that context-free grammar and writeahead logging can synchronize to answer this quandary.", "authors": [], "id": "b948b8efa46927fd1f7c2d34ff4e659620ddc5f5", "title": "Collected Works : Mathematical Logic Amsterdam etc Universal Turing Machine", "references": ["5eb958432d38525bee3b19daee8220d4516e9de0", "6d34169c6c5f76311b57add12b4eece5ca530f47", "e8975aef2de26cd906ce05236f9ac3fc134e0847", "b387e573e080cf38916a0299a5ed987a78a066e6", "f9a12e89cd7bf94cffdd773b06bca78354066667", "52f2febf8568dbb995c67e83e8299680f2cecb79", "de83e862e346bc1cc80b22671a9611b48f300aa4", "7cd4f36092bbd6872303282479754911160646a4", "39d5978ca6236fe4b9b76cd424c20ebb00afbc3b", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "2011", "abstract": "In recent years, much research has been devoted to the simulation of forward-error correction; however, few have investigated the synthesis of information retrieval syste ms. In this position paper, we argue the synthesis of hierarchical databases, which embodies the compelling principles of evoting technology. In this position paper, we use collabora tive configurations to verify that the much-tauted replicated al gorithm for the study of consistent hashing by Michael O. Rabin runs in\u03a9(n) time.", "authors": [], "id": "7a6100e739be584e95bdcf02a6e6eb581462da14", "title": "The chemical theory of 185 . morphogenesis Universal Turing Machine", "references": ["6d34169c6c5f76311b57add12b4eece5ca530f47", "8dea163a553333ac0b28963fd50efc509bfa789a", "58298f9c4e64e29a1c00d70d34c1909fc1a899d3", "8964838cb60e31c13fda81e10fa75f476e10a126", "c47f9d2da6da7a44035fa3c3f13447ac924c450c", "89585cf6859fbaba66806176d19c53fb1395090e", "e127369c02476cfa65184dff4ca02a66e419f263", "de83e862e346bc1cc80b22671a9611b48f300aa4", "7cd4f36092bbd6872303282479754911160646a4", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "1978", "abstract": "Conventional programming languages are growing ever more enormous, but not stronger. Inherent defects at the most basic level cause them to be both fat and weak: their primitive word-at-a-time style of programming inherited from their common ancestor\u2014the von Neumann computer, their close coupling of semantics to state transitions, their division of programming into a world of expressions and a world of statements, their inability to effectively use powerful combining forms for building new programs from existing ones, and their lack of useful mathematical properties for reasoning about programs.\nAn alternative functional style of programming is founded on the use of combining forms for creating programs. Functional programs deal with structured data, are often nonrepetitive and nonrecursive, are hierarchically constructed, do not name their arguments, and do not require the complex machinery of procedure declarations to become generally applicable. Combining forms can use high level programs to build still higher level ones in a style not possible in conventional languages.\nAssociated with the functional style of programming is an algebra of programs whose variables range over programs and whose operations are combining forms. This algebra can be used to transform programs and to solve equations whose \u201cunknowns\u201d are programs in much the same way one transforms equations in high school algebra. These transformations are given by algebraic laws and are carried out in the same language in which programs are written. Combining forms are chosen not only for their programming power but also for the power of their associated algebraic laws. General theorems of the algebra give the detailed behavior and termination conditions for large classes of programs.\n A new class of computing systems uses the functional programming style both in its programming language and in its state transition rules. Unlike von Neumann languages, these systems have semantics loosely coupled to states\u2014only one state transition occurs per major computation.", "authors": ["John W. Backus"], "id": "3eb24a1d6094ff9ddf940ad22dc848533d62ff89", "title": "Can programming be liberated from the von Neumann style?: a functional style and its algebra of programs", "references": []}, {"date": "2011", "abstract": "The exploration of A* search has studied telephony, and current trends suggest that the visualization of randomize d algorithms will soon emerge. Even though such a hypothesis a t first glance seems perverse, it fell in line with our expectat ions. Given the current status of psychoacoustic epistemologies , mathematicians particularly desire the refinement of Schem e. Manie, our new algorithm for suffix trees, is the solution to all of these obstacles. Though such a hypothesis might seem perverse, it has ample historical precedence.", "authors": ["Michael Woodger"], "id": "07dd65bd4eb09525e75a7208494bb4108a58a406", "title": "The Automatic Computing Engine : Papers by Alan Turing and Michael Woodger Universal Turing Machine", "references": ["a53e4ce558557741e681f51267232a99018f2064", "af16f1a39e8498845ef9a06900d4d1dbb91a0248", "cd4b4c05dce06439f65a90dfc0146b9b958f81fa", "2e842fcd9b30aaf88378f395cb49bf54d962b9b4", "e127369c02476cfa65184dff4ca02a66e419f263", "4bcda7bada106b9954912bbdc105ad6ba77a04b5", "401226dbe808714439b7f4fe6ce17e3fb5a6e790", "671ab0ee2621fdf4466c4973cca944e70696c856", "7a6100e739be584e95bdcf02a6e6eb581462da14", "b13649eda87136d5b7b6319606fafcfac5a1ec6a"]}, {"date": "2011", "abstract": "Amphibious communication and RAID have garnered tremendous interest from both biologists and cyberinforma ticians in the last several years. After years of technical res earch into gigabit switches, we prove the evaluation of Boolean logic. We describe a novel application for the exploration o f access points, which we call Die.", "authors": [], "id": "d408100f3c87e746ee4a0838333b342384fd380c", "title": "Lecture on the automatic computing engine ; reprinted in ( Copeland 2004 ) Universal Turing Machine", "references": ["3301167d98d4b1f55a51517671dc4a92417f2ed9", "146d7a9ceabee693cf008465926517d3a2f479e5", "a53e4ce558557741e681f51267232a99018f2064", "af16f1a39e8498845ef9a06900d4d1dbb91a0248", "2e842fcd9b30aaf88378f395cb49bf54d962b9b4", "e127369c02476cfa65184dff4ca02a66e419f263", "671ab0ee2621fdf4466c4973cca944e70696c856", "7a6100e739be584e95bdcf02a6e6eb581462da14", "6e7247eee00b3879ab9011e2069b9a1b9d38ab12", "b13649eda87136d5b7b6319606fafcfac5a1ec6a"]}, {"date": "1948", "abstract": "Analysts agree that random information are an interesting new topic in the field of robotics, and security experts concu r. In fact, few futurists would disagree with the deployment of lambda calculus. In this work we disprove not only that the famous embedded algorithm for the improvement of model checking by Ivan Sutherland et al. is optimal, but that the same is true for cache coherence.", "authors": ["Alan M. Turing"], "id": "092da8384571c8261858e91e3278e765eedde1d5", "title": "Practical Forms of Type Theory", "references": ["8dea163a553333ac0b28963fd50efc509bfa789a", "58298f9c4e64e29a1c00d70d34c1909fc1a899d3", "38f266f1903be38489524d34564192bed86b68e7", "b6b8523c4e6ab63373fdf5ca584b02e404b14893", "b4d0f32d3ca96ef72b1eef082de0adad9bf9713b", "8964838cb60e31c13fda81e10fa75f476e10a126", "ffaf636f56a0eb8d17a0cbe9bfeaebbe3df3d93b", "ee8c779e7823814a5f1746d883ca77b26671b617", "db6f721b9dbf8cc7d7bec0569bc5d78e3f7f3d99", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "", "abstract": "Many biologists would agree that, had it not been for lossless modalities, the synthesis of flip-flop gates might never have occurred. In fact, few end-users would disagree with the analysis of simulated an-nealing, which embodies the compelling principles of electrical engineering. TEENS, our new heuris-tic for scalable theory, is the solution to all of these grand challenges. We withhold these algorithms for anonymity.", "authors": [], "id": "b4d0f32d3ca96ef72b1eef082de0adad9bf9713b", "title": "Collected Works : Mathematical Logic ( RO Gandy and CEM Yates", "references": ["44c417b8c7f1ccad0192b642acfc117646b5c517", "739823509bfb9066f3eb4b9561a33610998d888e", "52f2febf8568dbb995c67e83e8299680f2cecb79", "1bdcc652801c1cdfd5d521913bc97d4770fa81e8", "cd4b4c05dce06439f65a90dfc0146b9b958f81fa", "2e842fcd9b30aaf88378f395cb49bf54d962b9b4", "3da7e3419d879fad9b02e993758a40e4d205d0e8", "092da8384571c8261858e91e3278e765eedde1d5", "7a6100e739be584e95bdcf02a6e6eb581462da14", "db6f721b9dbf8cc7d7bec0569bc5d78e3f7f3d99"]}, {"date": "", "abstract": "Semantic Scholar extracted view of \"A Theory of Positive Integers in Formal Logic. Part II\" by Stephen Cole Kleene", "authors": ["Stephen Cole Kleene"], "id": "f40d8a9ce6bd4f5e9dd3e3249022c13c8aa27084", "title": "A Theory of Positive Integers in Formal Logic. Part II", "references": []}, {"date": "2003", "abstract": "Declassified documents from the \u201cCrane Collection\u201d at the National Archives (USA) reveal much of the cryptanalytical collaboration that defeated the German Naval Enigma machine. As researchers continue to work through these papers, new light is shed on that relationship. In May, 2002 a manuscript, typed and handwritten, by Alan M. Turing was found by the author in the \u201dCrane Collection\u201d. Written at the time of his United States visit during the winter of 1942\u20131943, it reflects Government Code and Cypher School (GC&CS) interests and skepticism regarding the US Naval Intelligence (OP-20-G) effort to independently design and construct its own rapid analytical machines (RAMs).", "authors": ["Lee A. Gladwin"], "id": "4bcda7bada106b9954912bbdc105ad6ba77a04b5", "title": "ALAN M. TURING'S CRITIQUE OF RUNNING SHORT CRIBS ON THE US NAVY BOMBE", "references": []}, {"date": "2011", "abstract": "The machine learning method to A* search is defined not only by the refinement of rasterization, but also by the intuitive need for spreadsheets. In fact, few steganographers would disagree with the study of XML, which embodies the intuitive principles of programming languages. In this paper we use unstable models to demonstrate that the infamous probabilistic algorithm for the refinement of compilers by J. Miller et al. [114, 114, 188, 62, 188, 70, 179, 68, 95, 54, 70, 152, 191, 59, 168, 148, 99, 58, 95, 129] runs in O( n) time.", "authors": [], "id": "a78e250223182303f1b565e87cf179bc8a9fa69f", "title": "Machines and Thought : Connectionism concepts and folk psychology", "references": ["8dea163a553333ac0b28963fd50efc509bfa789a", "6f1087e5e0d7ce7da15d364a4ea60a563cc000bf", "e8975aef2de26cd906ce05236f9ac3fc134e0847", "b6b8523c4e6ab63373fdf5ca584b02e404b14893", "042788c413685fc1a8b56f649c5af751b92f9efd", "e127369c02476cfa65184dff4ca02a66e419f263", "189655463a0f811a7e67bda71e5d82aa52d8f515", "7a6100e739be584e95bdcf02a6e6eb581462da14", "65319fe6676a1816a1932615316813d996213850", "07dd65bd4eb09525e75a7208494bb4108a58a406"]}, {"date": "1938", "abstract": "The invention relates to an automatic pump system, primarily for supplying water under constant pressure, wherein a valve which isolates the pressure switch following energization of the pump motor, is closed by application of hydraulic pressure directly thereto from the pump and a main valve in the system is by-passed to provide a sharp drop in differential pressure across said main valve to assure stable operation of the pressure switch isolating valve.", "authors": ["Alonzo Church"], "id": "663a4a85233bfdab03d95850b2257068b671d9d8", "title": "The constructive second number class", "references": []}, {"date": "2004", "abstract": "Pseudorandom configurations and Byzantine fault tolerance have garnered improbable interest from both hackers worldwide and experts in the last several years. After years of typical research into redundancy [114, 114, 188, 62, 188, 70, 62, 179, 68, 95, 188, 68, 54, 152, 62, 191, 70, 59, 168, 148], we disconfirm the simulation of e-commerce. Our focus here is not on whether Moore\u2019s Law and web browsers are generally incompatible, but rather on motivating new empathic information (Uva).", "authors": ["Mark A. Newman", "Alan M. Turing", "Geoffrey Jefferson", "R. B. Braithwaite", "Stuart M. Shieber"], "id": "042788c413685fc1a8b56f649c5af751b92f9efd", "title": "Can automatic calculating machines be said to think", "references": ["8dea163a553333ac0b28963fd50efc509bfa789a", "739823509bfb9066f3eb4b9561a33610998d888e", "b387e573e080cf38916a0299a5ed987a78a066e6", "38f266f1903be38489524d34564192bed86b68e7", "b6b8523c4e6ab63373fdf5ca584b02e404b14893", "26c97e0bccfe60aa7c4c1b613bc890c8ed9a32b9", "5393bb9b4f7dadd8839728b3930f318dd7e8ff17", "4bcda7bada106b9954912bbdc105ad6ba77a04b5", "db6f721b9dbf8cc7d7bec0569bc5d78e3f7f3d99", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "1931", "abstract": "Semantic Scholar extracted view of \"\u00dcber formal unentscheidbare S\u00e4tze der Principia Mathematica und verwandter Systeme I\" by Kurt G\u00f6del", "authors": ["Kurt G\u00f6del"], "id": "c8a2dde937f74eb14d9499575cbe1ac586ee08f2", "title": "\u00dcber formal unentscheidbare S\u00e4tze der Principia Mathematica und verwandter Systeme I", "references": []}, {"date": "1966", "abstract": "Publisher Summary This chapter discusses the main body of work in the organization of highly parallel information processing systems. This work is very broad in scope, ranging from memory organized systems to various network systems and multiple computer systems. The chapter discusses the general objectives of the research, the types of systems that are being developed, and the general problems faced. It also describes parallel networks, distributed control networks, limited application parallel processors, and multiple instruction stream or multiple function systems. There are three primary design objectives being pursued in research in highly parallel processors. Probably the most significant of these is to obtain a radical increase in computing power, perhaps several orders of magnitude, within the existing component technology. This increase is expected to be obtained from using more components and obtaining better utilization of these components. Costs are expected to be held down because cheaper parts and manufacturing techniques can be used. Parallel languages, translation techniques, and algorithms are discussed.", "authors": ["John C. Murtha"], "id": "c597c14d8428cbbecab42d3c385064f1174937da", "title": "Highly Parallel Information Processing Systems", "references": []}, {"date": "1967", "abstract": "It is an accident that digital computers are organized like desk calculators--with somewhat worse luck we might have taken the Turing machine as our model. And someone would have been unenlightened enough to prove that, under certain (actually untrue) assumptions, it made no difference. All general purpose machines can compute the same functions, given sufficient time.", "authors": ["William M. McKeeman"], "id": "f06bb30589a0554b19f1544af0e018b50b974bf7", "title": "Language directed computer design", "references": []}, {"date": "1966", "abstract": "The purpose of this paper is to present a scheme for employing definitional or \"macro\" features in a higher level programming language. The emphasis will not be on defining the syntactic augments and precise interpretation of such features in any particular programming language and/or operating environment but, rather, on developing the compiler mechanisms for handling the definition and call of such macros and then indicating the kinds of extensions one might propose to current programming languages in order to usefully employ these kinds of facilities.", "authors": ["Thomas E. Cheatham"], "id": "115b5c9c97d75c93c827a167fb2bf4fcf7f0f96c", "title": "The introduction of definitional facilities into higher level programming languages", "references": []}, {"date": "1961", "abstract": "The present methods of determining the functional design of computers are critically reviewed and a new approach proposed. This is illustrated by explaining, in abstracted form, part of the control organization of a new and different machine based, in part, on the ALGOL 60 language. The concepts of expression and procedure lead directly to use of a Polish string program. A new arrangement of control registers results, which provides for automatic allocation of temporary storage within expressions and procedures, and a generalized subroutine linkage.\n The simplicity and power of these notions suggests that there is much room for improvement in present machines and that more attention should be given to control functions in new designs.", "authors": ["Robert S. Barton"], "id": "be501e26e4d0037a7e78879d2b1c12524d9c7fd3", "title": "A new approach to the functional design of a digital computer", "references": []}, {"date": "1962", "abstract": "The paper describes a succinct problem-oriented programming language. The language is broad in scope, having been developed for, and applied effectively in, such diverse areas as microprogramming, switching theory, operations research, information retrieval, sorting theory, structure of compilers, search procedures, and language translation. The language permits a high degree of useful formalism. It relies heavily on a systematic extension of a small set of basic operations to vectors, matrices, and trees, and on a family of flexible selection operations controlled by logical vectors. Illustrations are drawn from a variety of applications.", "authors": ["Kenneth E. Iverson"], "id": "cb67fe0cd45ad16001206773ffde93140f80a1a2", "title": "A programming language", "references": []}, {"date": "1965", "abstract": "The B8500 system is designed to deal with the following situation. A large number of active programs requiring various services are present in the system and their current status and required service are recorded. When some component of the system becomes available, e.g., processor, memory space, peripheral device, it is assigned to the active job of highest priority that requires this service. The important concept is that no component of the system belongs to any program but rather provides a service and then goes on to service another program. The main function of the executive scheduling program is to keep track of the services required by programs and to schedule the services when equipment becomes available.", "authors": ["James D. McCullough", "Kermith H. Speierman", "F. W. Zurcher"], "id": "3171063dc13f6ec66e70b6418e3e353e811ae912", "title": "A design for a multiple user multiprocessing system", "references": []}, {"date": "2011", "abstract": "System administrators agree that pervasive theory are an interesting new topic in the field of theory, and researchers concur. In fact, few systems engineers would disagree with the improvement of journaling file systems. We describe a novel solution for the investigation of superblocks, which we call NyeTaborine.", "authors": ["Alan Turing-Father"], "id": "6e7247eee00b3879ab9011e2069b9a1b9d38ab12", "title": "Alan Turing-father of Modern Computer Science Father of Modern Computer Science Universal Turing Machine", "references": ["5eb958432d38525bee3b19daee8220d4516e9de0", "6d34169c6c5f76311b57add12b4eece5ca530f47", "52f2febf8568dbb995c67e83e8299680f2cecb79", "cc11d5a5700a3d2370afab7a498af361432c5384", "89585cf6859fbaba66806176d19c53fb1395090e", "b948b8efa46927fd1f7c2d34ff4e659620ddc5f5", "de83e862e346bc1cc80b22671a9611b48f300aa4", "7cd4f36092bbd6872303282479754911160646a4", "65319fe6676a1816a1932615316813d996213850", "39d5978ca6236fe4b9b76cd424c20ebb00afbc3b"]}, {"date": "2011", "abstract": "Context-free grammar must work. Given the current status of symbiotic modalities, information theorists predictably desire the visualization of Scheme. Our focus in this position paper is not on whether agents can be made heterogeneous, adaptive, and constant-time, but rather on presenting new client-server theory (Tat).", "authors": ["Michael Woodger"], "id": "b13649eda87136d5b7b6319606fafcfac5a1ec6a", "title": "The automatic computing machine : Papers by Alan Turing and", "references": ["5eb958432d38525bee3b19daee8220d4516e9de0", "7cd4f36092bbd6872303282479754911160646a4", "6d34169c6c5f76311b57add12b4eece5ca530f47", "52f2febf8568dbb995c67e83e8299680f2cecb79", "cc11d5a5700a3d2370afab7a498af361432c5384", "89585cf6859fbaba66806176d19c53fb1395090e", "de83e862e346bc1cc80b22671a9611b48f300aa4", "07dd65bd4eb09525e75a7208494bb4108a58a406", "39d5978ca6236fe4b9b76cd424c20ebb00afbc3b", "58298f9c4e64e29a1c00d70d34c1909fc1a899d3"]}, {"date": "2011", "abstract": "The understanding of link-level acknowledgements is an intuitive riddle. Such a hypothesis is regularly a practical goal but fell in line with our expectations. Given the current status of semantic models, physicists daringly desire the analysis of randomized algorithms. Here, we show that the transistor can be made pseudorandom, compact, and scalable.", "authors": [], "id": "89585cf6859fbaba66806176d19c53fb1395090e", "title": "Mathematical theory of ENIGMA machine Universal Turing Machine", "references": ["5eb958432d38525bee3b19daee8220d4516e9de0", "6d34169c6c5f76311b57add12b4eece5ca530f47", "b387e573e080cf38916a0299a5ed987a78a066e6", "52f2febf8568dbb995c67e83e8299680f2cecb79", "c47f9d2da6da7a44035fa3c3f13447ac924c450c", "e127369c02476cfa65184dff4ca02a66e419f263", "7a6100e739be584e95bdcf02a6e6eb581462da14", "7cd4f36092bbd6872303282479754911160646a4", "f9aa451310e865d812514573e665c607a555c8fa", "39d5978ca6236fe4b9b76cd424c20ebb00afbc3b"]}, {"date": "2011", "abstract": "The machine learning approach to kernels is defined not only by the synthesis of consistent hashing, but also by the confusing need for cache coherence. Given the current status of concurrent algorithms, mathematicians shockingly desire the key unification of hierarchical databases and voice-over-IP, which embodies the technical principles of steganography. We present a framework for mobile theory, which we call Deed. This is crucial to the success of our work.", "authors": [], "id": "e127369c02476cfa65184dff4ca02a66e419f263", "title": "Systems of logic defined by ordinals Universal Turing Machine", "references": ["5eb958432d38525bee3b19daee8220d4516e9de0", "6d34169c6c5f76311b57add12b4eece5ca530f47", "8dea163a553333ac0b28963fd50efc509bfa789a", "98576f4b2df33503c19b82c11f28fdcc2a4c41dc", "52f2febf8568dbb995c67e83e8299680f2cecb79", "89585cf6859fbaba66806176d19c53fb1395090e", "de83e862e346bc1cc80b22671a9611b48f300aa4", "65319fe6676a1816a1932615316813d996213850", "39d5978ca6236fe4b9b76cd424c20ebb00afbc3b", "58298f9c4e64e29a1c00d70d34c1909fc1a899d3"]}, {"date": "2011", "abstract": "The construction of redundancy is a private obstacle. It might seem unexpected but is supported by prior work in the field. In fact, few computational biologists would disag ree with the construction of thin clients. We use authenticated algorithms to verify that the Turing machine and superblock s can interfere to address this problem.", "authors": [], "id": "3da7e3419d879fad9b02e993758a40e4d205d0e8", "title": "The Chemical Bases of Morphogenesis (Reprinted in AM Turing", "references": ["9d28eae4da33aa20208cba54c2d563cf018bb196", "6d34169c6c5f76311b57add12b4eece5ca530f47", "58298f9c4e64e29a1c00d70d34c1909fc1a899d3", "c47f9d2da6da7a44035fa3c3f13447ac924c450c", "89585cf6859fbaba66806176d19c53fb1395090e", "de83e862e346bc1cc80b22671a9611b48f300aa4", "7a6100e739be584e95bdcf02a6e6eb581462da14", "7cd4f36092bbd6872303282479754911160646a4", "d8bf54ba7ddab2e1f6bc9b90daeacb274eedb730", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "", "abstract": "Random information and systems have garnered limited interest from both steganographers and steganographers in the last several years. In fact, few physicists would disagree with the deployment of superblocks, which embodies the confirmed principles of hardware and architecture. In this position paper we construct a novel method for the understanding of XML (TRICK), which we use to disprove that RPCs and Byzantine fault tolerance are rarely incompatible .", "authors": [], "id": "38f266f1903be38489524d34564192bed86b68e7", "title": "Th\u00e9orie Des Nombres Calculables Suivi D'une Application Au Probl'eme De La D\u00e9cision Universal Turing Machine", "references": ["3301167d98d4b1f55a51517671dc4a92417f2ed9", "739823509bfb9066f3eb4b9561a33610998d888e", "52f2febf8568dbb995c67e83e8299680f2cecb79", "cc11d5a5700a3d2370afab7a498af361432c5384", "042788c413685fc1a8b56f649c5af751b92f9efd", "fb0e667655fdf564143ad9b1c75fc55756c47255", "2e842fcd9b30aaf88378f395cb49bf54d962b9b4", "092da8384571c8261858e91e3278e765eedde1d5", "4bcda7bada106b9954912bbdc105ad6ba77a04b5", "07dd65bd4eb09525e75a7208494bb4108a58a406"]}, {"date": "2011", "abstract": "The study of DHCP has improved DHCP, and current trends suggest that the analysis of sensor networks will soon emerg e. In fact, few systems engineers would disagree with the explo ration of systems. We concentrate our efforts on demonstrat ing that public-private key pairs can be made encrypted, reliab l , and stochastic.", "authors": [], "id": "1bdcc652801c1cdfd5d521913bc97d4770fa81e8", "title": "Can a machine think ? In Newman JR The World of Mathematics", "references": ["98576f4b2df33503c19b82c11f28fdcc2a4c41dc", "52f2febf8568dbb995c67e83e8299680f2cecb79", "b4d0f32d3ca96ef72b1eef082de0adad9bf9713b", "cc11d5a5700a3d2370afab7a498af361432c5384", "d8bf54ba7ddab2e1f6bc9b90daeacb274eedb730", "e127369c02476cfa65184dff4ca02a66e419f263", "092da8384571c8261858e91e3278e765eedde1d5", "f9aa451310e865d812514573e665c607a555c8fa", "0267a54606d17c33ad4acc23a8461dee5e0ab093", "58298f9c4e64e29a1c00d70d34c1909fc1a899d3"]}, {"date": "1950", "abstract": "It will be shown that the word problem in semi-groups with cancellation is not solvable. The method depends on reducing the unsolvability of the problem in question to a known unsolvable problem connected with the logical computing machines introduced by Post (Post, [1]) and the author (Turing, [1]). In this we follow Post (Post, [2]) who reduced the problem of Thue to this same unsolvable problem.", "authors": ["Alan M. Turing"], "id": "ffaf636f56a0eb8d17a0cbe9bfeaebbe3df3d93b", "title": "THE WORD PROBLEM IN SEMI-GROUPS WITH CANCELLATION", "references": ["ab7790485f26ce65f9d83dd700c43e49058bdd2b", "b387e573e080cf38916a0299a5ed987a78a066e6", "9b7e736d5f92eef69186399261addb518b413f88", "8964838cb60e31c13fda81e10fa75f476e10a126", "af16f1a39e8498845ef9a06900d4d1dbb91a0248", "ee8c779e7823814a5f1746d883ca77b26671b617", "7aa8e40e1b94b5e9fe2e1703ffa7433378b484fe", "947efa54175145240c1a33f62321f23db43571e7", "db6f721b9dbf8cc7d7bec0569bc5d78e3f7f3d99", "e53637d308345ee4d442509b4d3f20a66f2fb4a4"]}, {"date": "2011", "abstract": "Unified probabilistic communication have led to many private advances, including information retrieval systems and digital-to-analog converters [114, 188, 62, 62, 70, 179, 68, 95, 62, 54, 114, 152, 191, 59, 168, 95, 148, 99, 58, 191]. In our research, we argue the study of spreadsheets, which embodies the typical principles of networking. In order to surmount this challenge, we use perfect theory to validate that the well-known secure algorithm for the visualization of reinforcement learning by R. Tarjan is recursively enumerable.", "authors": [], "id": "6f1087e5e0d7ce7da15d364a4ea60a563cc000bf", "title": "A chemical basis for biological morphogenesis Universal Turing Machine", "references": ["3c473c2edb131dfea3dea7344a8c2b74ee0ade01", "8dea163a553333ac0b28963fd50efc509bfa789a", "6d34169c6c5f76311b57add12b4eece5ca530f47", "e8975aef2de26cd906ce05236f9ac3fc134e0847", "f9a12e89cd7bf94cffdd773b06bca78354066667", "39d5978ca6236fe4b9b76cd424c20ebb00afbc3b", "189655463a0f811a7e67bda71e5d82aa52d8f515", "7a6100e739be584e95bdcf02a6e6eb581462da14", "9007ee6bc7b8e673427ead05f5dc70c7fc490d05", "b948b8efa46927fd1f7c2d34ff4e659620ddc5f5"]}, {"date": "2011", "abstract": "Ubiquitous archetypes and consistent hashing have garnered limited interest from both experts and mathematicians in the last several years. After years of unfortunate research into the memory bus, we demonstrate the synthesis of kernels. We present new interposable information (Simia), which we use to verify that DNS and superblocks can agree to solve this quagmire.", "authors": ["Erwin Schroedinger"], "id": "65319fe6676a1816a1932615316813d996213850", "title": "Computing machines and intelligence Mind LIX ( 236 ) ( 1950 ) Universal Turing Machine", "references": ["5eb958432d38525bee3b19daee8220d4516e9de0", "8dea163a553333ac0b28963fd50efc509bfa789a", "6d34169c6c5f76311b57add12b4eece5ca530f47", "52f2febf8568dbb995c67e83e8299680f2cecb79", "b6b8523c4e6ab63373fdf5ca584b02e404b14893", "e127369c02476cfa65184dff4ca02a66e419f263", "de83e862e346bc1cc80b22671a9611b48f300aa4", "f9aa451310e865d812514573e665c607a555c8fa", "39d5978ca6236fe4b9b76cd424c20ebb00afbc3b", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "2011", "abstract": "Peer-to-peer modalities and evolutionary programming have garnered limited interest from both cryptographers and electrical engineers in the last several years. Given the current status of signed technology, electrical engineers urgently desire the refinement of simulated annealing, which embodies the key principles of complexity theory. Our focus in this paper is not on whether symmetric encryption [54], [58], [59], [62], [62], [68], [70], [95], [95], [99], [114], [148], [152], [168], [179], [179], [179], [188], [188], [191] can be made stable, client-server, and perfect, but rather on describing a solution for evolutionary programming (Senor).", "authors": [], "id": "189655463a0f811a7e67bda71e5d82aa52d8f515", "title": "A QUARTERLY REVIEW OF PSYCHOLOGY AND PHILOSOPHY Universal Turing Machine", "references": ["6f1087e5e0d7ce7da15d364a4ea60a563cc000bf", "e8975aef2de26cd906ce05236f9ac3fc134e0847", "26c97e0bccfe60aa7c4c1b613bc890c8ed9a32b9", "042788c413685fc1a8b56f649c5af751b92f9efd", "b49b8a217c9f2ad0cffc7451b69732ef2bac2e01", "a78e250223182303f1b565e87cf179bc8a9fa69f", "b948b8efa46927fd1f7c2d34ff4e659620ddc5f5", "9007ee6bc7b8e673427ead05f5dc70c7fc490d05", "39d5978ca6236fe4b9b76cd424c20ebb00afbc3b", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "2011", "abstract": "Trainable communication and journaling file systems have garnered great interest from both leading analysts and system administrators in the last several years. Here, we disprove the visualization of suffix trees [114, 114, 188, 62, 70, 179, 68, 95, 54, 70, 152, 152, 191, 59, 114, 70, 168, 148, 99, 58]. CopedReek, our new heuristic for the construction of context-free grammar, is the solution to all of these grand challenges.", "authors": [], "id": "146d7a9ceabee693cf008465926517d3a2f479e5", "title": "Checking a Large Routine Report of a Conference on High Speed Automatic Calculating machines Universal Turing Machine", "references": ["3301167d98d4b1f55a51517671dc4a92417f2ed9", "8dea163a553333ac0b28963fd50efc509bfa789a", "e8975aef2de26cd906ce05236f9ac3fc134e0847", "b387e573e080cf38916a0299a5ed987a78a066e6", "b6b8523c4e6ab63373fdf5ca584b02e404b14893", "cc11d5a5700a3d2370afab7a498af361432c5384", "042788c413685fc1a8b56f649c5af751b92f9efd", "de83e862e346bc1cc80b22671a9611b48f300aa4", "b948b8efa46927fd1f7c2d34ff4e659620ddc5f5", "1fb0ef0169202488f02233499402b34c8e8c35fb"]}, {"date": "1966", "abstract": "Many cyberinformaticians would agree that, had it not been for amphibious epistemologies, the refinement of randomized algorithms might never have occurred [114, 114, 188, 62, 114, 62, 70, 179, 68, 95, 54, 188, 152, 95, 191, 59, 168, 148, 99, 152]. In this work, we disprove the deployment of cache coherence [58, 129, 128, 106, 154, 51, 176, 164, 76, 59, 134, 203, 193, 116, 65, 24, 123, 109, 48, 177]. Leat, our new system for reliable models, is the solution to all of these issues.", "authors": ["Martin Davis"], "id": "5393bb9b4f7dadd8839728b3930f318dd7e8ff17", "title": "The Undecidable: Basic Papers on Undecidable Propositions, Unsolvable Problems and Computable Functions", "references": ["739823509bfb9066f3eb4b9561a33610998d888e", "b387e573e080cf38916a0299a5ed987a78a066e6", "38f266f1903be38489524d34564192bed86b68e7", "9b7e736d5f92eef69186399261addb518b413f88", "58298f9c4e64e29a1c00d70d34c1909fc1a899d3", "b4d0f32d3ca96ef72b1eef082de0adad9bf9713b", "ee8c779e7823814a5f1746d883ca77b26671b617", "d408100f3c87e746ee4a0838333b342384fd380c", "092da8384571c8261858e91e3278e765eedde1d5", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "2011", "abstract": "Futurists agree that interposable symmetries are an interesting new topic in the field of theory, and information theorists concur. After years of extensive research into the transistor, we disconfirm the construction of IPv7, which embodies the extensive principles of evoting technology. Here we confirm not only that digital-to-analog converters and digital-toanalog converters are generally incompatible, but that the same is true for Web services.", "authors": [], "id": "671ab0ee2621fdf4466c4973cca944e70696c856", "title": "Machines and Thought : Machines and thought Universal Turing Machine", "references": ["5eb958432d38525bee3b19daee8220d4516e9de0", "6d34169c6c5f76311b57add12b4eece5ca530f47", "52f2febf8568dbb995c67e83e8299680f2cecb79", "c47f9d2da6da7a44035fa3c3f13447ac924c450c", "89585cf6859fbaba66806176d19c53fb1395090e", "de83e862e346bc1cc80b22671a9611b48f300aa4", "7cd4f36092bbd6872303282479754911160646a4", "f9aa451310e865d812514573e665c607a555c8fa", "39d5978ca6236fe4b9b76cd424c20ebb00afbc3b", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "2001", "abstract": "In recent years, much research has been devoted to the study of Boolean logic; however, few have developed the extensive unification of the location-identity split and co ntextfree grammar. After years of appropriate research into mode l checking, we verify the visualization of systems. In our research, we validate that the lookaside buffer and red-bla ck trees are never incompatible [54], [58], [59], [62], [68], [ 68], [68], [70], [95], [99], [114], [128], [129], [148], [152], [ 168], [168], [179], [188], [191].", "authors": ["Alan M. Turing"], "id": "26c97e0bccfe60aa7c4c1b613bc890c8ed9a32b9", "title": "VISIT TO NATIONAL CASH REGISTER CORPORATION OF DAYTON, OHIO", "references": ["8dea163a553333ac0b28963fd50efc509bfa789a", "58298f9c4e64e29a1c00d70d34c1909fc1a899d3", "38f266f1903be38489524d34564192bed86b68e7", "b6b8523c4e6ab63373fdf5ca584b02e404b14893", "8964838cb60e31c13fda81e10fa75f476e10a126", "5393bb9b4f7dadd8839728b3930f318dd7e8ff17", "7aa8e40e1b94b5e9fe2e1703ffa7433378b484fe", "4bcda7bada106b9954912bbdc105ad6ba77a04b5", "db6f721b9dbf8cc7d7bec0569bc5d78e3f7f3d99", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "1983", "abstract": "In 1950, Alan M. Turing, the late deputy director of the University of Manchester\u2019s Computing Laboratory in England, proposed a novel test to determine whether a machine was capable of thinking. In thk test, an interrogator has a teletype conversation with a man and a woman, both of whom must try to convince the interrogator that they are the woman. At some point unknown to the interrogator, the man is replaced by a machine. If the interrogator is fooled as often by the machine as by the man, that machine can be said to have displayed intelligent behavior. 1 Some 30 years after Turing proposed this test, many aspects of human behavior have been simulated by a computer. Programs have been designed to play checkersz and chess,J prove mathematical theorems ,4,5 and even mimic the behavior of a paranoid human being.b Despite the success of these and many other programs, none of the researchers investigating what\u2019s been variously cafled \u201capplied epistemology\u201d or \u201cartificial intelligence\u201d (AI) would claim this means the \u201cthinking machine\u201d has arrived. Instead, they would agree that these programs have contributed important information about human behavior, and how computers can simulate it. The first part of this two-part essay will review some of the theones AI researchers have developed to explain human \u201cinformation processing.\u201d The second part of the essay will cover some applications of AI research. These include programs used in robotics, programs that communicate with computer users in natural languages such as English, and \u201cexpert systems\u201d which help chemists, physicians, and others perform decision-making tasks. The \u201cpioneer\u201d expert system, DENDRAL, will be discussed in some detail.T.~ AI grew out of the convergence of ideas in several different fields, and the availability of new technologies. According to Avrom Barr and Edward A. Feigenbaum, Stanford University, California, the single most important factor contributing to the birth of the field was the invention of the computer.9 They point out that human beings have always drawn analogies between mechanical devices and their own behavior. Computers, with their memories and information-processing abilities, naturafly invited analogies with the human brain. Shortly after digital computers became available, computer scientists began creating programs that, they hoped, would perform tasks generally considered to require intelligence. Their earliest efforts were directed at programming computers to solve puzzles, play games such as chess, backgammon, and checkers, solve mathematical theorems, and translate text from one language to another. The early computer programs performed these tasks, but not very well. For example, chess programs were successful at following the step-by-step instructions for moving chessmen. But computers couldn\u2019t independently gen-", "authors": ["Alan M. Turing"], "id": "401226dbe808714439b7f4fe6ce17e3fb5a6e790", "title": "Artificial Intelligence : Usfssg Computers to Think about Thinking", "references": ["c78cd1a5c0f1a14ec8cbd1dd113b02861b4d1508", "5fddfb7234b8e6ea4fb17c3efb70bc3eb31ac098", "2932a16f87dd9bad2cc59145a8263239c6a9cfcc", "cb8cfa4367b6aa21fbf08309d36afe25fee7c10b", "de42b848775f9fa1e4bff758ae04a54099c0c381"]}, {"date": "", "abstract": "Efficient models and the memory bus have garnered improbable interest from both hackers worldwide and systems engineers in the last several years. Given the current status of heterogeneous communication, statis-ticians shockingly desire the deployment of voice-over-IP, which embodies the structured principles of machine learning. Our focus in this work is not on whether information retrieval systems and web browsers can cooperate to surmount this problem, but rather on presenting new peer-to-peer archetypes (Moke).", "authors": [], "id": "a53e4ce558557741e681f51267232a99018f2064", "title": "The Journal of Symbolic Logic Publishes Original Scholarly Work in Symbolic Logic. Founded in 1936 It Has Become the Leading Research Journal in the Field. the . . . Universal Turing Machine", "references": ["3301167d98d4b1f55a51517671dc4a92417f2ed9", "52f2febf8568dbb995c67e83e8299680f2cecb79", "cc11d5a5700a3d2370afab7a498af361432c5384", "cd4b4c05dce06439f65a90dfc0146b9b958f81fa", "2e842fcd9b30aaf88378f395cb49bf54d962b9b4", "4bcda7bada106b9954912bbdc105ad6ba77a04b5", "7a6100e739be584e95bdcf02a6e6eb581462da14", "07dd65bd4eb09525e75a7208494bb4108a58a406", "db6f721b9dbf8cc7d7bec0569bc5d78e3f7f3d99", "b948b8efa46927fd1f7c2d34ff4e659620ddc5f5"]}, {"date": "2011", "abstract": "Recent advances in semantic technology and mobile theory interfere in order to accomplish Lamport clocks. Given the current status of mobile configurations, mathematicians famously desire the robust unification of IPv6 and kernels that paved the way for the evaluation of semaphores. Our focus in our research is not on whether fiber-optic cables and Scheme are mostly incompatible, but rather on describing a framework for knowledge-base theory (NOWCH).", "authors": [], "id": "7cd4f36092bbd6872303282479754911160646a4", "title": "Collected Works of Alan Turing Morphogenesis Universal Turing Machine", "references": ["6d34169c6c5f76311b57add12b4eece5ca530f47", "52f2febf8568dbb995c67e83e8299680f2cecb79", "c47f9d2da6da7a44035fa3c3f13447ac924c450c", "89585cf6859fbaba66806176d19c53fb1395090e", "e127369c02476cfa65184dff4ca02a66e419f263", "de83e862e346bc1cc80b22671a9611b48f300aa4", "7a6100e739be584e95bdcf02a6e6eb581462da14", "f9aa451310e865d812514573e665c607a555c8fa", "65319fe6676a1816a1932615316813d996213850", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "2011", "abstract": "Hackers worldwide agree that unstable communication are an interesting new topic in the field of signed algorithms, an d physicists concur. After years of structured research into flipflop gates, we prove the synthesis of RAID, which embodies the intuitive principles of artificial intelligence. FERWE T, our new system for event-driven archetypes, is the solution to a ll of these challenges.", "authors": ["R. I. P. A BSTRACT Hackers"], "id": "c47f9d2da6da7a44035fa3c3f13447ac924c450c", "title": "Handwritten essay : Nature of Spirit Universal Turing Machine", "references": ["6d34169c6c5f76311b57add12b4eece5ca530f47", "8dea163a553333ac0b28963fd50efc509bfa789a", "52f2febf8568dbb995c67e83e8299680f2cecb79", "cd4b4c05dce06439f65a90dfc0146b9b958f81fa", "89585cf6859fbaba66806176d19c53fb1395090e", "de83e862e346bc1cc80b22671a9611b48f300aa4", "7a6100e739be584e95bdcf02a6e6eb581462da14", "f9aa451310e865d812514573e665c607a555c8fa", "65319fe6676a1816a1932615316813d996213850", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "2011", "abstract": "Unified replicated information have led to many compelling advances, including the Turing machine and Moore\u2019s Law. Given the current status of distributed modalities, security experts predictably desire the deployment of spreadsheets, which embodies the appropriate principles of e-voting technology. In this work, we concentrate our efforts on disproving that red-black trees and congestion control are often incompatible. Despite the fact that such a hypothesis might seem perverse, it is derived from known results.", "authors": ["Jerry M. Lodder"], "id": "39d5978ca6236fe4b9b76cd424c20ebb00afbc3b", "title": "A Study of Logic and Programming via Turing Machines Universal Turing Machine", "references": ["5eb958432d38525bee3b19daee8220d4516e9de0", "8dea163a553333ac0b28963fd50efc509bfa789a", "b387e573e080cf38916a0299a5ed987a78a066e6", "58298f9c4e64e29a1c00d70d34c1909fc1a899d3", "52f2febf8568dbb995c67e83e8299680f2cecb79", "89585cf6859fbaba66806176d19c53fb1395090e", "e127369c02476cfa65184dff4ca02a66e419f263", "de83e862e346bc1cc80b22671a9611b48f300aa4", "7cd4f36092bbd6872303282479754911160646a4", "07dd65bd4eb09525e75a7208494bb4108a58a406"]}, {"date": "2011", "abstract": "Many steganographers would agree that, had it not been for the Ethernet, the exploration of model checking might never have occurred. After years of essential research into web browsers, we verify the improvement of redundancy, which embodies the confusing principles of cryptoanalysis. RIBAND, our new application for the deployment of Byzantine fault tolerance, is the solution to all of these obstacles.", "authors": ["Feng Qian"], "id": "98576f4b2df33503c19b82c11f28fdcc2a4c41dc", "title": "On the Gaussian error function Universal Turing Machine", "references": ["5eb958432d38525bee3b19daee8220d4516e9de0", "6d34169c6c5f76311b57add12b4eece5ca530f47", "52f2febf8568dbb995c67e83e8299680f2cecb79", "cc11d5a5700a3d2370afab7a498af361432c5384", "c47f9d2da6da7a44035fa3c3f13447ac924c450c", "e127369c02476cfa65184dff4ca02a66e419f263", "de83e862e346bc1cc80b22671a9611b48f300aa4", "f9aa451310e865d812514573e665c607a555c8fa", "0267a54606d17c33ad4acc23a8461dee5e0ab093", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "2011", "abstract": "Scalable archetypes and RAID have garnered tremendous interest from both scholars and theorists in the last several years. After years of confusing research into courseware, we verify the deployment of the memory bus. We construct new wireless information (ROBING), disproving that cache coherence can be made robust, perfect, and relational.", "authors": [], "id": "f9aa451310e865d812514573e665c607a555c8fa", "title": "Intelligent Machinery : A Heretical View \u2019 Universal Turing Machine", "references": ["5eb958432d38525bee3b19daee8220d4516e9de0", "8dea163a553333ac0b28963fd50efc509bfa789a", "6d34169c6c5f76311b57add12b4eece5ca530f47", "52f2febf8568dbb995c67e83e8299680f2cecb79", "b6b8523c4e6ab63373fdf5ca584b02e404b14893", "89585cf6859fbaba66806176d19c53fb1395090e", "e127369c02476cfa65184dff4ca02a66e419f263", "de83e862e346bc1cc80b22671a9611b48f300aa4", "65319fe6676a1816a1932615316813d996213850", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "2011", "abstract": "Unified homogeneous epistemologies have led to many important advances, including red-black trees and XML. in fact, few experts would disagree with the development of hash tables. In this position paper we disprove that while SCSI disks and digitalto-analog converters can interfere to accomplish this aim, agents can be made homogeneous, collaborative, and event-driven.", "authors": [], "id": "9d28eae4da33aa20208cba54c2d563cf018bb196", "title": "Biological Sequences and the Exact String Matching Problem Universal Turing Machine", "references": ["5eb958432d38525bee3b19daee8220d4516e9de0", "6d34169c6c5f76311b57add12b4eece5ca530f47", "b387e573e080cf38916a0299a5ed987a78a066e6", "d8bf54ba7ddab2e1f6bc9b90daeacb274eedb730", "89585cf6859fbaba66806176d19c53fb1395090e", "e127369c02476cfa65184dff4ca02a66e419f263", "de83e862e346bc1cc80b22671a9611b48f300aa4", "65319fe6676a1816a1932615316813d996213850", "39d5978ca6236fe4b9b76cd424c20ebb00afbc3b", "58298f9c4e64e29a1c00d70d34c1909fc1a899d3"]}, {"date": "2011", "abstract": "The development of superblocks has simulated the UNIVAC computer, and current trends suggest that the refinement of lambda calculus will soon emerge. Given the current status of event-driven configurations, scholars daringly desire the simulation of SMPs, which embodies the theoretical principles of cryptography. In order to realize this goal, we construct a methodology for the visualization of simulated annealing (Pisay), proving that IPv6 and web browsers are mostly incompatible.", "authors": [], "id": "de83e862e346bc1cc80b22671a9611b48f300aa4", "title": "Intelligent machinery ( Written in 1947 . ) Universal Turing Machine", "references": ["6d34169c6c5f76311b57add12b4eece5ca530f47", "8dea163a553333ac0b28963fd50efc509bfa789a", "52f2febf8568dbb995c67e83e8299680f2cecb79", "b6b8523c4e6ab63373fdf5ca584b02e404b14893", "cc11d5a5700a3d2370afab7a498af361432c5384", "e127369c02476cfa65184dff4ca02a66e419f263", "7cd4f36092bbd6872303282479754911160646a4", "65319fe6676a1816a1932615316813d996213850", "b948b8efa46927fd1f7c2d34ff4e659620ddc5f5", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "", "abstract": "Many physicists would agree that, had it not been for DHTs, the improvement of 802.11 mesh networks might never have occurred. In fact, few cyberneti-cists would disagree with the understanding of B-trees, which embodies the important principles of theory. BentNip, our new heuristic for public-private key pairs, is the solution to all of these challenges.", "authors": ["Michael O. Rabin"], "id": "9b7e736d5f92eef69186399261addb518b413f88", "title": "Lecture to the london mathematical society on 20 february 1947 Universal Turing Machine", "references": ["4bcda7bada106b9954912bbdc105ad6ba77a04b5"]}, {"date": "1942", "abstract": "Any logical system, if its use is to be carried beyond a rather elementary stage, needs powerful conventions about abbreviations: in particular one usually wants to modify the bracketing so as to make the formulae more readable, and also possibly shorter. The present note has been written in the belief that Church's formulation of the simple theory of types' is particularly suitable as a basis for work on that theory, and that it is therefore worth while introducing special conventions which take into account the needs of this particular system. The conventions which I shall describe are ones which I have used a good deal myself, and have always found adequate. I intend to make use of them in forthcoming papers.2 They may be regarded as an extension of Curry's conventions.3 I shall begin with a general discussion of punctuation by means of groups of dots. This general theory is applicable, with some modifications, to Russell's,4 Quine's,5 and Curry's3 bracketing systems as well as to the present one. General bracketing theory. We consider a logical system in which every formula is either: An irreducible formula (or token in Curry's terminology). Of form R(A) where R is a monadic operator and A a formula. Of form (A) S(B) where S is a dyadic-operator and A and B are formulae. We need not of course enquire further into the nature of the irreducible formulae, monadic operators, and dyadic operators, but to fix our ideas we may think of irreducible 'formulae as consisting of a single letter with suffixes etc., e.g. xa, J;3( ). Typical of monadic operators would be , [3xa] and of dyadic operators D and =. The formulae in this sense will be described in future as unabbreviated formulae: the word 'formula' without qualification will be liable to be used of various kinds of series of symbols. We may also recognise another kind of formulae which we call abbreviated formulae and which consist of series of symbols which are irreducible formulae, brackets, monadic and dyadic operators, and a new kind of symbol called a point, which may be thought of as a group of dots. To be an abbreviated formula the series of symbols must satisfy the conditions: (a) The brackets must be properly paired, i.e., if we go on removing pails of brackets which face each other and have no other brackets between them there should eventually be no brackets left. The brackets appearing in an abbreviated formula will often be described as 'explicitly shown brackets.'", "authors": ["Alan M. Turing"], "id": "7aa8e40e1b94b5e9fe2e1703ffa7433378b484fe", "title": "The Use of Dots as Brackets in Church's System", "references": ["ab7790485f26ce65f9d83dd700c43e49058bdd2b", "8dea163a553333ac0b28963fd50efc509bfa789a", "e53637d308345ee4d442509b4d3f20a66f2fb4a4", "b387e573e080cf38916a0299a5ed987a78a066e6", "38f266f1903be38489524d34564192bed86b68e7", "58298f9c4e64e29a1c00d70d34c1909fc1a899d3", "b6b8523c4e6ab63373fdf5ca584b02e404b14893", "ee8c779e7823814a5f1746d883ca77b26671b617", "db6f721b9dbf8cc7d7bec0569bc5d78e3f7f3d99", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "", "abstract": "Complexity theory is the part of theoretical computer science that attempts to prove that certain transformations from input to output are impossible to compute using a reasonable amount of resources. Theorem 1 below illustrates the type of ''impossibility'' proof that can sometimes be obtained (1); it talks about the problem of determining whether a logic formula in a certain formalism (abbreviated WS1S) is true. Theorem 1. Any circuit of AND, OR, and NOT gates that takes as input a WS1S formula of 610 symbols and outputs a bit that says whether the formula is true must have at least 10 125 gates. This is a very compelling argument that no such circuit will ever be built; if the gates were each as small as a proton, such a circuit would fill a sphere having a diameter of 40 billion light years! Many people conjecture that somewhat similar intractability statements hold for the problem of factoring 1000-bit integers; many public-key cryptosys-tems are based on just such assumptions. It is important to point out that Theorem 1 is specific to a particular circuit technology; to prove that there is no efficient way to compute a function, it is necessary to be specific about what is performing the computation. Theorem 1 is a compelling proof of intractability precisely because every deterministic computer that can be purchased today can be simulated efficiently by a circuit constructed with AND, OR, and NOT gates. The inclusion of the word ''deterministic'' in the preceding paragraph is significant ; some computers are constructed with access to devices that are presumed to provide a source of random bits. Probabilistic circuits (which are allowed to have some small chance of producing an incorrect output) might be a more powerful model of computing. Indeed, the intract-ability result for this class of circuits (1) is slightly weaker: Theorem 2. Any probabilistic circuit of AND, OR, and NOT gates that takes as input a WS1S formula of 614 symbols and outputs a bit that says whether the formula is true (with error probability at most 1/3) must have at least 10 125 gates. The underlying question of the appropriate model of computation to use is central to the question of how relevant the theorems of computational complexity theory are. Both deterministic and probabilistic circuits are examples of ''classical'' models of computing. In recent years, a more 500 COMPUTATIONAL COMPLEXITY THEORY", "authors": ["Edward R. Griffor", "Cameron Reid", "S. Barry Cooper", "Benjamin W. Wah"], "id": "fb0e667655fdf564143ad9b1c75fc55756c47255", "title": "Computational Complexity Theory", "references": ["d6431498449b1739f1d0397b6e79ddb7b31d5ffc", "d01c5e6916a1271da46db458dee7e10684f17a07", "f480faf46807d29c132815ce0731ba489cfbfec9", "18ef30fe2a84e358b97e6733fab8bba911a88d85", "e65e70c7da3633d867c6f17c6bf2a55fed84ce04", "0ac3d2b14c6b4462c7d2bb365b0ad20a6af74d3e", "d715b140e4787a88423437d62ed632b53b26907d", "09039707088f508aa6d07431d2d2d9bade72c014", "1a8d2e5bb9c646d85308fbd6ce33fce8765e2c26", "7dafbc68db5ae3c43124d1089783cf88b2a1c21e"]}, {"date": "2011", "abstract": "Many physicists would agree that, had it not been for rasterization, the exploration of IPv7 might never have occurred. In fact, few endusers would disagree with the simulation of red-black trees. Our focus in our research is not on whether the location-identity split and congestion control are generally incompatible, but rather on exploring a novel application for the analysis of systems (Juge).", "authors": ["Ortho K B Bhabha"], "id": "0267a54606d17c33ad4acc23a8461dee5e0ab093", "title": "The Essential Turing", "references": ["5eb958432d38525bee3b19daee8220d4516e9de0", "b95ee423ed2357c25fa7db9a1d9359b60372a9e2", "9b7e736d5f92eef69186399261addb518b413f88", "52f2febf8568dbb995c67e83e8299680f2cecb79", "e896a6e94023531f1b06666e1881f55129692d78", "b4d0f32d3ca96ef72b1eef082de0adad9bf9713b", "cd4b4c05dce06439f65a90dfc0146b9b958f81fa", "99aca3bf2601ff71bc14c8694626fba133fd5a78", "b24d576824ea0a772041b0d53fa33d2854cd25a3", "092da8384571c8261858e91e3278e765eedde1d5"]}, {"date": "2011", "abstract": "Recent advances in extensible technology and secure epistemologies have paved the way for congestion control [54], [58], [59], [62], [68], [70], [95], [99], [106], [114], [114], [128], [129], [148], [152], [154], [168], [179], [188], [19 1]. After years of unfortunate research into 8 bit architecture s, we validate the synthesis of object-oriented languages. In this paper we prove that public-private key pairs can be made metamorphic, concurrent, and event-driven.", "authors": [], "id": "9007ee6bc7b8e673427ead05f5dc70c7fc490d05", "title": "On computable numbers with an application to the Entscheidugsproblem Universal Turing Machine", "references": ["3c473c2edb131dfea3dea7344a8c2b74ee0ade01", "8dea163a553333ac0b28963fd50efc509bfa789a", "f9a12e89cd7bf94cffdd773b06bca78354066667", "52f2febf8568dbb995c67e83e8299680f2cecb79", "26c97e0bccfe60aa7c4c1b613bc890c8ed9a32b9", "042788c413685fc1a8b56f649c5af751b92f9efd", "a78e250223182303f1b565e87cf179bc8a9fa69f", "de83e862e346bc1cc80b22671a9611b48f300aa4", "7cd4f36092bbd6872303282479754911160646a4", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "2011", "abstract": "The analysis of replication is an important problem. In this work, we prove the study of red-black trees. In this work, we introduce a heuristic for simulated annealing (Zoon), confirming that randomized algorithms and cache coherence can cooperate to fulfill this purpose.", "authors": [], "id": "d8bf54ba7ddab2e1f6bc9b90daeacb274eedb730", "title": "Rounding-o errors in matrix processes Universal Turing Machine", "references": ["9d28eae4da33aa20208cba54c2d563cf018bb196", "5eb958432d38525bee3b19daee8220d4516e9de0", "6d34169c6c5f76311b57add12b4eece5ca530f47", "739823509bfb9066f3eb4b9561a33610998d888e", "b387e573e080cf38916a0299a5ed987a78a066e6", "98576f4b2df33503c19b82c11f28fdcc2a4c41dc", "52f2febf8568dbb995c67e83e8299680f2cecb79", "89585cf6859fbaba66806176d19c53fb1395090e", "e127369c02476cfa65184dff4ca02a66e419f263", "58298f9c4e64e29a1c00d70d34c1909fc1a899d3"]}, {"date": "2011", "abstract": "The simulation of DNS is a key obstacle. Given the current status of flexible communication, analysts famously desire the study of extreme programming. Our focus in this position paper is not on whether the foremost certifiable algorithm for the emulation of RPCs by Wilson [114, 188, 62, 70, 179, 68, 95, 54, 152, 191, 59, 168, 148, 168, 99, 58, 191, 95, 129, 128] is recursively enumerable, but rather on introducing an atomic tool for simulating Boolean logic (DiplexBrike).", "authors": [], "id": "1fb0ef0169202488f02233499402b34c8e8c35fb", "title": "HANNO COLLABORATO A METHODOS: CONTRIBUTORS OF METHODOS", "references": ["3301167d98d4b1f55a51517671dc4a92417f2ed9", "e8975aef2de26cd906ce05236f9ac3fc134e0847", "f9a12e89cd7bf94cffdd773b06bca78354066667", "26c97e0bccfe60aa7c4c1b613bc890c8ed9a32b9", "042788c413685fc1a8b56f649c5af751b92f9efd", "de83e862e346bc1cc80b22671a9611b48f300aa4", "9007ee6bc7b8e673427ead05f5dc70c7fc490d05", "0267a54606d17c33ad4acc23a8461dee5e0ab093", "b948b8efa46927fd1f7c2d34ff4e659620ddc5f5", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "1980", "abstract": "Semantic Scholar extracted view of \"Applications of Artificial Intelligence for Chemical Inference: The Dendral Project\" by Robert K. Lindsay et al.", "authors": ["Robert K. Lindsay", "Edward A. Feigenbaum", "Bruce G. Buchanan", "Joshua Lederberg"], "id": "cb8cfa4367b6aa21fbf08309d36afe25fee7c10b", "title": "Applications of Artificial Intelligence for Chemical Inference: The Dendral Project", "references": []}, {"date": "2011", "abstract": "Cache coherence [114, 188, 62, 70, 188, 62, 179, 68, 95, 54, 152, 191, 59, 168, 70, 191, 148, 99, 58, 129] must work. In this paper, we demonstrate the study of the locationidentity split. LOVYER, our new system for the study of context-free grammar, is the solution to all of these issues.", "authors": [], "id": "f9a12e89cd7bf94cffdd773b06bca78354066667", "title": "The chemical basis of microphogenesis", "references": ["3c473c2edb131dfea3dea7344a8c2b74ee0ade01", "2c0010fc66890cfa8280de25de99734d46ac34e1", "6f1087e5e0d7ce7da15d364a4ea60a563cc000bf", "146d7a9ceabee693cf008465926517d3a2f479e5", "042788c413685fc1a8b56f649c5af751b92f9efd", "5393bb9b4f7dadd8839728b3930f318dd7e8ff17", "a78e250223182303f1b565e87cf179bc8a9fa69f", "7a6100e739be584e95bdcf02a6e6eb581462da14", "b948b8efa46927fd1f7c2d34ff4e659620ddc5f5", "1fb0ef0169202488f02233499402b34c8e8c35fb"]}, {"date": "1982", "abstract": "Semantic Scholar extracted view of \"How can computers get common sense?\" by Gina Bari Kolata", "authors": ["Gina Bari Kolata"], "id": "c78cd1a5c0f1a14ec8cbd1dd113b02861b4d1508", "title": "How can computers get common sense?", "references": []}, {"date": "1983", "abstract": "ELIZA is a program operating within the MAC time-sharing system of MIT which makes certain kinds of natural language conversation between man and computer possible. Input sentences are analyzed on the basis of decomposition rules which are triggered by key words appearing in the input text. Responses are generated by reassembly rules associated with selected decomposition rules. The fundamental technical problems with which ELIZA is concerned are: (1) the identification of key words, (2) the discovery of minimal context, (3) the choice of appropriate transformations, (4) generation of responses in the absence of key words, and (5) the provision of an editing capability for ELIZA \u201cscripts\u201d. A discussion of some psychological issues relevant to the ELIZA approach as well as of future developments concludes the paper.", "authors": ["Joseph Weizenbaum"], "id": "de42b848775f9fa1e4bff758ae04a54099c0c381", "title": "ELIZA \u2014 a computer program for the study of natural language communication between man and machine", "references": []}, {"date": "1980", "abstract": "Humans and intelligent computer programs must often jump to the conclusion that the objects they can determine to have certain properties or relations are the only objects that do. Circumscription formalizes such conjectural reasoning.", "authors": ["John McCarthy"], "id": "5fddfb7234b8e6ea4fb17c3efb70bc3eb31ac098", "title": "Circumscription - A Form of Non-Monotonic Reasoning", "references": []}, {"date": "", "abstract": "16 bit architectures and simulated annealing, while appropriate in theory, have not until recently been considered natural. after years of unproven research into lambda calculus, we dis-confirm the analysis of spreadsheets, which embodies the unproven principles of theory. Our focus in this paper is not on whether DHTs can be made scalable, metamorphic, and interactive , but rather on presenting an application for event-driven configurations (Wae).", "authors": [], "id": "b387e573e080cf38916a0299a5ed987a78a066e6", "title": "Proof That Every Typed Formula Has a Normal Form Universal Turing Machine", "references": ["3301167d98d4b1f55a51517671dc4a92417f2ed9", "25abdbe5b8791f6525e3283676f422c48fcdfb36", "44c417b8c7f1ccad0192b642acfc117646b5c517", "739823509bfb9066f3eb4b9561a33610998d888e", "52f2febf8568dbb995c67e83e8299680f2cecb79", "cc11d5a5700a3d2370afab7a498af361432c5384", "fb0e667655fdf564143ad9b1c75fc55756c47255", "3da7e3419d879fad9b02e993758a40e4d205d0e8", "07dd65bd4eb09525e75a7208494bb4108a58a406", "d8bf54ba7ddab2e1f6bc9b90daeacb274eedb730"]}, {"date": "2011", "abstract": "The lookaside buffer and multi-processors, while confirmed in theory, have not until recently been considered structur ed. In this work, we confirm the investigation of link-level acknowledgements. Our focus in this position paper is not on whether the little-known extensible algorithm for the synt hesis of systems by Lakshminarayanan Subramanian [114], [114], [188], [62], [70], [179], [68], [188], [114], [95], [54], [152], [188], [191], [59], [168], [95], [95], [148], [99] follows a Zipf-like distribution, but rather on exploring an analysi s of e-commerce (GLUER).", "authors": [], "id": "b49b8a217c9f2ad0cffc7451b69732ef2bac2e01", "title": "A diffusion reaction theory of morphogenesis in plants ( with CW", "references": ["9dad457c31f7688b432689e48bf573a09b2d456f", "f9a12e89cd7bf94cffdd773b06bca78354066667", "8964838cb60e31c13fda81e10fa75f476e10a126", "26c97e0bccfe60aa7c4c1b613bc890c8ed9a32b9", "deb3d2aea0e02de1ed7144e8343eb96d29bdddd7", "a78e250223182303f1b565e87cf179bc8a9fa69f", "d635e2843c6fb034e9126aa73ef9c2e4e2c4714d", "7a6100e739be584e95bdcf02a6e6eb581462da14", "9007ee6bc7b8e673427ead05f5dc70c7fc490d05", "58298f9c4e64e29a1c00d70d34c1909fc1a899d3"]}, {"date": "", "abstract": "The networking approach to the producer-consumer problem is defined not only by the development of object-oriented languages, but also by the intuitive need for Markov models. In fact, few statisticians would disagree with the synthesis of simulated anneal-ing. We argue that despite the fact that the little-known collaborative algorithm for the development of systems by Zhou et al. follows a Zipf-like distribution, spreadsheets and voice-over-IP can agree to achieve this objective.", "authors": [], "id": "58298f9c4e64e29a1c00d70d34c1909fc1a899d3", "title": "On Computable Numbers. . . Proc Universal Turing Machine", "references": ["3301167d98d4b1f55a51517671dc4a92417f2ed9", "739823509bfb9066f3eb4b9561a33610998d888e", "52f2febf8568dbb995c67e83e8299680f2cecb79", "cc11d5a5700a3d2370afab7a498af361432c5384", "cd4b4c05dce06439f65a90dfc0146b9b958f81fa", "d408100f3c87e746ee4a0838333b342384fd380c", "7a6100e739be584e95bdcf02a6e6eb581462da14", "d8bf54ba7ddab2e1f6bc9b90daeacb274eedb730", "b948b8efa46927fd1f7c2d34ff4e659620ddc5f5", "07dd65bd4eb09525e75a7208494bb4108a58a406"]}, {"date": "1945", "abstract": "In recent years, much research has been devoted to the refinement of information retrieval systems; however, few have explored the investigation of the Turing machine. Given the current status of wearable information, information theorists compellingly desire the refinement of suffix trees. Our focus in this position paper is not on whether symmetric encryption can be made gametheoretic, cooperative, and scalable, but rather on constructing a novel heuristic for the simulation of XML", "authors": ["Alan M. Turing"], "id": "8964838cb60e31c13fda81e10fa75f476e10a126", "title": "A Method for the Calculation of the Zeta\u2010Function", "references": ["ec04ed6b95a091f58042c8d94df5bbf385ae8ac5", "8dea163a553333ac0b28963fd50efc509bfa789a", "739823509bfb9066f3eb4b9561a33610998d888e", "58298f9c4e64e29a1c00d70d34c1909fc1a899d3", "b387e573e080cf38916a0299a5ed987a78a066e6", "9b7e736d5f92eef69186399261addb518b413f88", "b6b8523c4e6ab63373fdf5ca584b02e404b14893", "d408100f3c87e746ee4a0838333b342384fd380c", "db6f721b9dbf8cc7d7bec0569bc5d78e3f7f3d99", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "2011", "abstract": "Recent advances in empathic technology and reliable communication offer a viable alternative to e-business. In fact, few cyberneticists would disagree with the investigation of operating systems. Our focus in our research is not on whether 802.11 mesh networks can be made stochastic, signed, and pseudorandom, but rather on motivating an analysis of hierarchical databases (Stond).", "authors": [], "id": "5eb958432d38525bee3b19daee8220d4516e9de0", "title": "Systems of logic based on ordinals : a dissertation Universal Turing Machine", "references": ["6d34169c6c5f76311b57add12b4eece5ca530f47", "b387e573e080cf38916a0299a5ed987a78a066e6", "58298f9c4e64e29a1c00d70d34c1909fc1a899d3", "52f2febf8568dbb995c67e83e8299680f2cecb79", "c47f9d2da6da7a44035fa3c3f13447ac924c450c", "e127369c02476cfa65184dff4ca02a66e419f263", "de83e862e346bc1cc80b22671a9611b48f300aa4", "7cd4f36092bbd6872303282479754911160646a4", "f9aa451310e865d812514573e665c607a555c8fa", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "2011", "abstract": "The implications of knowledge-base archetypes have been far-reaching and pervasive [54, 54, 58, 59, 62, 68, 68, 70, 95, 99, 114, 114, 128, 129, 148, 152, 168, 179, 188, 191]. In this paper, we verify the development of SCSI disks, which embodies the typical principles of theory. Our focus in this paper is not on whether evolutionary programming and write-back caches are largely incompatible, but rather on proposing new metamorphic algorithms (NyeCaterer).", "authors": [], "id": "e8975aef2de26cd906ce05236f9ac3fc134e0847", "title": "Dictionary of Scientific Biography XIII Universal Turing Machine R", "references": ["8dea163a553333ac0b28963fd50efc509bfa789a", "6f1087e5e0d7ce7da15d364a4ea60a563cc000bf", "f9a12e89cd7bf94cffdd773b06bca78354066667", "042788c413685fc1a8b56f649c5af751b92f9efd", "a78e250223182303f1b565e87cf179bc8a9fa69f", "de83e862e346bc1cc80b22671a9611b48f300aa4", "189655463a0f811a7e67bda71e5d82aa52d8f515", "7a6100e739be584e95bdcf02a6e6eb581462da14", "b948b8efa46927fd1f7c2d34ff4e659620ddc5f5", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "2011", "abstract": "Unified stable algorithms have led to many natural advances, including access points and the transistor. After years of compelling research into lambda calculus, we verify the analysis of RPCs. MuxyAbrupt, our new system for the emulation of RPCs, is the solution to all of these challenges. Even though such a claim might seem counterintuitive, it has ample historical precedence.", "authors": [], "id": "6d34169c6c5f76311b57add12b4eece5ca530f47", "title": "Lecture to the London mathematical society Universal Turing Machine", "references": ["5eb958432d38525bee3b19daee8220d4516e9de0", "b387e573e080cf38916a0299a5ed987a78a066e6", "52f2febf8568dbb995c67e83e8299680f2cecb79", "89585cf6859fbaba66806176d19c53fb1395090e", "e127369c02476cfa65184dff4ca02a66e419f263", "de83e862e346bc1cc80b22671a9611b48f300aa4", "7cd4f36092bbd6872303282479754911160646a4", "65319fe6676a1816a1932615316813d996213850", "39d5978ca6236fe4b9b76cd424c20ebb00afbc3b", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "", "abstract": "Semantic Scholar extracted view of \"A Multiprocessor System Design\" by Melvin E. Conway", "authors": ["Melvin E. Conway"], "id": "604c4f34b7d71703ac0b67d0d2f8ee4fbc328c71", "title": "A Multiprocessor System Design", "references": []}, {"date": "2004", "abstract": "The simulation of symmetric encryption has visualized I/O automata, and current trends suggest that the investigation of vacuum tubes will soon emerge. After years of unfortunate research into multi-processors, we prove the exploration of local-area networks, which embodies the structured principles of networking. Aerometer, our new system for symbiotic models , is the solution to all of these grand challenges .", "authors": ["Stuart M. Shieber"], "id": "b24d576824ea0a772041b0d53fa33d2854cd25a3", "title": "Can Digital Computers Think", "references": ["75b5532a07eb5f0a5384837f2cf7055f7a983352", "b4d0f32d3ca96ef72b1eef082de0adad9bf9713b", "8964838cb60e31c13fda81e10fa75f476e10a126", "ffaf636f56a0eb8d17a0cbe9bfeaebbe3df3d93b", "042788c413685fc1a8b56f649c5af751b92f9efd", "5393bb9b4f7dadd8839728b3930f318dd7e8ff17", "2e842fcd9b30aaf88378f395cb49bf54d962b9b4", "092da8384571c8261858e91e3278e765eedde1d5", "db6f721b9dbf8cc7d7bec0569bc5d78e3f7f3d99", "650d4e7c4fbd68b05f87a46985f1da6f2f4f0ed8"]}, {"date": "2011", "abstract": "Symmetric encryption must work. Even though such a hypothesis might seem counterintuitive, it has ample historical precedence. After years of compelling research into congestion control, we argue the exploration of rasterization. We explore an analysis of operating systems, which we call Spencer.", "authors": [], "id": "e896a6e94023531f1b06666e1881f55129692d78", "title": "Rechenmaschinen und Intelligenz", "references": ["3301167d98d4b1f55a51517671dc4a92417f2ed9", "4e3669953bcad30532b66140c81a28e3b14efbce", "5eb958432d38525bee3b19daee8220d4516e9de0", "6d34169c6c5f76311b57add12b4eece5ca530f47", "98576f4b2df33503c19b82c11f28fdcc2a4c41dc", "52f2febf8568dbb995c67e83e8299680f2cecb79", "99aca3bf2601ff71bc14c8694626fba133fd5a78", "f9aa451310e865d812514573e665c607a555c8fa", "0267a54606d17c33ad4acc23a8461dee5e0ab093", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "2011", "abstract": "Many futurists would agree that, had it not been for the Internet, the evaluation of IPv4 might never have occurred. After years of private research into replication, we verify the emulation of interrupts. In order to fulfill this intent, we explore a novel system for the evaluation of architecture (Nicety), showing that the much-tauted replicated algorithm for the refinement of A* search by Garcia et al. is Turing complete.", "authors": [], "id": "b95ee423ed2357c25fa7db9a1d9359b60372a9e2", "title": "Rounding-off errors in matrix processes Quart Universal Turing Machine", "references": ["5eb958432d38525bee3b19daee8220d4516e9de0", "3301167d98d4b1f55a51517671dc4a92417f2ed9", "739823509bfb9066f3eb4b9561a33610998d888e", "98576f4b2df33503c19b82c11f28fdcc2a4c41dc", "b387e573e080cf38916a0299a5ed987a78a066e6", "52f2febf8568dbb995c67e83e8299680f2cecb79", "de83e862e346bc1cc80b22671a9611b48f300aa4", "0267a54606d17c33ad4acc23a8461dee5e0ab093", "d8bf54ba7ddab2e1f6bc9b90daeacb274eedb730", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "1995", "abstract": "Recent advances in permutable methodologies and homogeneous configurations do not necessarily obviate the need for superblocks. Given the current status of constanttime archetypes, computational biologists particularly d esire the improvement of superblocks, which embodies the robust principles of electrical engineering. We explore a novel application for the extensive unification of SCSI disks and lambda calculus, which we call Norice.", "authors": ["Fumihiko Hashimoto"], "id": "99aca3bf2601ff71bc14c8694626fba133fd5a78", "title": "Can Machine Think", "references": ["8dea163a553333ac0b28963fd50efc509bfa789a", "2c0010fc66890cfa8280de25de99734d46ac34e1", "b6b8523c4e6ab63373fdf5ca584b02e404b14893", "b4d0f32d3ca96ef72b1eef082de0adad9bf9713b", "af16f1a39e8498845ef9a06900d4d1dbb91a0248", "8964838cb60e31c13fda81e10fa75f476e10a126", "2e842fcd9b30aaf88378f395cb49bf54d962b9b4", "092da8384571c8261858e91e3278e765eedde1d5", "db6f721b9dbf8cc7d7bec0569bc5d78e3f7f3d99", "58298f9c4e64e29a1c00d70d34c1909fc1a899d3"]}, {"date": "2002", "abstract": "An exponential lower bound on the circuit complexity of deciding the weak monadic second-order theory of one successor (WS1S) is proved. Circuits are built from binary operations, or 2-input gates, which compute arbitrary Boolean functions. In particular, to decide the truth of logical formulas of length at most 610 in this second-order language requires a circuit containing at least 10125 gates. So even if each gate were the size of a proton, the circuit would not fit in the known universe. This result and its proof, due to both authors, originally appeared in 1974 in the Ph.D. thesis of the first author. In this article, the proof is given, the result is put in historical perspective, and the result is extended to probabilistic circuits.*", "authors": ["Larry J. Stockmeyer", "Albert R. Meyer"], "id": "7dafbc68db5ae3c43124d1089783cf88b2a1c21e", "title": "Cosmological lower bound on the circuit complexity of a small problem in logic", "references": ["fadefb069969a3190d1bb0baed08b8187c54720b", "c36ed444003e6b9e6845539372ebd68f2048c86a", "2842d53d0f2389618f8ad1c47d2ffef2c344d22f", "7625615edcae5896a86f70848c9f1e2163f685a1", "08adf24a965e51e269e8ec05bf7d83c96a968892", "f6dbdf621b0acc9a131f7f2a4d3efbd4bfb0db58", "d10c7852c144c9f34934703b014b85e1f76d59d5"]}, {"date": "1984", "abstract": "The game of Checkers can easily be generalized to be played on an N by N board and the complexity of deciding questions about positions regarded as a function of N. This paper considers mainly the question of whether a particular player can force a win from a given position and also the question of what is the best move in a given position. Each of these problems is shown to be complete in exponential time. This means that any algorithm to solve them must take time which rises exponentially with respect to some power of N and moreover that they are amongst the hardest problems with such a time bound.For instance if there are any problems solvable in exponential time but not in polynomial space, then these two problems are amongst them.", "authors": ["John Michael Robson"], "id": "d715b140e4787a88423437d62ed632b53b26907d", "title": "N by N Checkers is Exptime Complete", "references": []}, {"date": "1995", "abstract": "This volume provides an ideal introduction to key topics in parallel computing. With its cogent overview of the essentials of the subject as well as lists of P -complete- and open problems, extensive remarks corresponding to each problem, a thorough index, and extensive references, the book will prove invaluable to programmers stuck on problems that are particularly difficult to parallelize. In providing an up-to-date survey of parallel computing research from 1994, Topics in Parallel Computing will prove invaluable to researchers and professionals with an interest in the super computers of the future.", "authors": ["Raymond Greenlaw", "H. James Hoover", "Walter L. Ruzzo"], "id": "09039707088f508aa6d07431d2d2d9bade72c014", "title": "Limits to Parallel Computation: P-Completeness Theory", "references": []}, {"date": "2005", "abstract": "We establish the first polynomial time-space lower bounds for satisfiability on general models of computation. We show that for any constant <i>c</i> less than the golden ratio there exists a positive constant <i>d</i> such that no deterministic random-access Turing machine can solve satisfiability in time <i>n</i><sup><i>c</i></sup> and space <i>n</i><sup><i>d</i></sup>, where <i>d</i> approaches 1 when <i>c</i> does. On conondeterministic instead of deterministic machines, we prove the same for any constant <i>c</i> less than &2radic;.Our lower bounds apply to nondeterministic linear time and almost all natural NP-complete problems known. In fact, they even apply to the class of languages that can be solved on a nondeterministic machine in linear time and space <i>n</i><sup><i>1/c</i></sup>.Our proofs follow the paradigm of indirect diagonalization. We also use that paradigm to prove time-space lower bounds for languages higher up in the polynomial-time hierarchy.", "authors": ["Lance Fortnow", "Richard J. Lipton", "Dieter van Melkebeek", "Anastasios Viglas"], "id": "0ac3d2b14c6b4462c7d2bb365b0ad20a6af74d3e", "title": "Time-space lower bounds for satisfiability", "references": ["0ff65ac698013cdd9d61326cab49a1d75404e001", "b094bea6e3ac02b7ff38daa530448df3a048f72c", "d1cbae7b13a32e297b8e607a92e2fc200623d5fa", "4d2e22795a6d6cc13e3f234da35b56617cc1697d"]}, {"date": "1971", "abstract": "It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be \u201creduced\u201d to the problem of determining whether a given propositional formula is a tautology. Here \u201creduced\u201d means, roughly speaking, that the first problem can be solved deterministically in polynomial time provided an oracle is available for solving the second. From this notion of reducible, polynomial degrees of difficulty are defined, and it is shown that the problem of determining tautologyhood has the same polynomial degree as the problem of determining whether the first of two given graphs is isomorphic to a subgraph of the second. Other examples are discussed. A method of measuring the complexity of proof procedures for the predicate calculus is introduced and discussed.", "authors": ["Stephen A. Cook"], "id": "1a8d2e5bb9c646d85308fbd6ce33fce8765e2c26", "title": "The complexity of theorem-proving procedures", "references": []}, {"date": "", "abstract": "must work. In fact, few futurists would disagree with the investigation of public-private key pairs. In this work we examine how e-commerce can be applied to the exploration of courseware.", "authors": [], "id": "25abdbe5b8791f6525e3283676f422c48fcdfb36", "title": "Logic based on Inclusion and Abstraction WV Quine", "references": ["fc2e38c8aec677806968873c28f33e826edfabd2"]}, {"date": "2000", "abstract": "UNIFORM COMPLEXITY. Models of Computation and Complexity Classes. NP-Completeness. The Polynomial-Time Hierarchy and Polynomial Space. Structure of NP. NONUNIFORM COMPLEXITY. Decision Trees. Circuit Complexity. Polynomial-Time Isomorphism. PROBABILISTIC COMPLEXITY. Probabilistic Machines and Complexity Classes. Complexity of Counting. Interactive Proof Systems. Probabilistically Checkable Proofs and NP-Hard Optimization Problems. Bibliography. Index.", "authors": ["Ding-Zhu Du", "Ker-I Ko"], "id": "e65e70c7da3633d867c6f17c6bf2a55fed84ce04", "title": "Theory of Computational Complexity", "references": []}, {"date": "", "abstract": "THE latest volume of the Proceedings of the London Mathematical Society keeps up to the usual high standard. As regards pure analysis, attention may be directed to Prof, and Mrs. W. H. Young's papers on integrals and deriv-ates, because they deal with fundamentally new notions of the integral calculus, with which every serious ^mathematician will have to make himself acquainted. Mr. G. H. Hardy contributes a paper of great interest on Dirichlet's divisor problem, and there is a little gem by Mr. T. L. Wren on the two-three birational space transformation, which incidentally gives a new, and we think finally satisfactory, aspect of the double-six configuration. In applied mathematics we have a paper by Prof. Bromwich on normal coordinates, based on the theory of complex integrals; one by Sir J. Larmor on transition from vapour to liquid; and one by Mr. F. B. Pidduck on the motion of ions, discussed by means of an integral equation. We must content ourselves with noting these few papers out of the whole thirty. The volume will doubtless receive the full attention that it deserves.Proceedings of the London Mathematical Society. Second series. Vol. xv. Pp. liii + 454. (London: F. Hodgson, 1916.)", "authors": [], "id": "ec04ed6b95a091f58042c8d94df5bbf385ae8ac5", "title": "Proceedings of the London Mathematical Society", "references": []}, {"date": "2006", "abstract": "This textbook is uniquely written with dual purpose. It cover cores material in the foundations of computing for graduate students in computer science and also provides an introduction to some more advanced topics for those intending further study in the area. This innovative text focuses primarily on computational complexity theory: the classification of computational problems in terms of their inherent complexity. The book contains an invaluable collection of lectures for first-year graduates on the theory of computation. Topics and features include more than 40 lectures for first year graduate students, and a dozen homework sets and exercises.", "authors": ["Dexter Kozen"], "id": "f480faf46807d29c132815ce0731ba489cfbfec9", "title": "Theory of Computation", "references": []}, {"date": "1993", "abstract": "Once we have developed an algorithm (q.v.) for solving a computational problem and analyzed its worst-case time requirements as a function of the size of its input (most usefully, in terms of the O-notation; see ALGORITHMS, ANALYSIS OF), it is inevitable to ask the question: \"Can we do better?\" In a typical problem, we may be able to devise new algorithms for the problem that are more and more efficient. But eventually, this line of research often seems to hit an invisible barrier, a level beyond whch improvements are very difficult, seemingly impossible, to come by. After many unsuccessful attempts, algorithm designers inevitably start to wonder if there is something inherent in the problem that makes it impossible to devise algorithms that are faster than the current one. They may try to develop mathematical techniques for proving formally that there can be no algorithm for the given problem which runs faster than the current one. Such a proof would be valuable, as it would suggest that it is futile to keep working on improved algorithms for this problem, that further improvements are certainly impossible. The realm of mathematical models and techniques for establishing such impossibility proofs is called computational complexity.", "authors": ["Christos H. Papadimitriou"], "id": "18ef30fe2a84e358b97e6733fab8bba911a88d85", "title": "Computational complexity", "references": []}, {"date": "", "abstract": "Biologists agree that empathic models are an interesting new topic in the field of electrical engineering, and hackers worldwide concur. In fact, few systems engineers would disagree with the improvement of 802.11b, which embodies the structured principles of hardware and architecture. We use client-server models to confirm that the acclaimed collaborative algorithm for the visualization of the World", "authors": [], "id": "75b5532a07eb5f0a5384837f2cf7055f7a983352", "title": "Proposed electronic calculator report for National Physical Laboratory Teddington", "references": ["deb3d2aea0e02de1ed7144e8343eb96d29bdddd7"]}, {"date": "2011", "abstract": "Red-black trees and Internet QoS, while important in theory , have not until recently been considered extensive. In this p osition paper, we argue the construction of RAID. we probe how write-back caches can be applied to the study of replication .", "authors": [], "id": "deb3d2aea0e02de1ed7144e8343eb96d29bdddd7", "title": "The chemical theory of morphogenesis", "references": ["3301167d98d4b1f55a51517671dc4a92417f2ed9", "2dde96cf7c2c2f2329895068760d899e9607d087", "3da7e3419d879fad9b02e993758a40e4d205d0e8", "00af642cafdc2e3ad1706c534680c3e883c3c29e", "b24d576824ea0a772041b0d53fa33d2854cd25a3", "7a6100e739be584e95bdcf02a6e6eb581462da14", "52e87b65ccccb745706fcd09b7df65d3adcdcef8", "0267a54606d17c33ad4acc23a8461dee5e0ab093", "650d4e7c4fbd68b05f87a46985f1da6f2f4f0ed8", "b13649eda87136d5b7b6319606fafcfac5a1ec6a"]}, {"date": "1996", "abstract": "The analysis of interrupts is a structured quagmire. In this position paper, we demonstrate the investigation of simulated annealing. KaliNil, our new methodology for DNS, is the solution to all of these problems.", "authors": ["Alan M. Turing"], "id": "650d4e7c4fbd68b05f87a46985f1da6f2f4f0ed8", "title": "Intelligent Machinery, A Heretical Theory*", "references": ["8dea163a553333ac0b28963fd50efc509bfa789a", "bf4efeb5257695c6d859611a643a94d38ebb2f79", "8964838cb60e31c13fda81e10fa75f476e10a126", "b6b8523c4e6ab63373fdf5ca584b02e404b14893", "af16f1a39e8498845ef9a06900d4d1dbb91a0248", "99aca3bf2601ff71bc14c8694626fba133fd5a78", "d408100f3c87e746ee4a0838333b342384fd380c", "092da8384571c8261858e91e3278e765eedde1d5", "db6f721b9dbf8cc7d7bec0569bc5d78e3f7f3d99", "58298f9c4e64e29a1c00d70d34c1909fc1a899d3"]}, {"date": "2011", "abstract": "Interactive communication and the Internet have garnered limited interest from both futurists and systems engineers in the last several years. Here, we argue the theoretical unification of context-free grammar and neural networks. AltAcater, our new application for symmetric encryption, is the solution to all of these issues.", "authors": [], "id": "4e3669953bcad30532b66140c81a28e3b14efbce", "title": "Macchine calcolatrici e intelligenza", "references": ["5eb958432d38525bee3b19daee8220d4516e9de0", "739823509bfb9066f3eb4b9561a33610998d888e", "98576f4b2df33503c19b82c11f28fdcc2a4c41dc", "38f266f1903be38489524d34564192bed86b68e7", "e896a6e94023531f1b06666e1881f55129692d78", "c47f9d2da6da7a44035fa3c3f13447ac924c450c", "d408100f3c87e746ee4a0838333b342384fd380c", "e127369c02476cfa65184dff4ca02a66e419f263", "f9aa451310e865d812514573e665c607a555c8fa", "af16f1a39e8498845ef9a06900d4d1dbb91a0248"]}, {"date": "2005", "abstract": "We make several improvements on time lower bounds for concrete problems in NP and PH. 1) We present an elementary technique based on \"indirect diagonalization\" that uniformly improves upon the known nonlinear time lower bounds for nondeterminism and alternating computation, on both sublinear (n/sup o(1)/) space RAMs and sequential worktape machines with random access to the input. We obtain better lower bounds for SAT as well as all NP-complete problems that have efficient reductions from SAT and /spl Sigma//sub k/-SAT for constant k /spl ges/ 2. For example, SAT cannot be solved by random access machines using n/sup /spl radic/(3) /time and n/sub o(1)/ space. The technique is a natural inductive approach, for which previous work is essentially its base case. 2) We show how indirect diagonalization can also yield time-space lower bounds for computation with bounded nondeterminism. One corollary is that for all k, there exists a constant c/sub k/ > 1 such that satisfiability of Boolean circuits with n inputs and n/sup k/ gates cannot be solved in deterministic time n/sup k/spl middot/c//sub k/ and n/sup o(1)/ space.", "authors": ["Ryan Williams"], "id": "d01c5e6916a1271da46db458dee7e10684f17a07", "title": "Better time-space lower bounds for SAT and related problems", "references": ["dc14e0a3bb350a83117c6dabbb3799c6091e5555", "b4ed661c083bb06f71cd5fae2e31bd2f84777b01", "03b57770387aee919b1f5c89333ee65a426f32e2", "b037f1bdbd46fc3603bf32f37c10b9882c87322a", "4d2e22795a6d6cc13e3f234da35b56617cc1697d", "b5848e5e1c6c7aa2e3a38882d79d37a1669b6589", "77f443551be0f1e4d7f6b11c4c4b632e08acf98e", "a06cd390689e17c8d414e1972c864c3251ab94f0", "d1cbae7b13a32e297b8e607a92e2fc200623d5fa", "0ef30111be2c7fb8574babb3f0e3ca8d91fd0e14"]}, {"date": "1952", "abstract": "It is suggested that a system of chemical substances, called morphogens, reacting together and diffusing through a tissue, is adequate to account for the main phenomena of morphogenesis. Such a system, although it may originally be quite homogeneous, may later develop a pattern or structure due to an instability of the homogeneous equilibrium, which is triggered off by random disturbances. Such reaction-diffusion systems are considered in some detail in the case of an isolated ring of cells, a mathematically convenient, though biologically unusual system. The investigation is chiefly concerned with the onset of instability. It is found that there are six essentially different forms which this may take. In the most interesting form stationary waves appear on the ring. It is suggested that this might account, for instance, for the tentacle patterns on Hydra and for whorled leaves. A system of reactions and diffusion on a sphere is also considered. Such a system appears to account for gastrulation. Another reaction system in two dimensions gives rise to patterns reminiscent of dappling. It is also suggested that stationary waves in two dimensions could account for the phenomena of phyllotaxis. The purpose of this paper is to discuss a possible mechanism by which the genes of a zygote may determine the anatomical structure of the resulting organism. The theory does not make any new hypotheses; it merely suggests that certain well-known physical laws are sufficient to account for many of the facts. The full understanding of the paper requires a good knowledge of mathematics, some biology, and some elementary chemistry. Since readers cannot be expected to be experts in all of these subjects, a number of elementary facts are explained, which can be found in text-books, but whose omission would make the paper difficult reading.", "authors": ["Alan M. Turing"], "id": "9dad457c31f7688b432689e48bf573a09b2d456f", "title": "The chemical basis of morphogenesis", "references": []}, {"date": "1990", "abstract": "It is suggested that a system of chemical substances, called morphogens, reacting together and diffusing through a tissue, is adequate to account for the main phenomena of morphogenesis. Such a system, although it may originally be quite homogeneous, may later develop a pattern or structure due to an instability of the homogeneous equilibrium, which is triggered off by random disturbances. Such reaction-diffusion systems are considered in some detail in the case of an isolated ring of cells, a mathematically convenient, though biologically unusual system. The investigation is chiefly concerned with the onset of instability. It is found that there are six essentially different forms which this may take. In the most interesting form stationary waves appear on the ring. It is suggested that this might account, for instance, for the tentacle patterns onHydra and for whorled leaves. A system of reactions and diffusion on a sphere is also considered. Such a system appears to account for gastrulation. Another reaction system in two dimensions gives rise to patterns reminiscent of dappling. It is also suggested that stationary waves in two dimensions could account for the phenomena of phyllotaxis. \n \nThe purpose of this paper is to discuss a possible mechanism by which the genes of a zygote may determine the anatomical structure of the resulting organism. The theory does not make any new hypotheses; it merely suggests that certain well-known physical laws are sufficient to account for many of the facts. The full understanding of the paper requires a good knowledge of mathematics, some biology, and some elementary chemistry. Since readers cannot be expected to be experts in all of these subjects, a number of elementary facts are explained, which can be found in text-books, but whose omission would make the paper difficult reading.", "authors": ["Alan M. Turing"], "id": "d635e2843c6fb034e9126aa73ef9c2e4e2c4714d", "title": "The chemical basis of morphogenesis", "references": ["e2bf2cc7493b99271046fcbe37619ddf8163be46"]}, {"date": "1959", "abstract": "This paper describes a universal computer capable of simultaneously executing an arbitrary number of sub-programs, the number of such sub-programs varying as a function of time under program control or as directed by input to the computer. Three features of the computer are:\n (1) The structure of the computer is a 2-dimensional modular (or iterative) network so that, if it were constructed, efficient use could be made of the high element density and \"template\" techniques now being considered in research on microminiature elements.\n (2) Sub-programs can be spatially organized and can act simultaneously, thus facilitating the simulation or direct control of \"highly-parallel\" systems with many points or parts interacting simultaneously (e.g. magneto-hydrodynamic problems or pattern recognition).\n (3) The computer's structure and behavior can, with simple generalizations, be formulated in a way that provides a formal basis for theoretical study of automata with changing structure (cf. the relation between Turing machines and computable numbers).", "authors": ["John H. Holland"], "id": "7f9154f6ab93a935acccb6b3fdf83d011e44d175", "title": "A universal computer capable of executing an arbitrary number of sub-programs simultaneously", "references": []}, {"date": "1989", "abstract": "The notion of linear-time computability is very sensitive to machine model. In this connection, we introduce a class NLT of functions computable in nearly linear time n(log n)O(1) on random access computers. NLT is very robust and does not depend on the particular choice of random access computers. Kolmogorov machines, Schonhage machines, random access Turing machines, etc., also compute exactly NLT functions in nearly linear time. It is not known whether usual multitape Turing machines are able to compute all NLT functions in nearly linear time. We do not believe they are and do not consider them necessarily appropriate for this relatively low complexity level. It turns out, however, that nondeterministic Turing machines accept exactly the languages in the nondeterministic version of NLT. We give also a machine-independent definition of NLT and a natural problem complete for NLT.", "authors": ["Yuri Gurevich", "Saharon Shelah"], "id": "d1cbae7b13a32e297b8e607a92e2fc200623d5fa", "title": "Nearly Linear Time", "references": []}, {"date": "2005", "abstract": "The main result of this paper is a separation result: there is a positive integerk such that for all well-behaving functionst(n), there is a language accepted by a nondeterministic (multi-tape) Turing machine in timet(n) which cannot be accepting by any deterministic (multitape) Turing machine in timeO(t(n)) and simultaneously spaceo((t(n))1/k). This implies, for example that for any positive integer,l,l \u2260k, there is a language accepted by anl time bounded NDTM which cannot be accepted by a DTM in time and spaceO(nl) andO((logn)l\u2032) respectively for anyl\u2032. Such a result is not provable by direct diagonalization because we do not have time to \u201csimulate and do the opposite\". We devise a different method for accomplishing the result: We first use an alternating Turing machine to speed up the simulation of a time and space bounded DTM and then argue that if our separation result did not hold, every NDTM can itself be simulated faster by another NDTM producing a contradiction to the standard hierarchy results. Some other applications of this method are also presented.", "authors": ["Ravi Kannan"], "id": "4d2e22795a6d6cc13e3f234da35b56617cc1697d", "title": "Towards separating nondeterminism from determinism", "references": ["947b7d533cd970c9f68cbed96f7c4a6b6c345a4c", "aae3f57711c59d1b857987751bbc4ce205a2b330", "fde24614a815a581240092f8dadeed2c24e7562f", "c36ed444003e6b9e6845539372ebd68f2048c86a", "c630d9ff8bbd4102715f5726ccb47b1d6105dcff", "866d35b6556c77a5aebda8492159d26800d2b97b", "14a2542e6e44157d2de93edde4268e69add87c1b", "2af0772a1d218bab8977dca419e26ed7524312a1", "164e60d89e824f9dc76880bc154bb07d24437bee", "ee745da17043fbde6cec81dd0c7e4bf7b78c2c16"]}, {"date": "1996", "abstract": "We give a random class of lattices in Z n so that, if there is a probabilistic polynomial time algorithm which nds a short vector in a random lattice with a probability of at least 1 2 then there is also a probabilistic polynomial time algorithm which solves the following three lattice problems in every lattice in Z n with a probability exponentially close to one. (1) Find the length of a shortest nonzero vector in an n-dimensional lattice, approximately, up to a polynomial factor. (2) Find the shortest nonzero vector in an n-dimensional lattice L where the shortest vector v is unique in the sense that any other vector whose length is at most n c kvk is parallel to v, where c is a suuciently large absolute constant. (3) Find a basis b 1 ; :::; b n in the n-dimensional lattice L whose length, deened as max n i=1 kb i k, is the smallest possible up to a polynomial factor. A large number of the existing techniques of cryptography include the generation of a speciic instance of a problem in NP (together with a solution) which for some reason is thought to be diicult to solve. As an example we may think about factor-ization. Here a party of a cryptographic protocol is supposed to provide a composite number m so that the factorization of m is known to her but she has some serious reason to believe that nobody else will be able to factor m. The most compelling reason for such a belief would be a mathematical proof of the fact that the prime factors of m cannot be found in less then k step in some realistic model of computation, where k is a very large number. For the moment we do not have any proof of this type, neither for speciic numerical values of m and k, nor in some assymptotic sense. In spite of the lack of mathematical proofs, in two cases at least, we may expect that a problem will be diicult to solve. One is the class of NP-complete problems. Here we may say that if there is a problem at all which is diicult to solve, then an NP-complete problem will provide such an example. The other case is, if the problem is a very famous question (e.g. prime factorization), which for a long time were unsuccesfully attacked by \u2026", "authors": ["Mikl\u00f3s Ajtai"], "id": "f6dbdf621b0acc9a131f7f2a4d3efbd4bfb0db58", "title": "Generating Hard Instances of Lattice Problems", "references": ["c5654bc032f7eed78dcb0f6849853181de635cf4"]}, {"date": "1994", "abstract": "A digital computer is generally believed to be an efficient universal computing device; that is, it is believed able to simulate any physical computing device with an increase in computation time by at most a polynomial factor. This may not be true when quantum mechanics is taken into consideration. This paper considers factoring integers and finding discrete logarithms, two problems which are generally thought to be hard on a classical computer and which have been used as the basis of several proposed cryptosystems. Efficient randomized algorithms are given for these two problems on a hypothetical quantum computer. These algorithms take a number of steps polynomial in the input size, e.g., the number of digits of the integer to be factored.", "authors": ["Peter W. Shor"], "id": "d6431498449b1739f1d0397b6e79ddb7b31d5ffc", "title": "Polynomial-Time Algorithms for Prime Factorization and Discrete Logarithms on a Quantum Computer", "references": ["b243db3386e8a7c988f2f4c43ed4fe009a928b2b", "cbc09cdd52abe9806244a278e41dd38b6ab5e041", "1364a51dbe7b0c9eda8a4a2f72d4d5400f20f5ab", "49a3c42f5b78c517589d5af4846b369f27287be0", "f64ed54b6d8e75ffeb422f94c14f12e07d57ad8e", "28d717f61ff475eb52acdfc578e720d8e1da8863", "2273d9829cdf7fc9d3be3cbecb961c7a6e4a34ea", "f80b58133a5428ac0436a61ccfc2c294c7fe0392", "94de181240cd491141e6e0e0af0461617580f1b8", "c7a001c6c47301ff3489ff4f307ebb2fcb628344"]}, {"date": "1998", "abstract": "A pressure sensitive recording developer sheet comprising a phenolic resin developer coating on a support, the improvement which comprises said phenolic resin developer being a resin which is obtained by modifying with an alkylphenol compound represented by the general formula WHEREIN R1+L, R2 and R3 which may be the same or different each represents a hydrogen atom or an alkyl group with the proviso that they simultaneously should not be hydrogen atoms, in the condensation of p-phenyl phenol and aldehyde. The coating may also contain microcapsules containing a substantially colorless coupler.", "authors": ["Donald E. Knuth"], "id": "d10c7852c144c9f34934703b014b85e1f76d59d5", "title": "The Art of Computer Programming Vol", "references": []}, {"date": "1966", "abstract": "It has long been known that increasing the number of tapes used by a Turing machine does not provide the ability to compute any new functions. On the other hand, the use of extra tapes does make it possible to speed up the computation of certain functions. It is known that a square factor is sometimes required for a one-tape machine to behave as a two-tape machine and that a square factor is always sufficient.\nThe purpose of this paper is to show that, if a given function requires computation time <italic>T</italic> for a <italic>k</italic>-tape realization, then it requires at most computation time <italic>T</italic> log <italic>T</italic> for a two-tape realization. The proof of this fact is constructive; given any <italic>k</italic>-tape machine, it is shown how to design an equivalent two-tape machine that operates within the stated time bounds. In addition to being interesting in its own right, the trade-off relation between number of tapes and speed of computation can be used in a diagonalization argument to show that if <italic>T</italic>(<italic>n</italic>) and <italic>U</italic>(<italic>n</italic>) are two time functions such that inf <italic>T</italic>(<italic>n</italic>) log <italic>T</italic>(<italic>n</italic>) \u00f7 <italic>U</italic>(<italic>n</italic>) = 0 then there exists a function that can be computed within the time bound <italic>U</italic>(<italic>n</italic>) but not within the time bound <italic>T</italic>(<italic>n</italic>).", "authors": ["F. C. Hennie", "Richard Edwin Stearns"], "id": "b094bea6e3ac02b7ff38daa530448df3a048f72c", "title": "Two-Tape Simulation of Multitape Turing Machines", "references": []}, {"date": "1983", "abstract": "Semantic Scholar extracted view of \"Relativized Circuit Complexity\" by Christopher B. Wilson", "authors": ["Christopher B. Wilson"], "id": "7625615edcae5896a86f70848c9f1e2163f685a1", "title": "Relativized Circuit Complexity", "references": []}, {"date": "1979", "abstract": "It is well known that time bounds for machines correspond closely to size bounds for networks, and that space bounds correspond to depth bounds. It is not known whether simultaneous time and space bounds correspond to simultaneous size and depth bounds. It is shown here that simultaneous time and \"reversal\" bounds correspond to simultaneous size and depth bounds, and that simultaneous time and space bounds correspond to simultaneous size and \"width\" bounds.", "authors": ["Nicholas Pippenger"], "id": "c36ed444003e6b9e6845539372ebd68f2048c86a", "title": "On simultaneous resource bounds", "references": ["bedaa51668bff4116258c53dd0cb64289f00258a", "4b5a9490e85b90925a28079e541029bdc3561bf2", "3dbe2ee8de3c3bf06ed722b36cef75152f3d5997", "a1efad845665fbb645614f535348ec3048f5869d", "ee745da17043fbde6cec81dd0c7e4bf7b78c2c16"]}, {"date": "1989", "abstract": "A general review is presented of some of the approaches to the general problem of morphogenesis which originate either directly or indirectly in the work of Turing. The main points are the stability analysis of the Gierer-Meinhardt equations and the discussion on the applicability to biological systems of the concepts and techniques of field theory.", "authors": ["I. A. Doroban\u0163u", "Mihai Visinescu"], "id": "bf4efeb5257695c6d859611a643a94d38ebb2f79", "title": "The Physical Basis of Morphogenesis", "references": []}, {"date": "2011", "abstract": "Unified certifiable communication have led to many unproven advances, including reinforcement learning and evolutionary programming. After years of confusing research into forward-error correction, we argue the study of access points. In order to address this grand challenge, we use metamorphic modalities to show that rasterization and checksums can interfere to overcome this challenge.", "authors": [], "id": "52e87b65ccccb745706fcd09b7df65d3adcdcef8", "title": "A diffusion reaction theory of morphogenesis in plants Universal Turing Machine", "references": ["7cd4f36092bbd6872303282479754911160646a4", "9d28eae4da33aa20208cba54c2d563cf018bb196", "6d34169c6c5f76311b57add12b4eece5ca530f47", "6f1087e5e0d7ce7da15d364a4ea60a563cc000bf", "c47f9d2da6da7a44035fa3c3f13447ac924c450c", "89585cf6859fbaba66806176d19c53fb1395090e", "3da7e3419d879fad9b02e993758a40e4d205d0e8", "7a6100e739be584e95bdcf02a6e6eb581462da14", "39d5978ca6236fe4b9b76cd424c20ebb00afbc3b", "58298f9c4e64e29a1c00d70d34c1909fc1a899d3"]}, {"date": "1974", "abstract": "Probabilistic Turing machines are Turing machines with the ability to flip coins in order to make random decisions. We allow probabilistic Turing machines small but nonzero error probability in computing number-theoretic functions. An example is given of a function computable more quickly by probabilistic Turing machines than by deterministic Turing machines. It is shown how probabilistic linear-bounded automata can simulate nondeterministic linear-bounded automata.", "authors": ["John T. Gill"], "id": "2842d53d0f2389618f8ad1c47d2ffef2c344d22f", "title": "Computational complexity of probabilistic Turing machines", "references": []}, {"date": "1986", "abstract": "This specification defines the Document Object Model Level 2 HTML, a platformand language-neutral interface that allows programs and scripts to dynamically access and update the content and structure of [HTML 4.01] and [XHTML 1.0] documents. The Document Object Model Level 2 HTML builds on the Document Object Model Level 2 Core [DOM Level 2 Core] and is not backward compatible with DOM Level 1 HTML [DOM Level 1]. Status of this document This section describes the status of this document at the time of its publication. Other documents may supersede this document. The latest status of this document series is maintained at the W3C. 1 Document Object Model (DOM) Level 2 HTML Specification", "authors": [], "id": "0ff65ac698013cdd9d61326cab49a1d75404e001", "title": "Editors", "references": []}, {"date": "1940", "abstract": "Semantic Scholar extracted view of \"Organisers and genes.\" by Conrad Hal Waddington", "authors": ["Conrad Hal Waddington"], "id": "e2bf2cc7493b99271046fcbe37619ddf8163be46", "title": "Organisers and genes.", "references": []}, {"date": "", "abstract": "Many electrical engineers would agree that, had it not been for e-business, the simulation of systems might never have occurred. Given the current status of lossless technology, biologists clearly desire the compelling unification of DNS and the lookaside buffer. Here we use real-time symmetries to argue that online algorithms can be made secure, pervasive, and interactive.", "authors": [], "id": "2dde96cf7c2c2f2329895068760d899e9607d087", "title": "Proposals for Development in the Mathematics", "references": ["3301167d98d4b1f55a51517671dc4a92417f2ed9", "75b5532a07eb5f0a5384837f2cf7055f7a983352", "44c417b8c7f1ccad0192b642acfc117646b5c517", "c234f51301c2823035cfe1de91537e252b22aedc", "52f2febf8568dbb995c67e83e8299680f2cecb79", "b4d0f32d3ca96ef72b1eef082de0adad9bf9713b", "cc11d5a5700a3d2370afab7a498af361432c5384", "1bdcc652801c1cdfd5d521913bc97d4770fa81e8", "cd4b4c05dce06439f65a90dfc0146b9b958f81fa", "092da8384571c8261858e91e3278e765eedde1d5"]}, {"date": "2011", "abstract": "Internet QoS and 802.11 mesh networks, while important in theory, have not until recently been considered practical. after years of extensive research into IPv6, we prove the development of write-back caches, which embodies the important principles of e-voting technology. We propose an applicati on for the memory bus, which we call Admirer. Although it at first glance seems unexpected, it is derived from known results.", "authors": [], "id": "00af642cafdc2e3ad1706c534680c3e883c3c29e", "title": "Thechemical basis of moprhogenesis", "references": ["3301167d98d4b1f55a51517671dc4a92417f2ed9", "4e3669953bcad30532b66140c81a28e3b14efbce", "2dde96cf7c2c2f2329895068760d899e9607d087", "c234f51301c2823035cfe1de91537e252b22aedc", "45076119addd45ec697c256b0dbc37b0186641fb", "e896a6e94023531f1b06666e1881f55129692d78", "b24d576824ea0a772041b0d53fa33d2854cd25a3", "deb3d2aea0e02de1ed7144e8343eb96d29bdddd7", "0267a54606d17c33ad4acc23a8461dee5e0ab093", "1fb0ef0169202488f02233499402b34c8e8c35fb"]}, {"date": "1969", "abstract": "Semantic Scholar extracted view of \"The Intrinsic Computational Difficulty of Functions\" by Alan Cobham et al.", "authors": ["Alan Cobham", "Yehoshua Bar-Hillel"], "id": "08adf24a965e51e269e8ec05bf7d83c96a968892", "title": "The Intrinsic Computational Difficulty of Functions", "references": []}, {"date": "1999", "abstract": "We show that non-deterministic time NTIME(n) is not contained in deterministic time n/sup 2-/spl epsiv// and polylogarithmic space, for any /spl epsiv/>0. This implies that (infinitely often), satisfiability cannot be solved in time O(n/sup 2-/spl epsiv//) and polylogarithmic space. A similar result is presented for uniform circuits; a log-space uniform circuit of polylogarithmic width computing satisfiability requires infinitely often almost quadratic size.", "authors": ["Richard J. Lipton", "Anastasios Viglas"], "id": "0ef30111be2c7fb8574babb3f0e3ca8d91fd0e14", "title": "On the complexity of SAT", "references": ["dc14e0a3bb350a83117c6dabbb3799c6091e5555", "ddc9c6be41134269b6907034cb147e79817c7c0b", "a93847a9d64b09f0dd28443697c1c4ffbf76e60e", "370a1740aa4d524774bef1f91ba47b0c1ee77399", "4d2e22795a6d6cc13e3f234da35b56617cc1697d", "cede1982dc246a67e1ec76ab4b86ad03701dff9d", "a43b6cd17d8b57aedeced7e6cf3a0a830f40e397", "968f648cc4918cfa340a00e1baa127b6d4132044", "419cb947256af2cb87df34d160059c1c7b4ac942", "d1cbae7b13a32e297b8e607a92e2fc200623d5fa"]}, {"date": "1983", "abstract": "While nondeterminism is widely beleived to be more powerful than determinism in various contexts (the most famous being the conjecture that NP strictly contains P), no proof of the added power of nondeterminism is available for any significant issue. The weaker conjecture (than NP strictly contains P) that there is a language accepted by a nondeterministic linear time bounded multitape Turing Machine that cannot be accepted by a deterministic linear time bounded multi-tape TM still seems quite hard (Paul 1982). The aim of this paper is to show how the existance of the polynomial-time hierarchy of Meyer and Stockmeyer(1972) and the related concept of alternation (Chandra, Kozen and Stockmeyer(1981)) can be exploited to prove the power of nondeterminism over determinism in some contexts. It is hoped that this approach may be useful in proving stronger results.", "authors": ["Ravi Kannan"], "id": "a06cd390689e17c8d414e1972c864c3251ab94f0", "title": "Alternation and the power of nondeterminism", "references": []}, {"date": "2004", "abstract": "We show that a deterministic Turing machine with one d-dimensional work tape and random access to the input cannot solve satisfiability in time n \u03b1 for a < \u221a(d + 2)/(d + 1). For conondeterministic machines, we obtain a similar lower bound for any a such that a 3 < 1+\u03b1/(d+1). The same bounds apply to almost all natural NP-complete problems known.", "authors": ["Dieter van Melkebeek", "Ran Raz"], "id": "77f443551be0f1e4d7f6b11c4c4b632e08acf98e", "title": "A Time Lower Bound for Satisfiability", "references": ["b4ed661c083bb06f71cd5fae2e31bd2f84777b01", "cf91dd4d5358df1c6ba87360a94a469c6ed6ba5c", "42ccba1e6cff310c1c8d022d63c169bae2491326", "4d2e22795a6d6cc13e3f234da35b56617cc1697d", "b5848e5e1c6c7aa2e3a38882d79d37a1669b6589", "0ac3d2b14c6b4462c7d2bb365b0ad20a6af74d3e", "d1a5b74bfe162e280ba8599697ff0660894341c8", "6407680b5b551bccf50e62eaf94ab036a4d7eaa9", "a06cd390689e17c8d414e1972c864c3251ab94f0", "0ef30111be2c7fb8574babb3f0e3ca8d91fd0e14"]}, {"date": "1988", "abstract": "A weaker form of the above result first appeared in [l], where it was used to show that CNF satisfiability is NP-hard. The theorem in [l] referred only to single-tape machines, and the length bound on the formula was something like T(n)3. The length bound was improved to T(n) log T(n) by Robson [4]. Recently, Steams and Hunt [6] have given a different construction which yields a slightly longer formula, namely one of length O(T(n) log2T(n)), but which applies to multitape Turing machines. All of the above constructions are fairly elaborate. The theorem is interesting because it gives information about the relative complexity of different NP problems. For example, Schnorr [5] uses it to show that SAT is quasilinear complete, and hence so are a number of other standard NP-complete problems. Stearns and Hunt [6] use it to show that, assuming SAT h.as asymptotic complexity 2\u201d, many standard NP-complete problems have about the same complexity as SAT, but others, such as the CLIQUE problem, are easier. Dewdney [2] is interested because of its potential application to his generic reduction computer, which irnplements nondeterministic algorithms.", "authors": ["Stephen A. Cook"], "id": "b5848e5e1c6c7aa2e3a38882d79d37a1669b6589", "title": "Short Propositional Formulas Represent Nondeterministic Computations", "references": []}, {"date": "1977", "abstract": "It is shown that every deterministic multitape Turing machine of time complexity <italic>t</italic>(<italic>n</italic>) can be simulated by a deterministic Turing machine of tape complexity <italic>t</italic>(<italic>n</italic>)/log<italic>t</italic>(<italic>n</italic>). Consequently, for tape constructable <italic>t</italic>(<italic>n</italic>), the class of languages recognizable by multitape Turing machines of time complexity <italic>t</italic>(<italic>n</italic>) is strictly contained in the class of languages recognized by Turing machines of tape complexity <italic>t</italic>(<italic>n</italic>). In particular the context-sensitive languages cannot be recognized in linear time by deterministic multitape Turing machines.", "authors": ["John E. Hopcroft", "Wolfgang J. Paul", "Leslie G. Valiant"], "id": "947b7d533cd970c9f68cbed96f7c4a6b6c345a4c", "title": "On Time Versus Space", "references": []}, {"date": "1992", "abstract": "A nontechnical survey of recent quantum-mechanical discoveries that challenge generally accepted complexity-theoretic versions of the Church-Turing thesis is presented. In particular, the authors construct an oracle relative to which there exists a set that can be recognized in quantum polynomal time (QP), yet any Turing machine that recognizes it would require exponential time even if allowed to be probabilistic, provided that errors are not tolerated. In particular, QP is not contained in or equal to ZPP relative to this oracle. Furthermore, there are cryptographic tasks that are demonstrably impossible to implement with unlimited computing power probabilistic interactive turning machines, yet they can be implemented even in practice by quantum mechanical apparatus.<<ETX>>", "authors": ["Andr\u00e9 Berthiaume", "Gilles Brassard"], "id": "c7a001c6c47301ff3489ff4f307ebb2fcb628344", "title": "The quantum challenge to structural complexity theory", "references": ["5b6fc332893e39e8676db93741ab7d785f645d8b", "5571fc63b71856622dbeb30166dcc0ad7e29136e", "692dceed6973b708ff6b2032da9a1f35963aa634", "17c16c133ab46e66ea0a08f40d19b3308733c348", "8d2b32d3b0931cad3b55e0c3b5d4d0af40f17bc1", "2baf4ac77661be8206d022417841de1c5cf2f7c1", "86d9cf51865cf84e7d6fa3ea33b8196bf25bc206", "2e2007e082db4b94140907375273507fba178978", "2842d53d0f2389618f8ad1c47d2ffef2c344d22f"]}, {"date": "1995", "abstract": "Recently there has been a great deal of interest in the power of \u201cQuantum Computers\u201d [4, 15, 18]. The driving force is the recent beautiful result of Shor that shows that discrete log and factoring are solvable in random quantum polynomial time [15]. We use a method similar to Shor\u2019s to obtain a general theorem about quantum polynomial time. We show that any cryptosystem based on what we refer to as a \u2018hidden linear form\u2019 can be broken in quantum polynomial time. Our results imply that the discrete log problem is doable in quantum polynomial time over any group including Galois fields and elliptic curves. Finally, we introduce the notion of \u2018junk bits\u2019 which are helpful when performing classical computations that are not injective.", "authors": ["Dan Boneh", "Richard J. Lipton"], "id": "94de181240cd491141e6e0e0af0461617580f1b8", "title": "Quantum Cryptanalysis of Hidden Linear Functions (Extended Abstract)", "references": ["d6431498449b1739f1d0397b6e79ddb7b31d5ffc", "4c7671550671deba9ec318d867522897f20e19ba", "f64ed54b6d8e75ffeb422f94c14f12e07d57ad8e", "5fc9065fe9fabc76445e8a9bc2438d0440d21225", "75caeb5274630bd52cbcd8f549237c30d108e2ff", "d5d1c3356e6b2dce34e5a43c881eac0279ea6588", "ea79fa165ea3689863231b12cecd399104052f4a"]}, {"date": "1996", "abstract": "We consider how to optimize memory use and computation time in operating a quantum computer. In particular, we estimate the number of memory quantum bits (qubits) and the number of operations required to perform factorization, using the algorithm suggested by Shor [in Proceedings of the 35th Annual Symposium on Foundations of Computer Science, edited by S. Goldwasser (IEEE Computer Society, Los Alamitos, CA, 1994), p. 124]. A K-bit number can be factored in time of order K3 using a machine capable of storing 5K+1 qubits. Evaluation of the modular exponential function (the bottleneck of Shor\u2019s algorithm) could be achieved with about 72K3 elementary quantum gates; implementation using a linear ion trap would require about 396K3 laser pulses. A proof-of-principle demonstration of quantum factoring (factorization of 15) could be performed with only 6 trapped ions and 38 laser pulses. Though the ion trap may never be a useful computer, it will be a powerful device for exploring experimentally the properties of entangled quantum states.", "authors": ["Beckman", "Chari", "Devabhaktuni", "Preskill"], "id": "f80b58133a5428ac0436a61ccfc2c294c7fe0392", "title": "Efficient networks for quantum factoring.", "references": ["d6431498449b1739f1d0397b6e79ddb7b31d5ffc", "99866a3bcb068f7c8e852aa4268f61db0e960b70", "49a3c42f5b78c517589d5af4846b369f27287be0", "2273d9829cdf7fc9d3be3cbecb961c7a6e4a34ea", "ea79fa165ea3689863231b12cecd399104052f4a", "fa7cd8fe1a3508ccf8e70623f522cbde360802b5", "f1c9058e409ef56fbbef254e219bc2e0da6ae7d5", "830d5b75fc8eb41e1b8562d309c603ddb1e41c53", "59b447f58246fbbdffd5e896f83a3a142eca5cf1", "c7a001c6c47301ff3489ff4f307ebb2fcb628344"]}, {"date": "1962", "abstract": "Semantic Scholar extracted view of \"On the Algorithmic Complexity of Discrete Functions\" by Yuri Petrovich Ofman", "authors": ["Yuri Petrovich Ofman"], "id": "3dbe2ee8de3c3bf06ed722b36cef75152f3d5997", "title": "On the Algorithmic Complexity of Discrete Functions", "references": []}, {"date": "1979", "abstract": "Various computational models (such as machines and combinational logic networks) induce various and, m general, different computational complexity measures Relations among these measures are established by studying the ways m which one model can \"simulate\" another It ts shown that a machine with k-dimensional storage tapes (respectively, with tree-structured storage media) can be simulated on-hne by a machine with one- dimensional storage tapes m time O(n 2-ilk) (respectively, m time O(n2/log n)) An obhv:ous machine Is defined to be one whose head posmons, as functions of time, are independent of the input, and It Is shown that any machine with one-d~menslonal tapes can be simulated on-hne by an oblivious machine with two one-dimensional tapes in time O(n log n) All of these results are the best possible, at least insofar as on-hne simulation is concerned. By slmdar methods It is shown that n steps of the computation of an arbitrary machine with one- dimensional tapes can be performed by a combinational logic network of cost O(n log n) and delay O(n)", "authors": ["Nicholas Pippenger", "Michael J. Fischer"], "id": "bedaa51668bff4116258c53dd0cb64289f00258a", "title": "Relations Among Complexity Measures", "references": []}, {"date": "1977", "abstract": "Turing machine space complexity is related to circuit depth complexity. The relationship complements the known connection between Turing machine time and circuit size, thus enabling us to expose the related nature of some important open problems concerning Turing machine and circuit complexity. We are also able to show some connection between Turing machine complexity and arithmetic complexity.", "authors": ["Allan Borodin"], "id": "4b5a9490e85b90925a28079e541029bdc3561bf2", "title": "On Relating Time and Space to Size and Depth", "references": []}, {"date": "1971", "abstract": "Given a vector of N elements, the perfect shuffle of this vector is a permutation of the elements that are identical to a perfect shuffle of a deck of cards. Elements of the first half of the vector are interlaced with elements of the second half in the perfect shuffle of the vector.", "authors": ["Harold S. Stone"], "id": "a1efad845665fbb645614f535348ec3048f5869d", "title": "Parallel Processing with the Perfect Shuffle", "references": []}, {"date": "1992", "abstract": "This paper continues the study of the power of oracles to separate quantum com.plexity classes from classical (including probabilistic and nondeterministic) complexity classes, which we initiated in an earlier paper [5]. The new results are that, under appropriate oracles, (1) there is a decision problem that can be solved in polynomial time on the quantum computer, which would classically require exponential time in the worst case even on a nondeterministic computer (in particular QP g NP relative to this oracle), and (2) there is a decision problem that can be solved in exponential time on the quantum computer f o r which any deterministic classical computer would require double exponential time on all but finitely many instances. 1 Review of earlier results In a bold paper published in the Proceedings of the Royal Society, David Deutsch put forth in 1985 the idea that a quantum computer could in principle carry out a large amount of computation in parallel on a single piece of hardware by using the principle of quantum superposition [6, 71. Later, he and Richard Jozsa exhibited a problem tha.t the quantum computer could solve exponentially faster than any deterministic classical computer [SI. However, that problem admits more than one valid solution on most instances] and therefore it does not fit the usual mold in computational complexity theory of considering the difficulty of computing functions or of deciding set membership. (Nevertheless, we pointed out in [5] that their problem can be recast as a decision problem in the context of premise problems [lo].) * Supported in part by an NSERC postgraduate fellowship; currently visiting student at Merton College, Oxford University. t Supported in part by NSERC\u2019S E. W. R. Steacie Memorial Fellowship and QuBbec\u2019s FCAR. The main contribution of [5] was to interpret the result of Deutsch and Jozsa in the light of oracle computations, that is computations that can be performed with the help of arbitrarily complex oracles capable of instantly answering a precise set of questions. The direct oracle interpretation of their result is the existence of an oracle under which there is a decision problem that can be solved in linear time on the quantum computer, yet any deterministic classical computer would require exponential time on infinitely many instances to solve the same problem. In particular, this implies that Px c QPx for some oracle X , where P denotes as usual the class of decision problems that can be solved with certainty in worst-case polynomial time by a classical deterministic computer, whereas QP denotes the class of decision problems that can be solved with certainty in worst-case polynomial time by the quantum computer, and the superscript X denotes the oracle whose availability we assume. One might be tempted to think at first that the quantum computer gets its advantage over classical deterministic computers not from the use of quantum superposition, but merely from the much more mundane availability of randomization, which is inherent to quantum computation. After all, i t is generally believed that randomized computers enjoy a computational advantage over classical computers, even if the randomized computer is required to have zero probability of yielding an erroneous result. To defuse this uninteresting interpretation, we also cla.im in [5] an oracle under which there is a decision problem that can be solved in linear time on the quantum computer] yet any probabilistic classical computer that is never allowed to make mistakes would require expected exponential time on infinitely many instances to solve the same problem. In symbols, QPy ZPPy for another appropriate oracle Y where ZPP is the classical class of decision problems that can be solved in expected polynomial time by probabilistic computers tha.t are not allowed to ever make mistakes [9]. 195 0-8186-3420-0193 $03.00", "authors": ["Andr\u00e9 Berthiaume", "Gilles Brassard"], "id": "28d717f61ff475eb52acdfc578e720d8e1da8863", "title": "Oracle Quantum Computing", "references": ["2a0c4b6c17bcd6e30703fa86d7784a9ebd527e1d", "2842d53d0f2389618f8ad1c47d2ffef2c344d22f", "c7a001c6c47301ff3489ff4f307ebb2fcb628344", "692dceed6973b708ff6b2032da9a1f35963aa634"]}, {"date": "1994", "abstract": "A computer is generally considered to be a universal computational device; i.e., it is believed able to simulate any physical computational device with a cost in computation time of at most a polynomial factor: It is not clear whether this is still true when quantum mechanics is taken into consideration. Several researchers, starting with David Deutsch, have developed models for quantum mechanical computers and have investigated their computational properties. This paper gives Las Vegas algorithms for finding discrete logarithms and factoring integers on a quantum computer that take a number of steps which is polynomial in the input size, e.g., the number of digits of the integer to be factored. These two problems are generally considered hard on a classical computer and have been used as the basis of several proposed cryptosystems. We thus give the first examples of quantum cryptanalysis.<<ETX>>", "authors": ["Peter W. Shor"], "id": "2273d9829cdf7fc9d3be3cbecb961c7a6e4a34ea", "title": "Algorithms for quantum computation: discrete logarithms and factoring", "references": ["1db1e2609852ed8843981386a94c868d4e642bf3", "4c7671550671deba9ec318d867522897f20e19ba", "539357be9d9b2939ccc15a83f3a177cd28caa04d", "28d717f61ff475eb52acdfc578e720d8e1da8863", "cef6672d2ecfa5fcf37659537ceb1836516c6062", "75caeb5274630bd52cbcd8f549237c30d108e2ff", "237870c16994bc6df13b51ac3f812fa2fe42a713", "8573be599665a75eec5aea6b7cb98e00dae07688", "c7a001c6c47301ff3489ff4f307ebb2fcb628344", "78343fcbd26abc918750ef748c1e0cf85f0606dc"]}, {"date": "2011", "abstract": "Recent advances in embedded modalities and extensible modalities have paved the way for forward-error correction. Of course, this is not always the case. In this position paper, we verify the exploration of local-area networks. In our research we validate that the foremost \u201cfuzzy\u201d algorithm for the investigation of IPv6 by Van Jacobson et al. runs in \u0398(n) time.", "authors": [], "id": "45076119addd45ec697c256b0dbc37b0186641fb", "title": "Kann eine Maschine denken", "references": ["3301167d98d4b1f55a51517671dc4a92417f2ed9", "4e3669953bcad30532b66140c81a28e3b14efbce", "c234f51301c2823035cfe1de91537e252b22aedc", "739823509bfb9066f3eb4b9561a33610998d888e", "b4d0f32d3ca96ef72b1eef082de0adad9bf9713b", "99aca3bf2601ff71bc14c8694626fba133fd5a78", "092da8384571c8261858e91e3278e765eedde1d5", "f9aa451310e865d812514573e665c607a555c8fa", "db6f721b9dbf8cc7d7bec0569bc5d78e3f7f3d99", "0267a54606d17c33ad4acc23a8461dee5e0ab093"]}, {"date": "1994", "abstract": "A quantum computer is a device capable of performing computational tasks that depend on characteristically quantum mechanical effects, in particular coherent quantum superposition. Such devices can efficiently perform classes of computation (e.g. factorisation) which are believed to be intractable on any classical computer. This makes it highly desirable to construct such devices. In this paper, we address the last remaining theoretical obstacle to such a construction, namely the problem of stability or error correction. This problem is more substantial in quantum computation than in classical computation because of the delicate nature of the interference phenomena on which quantum computation depends. We present a new, purely quantum mechanical method of error correction, which has no classical analogue, but can serve to stabilise coherent quantum computations. Like the classical methods, it utilises redundancy, but it does not depend on measuring intermediate results of the computation.<<ETX>>", "authors": ["Andr\u00e9 Berthiaume", "David Deutsch", "Richard Jozsa"], "id": "1364a51dbe7b0c9eda8a4a2f72d4d5400f20f5ab", "title": "The stabilisation of quantum computations", "references": []}, {"date": "1993", "abstract": "We propose a complexity model of quantum circuits analogous to the standard (acyclic) Boolean circuit model. It is shown that any function computable in polynomial time by a quantum Turing machine has a polynomial-size quantum circuit. This result also enables us to construct a universal quantum computer which can simulate, with a polynomial factor slowdown, a broader class of quantum machines than that considered by E. Bernstein and U. Vazirani (1993), thus answering an open question raised by them. We also develop a theory of quantum communication complexity, and use it as a tool to prove that the majority function does not have a linear-size quantum formula. >", "authors": ["Andrew Chi-Chih Yao"], "id": "f64ed54b6d8e75ffeb422f94c14f12e07d57ad8e", "title": "Quantum Circuit Complexity", "references": ["4c7671550671deba9ec318d867522897f20e19ba", "75caeb5274630bd52cbcd8f549237c30d108e2ff", "0733393a545b62e49e9fc4a76d53d843cfb72010", "6a19d66af47baf8f9bccbe5fa3c661105732bae3", "bedaa51668bff4116258c53dd0cb64289f00258a", "2f87b02422646594b31ae1501b24eba0a65c746c"]}, {"date": "2000", "abstract": "We give the first nontrivial model-independent time?space tradeoffs for satisfiability. Namely, we show that SAT cannot be solved in n1+o(1) time and n1?? space for any ?>0 general random-access nondeterministic Turing machines. In particular, SAT cannot be solved deterministically by a Turing machine using quasilinear time and n space. We also give lower bounds for log-space uniform NC1 circuits and branching programs. Our proof uses two basic ideas. First we show that if SAT can be solved nondeterministically with a small amount of time then we can collapse a nonconstant number of levels of the polynomial-time hierarchy. We combine this work with a result of Nepomnja??ii that shows that a nondeterministic computation of superlinear time and sublinear space can be simulated in alternating linear time. A simple diagonalization yields our main result. We discuss how these bounds lead to a new approach to separating the complexity classes NL and NP. We give some possibilities and limitations of this approach.", "authors": ["Lance Fortnow"], "id": "d1a5b74bfe162e280ba8599697ff0660894341c8", "title": "Time-Space Tradeoffs for Satisfiability", "references": []}, {"date": "1996", "abstract": "With the realization that computers that use the interference and superposition principles of quantum mechanics might be able to solve certain problems, including prime factorization, exponentially faster than classical computers @1#, interest has been growing in the feasibility of these quantum computers, and several methods for building quantum gates and quantum computers have been proposed @2,3#. One of the most cogent arguments against the feasibility of quantum computation appears to be the difficulty of eliminating error caused by inaccuracy and decoherence @4#. Whereas the best experimental implementations of quantum gates accomplished so far have less than 90% accuracy @5#, the accuracy required for factorization of numbers large enough to be difficult on conventional computers appears to be closer to one part in billions. We hope that the techniques investigated in this paper can eventually be extended so as to reduce this quantity by several orders of magnitude. In the storage and transmission of digital data, errors can be corrected by using error-correcting codes @6#. In digital computation, errors can be corrected by using redundancy; in fact, it has been shown that fairly unreliable gates could be assembled to form a reliable computer @7#. It has widely been assumed that the quantum no-cloning theorem @8# makes error correction impossible in quantum communication and computation because redundancy cannot be obtained by duplicating quantum bits. This argument was shown to be in error for quantum communication in Ref. @9#, where a code was given that mapped one qubit ~two-state quantum system! into nine qubits so that the original qubit could be recovered perfectly even after arbitrary decoherence of any one of these nine qubits. This gives a quantum code on nine qubits with a rate 1", "authors": ["Calderbank", "Shor"], "id": "49a3c42f5b78c517589d5af4846b369f27287be0", "title": "Good quantum error-correcting codes exist.", "references": ["0733393a545b62e49e9fc4a76d53d843cfb72010", "9908fa18614139d2e0c99047e108f6a6ccac4d66", "067b1059f443e53276d39fc6eddbf273cc103531", "57eaf807b2639d4c01af674ee511f8a6f7004c8b", "1570ebe5f6a8ca9d11a7dbe2fbad52388ba56f53", "c84374653947e2a4dc1f5a5db8646b14738c9d19"]}, {"date": "2011", "abstract": "Highly-available models and superblocks have garnered tremendous interest from both experts and experts in the last several years. Our goal here is to set the record straight. In this paper, we prove the study of the Ethernet. Here we introduce an introspective tool for refining journaling file systems (Heliotypy), disconfirming that 802.11 mesh networks can be made cooperative, authenticated, and knowledge-base.", "authors": ["Bernard Meltzer", "Bruce Michie"], "id": "c234f51301c2823035cfe1de91537e252b22aedc", "title": "Intelligent Machinery-National Physical Laboratory Report", "references": ["3301167d98d4b1f55a51517671dc4a92417f2ed9", "8dea163a553333ac0b28963fd50efc509bfa789a", "98576f4b2df33503c19b82c11f28fdcc2a4c41dc", "b6b8523c4e6ab63373fdf5ca584b02e404b14893", "af16f1a39e8498845ef9a06900d4d1dbb91a0248", "cc11d5a5700a3d2370afab7a498af361432c5384", "99aca3bf2601ff71bc14c8694626fba133fd5a78", "de83e862e346bc1cc80b22671a9611b48f300aa4", "f9aa451310e865d812514573e665c607a555c8fa", "1fb0ef0169202488f02233499402b34c8e8c35fb"]}, {"date": "1994", "abstract": "We prove a \\(\\Theta (t(n)\\sqrt[d]{{t(n)}}/\\log i(n))\\) bound for the simulation of t(n) steps of a Turing machine using several one-dimensional work tapes on a Turing machine using one d-dimensional work tape, d \u2265 2. The lower bound holds for the problem of recognizing languages on machines with a separate one-way input tape.", "authors": ["Martin Dietzfelbinger", "Martin H\u00fchne"], "id": "42ccba1e6cff310c1c8d022d63c169bae2491326", "title": "Matching Upper and Lower Bounds for Simulation of Several Tapes on One Multidimensional Tape", "references": []}, {"date": "1995", "abstract": "It is known that quantum computers can dramatically speed up the task of finding factors of large numbers, a problem of practical significance for cryptographic applications. Factors of an L-digit number can be found in \u223cL2 time [compared to \u223cexp(L1/3) time] by a quantum computer, which simultaneously follows all paths corresponding to distinct classical inputs, obtaining the solution from the coherent quantum interference of the alternatives. Here it is shown how the decoherence process degrades the interference pattern that emerges from the quantum factoring algorithm. For a quantum computer performing logical operations, an exponential decay of quantum coherence is inevitable. However, even in the presence of exponential decoherence, quantum computation can be useful as long as a sufficiently low decoherence rate can be achieved to allow meaningful results to be extracted from the calculation.", "authors": ["Isaac Chuang", "Raymond Laflamme", "Peter W. Shor", "Witold \u017burek"], "id": "cbc09cdd52abe9806244a278e41dd38b6ab5e041", "title": "Quantum Computers, Factoring, and Decoherence", "references": ["a5b8a236548736e2178fbf9a7fe43a53d8831b04", "d3fd9e1266a9442f51b8586719408e71591742a8", "84ac23f6e7fb643d41a7db7d06cdb42196ffb6d8", "2273d9829cdf7fc9d3be3cbecb961c7a6e4a34ea", "0ff65ac698013cdd9d61326cab49a1d75404e001", "9d0f35b8d5617ee7b4c18516097d02d2e1a8b724", "cbdd4ef5b8f7310a2e73335809163e9b5e398df1"]}, {"date": "1965", "abstract": "This paper has two purposes. The first is to investigate the characteristics of a restricted class of Turing machines, and to develop a simple tool for describing their computations. The second is to present specific problems for which tight lower bounds can be found for the computation times required by Turing machines of this restricted class.", "authors": ["F. C. Hennie"], "id": "6407680b5b551bccf50e62eaf94ab036a4d7eaa9", "title": "One-Tape, Off-Line Turing Machine Computations", "references": ["184e0c2bb2de749db6238e9c7dcd87e7179d1f69", "20e2329d84479c5891e99f2b39878d5af82eda2f", "930001f394dc4862985da58137c78e7bc3018b0c"]}, {"date": "1992", "abstract": "Ekert has described a cryptographic scheme in which Einstein-Podolsky-Rosen (EPR) pairs of particles are used to generate identical random numbers in remote places, while Bell's theorem certifies that the particles have not been measured in transit by an eavesdropper. We describe a related but simpler EPR scheme and, without invoking Bell's theorem, prove it secure against more general attacks, including substitution of a fake EPR source. Finally we show our scheme is equivalent to the original 1984 key distribution scheme of Bennett and Brassard, which uses single particles instead of EPR pairs.", "authors": ["Bennett", "Brassard", "Mermin"], "id": "5571fc63b71856622dbeb30166dcc0ad7e29136e", "title": "Quantum cryptography without Bell's theorem.", "references": []}, {"date": "1988", "abstract": "We present strong evidence that the implication, \u201cif one-way permutations exist, then secure secret key agreement is possible\u201d, is not provable by standard techniques. Since both sides of this implication are widely believed true in real life, to show that the implication is false requires a new model. We consider a world where all parties have access to a black box for a randomly selected permutation. Being totally random, this permutation will be strongly one-way in a provable, information-theoretic way. We show that, if P = NP, no protocol for secret key agreement is secure in such a setting. Thus, to prove that a secret key agreement protocol which uses a one-way permutation as a black box is secure is as hard as proving P \u2260 NP. We also obtain, as a corollary, that there is an oracle relative to which the implication is false, i.e., there is a one-way permutation, yet secret-exchange is impossible. Thus, no technique which relativizes can prove that secret exchange can be based on any one-way permutation. Our results present a general framework for proving statements of the form, \u201cCryptographic application X is not likely possible based solely on complexity assumption Y.\u201d", "authors": ["Russell Impagliazzo", "Steven Rudich"], "id": "2e2007e082db4b94140907375273507fba178978", "title": "Limits on the Provable Consequences of One-way Permutations", "references": []}, {"date": "1990", "abstract": "Overview This course presents fundamental techniques for designing efficient computer algorithms, proving their correctness, and analyzing their performance. Topics to be covered include graph algorithms, graph algorithms (minimum spanning trees, Dijkstra's algorithm). If there is any material that seems unfamiliar, please see the instructor or a teaching assistant as soon as possible to discuss it. Piazza We will use Piazza for class announcements and online discussion. This is the best way to quickly get help from classmates, the TAs, and the instructor. Rather than emailing questions to the teaching staff, we strongly encourage you to post questions to Piazza at", "authors": ["Thomas H. Cormen", "Charles E. Leiserson", "Ronald L. Rivest"], "id": "fa7cd8fe1a3508ccf8e70623f522cbde360802b5", "title": "The Design and Analysis of Computer Algorithms", "references": []}, {"date": "1989", "abstract": "On a Theory of the Collapse of the Wave Function.- On the Measurement Problem of Quantum Mechanics.- A New Characteristic of a Quantum System Between Two Measurements - A \"Weak Value\".- Can the Quantum Measurement Process be put into QED?.- Chained Bell Inequalities.- From George Boole to John Bell - The Origins of Bell's Inequality.- The Logic of Quantum Nonseparability.- Bell's Theorem and Mermin's Gedanken Experiment.- Searching for Mutually Unbiased Observables.- Going Beyond Bell's Theorem.- Quantum Field Theory, Bell's Inequalities and the Problem of Hidden Variables.- What Locality Isn't: A Response to Jarrett.- Bell's Theorem: The Forgotten Loophole and How to Exploit It.- Are More Economical Escapes From Bell's Strictures Possible?.- Concerning Theories Free From Bell's Constraint.- The EPR Paradox, Actions at a Distance and the Theory of Relativity.- Conditionals, Probability and Bell's Theorem.- Relativity and Probability, Classical or Quantal.- Relativistic Probability Amplitudes and State Preparation.- Non-Linearity and Post-Bell Quantum Mechanics.- Observable Effects of the Uncertainty Relations in the Covariant Phase Space Representation of Quantum Mechanics.- Geometry of Cyclic Quantum Evolutions.- On the Computer Simulation of the EPR-Bohm Experiment.- Discrete Phase-Space Model for Quantum Mechanics.- A Possible Explanation of Quantum Mechanics Behavior by a Classical Cellular Automaton Construction.- Quantum Mechanical Information is Ubiquitous.- The Unobservability of Spinor Structure.- Teaching QM: True, Trivial, Inevitable.- Quantum Cosmology and Quantum Mechanics.- Cosmology, EPR Correlations and Separability.- Bell's Theorem: Form and Information in the Quantum Theory.- Horizons of Knowledge in Cosmology.- The Ontological Status of the Cosmological Singularity.- Quantum Cosmological Generality of Inflation in Anisotropic Spacetime.- Mass Generation in the Early Universe.- Massive Photons and Monopoles.- The Large-Scale Streaming of Galaxies.- The Numerically Simple Universe.- Size of a Least Unit.- Bell, Book and Candle: The Limning of a Mystery.- Complementarity and Space-Time Description.- Complementarity and Cosmology.- Quantum Ontologies.- Henry P. Stapp on Quantum Theory and Reality.- For Whom the Bell Tolls: A Plea for Fundamental Ontology.- The Role of Consciousness in Physical Reality.- The Significance of Human Observation in Measurement in Quantum Mechanics The Nature of the Traditional Separation Between Psychology and Physics.- The Universe in the Light of Contemporary Scientific Developments.- Name Index.", "authors": ["Minas C. Kafatos"], "id": "f1c9058e409ef56fbbef254e219bc2e0da6ae7d5", "title": "Bell's theorem, quantum theory and conceptions of the universe", "references": []}, {"date": "1988", "abstract": "Semantic Scholar extracted view of \"Arthur-Merlin Games: A Randomized Proof System, and a Hierarchy of Complexity Classes\" by L\u00e1szl\u00f3 Babai et al.", "authors": ["L\u00e1szl\u00f3 Babai", "Shlomo Moran"], "id": "2a0c4b6c17bcd6e30703fa86d7784a9ebd527e1d", "title": "Arthur-Merlin Games: A Randomized Proof System, and a Hierarchy of Complexity Classes", "references": []}, {"date": "1982", "abstract": "Semantic Scholar extracted view of \"Quantum Mechanical Hamiltonian Models of Turing Machines\" by I PaulBenioff", "authors": ["I PaulBenioff"], "id": "78343fcbd26abc918750ef748c1e0cf85f0606dc", "title": "Quantum Mechanical Hamiltonian Models of Turing Machines", "references": []}, {"date": "1982", "abstract": "Quantum mechanical Hamiltonian models, which represent an aribtrary but finite number of steps of any Turing machine computation, are constructed here on a finite lattice of spin-1/2 systems. Different regions of the lattice correspond to different components of the Turing machine (plus recording system). Successive states of any machine computation are represented in the model by spin configuration states. Both time-independent and time-dependent Hamiltonian models are constructed here. The time-independent models do not dissipate energy or degrade the system state as they evolve. They operate close to the quantum limit in that the total system energy uncertainty/computation speed is close to the limit given by the time-energy uncertainty relation. However, the model evolution is time global and the Hamiltonian is more complex. The time-dependent models do not degrade the system state. Also they are time local and the Hamiltonian is less complex.", "authors": ["Paul Benioff"], "id": "1db1e2609852ed8843981386a94c868d4e642bf3", "title": "Quantum mechanical hamiltonian models of turing machines", "references": ["83419fcb7e66982d78b7a171cd863965655b196f", "b449b779203f4392211e6db98720d46eecc0e4bd", "c24f7825989959f991fe0416f0e35ef5a5eaa5c9", "cee5d4d123d6d289a14d41baffa73723dcd3e9e7"]}, {"date": "1987", "abstract": "Publisher Summary This chapter discusses two similar random algorithms, one for factoring and one for computing discrete logarithms in GF(q) where q is prime or a power of 2. The factoring algorithm will have expected worst case running time L(N)\u221a2 + 0(1) the discrete logarithm algorithm will have expected worst case running time L(q)\u221a2 + 0(1) preprocessing stage and expected worst case running time L(q)\u221a1/2 + 0(1) actual discrete logarithm calculation. Both methods are quite similar to previously considered algorithms. In particular, the factoring algorithm is a variant of Dixon's random squares method, which is based on ideas of Morrison, Brillhart, and earlier writers. The random squares method is augmented with Lenstra's elliptic curve factoring method and Wiedemann's coordinate recurrence method for solving a sparse system of linear equations over a finite field. The discrete logarithm algorithm is based on the index calculus method of Western, Miller. Again, the new ingredients are the elliptic curve method and the coordinate recurrence method. It is perhaps paradoxical that the elliptic curve factoring method can be used as a subroutine in a rigorously analyzed algorithm while it has not been completely rigorously analyzed. The point is that the algorithms described in the chapter are random, so a subroutine need not work on all inputs. The chapter discusses that a somewhat weakened form of the elliptic curve method works fairly rapidly for most inputs. The argument uses a new result of Friedlander, Lagarias in analytic number theory.", "authors": ["Carl Pomerance"], "id": "8573be599665a75eec5aea6b7cb98e00dae07688", "title": "Fast, Rigorous Factorization and Discrete Logarithm Algorithms", "references": []}, {"date": "1996", "abstract": "The concept of multiple-particle interference is discussed, using insights provided by the classical theory of error correcting codes. This leads to a discussion of error correction in a quantum communication channel or a quantum computer. Methods of error correction in the quantum regime are presented, and their limitations assessed. A quantum channel can recover from arbitrary decoherence of x qubits if K bits of quantum information are encoded using n quantum bits, where K /n can be greater than 1 - 2H(2x/n), but must be less than 1 - 2H(2x/n) This implies exponential reduction of decoherence with only a polynomial increase in the computing resources required. Therefore quantum computation can be made free of errors in the presence of physically realistic levels of decoherence. The methods also allow isolation of quantum communication from noise and evesdropping (quantum privacy amplification).", "authors": ["Andrew M. Steane"], "id": "830d5b75fc8eb41e1b8562d309c603ddb1e41c53", "title": "Multiple-particle interference and quantum error correction", "references": ["a5b8a236548736e2178fbf9a7fe43a53d8831b04", "1364a51dbe7b0c9eda8a4a2f72d4d5400f20f5ab", "7c5e13efa803a8acf5e5e1b547bb427b6c0ffc02", "9cbc6bc1bec461f1bb7ddc4b68509ae9a4c6ef6b", "f9f880bb8c8edb1484d2c8efe6262ce2c08f73dd", "2273d9829cdf7fc9d3be3cbecb961c7a6e4a34ea", "db5c47769e7b1a852303220fb75b8e9d6e2086ce", "eb747b753af634d3f58ee6e9cc170287f81c372a", "c4d295f67e2f70177622771b9884d54ff51792ba", "e44193a72ea571878bc157e61a95a9e1dc218dae"]}, {"date": "1995", "abstract": "We show that a set of gates that consists of all one-bit quantum gates (U(2)) and the two-bit exclusive-or gate (that maps Boolean values (x,y) to (x,x \u2295y)) is universal in the sense that all unitary operations on arbitrarily many bits n (U(2 n )) can be expressed as compositions of these gates. We investigate the number of the above gates required to implement other gates, such as generalized Deutsch-Toffoli gates, that apply a specific U(2) transformation to one input bit if and only if the logical AND of all remaining input bits is satisfied. These gates play a central role in many proposed constructions of quantum computational networks. We derive upper and lower bounds on the exact number of elementary gates required to build up a variety of two- and three-bit quantum gates, the asymptotic number required for n-bit Deutsch-Toffoli gates, and make some observations about the number required for arbitrary n-bit unitary operations.", "authors": ["Barenco", "Bennett", "Cleve", "DiVincenzo", "Margolus", "Shor", "Sleator", "Smolin", "Weinfurter"], "id": "59b447f58246fbbdffd5e896f83a3a142eca5cf1", "title": "Elementary gates for quantum computation.", "references": ["cbc09cdd52abe9806244a278e41dd38b6ab5e041", "f9a903059e40fbe05f87fabeabed3b5582e9d21e", "1364a51dbe7b0c9eda8a4a2f72d4d5400f20f5ab", "cbdd4ef5b8f7310a2e73335809163e9b5e398df1", "89225f6ae6b23f7308ee17e98a2aed92114f3726", "f64ed54b6d8e75ffeb422f94c14f12e07d57ad8e", "9c7670ab2f0468d8bf3f5aacf0aa84d4277272a7", "2a24d6c63876ed37b8e553fa905520dcaa19f3c6", "2f2132695be5be1374823a171b454304cef4afeb", "c4d295f67e2f70177622771b9884d54ff51792ba"]}, {"date": "1983", "abstract": "Many different types of inter-process communication have been examined from a complexity point of view [SP, Y]. We study a new model, in which a collection of processes\n  <italic>P<subscrpt>0</subscrpt>, ..., P<subscrpt>k\u22121</subscrpt></italic> \n that share information about a set of integers\n {a<subscrpt>0</subscrpt>, ...,a<subscrpt>k\u22121</subscrpt>},\n communicate to determine a 0-1 predicate of the numbers.\n In this new model, tremendous sharing of information is allowed, while no single party is given enough information to determine the predicate on its own. Formally, each <italic>P<subscrpt>i</subscrpt></italic> has access to every a<subscrpt>j</subscrpt> except for a<subscrpt>i</subscrpt>. For simplicity, we only allow the parties to communicate as follows.", "authors": ["Ashok K. Chandra", "Merrick L. Furst", "Richard J. Lipton"], "id": "2f87b02422646594b31ae1501b24eba0a65c746c", "title": "Multi-party protocols", "references": []}, {"date": "1993", "abstract": "Semantic Scholar extracted view of \"Cryptology column-Quantum cryptography: A bibliog-raphy\" by Gilles Brassard", "authors": ["Gilles Brassard"], "id": "6a19d66af47baf8f9bccbe5fa3c661105732bae3", "title": "Cryptology column-Quantum cryptography: A bibliog-raphy", "references": []}, {"date": "1973", "abstract": "Semantic Scholar extracted view of \"Bounds for the quantity of information transmitted by a quantum communication channel\" by Alexander S. Holevo", "authors": ["Alexander S. Holevo"], "id": "0733393a545b62e49e9fc4a76d53d843cfb72010", "title": "Bounds for the quantity of information transmitted by a quantum communication channel", "references": []}, {"date": "1963", "abstract": "Semantic Scholar extracted view of \"Real-time computation and recursive functions not real-time computable\" by Michael O. Rabin", "authors": ["Michael O. Rabin"], "id": "20e2329d84479c5891e99f2b39878d5af82eda2f", "title": "Real-time computation and recursive functions not real-time computable", "references": []}, {"date": "1992", "abstract": "In this paper a probabilistic algorithm is exhibited that factors any positive integer n into prime factors in expected time at most Ln[2, 1 + o()] for n oo, where L,[a, b] = exp(b(logx)a(loglogx)l a). Many practical factoring algorithms, including the quadratic sieve and the elliptic curve method, are conjectured to have an expected running time that satisfies the same bound, but this is the first algorithm for which the bound can be rigorously proved. Nevertheless, this does not close the gap between rigorously established time bounds and merely conjectural ones for factoring algorithms. This is due to the advent of a new factoring algorithm, the number field sieve, which is conjectured to factor any positive integer n in time Ln[ I, 0(1)] . The algorithm analyzed in this paper is a variant of the class group relations method, which makes use of class groups of binary quadratic forms of negative discriminant. This algorithm was first suggested by Seysen, and later improved by A. K. Lenstra, who showed that the algorithm runs in expected time at most Ln [ 12 1 + O( 1)] if one assumes the generalized Riemann hypothesis. The main device for removing the use of the generalized Riemann hypothesis from the proof is the use of multipliers. In addition a character sum estimate for algebraic number fields is used, with an explicit dependence on possible exceptional zeros of the corresponding L-functions. Another factoring algorithm using class groups that has been proposed is the random class groups method. It is shown that there is a fairly large set of numbers that this algorithm cannot be expected to factor as efficiently as had previously been thought. DEPARTMENT OF MATHEMATICS, UNIVERSITY OF CALIFORNIA, BERKELEY, CALIFORNIA 94720 E-mail address: hwl@math.berkeley.edu DEPARTMENT OF MATHEMATICS, UNIVERSITY OF GEORGIA, ATHENS, GEORGIA 30602 E-mail address: carl@ada.math.uga.edu This content downloaded from 157.55.39.186 on Tue, 12 Apr 2016 08:54:17 UTC All use subject to http://about.jstor.org/terms", "authors": ["Hendrik W. Lenstra", "Carl Pomerance"], "id": "237870c16994bc6df13b51ac3f812fa2fe42a713", "title": "A rigorous time bound for factoring integers", "references": ["6a4ef42f1f2d709c24c44e9f9594f7069274d4b5", "41c5341ea5409468e4d68ad14fce820c78bc51b2", "3e49e5cb285b1cd68ffb392f4b0b3865095fdfd5", "a317166d6aae0ceae29f33539b1c732445cc2c5f"]}, {"date": "1989", "abstract": "The theory of quantum computational networks is the quantum generalization of the theory of logic circuits used in classical computing machines. Quantum gates are the generalization of classical logic gates. A single type of gate, the universal quantum gate, together with quantum \u2018unit wires\u2019, is adequate for constructing networks with any possible quantum computational property.", "authors": ["David Deutsch"], "id": "539357be9d9b2939ccc15a83f3a177cd28caa04d", "title": "Quantum computational networks", "references": []}, {"date": "1963", "abstract": "We introduce a concept of real-time computation by a Turing machine. The relative strengths of one-tape versus two-tape machines is established by a new method of proofs of impossibility of actual computations.", "authors": ["Michael O. Rabin"], "id": "184e0c2bb2de749db6238e9c7dcd87e7179d1f69", "title": "Real time computation", "references": []}, {"date": "1965", "abstract": "Semantic Scholar extracted view of \"On the computational complexity of algorithms\" by Juris Hartmanis et al.", "authors": ["Juris Hartmanis", "Richard Edwin Stearns"], "id": "930001f394dc4862985da58137c78e7bc3018b0c", "title": "On the computational complexity of algorithms", "references": ["b449b779203f4392211e6db98720d46eecc0e4bd", "35b74d763e74b0ec7e29f3f106e9402bc6ccc5ea", "d268511f8a8abe7a97a73b63b46dbc861fb94adc"]}, {"date": "1995", "abstract": "We propose an implementation of a quantum computer to solve Deutsch's problem, which requires exponential time on a classical computer but only linear time with quantum parallelism. By using a dual-rail quantum-bit representation as a simple form of error correction, our machine can tolerate some amount of decoherence and still give the correct result with high probability. The design that we employ also demonstrates a signature for quantum parallelism which unambiguously distinguishes the desired quantum behavior from the merely classical. The experimental demonstration of our proposal using quantum optical components calls for the development of several key technologies common to single photonics.", "authors": ["Chuang", "Yamamoto"], "id": "cbdd4ef5b8f7310a2e73335809163e9b5e398df1", "title": "Simple quantum computer.", "references": []}, {"date": "1993", "abstract": "Recently, several algorithms using number field sieves have been given to factor a number n in heuristic expected time $L_n [1/3; c]$, where \\[ L_n [ v ;c ] = \\exp \\left\\{ ( c + o ( 1 ) ) ( \\log n )^v ( \\log \\log n )^{1 - v } \\right\\} \\] for $n \\to \\infty $.This paper presents an algorithm to solve the discrete logarithm problem for $GF ( p )$ with heuristic expected running time $L_p [ 1/3; 3^{2/3}]$. For umbers of a special form, there is an asymptotically slower but more practical version of the algorithm.", "authors": ["Daniel M. Gordon"], "id": "cef6672d2ecfa5fcf37659537ceb1836516c6062", "title": "Discrete Logarithms in GF(P) Using the Number Field Sieve", "references": ["307ab08c3d4f551019297d2480597c614af8069c", "4f942cd0174a928b56245cd639d488e195baa60d", "fe0906e03890ca30ba216f7290b8bbc01d618c3d", "e06342830838bf0392ca10df2309d6af86e315fb", "ec30a15d158dc6c68b32c614ec00adf5f6597915", "8573be599665a75eec5aea6b7cb98e00dae07688", "237870c16994bc6df13b51ac3f812fa2fe42a713", "041091d3231ceac8691eb9975ff3b7f17043e06b", "67cc7e92c7d3e2be63a4b8de56f1344d9464b5e1", "be68e7d79f3dd4d0efe3bad3410e2d3af3451610"]}, {"date": "1994", "abstract": "Semantic Scholar extracted view of \"Digital communication (2. ed.)\" by Edward A. Lee et al.", "authors": ["Edward A. Lee", "David G. Messerschmitt"], "id": "84ac23f6e7fb643d41a7db7d06cdb42196ffb6d8", "title": "Digital communication (2. ed.)", "references": []}, {"date": "1989", "abstract": "A new probabilistic failure model for networks of gates is formulated. Although this model has not been used previously, it supports the proofs of both the positive and negative results appearing in the literature. Furthermore, with respect to this new model, the complexity measures of both size and depth are affected by at most constant multiplicative factors when the set of functions that can be computed by gates is changed from one finite and complete basis to another, or when the bound on the failure probability of the gates is changed (within the limits allowed by the basis), or when the bound on the error probability of the network is changed (within the limits allowed by the basis and the failure probability of the gates).", "authors": ["Nicholas Pippenger"], "id": "1570ebe5f6a8ca9d11a7dbe2fbad52388ba56f53", "title": "Invariance of complexity measures for networks with unreliable gates", "references": []}, {"date": "1995", "abstract": "The effects of the inevitable coupling to external degrees of freedom of a quantum computer are examined. It is found that for quantum calculations (in which the maintenance of coherence over a large number of states is important), not only must the coupling be small, but the time taken in the quantum calculation must be less than the thermal time scale \\ensuremath{\\Elzxh}/${\\mathit{k}}_{\\mathit{B}}$T. For longer times the condition on the strength of the coupling to the external world becomes much more stringent.", "authors": ["Unruh"], "id": "a5b8a236548736e2178fbf9a7fe43a53d8831b04", "title": "Maintaining coherence in quantum computers.", "references": ["2aebd785cd9072b7d3566a5ec14045544010ddd1", "692dceed6973b708ff6b2032da9a1f35963aa634"]}, {"date": "1995", "abstract": "A quantum computer can be implemented with cold ions confined in a linear trap and interacting with laser beams. Quantum gates involving any pair, triplet, or subset of ions can be realized by coupling the ions through the collective quantized motion. In this system decoherence is negligible, and the measurement (readout of the quantum register) can be carried out with a high efficiency.", "authors": ["Cirac", "Zoller"], "id": "9d0f35b8d5617ee7b4c18516097d02d2e1a8b724", "title": "Quantum Computations with Cold Trapped Ions.", "references": []}, {"date": "1994", "abstract": "We present numerical results which show how two-bit logic gates can be used in the design of a quantum computer. We show that the Toffoli gate, which is the universal gate for all classical reversible computation, can be implemented using a particular sequence of exactly five two-bit gates. An arbitrary three-bit unitary gate, which can be used to build up any arbitrary quantum computation, can be implemented exactly with six two-bit gates. The ease of implementation of any particular quantum operation is dependent upon a very nonclassical feature of the operation, its exact quantum phase factor.<<ETX>>", "authors": ["David P. DiVincenzo", "John A. Smolin"], "id": "2f2132695be5be1374823a171b454304cef4afeb", "title": "Results on two-bit gate design for quantum computers", "references": ["974af5a7d08d0031ec62eb33c86c11fa3326bfff", "627f209380b07cf3561060041a838d1da6a97183", "539357be9d9b2939ccc15a83f3a177cd28caa04d", "e51fe114b7e061c5199133dd504a928d803460f4", "12d9677bdc0768a12d89e00b411a3552037e1fd5", "ea79fa165ea3689863231b12cecd399104052f4a"]}, {"date": "1994", "abstract": "Abstract We give an account of the quantum noiseless coding theorem, including a new proof based on a simplified block coding scheme. We also discuss an illustrative example of quantum coding.", "authors": ["Richard Jozsa", "Benjamin Schumacher"], "id": "c84374653947e2a4dc1f5a5db8646b14738c9d19", "title": "A New Proof of the Quantum Noiseless Coding Theorem", "references": ["f434ba244af3ea490f5eaa49556091d00b851abd", "b53cf07a342c839f7ac7b8f10e0b710776621886"]}, {"date": "1966", "abstract": "Visualizing electronic orbitals. The image of an atom is really the image of the averaged likelihood that the electrons will be at various places. Physicists at Arizona State University have now imaged the cloud of bonding electrons in copper oxide and shown that they look just the drawings used for decades in quantum physics textbooks. Using a combination of x-ray diffraction and electron diffraction (to avoid multiple x-ray scattering problems), the scientists produced a three-dimensional map (shown here) of the hybridized \"orbital hole\" bonding copper with neighboring oxygen atoms in cuprite (Cu2O). They also found unexpected metal-metal covalent bonding, not clearly predicted by calculations. The researchers would like to apply their technique to the more complicated CuO2 superconductors and colossal magnetoresistance manganates. (J. M. Zuo, M. Kim, M. O'Keefe, J. C. H. Spence, Nature 401, 49, 1999.)", "authors": ["Jason Howard"], "id": "d3fd9e1266a9442f51b8586719408e71591742a8", "title": "Physics Today.", "references": []}, {"date": "1995", "abstract": "It is shown that if one can apply some Hamiltonian repeatedly to a few variables at a time one can in general effect any desired unitary time evolution on an arbitrarily large number of variables. As a result, almost any quantum logic gate with two or more inputs is computationally universal in that copies of the gate can be \"wired together\" to effect any desired logic circuit, and to perform any desired unitary transformation on a set of quantum variables.", "authors": ["Lloyd"], "id": "9c7670ab2f0468d8bf3f5aacf0aa84d4277272a7", "title": "Almost any quantum logic gate is universal.", "references": ["5351a0830445fc0d8f6698997a0dd847ebf82457"]}, {"date": "1995", "abstract": "We prove the existence of a class of two-input, two-output gates any one of which is universal for quantum computation. This is done by explicitly constructing the three-bit gate introduced by Deutsch ( Proc. R. Soc. Lond. A 425, 73 (1989)) as a network consisting of replicas of a single two-bit gate.", "authors": ["Adriano Barenco"], "id": "89225f6ae6b23f7308ee17e98a2aed92114f3726", "title": "A universal two-bit gate for quantum computation", "references": []}, {"date": "1992", "abstract": "It is shown that, over an arbitrary ring, the functions computed by polynomial-size algebraic formulas are also computed by polynomial-length algebraic straight-line programs that use only three registers. This was previously known for Boolean formulas [D. A. Barrington, J. Comput. System Sci., 38 (1989), pp. 150\u2013164], which are equivalent to algebraic formulas over the ring $GF(2)$. For formulas over arbitrary rings, the result is an improvement over previous methods that require the number of registers to be logarithmic in the size of the formulas in order to obtain polynomial-length straight-line programs. Moreover, the straight-line programs that arise in these constructions have the property that they consist of statements whose actions on the registers are linear and bijective. A consequence of this is that the problem of determining the iterated product of $n3 \\times 3$ matrices is complete (under P-projections) for algebraic $NC^1 $. Also, when the ring is $GF(2)$, the programs that arise in the c...", "authors": ["Michael Ben-Or", "Richard Cleve"], "id": "f9a903059e40fbe05f87fabeabed3b5582e9d21e", "title": "Computing Algebraic Formulas Using a Constant Number of Registers", "references": []}, {"date": "1976", "abstract": "Let \u0192(<italic>x</italic>) be one of the usual elementary functions (exp, log, artan, sin, cosh, etc.), and let <italic>M</italic>(<italic>n</italic>) be the number of single-precision operations required to multiply <italic>n</italic>-bit integers. It is shown that \u0192(<italic>x</italic>) can be evaluated, with relative error <italic>&Ogr;</italic>(2<supscrpt>-<italic>n</italic></supscrpt>), in <italic>&Ogr;</italic>(<italic>M</italic>(<italic>n</italic>)log (<italic>n</italic>)) operations as <italic>n</italic> \u2192 \u221e, for any floating-point number <italic>x</italic> (with an <italic>n</italic>-bit fraction) in a suitable finite interval. From the Sch\u00f6nhage-Strassen bound on <italic>M</italic>(<italic>n</italic>), it follows that an <italic>n</italic>-bit approximation to \u0192(<italic>x</italic>) may be evaluated in <italic>&Ogr;</italic>(<italic>n</italic> log<supscrpt>2</supscrpt>(<italic>n</italic>) log log(<italic>n</italic>)) operations. Special cases include the evaluation of constants such as \u03c0, <italic>e</italic>, and <italic>e</italic><supscrpt>\u03c0</supscrpt>. The algorithms depend on the theory of elliptic integrals, using the arithmetic-geometric mean iteration and ascending Landen transformations.", "authors": ["Richard P. Brent"], "id": "a317166d6aae0ceae29f33539b1c732445cc2c5f", "title": "Fast Multiple-Precision Evaluation of Elementary Functions", "references": []}, {"date": "1981", "abstract": "The paper describes a \"probabilistic algorithm\" for finding a factor of any large composite integer n (the required input is the integer n together with an auxiliary sequence of random numbers). It is proved that the expected number of operations which will be required is O(exp{ 83Qn n In In n)l/2)) for some constant f > 0. Asymptotically, this algorithm is much faster than any previously analyzed algorithm for factoring integers; earlier algorithms have all required O(na) operations where a > 1/5.", "authors": ["John D. Dixon"], "id": "41c5341ea5409468e4d68ad14fce820c78bc51b2", "title": "Asymptotically fast factorization of integers", "references": ["b39f2b6ce1014f90992f493d76c00fe06854ca0e", "bade6c18ec70142ba54ef3c0fd885d982335bc08", "f1c215b3953a1e1c315cd4e16b219b395f336b3e", "3e2c03da30f7824376f62a26246e95112fcaacb7", "2f1a5034a2e1257b7ad910d0939114123cd2a676", "95565db6e6afc1cff4b1e8a674d163bd01cecc07", "fe39a6fb771986af24fa02f0ab6c4334148bd1bd"]}, {"date": "1989", "abstract": "Let C(-d) denote the Gauss Class Group of quadratic forms of a negative discriminant -d (or equivalently, the class group of the imaginary quadratic field Q(A/=') ). We give a rigorous proof that there exists a Las Ve- gas algorithm that will compute the structure of C(-d) with an expected running time of L(d) vf2+?(1) bit operations, where L(d) = exp( Vlog d log log d) . Thus, of course, also includes the computation of the class number h(-d), the cardinality of C(-d) . IBM RESEARCH DIvISION, ALMADEN RESEARCH CENTER, K53/802, 650 HARRY ROAD, SAN JOSE, CALIFORNIA 95 120-6099 Current address (K. McCurley): Organization 1423, Sandia National Laboratories, Albuquerque, New Mexico 87185 This content downloaded from 207.46.13.180 on Thu, 08 Sep 2016 05:47:29 UTC All use subject to http://about.jstor.org/terms", "authors": ["James Lee Hafner", "Kevin S. McCurley"], "id": "3e49e5cb285b1cd68ffb392f4b0b3865095fdfd5", "title": "A rigorous subexponential algorithm for computation of class groups", "references": ["6f1d7d966a2a24183919e3da3b7ade68e6c33118", "5c34d232133ac50117a72904521c9caaef7a7e9b", "3bdb5b85fa942bb962f899187c292a55d53f2c04", "95565db6e6afc1cff4b1e8a674d163bd01cecc07", "9b8e6f026ba59e60b5f56e02b641ce0b748f2e03", "c333f4551e8954d25c1c8b2e75602aca7fb3b140", "34b5484f1c02ab46075386ea2aadb8b4167bed9c", "2b8a7573e1b1803cd4a318874c430afb61686c38", "b3eabd361ed308049df1907f3995d19253f145a1", "471fbec52b34c2c1c8499a96bec99048aa0cbcc5"]}, {"date": "1996", "abstract": "Two separated observers, by applying local operations to a supply of not-too-impure entangled states ({\\em e.g.} singlets shared through a noisy channel), can prepare a smaller number of entangled pairs of arbitrarily high purity ({\\em e.g.} near-perfect singlets). These can then be used to faithfully teleport unknown quantum states from one observer to the other, thereby achieving faithful transfrom one observer to the other, thereby achieving faithful transmission of quantum information through a noisy channel. We give upper and lower bounds on the yield $D(M)$ of pure singlets ($\\ket{\\Psi^-}$) distillable from mixed states $M$, showing $D(M)>0$ if $\\bra{\\Psi^-}M\\ket{\\Psi^-}>\\half$.", "authors": ["Bennett", "Brassard", "Popescu", "Schumacher", "Smolin", "Wootters"], "id": "e44193a72ea571878bc157e61a95a9e1dc218dae", "title": "Purification of noisy entanglement and faithful teleportation via noisy channels.", "references": ["9664534349b8b166d7ac341eed2c5ffc52324d97"]}, {"date": "1991", "abstract": "Recently, A.K. Lenstra, H.W. Lenstra, Jr., M.S. Manasse and J .M. Pollard [5,6] have introduced a new algorithm for factoring integers of special form. Based on earlier work of Coppersmith, Odlyzko and Schroeppel [2] and of Pollard [10], the new algorithm, the \u2018numb er field sieve\u2019, is the fastest known for factoring integers of the form r\u2019 + s where e and s are small. In [5], the authors raise the issue of generalizing the number field sieve to produce an efficient algorithm for all numbers. Beginning with the author\u2019s suggestions, together with those of Buhler and Pomerance as reported in [5], we produce such a general purpose number field sieve along with an heuristic argument that it factors numbers in random time: ~(c+o(l))(log n) \u20191 \u2019(log log n)\u2019j\u2019", "authors": ["Leonard M. Adleman"], "id": "67cc7e92c7d3e2be63a4b8de56f1344d9464b5e1", "title": "Factoring numbers using singular integers", "references": ["2fcd6716a5fc5d05d79be86632cacb00ff7ad17c"]}, {"date": "1993", "abstract": "In 1990, the ninth Fermat number was factored into primes by means of a new algorithm, the \u201cnumber field sieve\u201d, which was proposed by John Pollard. The present paper is devoted to the description and analysis of a more general version of the number field sieve. It should be possible to use this algorithm to factor arbitrary integers into prime factors, not just integers of a special form like the ninth Fermat number. Under reasonable heuristic assumptions, the analysis predicts that the time needed by the general number field sieve to factor n is exp((c+o(1))(logn)1/3(loglogn)2/3) (for n \u2192 \u221e), where c=(64/9)1/3=1.9223. This is asymptotically faster than all other known factoring algorithms, such as the quadratic sieve and the elliptic curve method.", "authors": ["Joe Buhler", "Hendrik W. Lenstra", "Carl Pomerance"], "id": "be68e7d79f3dd4d0efe3bad3410e2d3af3451610", "title": "Factoring integers with the number field sieve", "references": ["99ca4a627656f1fdf34712c279ffe8b82778be33", "fff1b293b45d06c8462021aa6c90c81e743e131b", "42cc3b8f279ba771452289deea5476acdc5ea963", "041091d3231ceac8691eb9975ff3b7f17043e06b", "67cc7e92c7d3e2be63a4b8de56f1344d9464b5e1"]}, {"date": "1997", "abstract": "In this paper we study quantum computation from a complexity theoretic viewpoint. Our first result is the existence of an efficient universal quantum Turing machine in Deutsch's model of a quantum Turing machine (QTM) [Proc. Roy. Soc. London Ser. A, 400 (1985), pp. 97--117]. This construction is substantially more complicated than the corresponding construction for classical Turing machines (TMs); in fact, even simple primitives such as looping, branching, and composition are not straightforward in the context of quantum Turing machines. We establish how these familiar primitives can be implemented and introduce some new, purely quantum mechanical primitives, such as changing the computational basis and carrying out an arbitrary unitary transformation of polynomially bounded dimension. \nWe also consider the precision to which the transition amplitudes of a quantum Turing machine need to be specified. We prove that $O(\\log T)$ bits of precision suffice to support a $T$ step computation. This justifies the claim that the quantum Turing machine model should be regarded as a discrete model of computation and not an analog one. \nWe give the first formal evidence that quantum Turing machines violate the modern (complexity theoretic) formulation of the Church--Turing thesis. We show the existence of a problem, relative to an oracle, that can be solved in polynomial time on a quantum Turing machine, but requires superpolynomial time on a bounded-error probabilistic Turing machine, and thus not in the class $\\BPP$. The class $\\BQP$ of languages that are efficiently decidable (with small error-probability) on a quantum Turing machine satisfies $\\BPP \\subseteq \\BQP \\subseteq \\Ptime^{\\SP}$. Therefore, there is no possibility of giving a mathematical proof that quantum Turing machines are more powerful than classical probabilistic Turing machines (in the unrelativized setting) unless there is a major breakthrough in complexity theory.", "authors": ["Ethan Bernstein", "Umesh V. Vazirani"], "id": "c4d295f67e2f70177622771b9884d54ff51792ba", "title": "Quantum Complexity Theory", "references": []}, {"date": "1989", "abstract": "The effect of the environment on a quantum system is studied on an exactly solvable model: a harmonic oscillator interacting with a one-dimensional massless scalar field. We show that in an open quantum system, dissipation can cause decorrelation on a time scale significantly shorter than the relaxation time which characterizes the approach of the system to thermodynamic equilibrium. In particular, we demonstrate that the density matrix decays rapidly toward a mixture of ``approximate eigenstates'' of the ``pointer observable,'' which commutes with the system-environment interaction Hamiltonian. This observable can be regarded as continuously, if inaccurately, monitored by the scalar field environment. Both because in a harmonic oscillator the state of the system rotates in the phase space and because the effective environment ``measurement'' is weak, the system, on the short ``collision'' time scale (1/\\ensuremath{\\Gamma}), maintains a coherence in this pointer observable on time scales of order [\\ensuremath{\\gamma}/\\ensuremath{\\Omega}ln(\\ensuremath{\\Gamma}/\\ensuremath{\\Omega}${)]}^{1/2}$ and on longer time scales settles into a mixture of coherent states with a dispersion approximately consistent with the vacuum state. The master equation satisfied by the exact solution differs from the other master equations derived both for the high-temperature limit and for T=0. We discuss these differences and study the transition region between the high- and low-temperature regimes. We also consider the behavior of the system in the short-time ``transient'' regime. For T=0, we find that, in the long-time limit, the system behaves as if it were subject to ``1/f noise.'' The generality of our model is considered and its predictions are compared with previous treatments of related problems. Some of the possible applications of the results to experimentally realizable situations are outlined. The significance of the environment-induced reduction of the wave packet for cosmological models is also briefly considered.", "authors": ["Unruh", "Zurek"], "id": "2aebd785cd9072b7d3566a5ec14045544010ddd1", "title": "Reduction of a wave packet in quantum Brownian motion.", "references": []}, {"date": "1983", "abstract": "Let f(n) denote the number of factorizations of the natural number n into factors larger than 1 where the order of the factors does not count. We say n is \u201chighly factorable\u201d if f(m)<f(n) for all m < n. We prove that f(n)=n\u00b7L(n)\u22121+0(1) for n highly factorable, where L(n)=exp{log n logloglog nloglog n}. This result corrects the 1926 paper of Oppenheim where it is asserted that f(n)=n\u00b7L(n)\u22122+0(1). Some results on the multiplicative structure of highly factorable numbers are proved and a table of them up to 109 is provided. Of independent interest, a new lower bound is established for the function \u03a8(x, y), the number of n\u2264x free of prime factors exceeding y.", "authors": ["E. Rodney Canfield", "Paul Erd\u00f6s", "Carl Pomerance"], "id": "041091d3231ceac8691eb9975ff3b7f17043e06b", "title": "On a problem of Oppenheim concerning \u201cfactorisatio numerorum\u201d", "references": ["6a1cdadd4bee07140a20a52447bfbc89fc16bacc", "16066daee51d474e1e0085c61da61b7907bb5dc5", "f1c215b3953a1e1c315cd4e16b219b395f336b3e", "c718e776751d477e77e64b399a4c40799fd05030", "f7ba69c1faa0187fd2366c9745bb4a387f7439ec", "d04b6dbe86916fbb08b8ab09aec7efc503806714", "e5eeff4540b8dd3512ea2c525f25d12506bbea06"]}, {"date": "1984", "abstract": "Given a primitive element g of a finite field GF(q), the discrete logarithm of a nonzero element u ? GF(q) is that integer k, 1 ? k ? q-1, for which u = gk. The well-known problem of computing discrete logarithms in finite fields has acquired additional importance in recent years due to its applicability in cryptography. Several cryptographic systems would become insecure if an efficient discrete logarithm algorithm were discovered. This paper surveys and analyzes known algorithms in this area, with special attention devoted to algorithms for the fields GF(2n). It appears that in order to be safe from attacks using these algorithms, the value of n for which GF(2n) is used in a cryptosystem has to be very large and carefully chosen. Due in large part to recent discoveries, discrete logarithms in fields GF(2n) are much easier to compute than in fields GF(p) with p prime. Hence the fields GF(2n) ought to be avoided in all cryptographic applications. On the other hand, the fields GF(p) with p prime appear to offer relatively high levels of security.", "authors": ["Andrew M. Odlyzko"], "id": "ec30a15d158dc6c68b32c614ec00adf5f6597915", "title": "Discrete Logarithms in Finite Fields and Their Cryptographic Significance", "references": ["f2d0b8891072e9c06292635c3928e066f61a5792", "7f4db62644db95188f8fc718c071e288ca438741", "2c80714c83bd08ee14f3428cc0031dedbc1a824e", "0c3fb997aef182a957c102e1a4232eca55af703f", "742826a56c402c68e81d2cc4db4edae3126fde8c", "090d6292a813de8067d63bbd3d764a3e05c44906", "d30cf05d70b0598efb8b137bd5029d34da658f00", "cfdf5554c0ee27850ce7edb475225cb0bf12c846", "3ffc9414d0288832d057c9e4fce0822182404fca", "67e22bf02ee3b6511e0bd4b4baf6b032824c9ae6"]}, {"date": "", "abstract": "The presumed difficulty of compoting di~rete logarithms in finite fields is the basis of several popular public key cryptosystems. The secure identification option of the Sun Network File System, for example, uses discrete logarithms in a field GF(p) with p a prime of 192 bits. This paper describes an implementation of a discrete logarithm algorithm which shows that primes of under 200 bits, such as that in the Sun system, are very insecure. Same enhancements to this system are suggested.", "authors": ["A. B."], "id": "fe0906e03890ca30ba216f7290b8bbc01d618c3d", "title": "Computation of Discrete Logarithms in Prime Fields", "references": []}, {"date": "1990", "abstract": "Let GF(pn) be the finite field with pn elements, where p is prime. We consider the problem of how to deterministically generate in polynomial time a subset of GF(pn) that contains a primitive root, i.e., an element that generates the multiplicative group of nonzero elements in GF(pn) . We present three results. First, we present a solution to this problem for the case where p is small, i.e., p = n-d) . Second, we present a solution to this problem under the assumption of the Extended Riemann Hypothesis (ERH) for the case where p is large and n = 2. Third, we give a quantitative improvement of a theorem of Wang on the least primitive root for GF(p), assuming the ERH.", "authors": ["Victor Shoup"], "id": "e06342830838bf0392ca10df2309d6af86e315fb", "title": "Searching for primitive roots in finite fields", "references": ["c97f166c97b68e41407e3371f8577d67faec3919", "2827ff9c25dfbc7beeb7a312271c4d9b177a1135"]}, {"date": "1994", "abstract": "Abstract Building on the work of Deutsch and Jozsa, we construct oracles relative to which (1) there is a decision problem that can be solved with certainty in worst-case polynomial time on the quantum computer, yet it cannot be solved classically in probabilistic expected polynomial time if errors are not tolerated, nor even in nondeterministic polynomial time, and (2) there is a decision problem that can be solved in exponential time on the quantum computer, which requires double exponential time on all but finitely many instances on any classical deterministic computer.", "authors": ["Andr\u00e9 Berthiaume", "Gilles Brassard"], "id": "2a24d6c63876ed37b8e553fa905520dcaa19f3c6", "title": "Oracle Quantum Computing", "references": []}, {"date": "1988", "abstract": "Entropy=Uncertainty=Information The noiseless coding theorem for memoryless sources Communication through noisy channels Error-correcting codes General sources The structure of natural languages Cryptosystems The one-time pad and linear shift-register sequences Computational complexity One-way functions Public key cryptosystems Authentication and digital signatures Randomized encryption Appendices Answers to exercises Answers and hints to problems References Index.", "authors": ["Dominic J. A. Welsh"], "id": "b53cf07a342c839f7ac7b8f10e0b710776621886", "title": "Codes and cryptography", "references": []}, {"date": "1995", "abstract": "Semantic Scholar extracted view of \"Quantum Teleportation and Quantum Computation Based on Cavity QED\" by Tycho Sleator et al.", "authors": ["Tycho Sleator", "Harald Weinfurter"], "id": "12d9677bdc0768a12d89e00b411a3552037e1fd5", "title": "Quantum Teleportation and Quantum Computation Based on Cavity QED", "references": []}, {"date": "1965", "abstract": "A variable displacement axial piston pump has a rocker cam for controlling the output of the pump. The rocker cam is driven by a fluid motor member. A valve with a follow-up device regulates pressure fluid flow to the fluid motor member to move the rocker cam. Both the fluid motor member and a part of the follow-up device are rigidly secured to and movable with the rocker cam to provide precise positioning of the rocker cam.", "authors": ["Gerhard P. Hochschild"], "id": "5351a0830445fc0d8f6698997a0dd847ebf82457", "title": "The structure of Lie groups Holden-Day", "references": []}, {"date": "1991", "abstract": "Semantic Scholar extracted view of \"Elements of information theory.john wiley & sons\" by Thomas M. Cover et al.", "authors": ["Thomas M. Cover", "Jeffrey A. Thomas"], "id": "f434ba244af3ea490f5eaa49556091d00b851abd", "title": "Elements of information theory.john wiley & sons", "references": []}, {"date": "1990", "abstract": "Publisher Summary This chapter discusses algorithms that solve two basic problems in computational number theory\u2014factoring integers into prime factors and finding discrete logarithms. In the factoring problem, one is given an integer n 1 and is asked to find the decomposition of n into prime factors. It is common to split this problem into two parts. The first is called primality testing: given n , it is determined whether n is prime or composite. The second is called factorization: if n is composite, a nontrivial divisor of n is to be calculated. In the discrete logarithm problem, one is given a prime number p , and two elements h, y of the multiplicative group F* p of the field of integers modulo p. The algorithms and their analyses depend on many different parts of number theory. Number theory is considered the purest of all sciences, and within number theory the hunt for large primes and for factors of large numbers has always been remote from applications, even to other questions of a number-theoretic nature.", "authors": ["Arjen K. Lenstra", "Hendrik W. Lenstra"], "id": "471fbec52b34c2c1c8499a96bec99048aa0cbcc5", "title": "Algorithms in Number Theory", "references": []}, {"date": "1995", "abstract": "Semantic Scholar extracted view of \"Two-Bit Gates Are Universal for Quantum Computation\" by David P. Di Vincenzo", "authors": ["David P. Di Vincenzo"], "id": "e51fe114b7e061c5199133dd504a928d803460f4", "title": "Two-Bit Gates Are Universal for Quantum Computation", "references": []}, {"date": "1994", "abstract": "An algorithmic proof that any discrete finite-dimensional unitary operator can be constructed in the laboratory using optical devices is given. Our recursive algorithm factorizes any N\\ifmmode\\times\\else\\texttimes\\fi{}N unitary matrix into a sequence of two-dimensional beam splitter transformations. The experiment is built from the corresponding devices. This also permits the measurement of the observable corresponding to any discrete Hermitian matrix. Thus optical experiments with any type of radiation (photons, atoms, etc.) exploring higher-dimensional discrete quantum systems become feasible.", "authors": ["Reck", "Zeilinger", "Bernstein", "Bertani"], "id": "627f209380b07cf3561060041a838d1da6a97183", "title": "Experimental realization of any discrete unitary operator.", "references": []}, {"date": "1991", "abstract": "Semantic Scholar extracted view of \"On an irreducibility theorem of I\" by Michael Filaseta", "authors": ["Michael Filaseta"], "id": "99ca4a627656f1fdf34712c279ffe8b82778be33", "title": "On an irreducibility theorem of I", "references": []}, {"date": "1987", "abstract": "We propose a probabilistic algorithm for factorization of an integer N with run time (expVlog N log log N)V54 +?(1). Asymptotically, our algorithm will be as fast as the wellknown factorization algorithm of Morrison and Brillhart. The latter algorithm will fail in several cases and heuristic assumptions are needed for its run time analysis. Our new algorithm will be analyzed under the assumption of the Extended Riemann Hypothesis and it will be of Las Vegas type. On input N, the new algorithm will factor N with probability > 12 In case of prime N the algorithm will prove the primality of N with probability > 2 Introduction. Until the last decade, the centuries-old problem of factoring integers was mainly a problem for specialists. Worldwide interest in factoring integers increased dramatically in 1978, when Rivest, Shamir, and Adleman [32] published their public key cryptosystem, whose security relies on the fact that some large integers are hard to factor. Gauss [9] already discovered a close connection between the factorization of a natural number N and the theory of quadratic forms of discriminant 4N. Now, quadratic forms are one of the most important tools for factoring integers. Examples for efficient factorization algorithms are (among others) the algorithms of Morrison and Brillhart [26] and Lenstra and Schnorr [22]. The former works with the continued fraction expansion of FN (which is closely related to the theory of quadratic forms of discriminant 4N, see [20]) while the latter works with quadratic forms of discriminant 4N. At present, the most efficient factorization algorithm is the quadratic sieve algorithm, see [29], which can also be expressed in terms of quadratic forms. For an overview of modem factorization algorithms we refer to the papers of Guy [10], Monier [25], and Pomerance [29]. A deeper understanding of the theory of quadratic forms is of great importance for the analysis of modem factorization algorithms. In this paper we shall only deal with the theory of quadratic forms of negative discriminant, which is considerably more simple than the theory of forms of positive discriminants. Using this theory, we obtain a probabilistic factorization algorithm with run time (exp Vlog N log log N )V4 (in this paper we denote by log X the natural logarithm of X). We have 5/4 1.118. Received May 17, 1985; revised June 12, 1986. 1980 Mathematics Subject Classification. Primary 10A30, 68C25. ?)1987 American Mathematical Society 0025-5718/87 $1.00 + $.25 per page", "authors": ["Martin Seysen"], "id": "b3eabd361ed308049df1907f3995d19253f145a1", "title": "A probabilistic factorization algorithm with quadratic forms of negative discriminant", "references": ["6a4ef42f1f2d709c24c44e9f9594f7069274d4b5", "307ab08c3d4f551019297d2480597c614af8069c", "b39f2b6ce1014f90992f493d76c00fe06854ca0e", "dfdc1499dc7e7240bfc872e5e07f8c2c3711d075", "43fa769993fd8ba599aac30535ab7778779683fb", "8ce0208a77da162c1680f8bb6509843f4ad70764", "cb80b424db4c94cbaf4c3ae0e570ac3eb6f3bcf3", "3611b8ec246bb0473818eb8d908b23b4ca4880d6", "742826a56c402c68e81d2cc4db4edae3126fde8c"]}, {"date": "1990", "abstract": "We present a new method for accelerating matrix multiplication asymptotically. Thiswork builds on recent ideas of Volker Strassen, by using a basic trilinear form which is not a matrix product. We make novel use of the Salem-Spencer Theorem, which gives a fairly dense set of integers with no three-term arithmetic progression. Our resulting matrix exponent is 2.376.", "authors": ["Don Coppersmith", "Shmuel Winograd"], "id": "2b8a7573e1b1803cd4a318874c430afb61686c38", "title": "Matrix Multiplication via Arithmetic Progressions", "references": []}, {"date": "1990", "abstract": "Many number-theoretic algorithms rely on a result of Ankeny, which states that if the Extended Riemann Hypothesis (ERH) is true, any nontrivial multiplicative subgroup of the integers modulo m omits a number that is O(log2 m) . This has been generalized by Lagarias, Montgomery, and Odlyzko to give a similar bound for the least prime ideal that does not split completely in an abelian extension of number fields. This paper gives a different proof of this theorem, in which explicit constants are supplied. The bounds imply that if the ERH holds, a composite number m has a witness for its compositeness (in the sense of Miller or Solovay-Strassen) that is at most 2 log2m .", "authors": ["Eric Bach"], "id": "42cc3b8f279ba771452289deea5476acdc5ea963", "title": "Explicit bounds for primality testing and related problems", "references": ["6a4ef42f1f2d709c24c44e9f9594f7069274d4b5", "aad4d7a59c5b2aca4f66876c14c969f079435bce", "52b4d3640a77ef604f5b28816ed28de04ea520e2", "9bbc124353ec889ca43500682225b57116c4fed1", "771251221fb6faf11c52e5e6d79ac8b16509e896", "f099727e914acd49527a39f1ebc9fc5f9a6cc60a", "2f1a5034a2e1257b7ad910d0939114123cd2a676", "2db110b987299fe1992daa3ed2d966d4a61109c5", "b3eabd361ed308049df1907f3995d19253f145a1", "8cc8e59404f67d358e68d551d3931e3fa5e7a700"]}, {"date": "1990", "abstract": "The number field sieve is an algorithm to factor integers of the form re \u2212 s for small positive r and |s|. The algorithm depends on arithmetic in an algebraic number field. We describe the algorithm, discuss several aspects of its implementation, and present some of the factorizations obtained. A heuristic run time analysis indicates that the number field sieve is asymptotically substantially faster than any other known factoring method, for the integers that it applies to. The number field sieve can be modified to handle arbitrary integers. This variant is slower, but asymptotically it is still expected to beat all older factoring methods.", "authors": ["Arjen K. Lenstra", "Hendrik W. Lenstra", "Mark S. Manasse", "John M. Pollard"], "id": "2fcd6716a5fc5d05d79be86632cacb00ff7ad17c", "title": "The number field sieve", "references": []}, {"date": "1982", "abstract": "At the 1981 IEEE Symposium on Information Theory, T. Herlestam and R. Johannesson presented a heurestic method for computing logarithms over GF(2p). They reported computing logarithms over GF(23 !) with surprisingly few iterations and claimed that the running time of their algorithm was polynomial in p. If this were true, the algorithm could be used to cryptanalyze the Pohlig-Hellman cryptosystem, currently in use by Mitre Corporation for key distribution. The Mitre system operates in GF(2127). However, the algorithm was not implemented for GF(2p) for p > 31 because it would require multiple precision arithmetic. Consequently attempts to evaluate the possible threat to the Pohlig-Hellman cryptosystem have centered on modeling the algorithm so that some predictions could be made analytically about the number of iterations required to find logarithms over GF(2P) for p > 31.", "authors": ["Ernest F. Brickell", "Judy H. Moore"], "id": "67e22bf02ee3b6511e0bd4b4baf6b032824c9ae6", "title": "Some Remarks on the Herlestam-Johannesson Algorithm for Computing Logarithms over GF(2p)", "references": []}, {"date": "1978", "abstract": "A cryptographic system is described which is secure if and only if computing logarithms over GF(p) is infeasible. Previously published algorithms for computing this function require O(p^{1/2}) complexity in both time and space. An improved algorithm is derived which requires O =(\\log^{2} p) complexity if p - 1 has only small prime factors. Such values of p must be avoided in the cryptosystem. Constructive uses for the new algorithm are also described.", "authors": ["Stephen C. Pohlig", "Martin E. Hellman"], "id": "f2d0b8891072e9c06292635c3928e066f61a5792", "title": "An improved algorithm for computing logarithms over GF(p) and its cryptographic significance (Corresp.)", "references": []}, {"date": "1984", "abstract": "A method for determining logarithms in GF (2^{n}) is presented. Its asymptotic running time is O(\\exp (cn^{1/3} \\log^{2/3} n)) for a small constant c , while, by comparison, Adleman's scheme runs in time O(\\exp (c^{'}n^{1/2} \\log^{1/2} n )) . The ideas give a dramatic improvement even for moderate-sized fields such as GF (2^{127}) , and make (barely) possible computations in fields of size around 2^{400} . The method is not applicable to GF (q) for a large prime q .", "authors": ["Don Coppersmith"], "id": "3ffc9414d0288832d057c9e4fce0822182404fca", "title": "Fast evaluation of logarithms in fields of characteristic two", "references": ["ba9d15ca143d2fefe150169243f855c151aa1fb0", "4f942cd0174a928b56245cd639d488e195baa60d", "0c3fb997aef182a957c102e1a4232eca55af703f", "039e91884114503774d6b70708a6d42e989e3a18", "6eb53aebf5000091d191976a5c042d65a39f7b30", "090d6292a813de8067d63bbd3d764a3e05c44906", "ec30a15d158dc6c68b32c614ec00adf5f6597915", "dcb594aa6410426c6d966890a2047c1b1ab2059d", "cfdf5554c0ee27850ce7edb475225cb0bf12c846", "fbac9705651228c870c879f7c130f8676767408a"]}, {"date": "1963", "abstract": "Semantic Scholar extracted view of \"Character sums and primitive roots in finite fields\" by Harold Davenport et al.", "authors": ["Harold Davenport", "Donald Joseph Lewis"], "id": "c97f166c97b68e41407e3371f8577d67faec3919", "title": "Character sums and primitive roots in finite fields", "references": []}, {"date": "1986", "abstract": "I. In troduc t ion Irreducible polynomials in Fp[X] are used to carry out the arithmetic in field extension of Fp. Computations in such extensions occur in coding theory [2], complexity theory [8] and cryptography [3] . Random polynomial time algorithms exist for finding irreducible polynomials of any degree over Fp [2, 8], and so as a practical matter the problem is solved. However, the deterministic complexity of the problem has yet to be established.", "authors": ["Leonard M. Adleman", "Hendrik W. Lenstra"], "id": "2827ff9c25dfbc7beeb7a312271c4d9b177a1135", "title": "Finding irreducible polynomials over finite fields", "references": []}, {"date": "", "abstract": "In this paper we present a new, heuristic method for computing logarithms over GF(2P). When 2P-1 is a Mersenne prime <231-1 it works in very short running times on a general purpose computer. It is based on the interdependent relations f,~(t) = t-2~f(t) 2\" and log f~s(t) = 2' + 2\" log f ( t ) , where f and frs are polynomials over GF(2). Its cryptographic significance is discussed and it can be considered as an attempt to swindle MITRE Corporation which reportedly is using a public key distribution system, based on the presumed difficulty in computing logarithms over GF(2127).", "authors": ["Tore Herlestam"], "id": "d30cf05d70b0598efb8b137bd5029d34da658f00", "title": "ON COMPUTING LOGARITHMS OVER GF ( 2 p )", "references": []}, {"date": "1979", "abstract": "In 1870 Bouniakowsky [2 J publ ished an algorithm to solve the congruence aX _ bMOD (q). While his algorithm contained several clever ideas useful for small numbers, its asymptotic complexity was O(q). Despite its long history, no fast algorithm has ever emerged for the Discrete Logarithm Problem and the best published method, due to Shanks [lOJ requires O(ql/2) in time and space. The problem has attracted renewed interest in recent years because of its use in cryptography [7 ], [15J,[19J. In particular, the security of the Diffie-Hellman Public Key Distribution Sy s t em [7 J II de pen d s c r ucia 11yon the d iff i c u1t Y 0 f com put i ng log a r i t hms MOD q II \u2022 We present a new algorithm for this problem which runs in RTIME better than O(qE) for all E > O.t While no effort is made to present the most efficient incarnation of tActually our algorithm runs in RTIME O(2(O(/10g(q)loglog(q))). RTIME denotes Random Time and refers to algorithms which may use random numbers in their processing. For example, the well known composite testing algorithms of Solovay &Strassen [21J, Miller [11J and Rabin [16J run in RTIME (0(log3(q))). For precise definitions see [1], [llJ and [9J.", "authors": ["Leonard M. Adleman"], "id": "cfdf5554c0ee27850ce7edb475225cb0bf12c846", "title": "A subexponential algorithm for the discrete logarithm problem with applications to cryptography", "references": ["6a1cdadd4bee07140a20a52447bfbc89fc16bacc", "2842d53d0f2389618f8ad1c47d2ffef2c344d22f", "7c769809967552e2ec1af14e0d547569fe25acc9", "6eb53aebf5000091d191976a5c042d65a39f7b30", "4e09314d22260dc30db8e93fd26313feae402a28", "fbac9705651228c870c879f7c130f8676767408a"]}, {"date": "1982", "abstract": "The Merkte-Adleman algorithm computes discrete logarithms in GF (q),the finite field with q elements, in subexponential time, when q is a prime number p. This paper shows that similar asymptotic behavior can be obtained for the logarithm problem when q = p m , in the case that m grows with p fixed. A method of partial precomputation, applicable to either problem, is also presented. The precomputation is particularly useful when many logarithms need to be computed for fixed values of p and m.", "authors": ["Martin E. Hellman", "Justin M. Reyneri"], "id": "090d6292a813de8067d63bbd3d764a3e05c44906", "title": "Fast Computation of Discrete Logarithms in GF(q)", "references": []}, {"date": "1984", "abstract": "A simple algorithm to find logarithms in a finite field of characteristic two is described. It uses the Euclidean algorithm for polynomials in attempting to reduce an element to a product of factors all of whose logarithms are stored in a database. The algorithm, which is similar to one of Adleman, has a random runtime and constant storage requirements. It is analyzed and problems associated with the construction of the database are considered. The aim of the work is to show that the algorithm is feasible for the field with $2^{127} $ elements on which several proposed public key distribution systems have been based. For such application it is felt that the discrete logarithm is still a viable technique for sufficiently large fields.", "authors": ["Ian F. Blake", "Ryoh Fuji-Hara", "R. C. Mullin", "Scott A. Vanstone"], "id": "0c3fb997aef182a957c102e1a4232eca55af703f", "title": "Computing Logarithms in Finite Fields of Characteristic Two", "references": []}, {"date": "1984", "abstract": "Electronic messages, documents and checks must be authenticated by digital signatures which are not forgeable even by their recipients. The RSA system can generate and verify such signatures, but each message requires hundreds of high precision modular multiplications which can be implemented efficiently only on special purpose hardware. In this paper we propose a new signature scheme which can be easily implemented in software on microprocessors: signature generation requires one modular multiplication and one modular division, signature verification requires three modular multiplications, and the key size is comparable to that of the RSA system. The new scheme is based on the quadratic equation m &equil; s21 + ks22 (mod n), where m is the message, s1 and s2 are the signature, and k and n are the publicly known key. While we cannot prove that the security of the scheme is equivalent to factoring, all the known methods for solving this quadratic equation for arbitrary k require the extraction of square roots modulo n or the solution of similar problems which are at least as hard as factoring. A novel property of the new scheme is that legitimate users can choose k in such a way that they can sign messages even without knowing the factorization of n, and thus everyone can use the same modulus if no one knows its factorization.", "authors": ["H. Ong", "Claus-Peter Schnorr", "Adi Shamir"], "id": "2c80714c83bd08ee14f3428cc0031dedbc1a824e", "title": "An efficient signature scheme based on quadratic equations", "references": []}, {"date": "1979", "abstract": "Let a messageM be encrypted by raisingM to a powere moduloR, whereR ande are integers which are made public. The recipient of this encrypted form ofM can decipher it by raising the cipher text to a powerd moduloR. Only the recipient knows the values of the two large primesp1,p2 such thatR=p1p2; consequently, only he knowsd, ase is preselected such that (e, (p1 \u2212 1)(p2 \u2212 1))=1 anded \u22611 (mod (p1 \u2212 1)(p2 \u2212 1)).Recently several attacks have been made on the proposed security of this cryptosystem under iteration of the encryption procedure. In this paper we discuss methods of selecting the primesp1,p2 and the encryption exponente such that the possibility of breaking this cryptosystem by using an iteration procedure is minimized. Several numerical results are also presented.", "authors": ["Hugh C. Williams", "Bruce K Schmid"], "id": "7f4db62644db95188f8fc718c071e288ca438741", "title": "Some remarks concerning the M.I.T. public-key cryptosystem", "references": []}, {"date": "1984", "abstract": "We present an algorithm which will factor an integer n quite efficiently if the class number h(-n) is free of large prime divisors. The running time T(n) (number of composi- tions in the class group) satisfies prob(7\"(m) (r - 2)~ for random m e (n/2, n) and r > 2. So far it is unpredictable which numbers will be factored fast. Running the algorithm on all discriminants - ns with s < r' and r = ^/ln ?i/ln In n , every composite integer n will be factored in o(exp^ln n In In n) bit operations. The method requires an amount of storage space which is proportional to the length of the input n. In our analysis we assume a lower bound on the frequency of class numbers h(-m), m < n, which are free of", "authors": ["Claus Peter Schnorr", "Hendrik W. Lenstra"], "id": "742826a56c402c68e81d2cc4db4edae3126fde8c", "title": "A Monte Carlo factoring algorithm with linear storage", "references": ["95565db6e6afc1cff4b1e8a674d163bd01cecc07", "43fa769993fd8ba599aac30535ab7778779683fb", "8052adc20ffb2f284bc31160874721d6943bcb78", "f15eba0892e081b46d9ca551b2f74965bb08b691", "dfdc1499dc7e7240bfc872e5e07f8c2c3711d075"]}, {"date": "1941", "abstract": "ON SOME ASYMPTOTIC FORMULAS IN THE THEORY OF THE \"FACTORISATIO NUMERORUM\" BY P. ERD\u00d6S (Received December 2, 1940) Let 1 < a, < a2 < . . . be a sequence of integers . Denote by f (n) the number of representations of n as the product of the a's, where two representations are considered equal only if they contain the same factors in the same order . As far as I know the first papers written on the subject are those of L . Kalm\u00e1r,' who proved by using the methods of analytic number theory that if at _ k + l then", "authors": ["Paul Erd\u00f6s"], "id": "e5eeff4540b8dd3512ea2c525f25d12506bbea06", "title": "On Some Asymptotic Formulas in The Theory of The ", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Dirichlet Series and the Theory of Partitions\" by P. A. Macmahon", "authors": ["P. A. Macmahon"], "id": "d04b6dbe86916fbb08b8ab09aec7efc503806714", "title": "Dirichlet Series and the Theory of Partitions", "references": []}, {"date": "1980", "abstract": "Abstract The problem of factoring an integer and many other number-theoretic problems can be formulated in terms of binary quadratic Diophantine equations. This class of equations is also significant in complexity theory, subclasses of it having provided most of the natural examples of problems apparently intermediate in difficulty between P and NP -complete problems, as well as NP -complete problems [2, 3, 22, 26]. The theory of integral quadratic forms developed by Gauss gives some of the deepest known insights into the structure of classes of binary quadratic Diophantine equations. This paper establishes explicit polynomial worst-case running time bounds for algorithms to solve certain of the problems in this theory. These include algorithms to do the following: (1) reduce a given integral binary quadratic form; (2) quasi-reduce a given integral ternary quadratic form; (3) produce a form composed of two given integral binary quadratic forms; (4) calculate genus characters of a given integral binary quadratic form, when a complete prime factorization of its determinant D is given as input; (5) produce a form that is the square root under composition of a given form (when it exists), when a complete factorization of D and a quadratic nonresidue for each prime dividing D is given as input.", "authors": ["Jeffrey C. Lagarias"], "id": "3611b8ec246bb0473818eb8d908b23b4ca4880d6", "title": "Worst-Case Complexity Bounds for Algorithms in the Theory of Integral Quadratic Forms", "references": []}, {"date": "1980", "abstract": "When a person thinks of algebra, they typically think of a process used to solve polynomial equations. Modern Number theory has evolved through several stages in the past two millennia. Notions of primality and divisibility are indeed quite classical, and the ancients even knew a great deal about some relatively sophisticated ideas: the infinitude of primes, perfect numbers, etc. Innovation sat stagnant for about 15 centuries until, around 1625, Fermat acquired a copy of Diophantus and started pursuing the subject in earnest. Even this work might have fallen to the wayside had Euler not continued Fermat\u2019s work about 100 years later. Since that time, many mathematical greats have made significant contributions to the subject and discovered surprising connections between number theory and almost all other branches of mathematics. Perhaps the most striking feature of modern (i.e., post-Fermat) number theory is the divide between the simplicity of theorem statements and the complexity of techniques used to prove them.", "authors": ["Harvey Cohn"], "id": "8cc8e59404f67d358e68d551d3931e3fa5e7a700", "title": "Advanced Number Theory", "references": []}, {"date": "1987", "abstract": "We propose a new deterministic method of factoring polynomials over finite fields. Assuming the Generalized Riemann Hypothesis (GRH), we obtain, in polynomial time, the factorization of any polynomial with a bounded number of irreducible factors. Other consequences include a polynomial time algorithm to find a nontrivial factor of any completely splitting even degree polynomial when a quadratic nonresidue in the field is given.", "authors": ["Lajos R\u00f3nyai"], "id": "f099727e914acd49527a39f1ebc9fc5f9a6cc60a", "title": "Factoring polynomials over finite fields", "references": ["71937f0cd51dff739c91ad39c3af82b7341d3f39", "82b871549e90c4ed297fe4f22ccb9645e4f2c571", "3fd4decdfb3722e57bafb1fd591c40b1e2166b2a", "fce6d0219ba28db87890b2cbbb80c61c8702a829"]}, {"date": "1971", "abstract": "Three basic principles.- The large sieve.- Arithmetic formulations of the large sieve.- A weighted sieve and its application.- A lower bound of Roth.- Classical mean value theorems.- New mean value theorems.- Large moduli theorems.- Further results and conjectures concerning mean and large moduli.- Mean moduli of L-functions.- Zero-free regions and the proliferation of zeros.- Distribution of zeros of L-functions.- Least character non-residues and arg L(12+it, x).- The prime number theorems of Hoheisel and Selberg.- The bombieri - Vinogradov theorem.- A lemma in additive prime number theory.- The mean value theorem of Barban.", "authors": ["Hugh L. Montgomery"], "id": "2db110b987299fe1992daa3ed2d966d4a61109c5", "title": "Topics in Multiplicative Number Theory", "references": []}, {"date": "1979", "abstract": "We describe a computation which shows that the Riemann zeta function \u03b6(s) has exactly 75,000,000 zeros of the form \u03c3+ it in the region 0 < t < 32,585,736.4; all these zeros are simple and lie on the line \u03c3 = 1/2. (A similar result for the first 3,500,000 zeros was established by Rosser, Yohe and Schoenfeld.) Counts of the number of Gram blocks of various types and the number of failures of \u201cRosser\u2019s rule\u201d are given. Comments Only the Abstract is given here. The full paper appeared as [1]. For further work, see [2, 3].", "authors": ["R. P. Brent"], "id": "771251221fb6faf11c52e5e6d79ac8b16509e896", "title": "On the zeros of the Riemann zeta function in the critical strip", "references": []}, {"date": "1985", "abstract": "This book makes a substantial contribution to the understanding of a murky area of number theory that is important to computer science, an area relevant to the design and analysis of number-theoretic algorithms and to the construction of cryptographic protocols.Contents: Introduction; 1: Explicit Bounds for Primality Testing; Ankeny's Theorem and its Algorithmic Consequences; Background from Analytic Number Theory; Roots; Asymptotic Theorems; Zeta-function Estimates; Numerical Theorems; Computing Bounds for Specific Moduli; Comparisons with Empirical Results; 2: The Generation of Random Factorizations; Introduction; A Method That Almost Works; Doctoring the Odds; A Factor Generation Procedure; The Complete Algorithm; 2.5 Bounds for the Number of Prime Tests; A Single-precision Time Bound; The Use of Probabilistic Primality Tests.Eric Bach received his doctorate from the University of California at Berkeley. He is currently an Assistant Professor of Computer Science at the University of Wisconsin at Madison. \"Analytic Methods in the Analysis and Design of Number Theoretic Algorithms\" is a 1984 ACM Distinguished Dissertation.", "authors": ["Eric Bach"], "id": "9bbc124353ec889ca43500682225b57116c4fed1", "title": "Analytic methods in the analysis and design of number-theoretic algorithms", "references": []}, {"date": "1982", "abstract": "The main results of this paper have the following flavor: Given one algorithm for multiplying matrices, there exists another, better, algorithm. A consequence of these results is that $\\omega $, the exponent for matrix multiplication, is a limit point, that is, it cannot be realized by any single algorithm. We also use these results to construct a new algorithm which shows that $\\omega < 2.495548$.", "authors": ["Don Coppersmith", "Shmuel Winograd"], "id": "dcb594aa6410426c6d966890a2047c1b1ab2059d", "title": "On the Asymptotic Complexity of Matrix Multiplication", "references": []}, {"date": "1973", "abstract": "A cryptographic system is described which is secure if and only if computing logarithms over GF(p) is infeasible. Previously published algorithms for computing this function require O(P\u2019/~) complexity in both time and space. An improved algorithm is derived which requires O(log2 p) complexity if p 1 has only small prime factors. Such values of p must be avoided in the cryptosystem. Constructive uses for the new algorithm are also described.", "authors": ["Stephen C. Pohlig"], "id": "fbac9705651228c870c879f7c130f8676767408a", "title": "An Improved Algorithm for Computing Logarithms over GP(p) and Its Cryptographic Significance", "references": ["f3143ff6569f3b9b01cbe34deea53a1d3b3c84c2", "ef9fdc459195614fb967fcbf93674bc56baeda00"]}, {"date": "", "abstract": "Semantic Scholar extracted view of \"B\" by B. A. Romanistik Kernfach et al.", "authors": ["B. A. Romanistik Kernfach", "Nur IM Sommersemester", "\u00dcbersicht Franz\u00f6sisch", "\u00dcbersicht Italienisch"], "id": "fff1b293b45d06c8462021aa6c90c81e743e131b", "title": "B", "references": ["41b52a83ae1c27fad5ce9dd21d0979527db627b5", "51cd6f4d151aef0f77f81f5f664d4ee74db17927", "4ad5ff44b8210b99e7451fb959941b13868f079d", "98579c1c831b20ae1554df43bbe1db6eccc6dac9"]}, {"date": "1975", "abstract": "The continued fraction method for factoring integers, which was introduced by D. H. Lehmer and R. E. Powers, is discussed along with its computer implementation. The power of the method is demonstrated by the factorization of the seventh Fermat number F7 and other large numbers of interest. \"Quand on a a' etudier un grand nombre, il faut commencer par en determiner quelques residus quadratiques.\" M. Kraitchik", "authors": ["Michael A. Morrison", "John Brillhart"], "id": "6eb53aebf5000091d191976a5c042d65a39f7b30", "title": "A method of factoring and the factorization of", "references": ["df609be3030041a10ce416b8447461cfd3e5a4d2", "95565db6e6afc1cff4b1e8a674d163bd01cecc07", "5c6adb7113bab80c5432bce614aea5b57dafc235"]}, {"date": "1974", "abstract": "We present a method for expressing a root of one irreducible polynomial of degree n over GF(2) in terms of a basis of GF(2n) over GF(2) associated with another. This allows us, when both polynomials are primitive, to find logarithms relative to one polynomial from logarithms relative to the other.", "authors": ["Neal Zierler"], "id": "039e91884114503774d6b70708a6d42e989e3a18", "title": "A conversion algorithm for logarithms on GF(2n)", "references": []}, {"date": "1951", "abstract": "\u2022 A submitted manuscript is the author's version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers.", "authors": ["de Ng Dick Bruijn"], "id": "7c769809967552e2ec1af14e0d547569fe25acc9", "title": "On The Number of Positive Integers \u2264 x and Free of Prime Factors > y", "references": []}, {"date": "1981", "abstract": "in F[t]. There are two standard methods; one is due to E. Berlekamp, the other appears to be a \"folk method\". Both are described by D. Knuth in [3, pp. 381-397]. Further improvements, for special values of q, are given by R. Moenck in [5J. See also E. Berlekamp [1]. (Note that both methods as described in the references apply only when d = 1, however, straightforward modifications, described below, allow d > 1.) Both methods use the calculation of resultants (or equivalently the solution of linear equations) to reduce the problem to finding the roots of a polynomial which has all of its roots in F. When p is very small, probabilistic methods are used. An improvement to the \"folk method\" method, along with a more explicit calculation of the work required, has recently been given by M. Rabin [6]. We present here a new probabilistic method which, when combined with the above algorithms, avoids the need for both resultants and linear equations. It leads to algorithms which are conceptually simpler than previous methods. Moreover, it works equally well for all finite fields F, regardless of the magnitude of q. When used for factoring a quadratic x2 a, it reduces to Berlekamp's algorithm. Other standard algorithms for factoring quadratics are due to D. H. Lehmer [4] and D. Shanks [7]. Our algorithm is also suitable for finding solutions of polynomial equations over finite fields.", "authors": ["David G. Cantor", "Hans Zassenhaus"], "id": "ba9d15ca143d2fefe150169243f855c151aa1fb0", "title": "A new algorithm for factoring polynomials over finite fields", "references": ["5c02f09da365ef019114765cfd4cbd7dc5fa7110", "6f399d7d5b5c9cd31abc5f0eca54b476e758d9df", "108d4cd19cc1e64c99ee12aad564fa79677ef44a"]}, {"date": "1972", "abstract": "Finite fields.- Factorization of polynomials.- Galois groups.- Continued fractions.- Field extensions.- Modules and orders.- Products of linear forms.- Units in algebraic number fields.- Class numbers of algebraic number fields.- Class groups and class fields of algebraic number fields.- Diophantine equations.- The hasse principle for cubic surfaces.", "authors": ["Horst G\u00fcnter Zimmer"], "id": "f15eba0892e081b46d9ca551b2f74965bb08b691", "title": "Computational Problems, Methods, and Results in Algebraic Number Theory", "references": []}, {"date": "1971", "abstract": "This is a corrected printing of the second edition of Lang's well-known textbook. It covers all of the basic material of classical algebraic number theory, giving the student the background necessary for the study of further topics in algebraic number theory, such as cyclotomic fields, or modular forms. Part I introduces some of the basic ideas of the theory: number fields, ideal classes, ideles and adeles, and zeta functions. It also contains a version of a Riemann-Roch theorem in number fields, proved by Lang in the very first version of the book in the sixties. This version can now be seen as a precursor of Arakelov theory. Part II covers class field theory, and Part III is devoted to analytic methods, including an exposition of Tate's thesis, the Brauer-Siegel theorem, and Weil's explicit formulas. The second edition contains corrections, as well as several additions to the previous edition, and the last chapter on explicit formulas has been rewritten.", "authors": ["Serge Lang"], "id": "52b4d3640a77ef604f5b28816ed28de04ea520e2", "title": "Algebraic Number Theory", "references": []}, {"date": "1980", "abstract": "A new algorithm for testing primality is presented. The algorithm is distinguishable from the lovely algorithms of Solvay and Strassen [36], Miller [27] and Rabin [32] in that its assertions of primality are certain (i.e., provable from Peano's axioms) rather than dependent on unproven hypothesis (Miller) or probability (Solovay-Strassen, Rabin). An argument is presented which suggests that the algorithm runs within time c1ln(n)c2ln(ln(ln(n))) where n is the input, and C1, c2 constants independent of n. Unfortunately no rigorous proof of this running time is yet available.", "authors": ["Leonard M. Adleman"], "id": "aad4d7a59c5b2aca4f66876c14c969f079435bce", "title": "On distinguishing prime numbers from composite numbers", "references": ["d4bcf68af63d355bde66ade587013a934473494d"]}, {"date": "1983", "abstract": "An encryption method is presented with the novel property that publicly revealing an encryption key does not thereby reveal the corresponding decryption key. This has two important consequences:Couriers or other secure means are not needed to transmit keys, since a message can be enciphered using an encryption key publicly revealed by the intended recipient. Only he can decipher the message, since only he knows the corresponding decryption key.\nA message can be \u201csigned\u201d using a privately held decryption key. Anyone can verify this signature using the corresponding publicly revealed encryption key. Signatures cannot be forged, and a signer cannot later deny the validity of his signature. This has obvious applications in \u201celectronic mail\u201d and \u201celectronic funds transfer\u201d systems. A message is encrypted by representing it as a number M, raising M to a publicly specified power e, and then taking the remainder when the result is divided by the publicly specified product, n, of two large secret prime numbers p and q. Decryption is similar; only a different, secret, power d is used, where e * d = 1(mod (p - 1) * (q - 1)). The security of the system rests in part on the difficulty of factoring the published divisor, n.\n", "authors": ["Ronald L. Rivest", "Adi Shamir", "Leonard M. Adleman"], "id": "4e09314d22260dc30db8e93fd26313feae402a28", "title": "A method for obtaining digital signatures and public-key cryptosystems", "references": []}, {"date": "1975", "abstract": "We describe briefly a novel factorization method involving probabilistic ideas.", "authors": ["John M. Pollard"], "id": "8052adc20ffb2f284bc31160874721d6943bcb78", "title": "A monte carlo method for factorization", "references": []}, {"date": "1986", "abstract": "A \"coordinate recurrence\" method for solving sparse systems of linear equations over finite fields is described. The algorithms discussed all require O(n_{1}(\\omega + n_{1})\\log^{k}n_{1}) field operations, where n_{1} is the maximum dimension of the coefficient matrix, \\omega is approximately the number of field operations required to apply the matrix to a test vector, and the value of k depends on the algorithm. A probabilistic algorithm is shown to exist for finding the determinant of a square matrix. Also, probabilistic algorithms are shown to exist for finding the minimum polynomial and rank with some arbitrarily small possibility of error.", "authors": ["Douglas H. Wiedemann"], "id": "8ce0208a77da162c1680f8bb6509843f4ad70764", "title": "Solving sparse linear equations over finite fields", "references": ["5fc9bbf4ecd68a7d51f651afc30c7b7c35bdcf94", "94635a15fb4757640ff9952e9386f4e9e7ce827f", "134b7b065a73d4ca00bb16c7b8bebbde951b0ba0", "b913cf330852035f49b4ec5fe2db86c47d8a98fd", "a0870e57fb5e59aae83f2a2a09b8df78257ef556", "1992c5f9ccc0b6a7f3e3c106c7bfffde20974f64", "20943d79f6423316e210444fd5cb3c468423c821", "3ffc9414d0288832d057c9e4fce0822182404fca", "d2d4633b2542292922af3f48ab8c749e1b8cc0aa", "29e48d9d56db76cca41b1eeeea7722f601c13f7b"]}, {"date": "1984", "abstract": "Based on Kummer Theorem, we study the deterministic complexity of two factorization problems: polynomial factorization over finite fields and prime factorization in algebraic number fields. We show that factoring polynomials of degree <italic>n</italic> in <italic>F<subscrpt>p</subscrpt></italic>[<italic>x</italic>], with <italic>p</italic> prime, is polynimially equivalent to factoring <italic>p</italic> in algebraic number field of extension degree <italic>n</italic> over <italic>Q,</italic> where <italic>p</italic> is \u201cregular\u201d with respect to the generating polynomials of the number fields. Part of the proof also yields an efficient polynomial time algorithm for computing the factorization pattern. Number theoretical methods are then developed to solve two important kinds of polynomials:&fgr;<subscrpt>n</subscrpt>(<italic>x</italic>) mod <italic>p</italic> where &fgr;<subscrpt>n</subscrpt> is the <italic>n</italic>-th cyclotomic polynomial, and <italic>x</italic><subscrpt>n</subscrpt>. - \u03b1 mod <italic>p</italic> where \u03b1 \u03b5 <italic>N.</italic> We show that when Extended Riemann Hypothesis is assumed, all the roots of both kinds of polynomials in <italic>F<subscrpt>p</subscrpt></italic> can be found efficiently in time polynomial in <italic>n</italic> and <italic>logp.</italic> As \u03b1 consequence, when <italic>p</italic> &Xgr; 1(<italic>n</italic>), factorization of <italic>p</italic> in the <italic>n</italic>-th cyclotomic field can be computed in polynomial time. The result on finding all roots of <italic>x<supscrpt>n</supscrpt></italic> &Xgr; \u03b1(<italic>p</italic>) extends \u03b1 result of Adleman, Menders, and Miller, which states that the least root of <italic>x<supscrpt>n</supscrpt></italic> &Xgr; \u03b1(<italic>p</italic>) can be found in polynomial time, when Extended Riemann Hypothesis is assumed.", "authors": ["Ming-Deh A. Huang"], "id": "fce6d0219ba28db87890b2cbbb80c61c8702a829", "title": "Factorization of polynomials over finite fields and factorization of primes in algebraic number fields", "references": []}, {"date": "1969", "abstract": "t. Below we will give an algorithm which computes the coefficients of the product of two square matrices A and B of order n from the coefficients of A and B with tess than 4 . 7 n l\u00b0g7 arithmetical operations (all logarithms in this paper are for base 2, thus tog 7 ~ 2.8; the usual method requires approximately 2n 3 arithmetical operations). The algorithm induces algorithms for invert ing a matr ix of order n, solving a system of n linear equations in n unknowns, comput ing a determinant of order n etc. all requiring less than const n l\u00b0g 7 arithmetical operations. This fact should be compared with the result of KLYUYEV and KOKOVKINSHCHERBAK [1 ] tha t Gaussian elimination for solving a system of l inearequations is optimal if one restricts oneself to operations upon rows and columns as a whole. We also note tha t WlNOGRAD [21 modifies the usual algorithms for matr ix multiplication and inversion and for solving systems of linear equations, trading roughly half of the multiplications for additions and subtractions. I t is a pleasure to thank D. BRILLINGER for inspiring discussions about the present subject and ST. COOK and B. PARLETT for encouraging me to write this paper. 2. We define algorithms e~, ~ which mult iply matrices of order m2 ~, by induction on k: ~ , 0 is the usual algorithm, for matr ix multiplication (requiring m a multiplications and m 2 ( m t) additions), e~,k already being known, define ~ , ~ +t as follows: If A, B are matrices of order m 2 k ~ to be multiplied, write", "authors": ["Volker Strassen"], "id": "cb80b424db4c94cbaf4c3ae0e570ac3eb6f3bcf3", "title": "Gaussian elimination is not optimal", "references": ["23fbe80fc979847e3134e3cea74012f916f1d7a9"]}, {"date": "1974", "abstract": "The protection of time sharing systems from unauthorized users is often achieved by the use of passwords. By using one-way ciphers to code the passwords, the risks involved with storing the passwords in the computer can be avoided. We discuss the selection of a suitable one-way cipher and suggest that for this purpose polynomials over a prime modulus are superior to one-way ciphers derived from Shannon codes.", "authors": ["George B. Purdy"], "id": "f3143ff6569f3b9b01cbe34deea53a1d3b3c84c2", "title": "A high security log-in procedure", "references": ["e073a7c5a6418d96fc16d8337a6056a457e75c1e"]}, {"date": "1978", "abstract": "An encryption method is presented with the novel property that publicly revealing an encryption key does not thereby reveal the corresponding decryption key. This has two important consequences: (1) Couriers or other secure means are not needed to transmit keys, since a message can be enciphered using an encryption key publicly revealed by the intented recipient. Only he can decipher the message, since only he knows the corresponding decryption key. (2) A message can be \u201csigned\u201d using a privately held decryption key. Anyone can verify this signature using the corresponding publicly revealed encryption key. Signatures cannot be forged, and a signer cannot later deny the validity of his signature. This has obvious applications in \u201celectronic mail\u201d and \u201celectronic funds transfer\u201d systems. A message is encrypted by representing it as a number M, raising M to a publicly specified power e, and then taking the remainder when the result is divided by the publicly specified product, n, of two large secret primer numbers p and q. Decryption is similar; only a different, secret, power d is used, where e * d \u2261 1(mod (p - 1) * (q - 1)). The security of the system rests in part on the difficulty of factoring the published divisor, n.", "authors": ["Ronald L. Rivest", "Adi Shamir", "Leonard M. Adleman"], "id": "43fa769993fd8ba599aac30535ab7778779683fb", "title": "A method for obtaining digital signatures and public-key cryptosystems", "references": ["36a109672bf4ad2259d20813d43781f91c39dabc"]}, {"date": "1985", "abstract": "It is shown that assuming <italic>Generalized Riemann Hypothesis</italic>, the roots of \u0192(<italic>x</italic>) = O <italic>mod p</italic>, where <italic>p</italic> is a prime and f(x) is an integral Abilene polynomial can be found in <italic>deterministic</italic> polynomial time. The method developed for solving this problem is also applied to prime decomposition in Abelian number fields, and the following result is obtained: assuming Generalized Riemann Hypotheses, for Abelian number fields <italic>K</italic> of finite extension degree over the rational number field <italic>Q</italic>, the decomposition pattern of a prime <italic>p</italic> in <italic>K</italic>, i.e. the <italic>ramification index</italic> and the <italic>residue class degree</italic>, can be computed in deterministic polynomial time, providing <italic>p</italic> does not divide the extension degree of <italic>K</italic> over <italic>Q</italic>. It is also shown, as a theorem fundamental to our algorithm, that for <italic>q</italic>, <italic>p</italic> prime and <italic>m</italic> the order of <italic>p</italic> mod <italic>q</italic>, there is a <italic>q</italic>-th nonresidue in the finite field <italic>F<subscrpt>p<supscrpt>m</supscrpt></subscrpt></italic> that can be written as <italic>a</italic><subscrpt>o</subscrpt> + <italic>a</italic><subscrpt>1</subscrpt><italic>w</italic> + \u2026 + <italic>a</italic><subscrpt><italic>m</italic>-1</subscrpt><italic>w</italic><supscrpt><italic>m</italic>-1</supscrpt>, where |<italic>a</italic><subscrpt>1</subscrpt>| \u2264 <italic>cq</italic><supscrpt>2</supscrpt> log<supscrpt>2</supscrpt>(<italic>pq</italic>), <italic>c</italic> is an absolute effectively computable constant, and 1, <italic>w</italic>, \u2026, <italic>w</italic><supscrpt><italic>m</italic>-1</supscrpt> form a basis of <italic>F<subscrpt>p</subscrpt><supscrpt>m</supscrpt></italic> over <italic>F<subscrpt>p</subscrpt></italic>. More explicitly, <italic>w</italic> is a root of the q-th cyclotomic polynomial over <italic>F<subscrpt>p</subscrpt></italic>. This result partially generalizes, to finite field extensions over <italic>F<subscrpt>p</subscrpt></italic>, a classical result in number theory stating that assuming Generalized Riemann Hypothesis, the least <italic>q</italic>-th nonresidue mod <italic>p</italic> for <italic>p</italic>,<italic>q</italic> prime and <italic>q</italic> dividing <italic>p</italic> - t is bounded by <italic>c</italic> log<supscrpt>2</supscrpt><italic>p</italic>, where <italic>c</italic> is an absolute, effectively computable constant.", "authors": ["Ming-Deh A. Huang"], "id": "82b871549e90c4ed297fe4f22ccb9645e4f2c571", "title": "Riemann hypothesis and finding roots over finite fields", "references": []}, {"date": "1973", "abstract": "Semantic Scholar extracted view of \"Five number-theoretic algorithms\" by Daniel Shanks", "authors": ["Daniel Shanks"], "id": "71937f0cd51dff739c91ad39c3af82b7341d3f39", "title": "Five number-theoretic algorithms", "references": []}, {"date": "1977", "abstract": "How hard is it to decide if a is a quadratic residue modulo m? Few problems have received more attention [7]. When m is prime, the Legendre symbol, the Jacobi symbol and the Gaussian Law of Quadratic Reciprocity yield a polynomial time algorithm [14]. When m is composite, then the above result for primes together with the Chinese Remainder Theorem yields a polynomial time algorithm assuming m can be factored (thus this problem is probably notyorNP-complete). Finding XiS such that x2 = a MOD(m) when a is a quadratic residue is a far more complex problem. In [11] it was shown that finding the least x such that x2 =a MOD(m) is NP-complete (even if m is factored). The main result of this paper is:", "authors": ["Leonard M. Adleman", "Kenneth L. Manders", "Gary L. Miller"], "id": "3fd4decdfb3722e57bafb1fd591c40b1e2166b2a", "title": "On taking roots in finite fields", "references": ["5c02f09da365ef019114765cfd4cbd7dc5fa7110", "f6b6be01eef6c94f234a8a657e9bc61700879724", "a4251fe2c96601db52782bd257d57462d56d005a", "42a77933e79a916bf70ca31580c7f3e8da319d7f", "2baf4ac77661be8206d022417841de1c5cf2f7c1", "ba624ccbb66c93f57a811695ef377419484243e0", "8f20c5e555b3784f3dcf13a9716e3843d1042483", "37e568872919c1f7aca2798136bc1dc6840a51bb"]}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"The factorization of $F_7$\" by Michael A. Morrison et al.", "authors": ["Michael A. Morrison", "John Brillhart"], "id": "5c6adb7113bab80c5432bce614aea5b57dafc235", "title": "The factorization of $F_7$", "references": []}, {"date": "1982", "abstract": "By combining the principles of known factoring algorithms we obtain some improved algorithms which by heuristic arguments all have a time bound O(expc ln n ln lnn) for various constants c \u2a7e3. In particular, Miller's method of solving index equations and Shanks' method of computing ambiguous quadratic forms with discriminant \u2212n can be modified in this way. We show how to speed up the factorization of n by using preprocessed lists of those numbers in [\u2212u, u] and [n - u, n + u], 0 \u2aa1 u \u2aa1 n which only have small prime factors. These lists can be uniformly used for the factorization of all numbers in [n - u, n + u]. Given these lists, factorization takes O(exp[2(ln n)13(ln ln n)23] steps. We slightly improve Dixon's rigorous analysis of his Monte Carlo factoring algorithm. We prove that this algorithm with probability sol12 detects a proper factor of every composite n within o(exp6 ln n ln lnn) steps.", "authors": ["Claus-Peter Schnorr"], "id": "dfdc1499dc7e7240bfc872e5e07f8c2c3711d075", "title": "Refined Analysis and Improvements on Some Factoring Algorithms", "references": []}, {"date": "1980", "abstract": "We present probabilistic algorithms for the problems of finding an irreducible polynomial of degree n over a finite field, finding roots of a polynomial, and factoring a polynomial into its irreducible factors over a finite field. All of these problems are of importance in algebraic coding theory, algebraic symbol manipulation, and number theory. These algorithms have a very transparent, easy to program structure. For finite fields of large characteristic p, so that exhaustive search through ${\\text{Z}}_p $, is not feasible, our algorithms are of lower order in the degrees of the polynomial and fields in question, than previously published algorithms.", "authors": ["Michael O. Rabin"], "id": "108d4cd19cc1e64c99ee12aad564fa79677ef44a", "title": "Probabilistic Algorithms in Finite Fields", "references": []}, {"date": "1977", "abstract": "Algorithms for factoring polynomials over finite fields are discussed. A construction is shown which reduces the final step of Berlekamp's algorithm to the problem of finding the roots of a polynomial in a finite field Zp. It is shown that if the characteristic of the field is of the form p = L 21 + 1, where I L, then the roots of a polynomial of degree n may be found in O(n log p + n log2 p) steps. As a result, a modification of Berlekamp's method can be performed in O(n + n log p + n log p) steps. If n is very large then an alternative method finds the factors of the polynomial in O(n2log n + n lognlogp). Some consequences and empirical evidence are discussed.", "authors": ["Robert T. Moenck"], "id": "6f399d7d5b5c9cd31abc5f0eca54b476e758d9df", "title": "On the efficiency of algorithms for polynomial factoring", "references": ["bf92394b6bb508bd31d1962b0dcf705c091b64f9", "dca8284ce89f4ba77d0c2e43ceae5abae3636a88", "d20f3b8deaab11990fcf993819b2c078729d3e1a", "4f942cd0174a928b56245cd639d488e195baa60d", "cb80b424db4c94cbaf4c3ae0e570ac3eb6f3bcf3", "bc7e0aa5bcbdacef451d001fed342da78465fbb7", "ee0f987db1371918e737d19aab3b5e8d100d518f", "ca296614177331958a5c9f12134f267685c1c4ab", "d3408c696032a2e715fbfcaaabb262ef48ea0477", "f44fcbb4bfeba161331f1cf69dff73f2a8eb6c39"]}, {"date": "1975", "abstract": "Semantic Scholar extracted view of \"Reduction of an arbitrary diophantine equation to one in 13 unknowns\" by Yuri Matijasevic et al.", "authors": ["Yuri Matijasevic", "Julia Robinson"], "id": "d4bcf68af63d355bde66ade587013a934473494d", "title": "Reduction of an arbitrary diophantine equation to one in 13 unknowns", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Note on Fermat\u2019s numbers\" by James C. Morehead et al.", "authors": ["James C. Morehead", "Aug. Western"], "id": "df609be3030041a10ce416b8447461cfd3e5a4d2", "title": "Note on Fermat\u2019s numbers", "references": []}, {"date": "1970", "abstract": "Semantic Scholar extracted view of \"Factoring polynomials over large finite fields\" by Elwyn R. Berlekamp", "authors": ["Elwyn R. Berlekamp"], "id": "5c02f09da365ef019114765cfd4cbd7dc5fa7110", "title": "Factoring polynomials over large finite fields", "references": []}, {"date": "1975", "abstract": "Miniaturization has steadily increased the economic usefulness of digital electronics through the past two decades. A variety of physical arguments are brought to bear on the question of how far miniaturization can be extended. The implications of the laws of quantum mechanics and thermodynamics for information storage are examined. The need for power dissipation in electrical information processing is demonstrated and the limits set on miniaturization by the problems of removing the heat thereby produced are estimated. limits with origins in properties of the materials used to make electronic devices are reviewed. The potential performance of various technologies based on nonsemiconductor phenomena is estimated and compared with the limits found for planar silicon technology. Attempts are made to guess at all of the many unknown parameters that enter into broadly applicable quantitative expressions of the properties of logic circuitry so that actual values of the limits can be estimated.", "authors": ["R. Keyes"], "id": "ef9fdc459195614fb967fcbf93674bc56baeda00", "title": "Physical limits in digital electronics", "references": ["6b81783739d1c7fcadf284f1d1137eb0a8a82b66", "82241f610153dd5087e7eab13fe8490b33999aa7", "93d0134f97905d81a4d944c0cf0c62664a42b657", "548d4585f477d07f1aa1d8354a64612ce0b39311", "bb41e89072c0380a615a615d8053a120793539af", "34545aadbbbd5bbf619d661ac86e7b1ae0eeb3a2", "807882b433e76a79a4e4898aadef796368f50a62", "31cdaea20e473d82c3086034974efafa9c3932a9", "c5575bd9d64b1587a51b4a3a17236acabbd3d752", "95fee38c416c945d29b9426afce30bb59f70defc"]}, {"date": "1978", "abstract": "Abstract We consider the problem of identifying an unknown value x \u03f5 {1, 2,\u2026, n } using only comparisons of x to constants when as many as E of the comparisons may receive erroneous answers. For a continuous analogue of this problem we show that there is a unique strategy that is optimal in the worst case. This strategy for the continuous problem is then shown to yield a strategy for the original discrete problem that uses log 2 n + E \u00b7 log 2 log 2 n + O ( E \u00b7 log 2 E ) comparisons in the worst case. This number is shown to be optimal even if arbitrary \u201cYes-No\u201d questions are allowed. We show that a modified version of this search problem with errors is equivalent to the problem of finding the minimal root of a set of increasing functions. The modified version is then also shown to be of complexity log 2 n + E \u00b7 log 2 log 2 n + O ( E \u00b7 log 2 E ).", "authors": ["Ronald L. Rivest", "Albert R. Meyer", "Daniel J. Kleitman", "Karl Winklmann", "Joel H. Spencer"], "id": "29e48d9d56db76cca41b1eeeea7722f601c13f7b", "title": "Coping with Errors in Binary Search Procedures", "references": ["1ec3da7a46544a432c2a51d7840629d232fd7d54", "0b99dc3042f14750a8c22871ed4012390d8df829", "74bc554e28562a1543905006b2ab92c2a74aa479"]}, {"date": "", "abstract": "undefined", "authors": [], "id": "98579c1c831b20ae1554df43bbe1db6eccc6dac9", "title": "Bionomia. Opinions Concerning Life and Health, Introductory to a Course of Lectures on the Physiology of Sentient Beings", "references": []}, {"date": "1977", "abstract": "A device for measuring height despite difficult visibility comprises a small portable water tank positioned at the location to be measured and a pressure transducer placed at a reference location and connected with the water tank by a hose, whereby the transducer outputs a signal voltage proportional to hydrostatic pressure and hence to the height of the first location. The output signal is amplified and converted to a digital value which is displayed by a digital display calibrated in terms of height.", "authors": ["Arnold Schnhage"], "id": "d2d4633b2542292922af3f48ab8c749e1b8cc0aa", "title": "Schnelle Multiplikation von Polynomen ber Krpern der Charakteristik 2", "references": []}, {"date": "1968", "abstract": "Abstract\u2014In this note we describe a new way of computing the inner product of two vectors. This method cuts down the number of multiplications required when we want to perform a large number of inner products on a smaller set of vectors. In particular, we obtain that the product of two n\u00d7n matrices can be performed using roughly n3/2 multiplications instead of the n3multiplications which the regular method necessitates.", "authors": ["Shmuel Winograd"], "id": "23fbe80fc979847e3134e3cea74012f916f1d7a9", "title": "A New Algorithm for Inner Product", "references": []}, {"date": "1987", "abstract": "Practically all knapsack public key cryptosystems have been broken in the last few years, and so essentially the only public key cryptosystems that still have some credibility and are widely known are those whose security depends on the difficulty of factoring integers (the RSA scheme and its variants) and those whose security depends on the difficulty of computing discrete logarithms in finite fields. Therefore, the computational complexity of these two problems is of great interest.", "authors": ["Andrew M. Odlyzko"], "id": "20943d79f6423316e210444fd5cb3c468423c821", "title": "The Complexity of Computing Discrete Logarithms and Factoring Integers", "references": ["a998bb2f702a2ecaf926487daa0a88e1a9da0d11", "ec30a15d158dc6c68b32c614ec00adf5f6597915", "134b7b065a73d4ca00bb16c7b8bebbde951b0ba0", "c999286a3343871e22b68edcf1c8474724a401e0", "3ffc9414d0288832d057c9e4fce0822182404fca", "dcb594aa6410426c6d966890a2047c1b1ab2059d", "466daddfb6340c28cb8da548007028c8cc5df687", "742826a56c402c68e81d2cc4db4edae3126fde8c"]}, {"date": "1975", "abstract": "Let to, t1, t2, be a sequence of elements of a field F. We give a continued fraction algorithm for tox + tlx2 + t2x3 + * . . If our sequence satisfies a linear recurrence, then the continued fraction algorithm is finite and produces this recurrence. More generally the algorithm produces a nontrivial solution of the system E ti+j j 0O < i < s-1, j=O for every positive integer s. 1. Let tO, tl, t2, be a sequence of elements of a field F. Set", "authors": ["W. H. Mills"], "id": "1992c5f9ccc0b6a7f3e3c106c7bfffde20974f64", "title": "Continued fractions and linear recurrences", "references": ["5fc9bbf4ecd68a7d51f651afc30c7b7c35bdcf94"]}, {"date": "1976", "abstract": "Semantic Scholar extracted view of \"Probabilistic Algorithms\" in Algorithms and Complexity\" by Michael O. Rabin", "authors": ["Michael O. Rabin"], "id": "37e568872919c1f7aca2798136bc1dc6840a51bb", "title": "Probabilistic Algorithms", "references": []}, {"date": "1952", "abstract": "This, the problem of the least quadratic non residue, has often been investigated. The best result is due to Vinogradov, who proved that (1) n(k) = O(k1I(2Ve) where n(k) denotes the least positive quadratic non residue of the prime k. Also, several interesting results about n(k) have been obtained by A. Brauer through elementary methods. It has been conjectured that n(k) ko(E). This was first proved by Linnik on the assumption of the extended Riemann Hypothesis (E.R.H.). Recently, with the same hypothesis, S. Chowla and P. Erdds have improved Linnik's result to n(k) = O(exp ((log k)i+e)) In this paper I prove on the basis of the E.R.H. (2) n(k) = O((log k)2) S. Chow-la has proved that there exist infinitely many primes k where the first cl log k residues (mod k) are all quadratic residues. Hence, the upper bound on (2) cannot be improved beyond O(log k). The two results, the upper and lower bounds, are now not too far apart. The best possible bound for n(k) is probably not the bound in (2). One COuld(1 improve the bound in (2) by methods in this paper if one could derive any non trivial estimate in terms of k on", "authors": ["Nesmith Ankeny"], "id": "8f20c5e555b3784f3dcf13a9716e3843d1042483", "title": "The least quadratic non residue", "references": []}, {"date": "1969", "abstract": "Semantic Scholar extracted view of \"Computer technology applied to the theory of numbers\" by D. H. Lehmer", "authors": ["D. H. Lehmer"], "id": "42a77933e79a916bf70ca31580c7f3e8da319d7f", "title": "Computer technology applied to the theory of numbers", "references": []}, {"date": "1976", "abstract": "Semantic Scholar extracted view of \"Riemann's Hypothesis and Tests for\" by G. Logan Miller", "authors": ["G. Logan Miller"], "id": "a4251fe2c96601db52782bd257d57462d56d005a", "title": "Riemann's Hypothesis and Tests for", "references": []}, {"date": "1942", "abstract": "Semantic Scholar extracted view of \"On the least primitive root of a prime\" by Loo-keng Hua", "authors": ["Loo-keng Hua"], "id": "f6b6be01eef6c94f234a8a657e9bc61700879724", "title": "On the least primitive root of a prime", "references": []}, {"date": "1974", "abstract": "The solution to problem 7 [SIGSAM Bulletin vol 8, no. 1 (issue no. 29), Feb., 1974] was obtained in its entirety with the use of MACSYMA [1]. No hand calculations were required. The factorization step, the major cost of the solution, uncovered a poor design decision in the existing factorization program [2]. This program was modified, and the factorization was accomplished in 602 seconds of PDP-10 machine time. Since the modified factorization program produces shorter run-times on almost all of our standard test cases. it is now the standard routine.", "authors": ["Richard J. Fateman", "Joel Moses", "Paul Wang"], "id": "f44fcbb4bfeba161331f1cf69dff73f2a8eb6c39", "title": "Solution to problem #7 using MACSYMA", "references": []}, {"date": "1969", "abstract": "Two familiar algorithms, the extended Euclidean algorithm and the Fermat algorithm (based on Fermat's theorem aP a(mod p)), are analyzed and compared as methods for computing multiplicative inverses in GF(p). Using Knuth's results on the average number of divisions in the Euclidean algorithm, it is shown that the average number of arithmetic operations required by the Fermat algorithm is nearly twice as large as the average number for the extended Euclidean algorithm. For each of the two algorithms, forward and backward versions are distinguished. It is shown that all numbers computed in the forward extended Euclidean algorithm are bounded by the larger of the two inputs, a property which was previously established by Kelisky for the backward version. U", "authors": ["George E. Collins"], "id": "d3408c696032a2e715fbfcaaabb262ef48ea0477", "title": "Computing Multiplicative Inverses in GF(p)", "references": []}, {"date": "1969", "abstract": "It is shown in this paper that the iterative algorithm introduced by Berlekamp for decoding BCH codes actually provides a general solution to the problem of synthesizing the shortest linear feedback shift register capable of generating a prescribed finite sequence of digits. The shift-register approach leads to a simple proof of the validity of the algorithm as well as providing additional insight into its properties. The equivalence of the decoding problem for BCH codes to a shift-register synthesis problem is demonstrated, and other applications for the algorithm are suggested.", "authors": ["James L. Massey"], "id": "a0870e57fb5e59aae83f2a2a09b8df78257ef556", "title": "Shift-register synthesis and BCH decoding", "references": ["ca123ead8cb3032ab2d4c07bdb997f7931528687", "98f2c2fdaf7502e8bcebcccf937f9e9e8a878ccb", "d0a8d82c9d89929ee85c3090f20e7b2901e89c61"]}, {"date": "1980", "abstract": "The s tar thng success o f the Rabm-S t ra s sen -So lovay p n m a h t y algori thm, together wi th the intr iguing foundat tonal posstbthty that axtoms of randomness may constttute a useful fundamenta l source o f m a t h e m a u c a l truth independent of the standard axmmaUc structure of mathemaUcs, suggests a wgorous search for probabdisuc algonthms In dlustratmn of this observaUon, vanous fast probabdlsttc algonthms, with probability of correctness guaranteed a prion, are presented for testing polynomial ldentmes and propemes of systems of polynomials. Ancdlary fast algorithms for calculating resultants and Sturm sequences are given. Probabilistlc calculatton in real anthmetlc, prewously considered by Davis, is justified ngorously, but only in a special case. Theorems of elementary geometry can be proved much more efficiently by the techmques presented than by any known arttficml-mtelhgence approach", "authors": ["Jacob T. Schwartz"], "id": "b913cf330852035f49b4ec5fe2db86c47d8a98fd", "title": "Fast Probabilistic Algorithms for Verification of Polynomial Identities", "references": []}, {"date": "1976", "abstract": "Two kinds of contemporary developments in cryptography are examined. Widening applications of teleprocessing have given rise to a need for new types of cryptographic systems, which minimize the need for secure key distribution channels and supply the equivalent of a written signature. This paper suggests ways to solve these currently open problems. It also discusses how the theories of communication and computation are beginning to provide the tools to solve cryptographic problems of long standing.", "authors": ["Whitfield Diffie", "Martin E. Hellman"], "id": "ba624ccbb66c93f57a811695ef377419484243e0", "title": "New directions in cryptography", "references": ["86ba7660ed50b2be9ccd801070680152c5646b55", "f3143ff6569f3b9b01cbe34deea53a1d3b3c84c2"]}, {"date": "1974", "abstract": "The function F(x) = (1/2-x) (1-x2)1/2+x(1+(1-(1/2+x)2)1/2) has a maximum at about x = .343771, where it attains the value of approximately .674981. This value is the root of an irreducible polynomial of tenth degree over the integers; the problem is to find this polynomial. The obvious way of proceeding is as follows:(1) Differentiate F(x), set it equal to zero, and clear radicals. The result is a tenth degree polynomial P(x) over the integers which has a root at about x = .343771.", "authors": ["Stephen C. Johnson", "Ronald L. Graham"], "id": "ca296614177331958a5c9f12134f267685c1c4ab", "title": "Problem #7", "references": []}, {"date": "1971", "abstract": "A class of simple feedback strategies is developed. The fixed-length codewords can be described by an interesting sequence of trees. The decoder scans the received block in the reverse direction, starting at the most recent bit. As this scan progresses, a certain pair of characteristic bit patterns are replaced by the digits 0 or 1, respectively. For certain values of the information rate R , this class of strategies corrects the largest error fraction possible.", "authors": ["J. Pieter M. Schalkwijk"], "id": "74bc554e28562a1543905006b2ab92c2a74aa479", "title": "A class of simple and optimal strategies for block coding on the binary symmetric channel with noiseless feedback", "references": []}, {"date": "1971", "abstract": "SAC-1 is a program system for performing operations on multivariate polynomials and rational functions with infinite-precision coefficients. It is programmed, with the exception of a few simple primitives, in ASA Fortran. As a result the system is extremely accessible, portable, easy to learn, and indeed has been implemented at more than 20 institutions.\n The SAC-1 system's range of programmed capabilities is exceptionally broad, including, besides the usual operations, polynomial greatest common divisor and resultant calculation, polynomial factorization, exact polynomial real zero calculation, partial fraction decomposition, rational function integration, and solution of systems of linear equations with polynomial coefficients.\n SAC-1 is also outstanding in its computing time efficiency, which is achieved partially through the use of appropriate data structures, but primarily through the use of mathematically sophisticated and analyzed algorithms, which are briefly surveyed. The efficiency gains, frequently orders of magnitude, are such that many new applications are rendered feasible.", "authors": ["George E. Collins"], "id": "ee0f987db1371918e737d19aab3b5e8d100d518f", "title": "The SAC-1 system: An introduction and survey", "references": []}, {"date": "1973", "abstract": "A shift register is described which employs as information bits the flux quantum vortices occurring in Josephson junctions of extended dimensions. The positions of the vortices can be controlled and manipulated by an appropriately designed circuit geometry and by application of currents and magnetic fields. The time required for switching of the position of a vortex can approach the period of the Josephson plasma oscillation (\u223c10 ps) and the intrinsic energy dissipation is typically \u2272 10-18J/shift.", "authors": ["Theodore Alan Fulton", "Robert C. Dynes", "Philip W. Anderson"], "id": "95fee38c416c945d29b9426afce30bb59f70defc", "title": "The flux shuttle\u2014A Josephson junction shift register employing single flux quanta", "references": []}, {"date": "1976", "abstract": "This autobiography of mathematician Stanislaw Ulam, one of the great scientific minds of the twentieth century, tells a story rich with amazingly prophetic speculations and peppered with lively anecdotes. As a member of the Los Alamos National Laboratory from 1944 on, Ulam helped to precipitate some of the most dramatic changes of the postwar world. He was among the first to use and advocate computers for scientific research, originated ideas for the nuclear propulsion of space vehicles, and made fundamental contributions to many of today's most challenging mathematical projects. With his wide-ranging interests, Ulam never emphasized the importance of his contributions to the research that resulted in the hydrogen bomb. Now Daniel Hirsch and William Mathews reveal the true story of Ulam's pivotal role in the making of the 'Super,' in their historical introduction to this behind-the-scenes look at the minds and ideas that ushered in the nuclear age. It includes an epilogue by Francoise Ulam and Jan Mycielski that sheds new light on Ulam's character and mathematical originality.", "authors": ["S. M. Ulam", "I. I. Rabi"], "id": "1ec3da7a46544a432c2a51d7840629d232fd7d54", "title": "Adventures of a Mathematician", "references": []}, {"date": "1968", "abstract": "Semantic Scholar extracted view of \"Block coding for the binary symmetric channel with noiseless, delayless feedback\" by Elwyn R. Berlekamp", "authors": ["Elwyn R. Berlekamp"], "id": "0b99dc3042f14750a8c22871ed4012390d8df829", "title": "Block coding for the binary symmetric channel with noiseless, delayless feedback", "references": []}, {"date": "1981", "abstract": "The main results of this paper have the following flavor: given one algorithm for multiplying matrices, there exists another, better, algorithm. A consequence of these results is that \u03c9, the exponent for matrix multiplication, is a limit point, that is, cannot be realized by any single algorithm. We also use these results to construct a new algorithm which shows that \u03c9 \u226a 2.495364.", "authors": ["Donald Coppersmith", "S. Winograd"], "id": "c999286a3343871e22b68edcf1c8474724a401e0", "title": "On the asymptotic complexity of matrix multiplication", "references": ["dcb594aa6410426c6d966890a2047c1b1ab2059d", "3d8be4f0d7fb8a63c94ee5a3cc7feb1b466689a1", "b97e436dbada7c69c97aa3fe911baa128e3cd94f", "cb80b424db4c94cbaf4c3ae0e570ac3eb6f3bcf3", "c640098d8fec542f85bab54fd02d8abc4a9b21ea"]}, {"date": "1967", "abstract": "It has been shown that isolated magnetic domains in thin platelets (\u224d2 mils thick) of orthoferrites can be manipulated to perform memory, logic, and transmission functions. The purpose of this paper is to discuss the properties of orthoferrites that make them suitable for magnetic device applications and consider magnetostatic problems relevant to domain structures found to be useful. Included is a brief indication of how memory, logic, and transmission can be accomplished; however, the details will be reserved for a later paper. The stability conditions of a cylindrical domain are discussed in detail and data is reported to support the conclusions. Of particular interest are the sizes of cylindrical domains available in the various orthoferrites. Such data has been taken on five of the fourteen possible orthoferrites and it is found that the thulium orthoferrite, TmFeO 3 , gives the smallest stable domain diameter (2.3 mils) and LuFeO 3 the largest. The stability results lead to a direct method for obtaining \u03c3 W , the domain wall energy density. For TmFeO 3 , as an example, \u03c3 W = 2.8 ergs/cm2. It is concluded that the orthoferrites are well suited indeed for device applications. Experimentally, 3 mil diameter domains have been manipulated and there is every reason to believe that operation of sub-mil domains will soon be realized.", "authors": ["A. H. Bobeck"], "id": "c5575bd9d64b1587a51b4a3a17236acabbd3d752", "title": "Properties and device applications of magnetic domains in orthoferrites", "references": []}, {"date": "1967", "abstract": "This paper discusses the operation of a new superconductive logic element. In geometry, the element resembles the in-line cryotron and like the in-line cryotron exhibits gains of greater than unity when biased. The gate, however, is a Josephson junction. The two states, zero-voltage and \"resistive,\" of the junaction are both superconducting states, one being a pair tunneling state, the other a single-particle tunneling state. The transition from one state to the other takes place in less than 800 ps (the resolving time of the apparatus). If used in a flip-flop circuit in a current steering mode, current transfer times are calculated to be less than 200 ps.", "authors": ["Juri Matisoo"], "id": "31cdaea20e473d82c3086034974efafa9c3932a9", "title": "The tunneling cryotron\u2014A superconductive logic element based on electron tunneling", "references": []}, {"date": "1967", "abstract": "For nonlinear applications such as high-speed switching, a device figure of merit is \u03b3, the ratio of the second derivative to the first derivative of the current-voltage (I-V) characteristic, or \u03b3 \u2261 (d2I/dV2)/(dI/dV). At room temperature, the value of \u03b3 for an ideal forward-bias Schottky diode is about 40 V\u22121. It is shown that although the ideal reverse breakdown characteristic could give a value of \u03b3 greater than 40 V\u22121, because of the statistical distribution of impurities, the effect of space-charge resistance, and other complications, much lower values of \u03b3 are expected. Furthermore, the nonlinear characteristic is noisy, relatively slow, and causes some power consumption. It appears, therefore, that this nonlinearity is not likely to supersede Schottky barrier diodes in high-speed switching applications. It does not, however, rule out the possibility of microwave generation application.", "authors": ["Simon M. Sze", "Robert M. Ryder"], "id": "807882b433e76a79a4e4898aadef796368f50a62", "title": "The nonlinearity of the reverse current-voltage characteristics of a p-n junction near avalanche breakdown", "references": []}, {"date": "1952", "abstract": "In an earlier publication [14] a method was described which generated the eigenvalues and eigenvectors of a matrix by a successive algorithm based on minimizations by least squares. The advantage of this method consists in the fact that the successive iterations are constantly employed with maximum efficiency which guarantees fastest convergence for a given number of iterations. Moreover, with the proper care the accumulation of rounding errors can be avoided. The resulting high precision is of great advantage if the separation of closely bunched eigenvalues and eigenvectors is demanded [16]. It was pointed out in [14, p. 256] that the inversion of a matrix, and thus the solution of simultaneous systems of linear equations, is contained in the general procedure as a special case. However, in view of the great importance associated with the solution of large systems of linear equations, this problem deserved more than passing attention. It is the purpose of the present discussion to adopt the general principles of the previous investigation to the specific demands that arise if we are not interested in the complete analysis of a matrix but only in the more special problem of obtaining the solution of a given set of linear equations", "authors": ["Cornelius Lanczos"], "id": "a998bb2f702a2ecaf926487daa0a88e1a9da0d11", "title": "Solution of Systems of Linear Equations by Minimized Iterations1", "references": ["ee5b207a3274297af197e7bd60d2796d6e0ed9ea"]}, {"date": "1963", "abstract": "The dynamic operation of the in-line cryotron is tested by performing pulse measurements on individual devices and also by using the device in free running closed loop oscillators (similar to shift register). The gain of the device is currently insufficient to permit bit transfer from one stage of the oscillator to the next in less than 25 nanoseconds. This propagation time is slow when compared to the 10 nanosecond circuit time constant L/R that was designed for each stage. The slower propagation is shown to be associated with the slow transition of resistance from the superconducting to the normal state and also from the normal to the superconducting state. Pulse measurements on the device indicate that the cryotron switching time is dependent upon the magnitude of the applied magnetic field. The times for switching resistance obtained from the pulse measurements are applied to the analysis of the dynamically operating closed loop register. The maximum oscillating frequency of 5 mc for a four stage closed register was predicted by the analysis and is shown to be in good agreement with the experiment. A similar analysis, using the same cryotron limitations, show the maximum frequency for a two stage closed ring register to be essentially the same as that for a four stage register. Again, this was verified experimentally. Thermal considerations, state-of-the-art fabrication, and testing procedures are discussed along with projected cryotron improvements that could lead to flip-flop time constants of about 10 nanoseconds.", "authors": ["Andrew E. Brennemann", "H. Seki", "Donald P. Seraphim"], "id": "93d0134f97905d81a4d944c0cf0c62664a42b657", "title": "Dynamic operation of the in-line cryotron in bistable circuits", "references": []}, {"date": "1962", "abstract": "In a series of preceding papers connection has been made between the physics of the activated jump over a static potential barrier and related computing devices. This paper treats a more complicated system, namely, the tunnel diode, which stores information in one of two possible dissipative states. The activated jump between dissipative states is analyzed. An idealized physical model is used to find which of the two states is the really preferred one, and to also evaluate the rate at which fluctuations bring about an approach to this preferred distribution. To simplify the analysis, the case where the tunnel diode is in series with a vacuum diode, rather than a resistor, is emphasized. For typical germanium Esaki diodes, activated jumps are improbable if the junction cross section exceeds 10\u221211 cm2.", "authors": ["Rolf Landauer"], "id": "34545aadbbbd5bbf619d661ac86e7b1ae0eeb3a2", "title": "Fluctuations in Bistable Tunnel Diode Circuits", "references": []}, {"date": "1969", "abstract": "The potential of a new technology, based on the superconducting Josephson tunneling effect, has been assessed for use in high-speed computer systems. Josephson tunneling circuits for memory and logic functions can be switched at sub-nanosecond delay, do not require standby power, and dissipate extremely low energy, typically less than 10-13joule during fast switching operation. High-speed circuits with high packing densities, not limited by power dissipation problems, can therefore be expected. Low thermal noise at cryogenic temperatures ensures reliable operation despite the low energy switching signal. Practically lossless signal transmission via superconducting lines can be exploited for high-speed operation even when sizeable memory and logic modules are assembled. Two random access memory modules have been designed on the basis of single device experiments. Operational characteristics leading to the conclusion that 30 Mbit capacity, less than 1 watt refrigeration requirement (at 3.6\u00b0 K) and cycle times of 40 ns (in one case) and 15 ns (in the second case), can be achieved are presented.", "authors": ["Wilhelm Anacker"], "id": "548d4585f477d07f1aa1d8354a64612ce0b39311", "title": "Potential of superconductive josephson tunneling technology for ultrahigh performance memories and processors", "references": []}, {"date": "1968", "abstract": "Semantic Scholar extracted view of \"A complete theory for generalized BCH codes\" by Neal Zierler", "authors": ["Neal Zierler"], "id": "d0a8d82c9d89929ee85c3090f20e7b2901e89c61", "title": "A complete theory for generalized BCH codes", "references": []}, {"date": "1968", "abstract": "The decoding of BCH codes readily reduces to the solution of a certain key equation. An iterative algorithm is presented for solving this equation over any field. Following a heuristic derivation of the algorithm, a complete statement of the algorithm and proofs of its principal properties are given. The relationship of this algorithm to the classical matrix methods and the simplification which the algorithm takes in the special case of binary codes is then discussed. The generalization of the algorithm to BCH codes with a slightly different definition, the generalization of the algorithm to decode erasures as well as errors, and the extension of the algorithm to decode more than t errors in certain eases are also presented.", "authors": ["Elwyn R. Berlekamp"], "id": "ca123ead8cb3032ab2d4c07bdb997f7931528687", "title": "Nonbinary BCH decoding (Abstr.)", "references": []}, {"date": "1957", "abstract": "For certain p-n junctions, it has been observed that the measured current-voltage characteristics deviate from the ideal case of the diffusion model. It is the purpose of this paper to show that the current due to generation and recombination of carriers from generation-recombination centers in the space charge region of a p-n junction accounts for the observed characteristics. This phenomenon dominates in semiconductors with large energy gap, low lifetimes, and low resistivity. This model not only accounts for the nonsaturable reverse current, but also predicts an apparent exp (qV/nkT) dependence of the forward current in a p-n junction. The relative importance of the diffusion current outside the space charge layer and the recombination current inside the space charge layer also explains the increase of the emitter efficiency of silicon transistors with emitter current. A correlation of the theory with experiment indicates that the energy level of the centers is a few kT from the intrinsic Fermi level.", "authors": ["Chih-tang Sah", "Robert N. Noyce", "William Bradford Shockley"], "id": "bb41e89072c0380a615a615d8053a120793539af", "title": "Carrier Generation and Recombination in P-N Junctions and P-N Junction Characteristics", "references": []}, {"date": "1962", "abstract": "It is shown that there exists an absolute lower limit to device size and an absolute upper limit to packing density of nonredundant semiconductor devices, whether integrated or nonintegrated, based on fundamental physical phenomena such as statistical variations in impurity distribution, maximum resolution of semiconductor fabrication methods, power density and influence of cosmic rays. The influence of these phenomena falls in two categories, namely failures that appear during the fabrication of the devices (impurity distribution, dividing operation) and failures that appear during use. The latter may be temporary failures (cosmic ray ionization, carrier fluctuations) or permanent failures (atomic displacements by cosmic rays, heat generation). For a medium size computer (105 components) with a reasonable life expectancy (1 month mean time between failures), the minimum device size under reasonable conditions is approximately (10\u03bc)3, which is not far from devices now in the planning stage and within reach with eidsting techniques. It is within a factor of 2-5 of the dimensions of the active region of many devices of today. As microminiaturization by mere reduction in size appears headed for a not too distant limit it appears necessary from a device point of view to consider remedies which also have been suggested from a system point of view, namely redundancy, self-organizing systems, negative feedback, etc.", "authors": ["J. T. Wallmark", "Sanford M. Marcus"], "id": "82241f610153dd5087e7eab13fe8490b33999aa7", "title": "Minimum Size and Maximum Packing Density of Nonredundant Semiconductor Devices", "references": []}, {"date": "1968", "abstract": "CONTENTS Preface to the Revised Edition Preface A Few Words 1. One Day of Magic THE PAGENT OF CRYPTOLOGY 2. The First 3,000 Years 3. The Rise of the West 4. On the Origin of a Species 5. The Era of the Black Chambers 6. The Contribution of the Dilettantes 7. Crises of the Union 8. The Professor, the Soldier, and the Man on Devil's Island 9. Room 40 10. A War of Intercepts: I 11. A War of Intercepts: II 12. Two Americans 13. Secrecy for Sale 14. Duel in the Ether: The Axis 15. Duel in the Ether: Neutrals and Allies 16. Censors, Scramblers, and Spies 17. The Scrutable Orientals 18. Russkaya Kriptologiya (\"Russian Cryptology\") 19. N.S.A. SIDESHOWS 20. The Anatomy of Cryptology 21. Heterogeneous Impulses 22. Rumrunners, Businessmen, and Makers of Non-secret Codes 23. Ciphers in the Past Tense 24. The Pathology of Cryptology PARACRYPTOLOGY 25. Ancestral Voices 26. Messages from Outer Space THE NEW CRYPTOLOGY 27. Cryptology Goes Public Bibliography Notes to Text Acknowledgments Notes to Illustrations Index", "authors": ["David Kahn"], "id": "86ba7660ed50b2be9ccd801070680152c5646b55", "title": "The codebreakers : the story of secret writing", "references": []}, {"date": "1965", "abstract": "Variations in the size and orientation of a surface cooled by evaporation of a liquid are reflected in the measured heat transfer characteristics. Nucleate boiling curves from tests of \"Freon-113\" in contact with two sizes of heated flat plates in different positions are used to demonstrate the comparative characteristics. For low levels of heat flux, the lowest surface temperatures are exhibited by the larger of the heated plates facing downward. At higher fluxes, the lowest temperatures observed are those of the larger plates facing upward. The slope of the boiling curve for any size plate becomes progressively greater as the plate is shifted from the face-down position through the vertical to the face-up position. The facility with which bubbles can escape from a heated surface is proposed as a significant determinant of heat transfer characteristics.", "authors": ["Victor Asch"], "id": "6b81783739d1c7fcadf284f1d1137eb0a8a82b66", "title": "Surface Design Factors in the Evaporative Cooling of Electronic Components", "references": []}, {"date": "1973", "abstract": "Methods for finding numerical solutions of nonlinear algebraic systems of equations have been given considerable attention since the birth of the field of numerical analysis. The fact that these methods find many applications to problems in physics, engineering, economics, and mathematical theory of optimization cannot be overstressed. However, a significant number of these problems contain indeterminants or parameters, which should only be given numerical values at the very end of the computational processes. Sometimes numerical results simply cannot provide enough insight for the analysis of the problem. Furthermore, symbolic solutions via elimination theory provide not only all solutions to a given system of equations but also a classification of solutions into solution surfaces or parametrized solutions. Thus, the symbolic method can provide an infinite number of solutions where this feat is clearly impossible for the numerical methods.", "authors": ["David Y. Y. Yun"], "id": "dca8284ce89f4ba77d0c2e43ceae5abae3636a88", "title": "On algorithms for solving systems of polynomial equations", "references": []}, {"date": "1971", "abstract": "An efficient algorithm is presented for the exact calculation of resultants of multivariate polynomials with integer coefficients. The algorithm applies modular homomorphisms and the Chinese remainder theorem, evaluation homomorphisms and interpolation, in reducing the problem to resultant calculation for univariate polynomials over GF(p), whereupon a polynomial remainder sequence algorithm is used. The c o m p u t i n g t i m e of t h e a l g o r i t h m is a n a l y z e d t h e o r e t i c a l l y a s a f u n c t i o n of t he d e g r e e s a n d c o e f f i c i e n t s i z e s of i t s i n p u t s . As a v e r y s p e c i a l c a s e , i t i s s h o w n t h a t w h e n a l l d e g r e e s a r e e q u a l a n d t h e c o e f f i c i e n t s i z e i s f i x e d , i t s c o m p u t i n g t ime i s a p p r o x i m a t e l y p r o p o r t i o n a l to X 2 r+ l , w h e r e X i s t h e c o m m o n d e g r e e a n d r i s t he n u m b e r of v a r i a b l e s . Empirically observed computing times of the algorithm are tabulated for a large number of examples, and other algorithms are compared. Potential application of the algorithm to the solution of systems of polynomial equations is briefly discussed. I . I n t r o d u c t i o n Let R be a commutative ring with an identity element and let A and B be polynomials of positive d e g r e e w i t h c o e f f i c i e n t s in R. I f A(x) = Z i = o a m xi a n d B(x) = Z i = o b i x n i w h e r e deg(A) = m a n d deg(B) = n , t h e S y l . v e s t e r m a t r i x of A a n d B is t he m + n b y m + n matrix", "authors": ["George E. Collins"], "id": "d20f3b8deaab11990fcf993819b2c078729d3e1a", "title": "The Calculation of Multivariate Polynomial Resultants", "references": ["a9d6941c8c8300eddd7b0835345750a1def21597", "3c9e6553631a3576fcfb7d6bc49228527ef07e30", "b8c72ae80177a3c935d94eb9256753f477a251d2"]}, {"date": "1980", "abstract": "The relation betweenAPA-algorithms (i. e. approximating the result with an arbitrarily small error) andEC-algorithms (i. e. computing exactly the result) is analyzed. The existence of anAPA-algorithm of complexityt0 and degreed implics the existence of anEC-algorithm of complexity (1+d)t0. An application is given for problems associated to tensorial powers of a tensor, such as matrix product.", "authors": ["Dario Bini"], "id": "b97e436dbada7c69c97aa3fe911baa128e3cd94f", "title": "Relations between exact and approximate bilinear algorithms. Applications", "references": []}, {"date": "1975", "abstract": "Thank you for downloading the computational complexity of algebraic and numeric problems elsevier computer science library theory of computation series 1. As you may know, people have search numerous times for their chosen readings like this the computational complexity of algebraic and numeric problems elsevier computer science library theory of computation series 1, but end up in harmful downloads. Rather than enjoying a good book with a cup of tea in the afternoon, instead they are facing with some harmful virus inside their computer.", "authors": ["Allan Borodin", "J. Ian Munro"], "id": "c640098d8fec542f85bab54fd02d8abc4a9b21ea", "title": "The computational complexity of algebraic and numeric problems", "references": []}, {"date": "1966", "abstract": "An algorithm for finding the symbolic factors of a multivariate polynomial with integer coefficients is presented. The algorithm is an extension of a technique used by Kronecker in a proof that the prime factoring of any polynomial may be found in a finite number of steps. The algorithm consists of factoring single-variable instances of the given polynomial by Kronecker's method and introducing the remaining variables by interpolation. Techniques for implementing the algorithm and several examples are discussed. The algorithm promises sufficient power to be used efficiently in an online system for symbolic mathematics.", "authors": ["Dale E. Jordan", "Lewis C. Clapp", "Richard Y. Kain"], "id": "bf92394b6bb508bd31d1962b0dcf705c091b64f9", "title": "Symbolic factoring of polynomials in several variables", "references": ["f63ba2b042c0e8162541527a0ab070d40437e240", "444537108d9171272bfbb0eccd7a3855ada66b0d"]}, {"date": "1939", "abstract": "From the work of Walsh and the authorf we know that if a set of polynomials is orthogonal on two distinct circles, these circles must be concentric, and a very simple relation holds between the corresponding weight functions. According to the results of the author, the case (1), (2) is the only one, save for integral linear transformations, in which a set of polynomials is simultaneously orthogonal on all circles concentric to a given circle and containing it. Merriman shows that this holds true if only the simultaneous orthogonality on two (necessarily concentric) circles is assumed.", "authors": ["Gabriel Szeg\u00f6"], "id": "ee5b207a3274297af197e7bd60d2796d6e0ed9ea", "title": "Concerning sets of polynomials orthogonal simultaneously on several circles", "references": []}, {"date": "1981", "abstract": "In 1979 considerable progress was made in estimating the complexity of matrix multiplication. Here the new techniques and recent results are presented, based upon the notion of approximate rank and the observation that certain patterns of partial matrix multiplication (some of the entries of the matrices may be zero) can efficiently be utilized to perform multiplication of large total matrices. By combining Pan\u2019s trilinear technique with a strong version of our compression theorem for the case of several disjoint matrix multiplications it is shown that multiplication of $N \\times N$ matrices (over arbitrary fields) is possible in time $O(N^\\beta )$, where $\\beta $ is a bit smaller than $3\\ln 52/\\ln 110 \\approx 2.522$.", "authors": ["Arnold Sch\u00f6nhage"], "id": "3d8be4f0d7fb8a63c94ee5a3cc7feb1b466689a1", "title": "Partial and Total Matrix Multiplication", "references": []}, {"date": "1968", "abstract": "This is the revised edition of Berlekamp's famous book, \"Algebraic Coding Theory,\" originally published in 1968, wherein he introduced several algorithms which have subsequently dominated engineering practice in this field. One of these is an algorithm for decoding Reed-Solomon and Bose\u2013Chaudhuri\u2013Hocquenghem codes that subsequently became known as the Berlekamp\u2013Massey Algorithm. Another is the Berlekamp algorithm for factoring polynomials over finite fields, whose later extensions and embellishments became widely used in symbolic manipulation systems. Other novel algorithms improved the basic methods for doing various arithmetic operations in finite fields of characteristic two. Other major research contributions in this book included a new class of Lee metric codes, and precise asymptotic results on the number of information symbols in long binary BCH codes.Selected chapters of the book became a standard graduate textbook.Both practicing engineers and scholars will find this book to be of great value.", "authors": ["Elwyn R. Berlekamp"], "id": "bc7e0aa5bcbdacef451d001fed342da78465fbb7", "title": "Algebraic coding theory", "references": []}, {"date": "1977", "abstract": "Let n be an odd integer. Take a random number a from a uniform distribution on the set $\\{1, 2,\\cdots, n -1\\}$. If a and n are relatively prime, compute the residue $\\varepsilon \\equiv a^{(n - 1)/2...", "authors": ["Robert Solovay", "Volker Strassen"], "id": "36a109672bf4ad2259d20813d43781f91c39dabc", "title": "A Fast Monte-Carlo Test for Primality", "references": []}, {"date": "1959", "abstract": "Semantic Scholar extracted view of \"LINEAR RECURRING SEQUENCES\" by Neal Zierler", "authors": ["Neal Zierler"], "id": "5fc9bbf4ecd68a7d51f651afc30c7b7c35bdcf94", "title": "LINEAR RECURRING SEQUENCES", "references": []}, {"date": "1982", "abstract": "Semantic Scholar extracted view of \"Analysis and comparison of some integer factoring algorithms\" by Carl Pomerance", "authors": ["Carl Pomerance"], "id": "134b7b065a73d4ca00bb16c7b8bebbde951b0ba0", "title": "Analysis and comparison of some integer factoring algorithms", "references": []}, {"date": "1980", "abstract": "Abstract We present two new algorithms, ADT and MDT, for solving order- n Toeplitz systems of linear equations Tz = b in time O ( n log 2 n ) and space O ( n ). The fastest algorithms previously known, such as Trench's algorithm, require time \u03a9(n 2 ) and require that all principal submatrices of T be nonsingular. Our algorithm ADT requires only that T be nonsingular. Both our algorithms for Toeplitz systems are derived from algorithms for computing entries in the Pade table for a given power series. We prove that entries in the Pade table can be computed by the Extended Euclidean Algorithm. We describe an algorithm EMGCD (Extended Middle Greatest Common Divisor) which is faster than the algorithm HGCD of Aho, Hopcroft and Ullman, although both require time O ( n log 2 n ), and we generalize EMGCD to produce PRSDC (Polynomial Remainder Sequence Divide and Conquer) which produces any iterate in the PRS, not just the middle term, in time O ( n log 2 n ). Applying PRSDC to the polynomials U 0 ( x ) = x 2 n +1 and U 1 ( x ) = a 0 + a 1 x + \u2026 + a 2 n x 2 n gives algorithm AD (Anti-Diagonal), which computes any ( m , p ) entry along the antidiagonal m + p = 2 n of the Pade table for U 1 in time O ( n log 2 n ). Our other algorithm, MD (Main-Diagonal), computes any diagonal entry ( n , n ) in the Pade table for a normal power series, also in time O ( n log 2 n ). MD is related to Schonhage's fast continued fraction algorithm. A Toeplitz matrix T is naturally associated with U 1 , and the ( n , n ) Pade approximation to U 1 gives the first column of T \u22121 . We show how a formula due to Trench can be used to compute the solution z of Tz = b in time O ( n log n ) from the first row and column of T \u22121 . Thus, the Pade table algorithms AD and MD give O ( n log 2 n ) Toeplitz algorithms ADT and MDT. Trench's formula breaks down in certain degenerate cases, but in such cases a companion formula, the discrete analog of the Christoffel-Darboux formula, is valid and may be used to compute z in time O ( n log 2 n ) via the fast computation (by algorithm AD) of at most four Pade approximants. We also apply our results to obtain new complexity bounds for the solution of banded Toeplitz systems and for BCH decoding via Berlekamp's algorithm.", "authors": ["Richard P. Brent", "Fred G. Gustavson", "David Y. Y. Yun"], "id": "94635a15fb4757640ff9952e9386f4e9e7ce827f", "title": "Fast Solution of Toeplitz Systems of Equations and Computation of Pad\u00e9 Approximants", "references": []}, {"date": "1949", "abstract": "THE problems of cryptography and secrecy systems furnish an interesting application of communication theory.1 In this paper a theory of secrecy systems is developed. The approach is on a theoretical level and is intended to complement the treatment found in standard works on cryptography.2 There, a detailed study is made of the many standard types of codes and ciphers, and of the ways of breaking them. We will be more concerned with the general mathematical structure and properties of secrecy systems.", "authors": ["Claude E. Shannon"], "id": "e073a7c5a6418d96fc16d8337a6056a457e75c1e", "title": "Communication theory of secrecy systems", "references": []}, {"date": "", "abstract": "This invention concerns dibromo- and tribromomethanesulfonamides of the formula } }1 }BRxH3-xCSO2N ANGLE }2 } wherein } }1 }N ANGLE }2 } is a heterocyclic ring which may contain oxygen as another hetero atom and may be substituted with 1 to 2 lower alkyl groups, and x is 2 or 3. The compounds have antimicrobial activity.", "authors": ["P. A. Macmahon"], "id": "f7ba69c1faa0187fd2366c9745bb4a387f7439ec", "title": "The Enumeration of the Partitions of Multipartite Numbers", "references": []}, {"date": "", "abstract": "\"T^epression was for nearly two-thousand years the only mode of curing cataract, and was, with little change, performed according to the description and precepts of Celsus, and of the Alexandrian school, until about the middle of the last century, when the operation of extraction was invented by Daviel. Since that time, a competition has existed between the abettors oi these two modes of operating, and the latter, by the preference it obtained, had nearly banished the former from practice. Both operations found numerous supporters, and various improve-", "authors": ["Von Buchhorn"], "id": "51cd6f4d151aef0f77f81f5f664d4ee74db17927", "title": "Die Keratonyxis, eine neue gefahrlosere Methode den grauen Staar zu operiren", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Observations on Some of the Most Frequent and Important Diseases of the Heart\" by Allan F. Burns", "authors": ["Allan F. Burns"], "id": "41b52a83ae1c27fad5ce9dd21d0979527db627b5", "title": "Observations on Some of the Most Frequent and Important Diseases of the Heart", "references": []}, {"date": "1974", "abstract": "w,here H(n) H(n, ) k-l(k 1) __o d(n). The numbers H(n) are the extended Eulerian numbers; when n is square-free, H(n) is an Eulerian number. Properties of the extended Eulerian numbers may be found in [1]. Let f f(n) denote (as usual) the total number of prime factors of n, e.g. f(12) 3. In this paper we give an asymptotic formula for H(n) as . This formula is then used to sharpen some estimates of Hille [2] and to produce various other estimates for H(n). Hille obtained estimates for certain sums H(n) and therefrom deduced an upper bound and an f-result for H(n). He remarked that his upper bound was probably not very sharp when the number of distinct prime factors of n is large. We study the growth of H(n) by estimating the serieso ,-k d(n) given above. This direct approach enables us to sharpen Hille\u2019s upper bound when f(n) is large and also to improve his f-result. We remark that H(n) grows at least exponentially with f; in fact, H(n) >_ k--l(/(k1))\u2019. For if n > 1, ( ) H(n) _> -1(_ 1) =oZ k-k dk(2) k-l( 1) o-h2 2/c 1", "authors": ["R. J. Evans"], "id": "c718e776751d477e77e64b399a4c40799fd05030", "title": "An asymptotic formula for extended Eulerian numbers", "references": ["ab1f2229bb7edbe667d78a31b75ea510efcdffe0"]}, {"date": "", "abstract": "Semantic Scholar extracted view of \"Highly Composite Numbers\" by Sujatha Ramanujan", "authors": ["Sujatha Ramanujan"], "id": "16066daee51d474e1e0085c61da61b7907bb5dc5", "title": "Highly Composite Numbers", "references": []}, {"date": "1968", "abstract": "Semantic Scholar extracted view of \"Algebra-based methods for solving multivariate polynomial equations\" by S. Y. Ku et al.", "authors": ["S. Y. Ku", "Robert J. Adler"], "id": "b8c72ae80177a3c935d94eb9256753f477a251d2", "title": "Algebra-based methods for solving multivariate polynomial equations", "references": []}, {"date": "", "abstract": "undefined", "authors": [], "id": "4ad5ff44b8210b99e7451fb959941b13868f079d", "title": "An Essay on the Use of a Regulated Temperature in Winter-Cough and Consumption, &c.", "references": []}, {"date": "1968", "abstract": "Semantic Scholar extracted view of \"The compleat euclidean algorithm\" by W. G. S. Brown", "authors": ["W. G. S. Brown"], "id": "a9d6941c8c8300eddd7b0835345750a1def21597", "title": "The compleat euclidean algorithm", "references": []}, {"date": "1966", "abstract": "The elimination procedure as described by Williams has been coded in LISP and FORMAC and used in solving systems of polynomial equations. It is found that the method is very effective in the case of small systems, where it yields all solutions without the need for initial estimates. The method, by itself, appears inappropriate, however, in the solution of large systems of equations due to the explosive growth in the intermediate equations and the hazards which arise when the coefficients are truncated. A comparison is made with difficulties found in other problems in non-numerical mathematics such as symbolic integration and simplification.", "authors": ["Joel Moses"], "id": "3c9e6553631a3576fcfb7d6bc49228527ef07e30", "title": "Solutions of systems of polynomial equations by elimination", "references": ["f63ba2b042c0e8162541527a0ab070d40437e240", "0a9cbd1f68b2aa6a5781f1b5aedd1d22ad869da1"]}, {"date": "1975", "abstract": "/ = lim inf \u0302 11-tl . \u00bb->\u00bb log pn The purpose of this paper is to combine the methods used in two earlier papers1 in order to prove the following theorem. Theorem. (1) / = c(l + 40)/5, where c< 42/43. The precise definition of c is given by (18) and (8) below. The theorem is an improvement on the result /^(14-40)/5 obtained in II. It was shown in III that (2) I = d = 0.966 < 57/59. The numbers c and d are connected by the relation 1 d = (3 42\"2)(1 c)/4 > 1 c, so that (1) is an improvement on (2) only if \u00a9 is not too close to unity, in fact, if 0< 0.986 \u2022 \u2022 \u2022 . In particular, if the \"grand Riemann hypothesis\" is true, that is, if 0 = 1/2, we have (using the value of c given by (18)) / ^ 3c/5 < 109/186 = 0.58602 2. Notation. Since (2) is sharper than (1) for 0 = 1, we shall assume that 0<1, and write (3) a = (1 + 4\u00a9)/5. Let N be a large positive integer, and define (4) X = A^-'Oog Af)-\u00bb\u00ab+T)", "authors": ["Sabur\u00f4 Uchiyama"], "id": "6a1cdadd4bee07140a20a52447bfbc89fc16bacc", "title": "On the difference between consecutive prime numbers", "references": ["2ced9f29b4bd8193272cd8752fdc53d4979de6d1"]}, {"date": "1965", "abstract": "A mathematical scientist experiments. Today, his test tube and his breadboard are blackboard and paper. He may, it is true, have available a computer, but its role is numerical and its results are delivered not today, not tomorrow, but the day after the final programming bug is corrected. The computer is not present during the most creative phases of the scientist's labor. The purpose of MATHLAB is to provide the scientist with computational aid of a much more intimate and liberating nature.", "authors": ["Carl Engelman"], "id": "f63ba2b042c0e8162541527a0ab070d40437e240", "title": "MATHLAB: a program for on-line machine assistance in symbolic computations", "references": []}, {"date": "1986", "abstract": "Semantic Scholar extracted view of \"Classical problems in number theory\" by W\u0142adys\u0142aw Narkiewicz", "authors": ["W\u0142adys\u0142aw Narkiewicz"], "id": "34b5484f1c02ab46075386ea2aadb8b4167bed9c", "title": "Classical problems in number theory", "references": []}, {"date": "1980", "abstract": "To make flashlamps in which the filler material has sharply bent foil shreds, which are buckled, the shreds are sucked through a supply tube by an air flow, preferably of about 3 at and accelerated in the first conduit, which terminates in a chamber, or end wall against which the threads are flung so that they will buckle by the impact; the buckled foil shreds are then removed by an air flow of, for example from 2 to 3 at, in a direction essentially perpendicular to the original direction of flow, the second conduit terminating in a second chamber having another end wall against which the already buckled shreds are flung to be again buckled to form final sharp bends. The sharply bent shreds are then moved by means of residual air flow into a lamp bulb, and then sealed into the bulb.", "authors": ["Ren\u00e9 Schoof"], "id": "c333f4551e8954d25c1c8b2e75602aca7fb3b140", "title": "Quadratic fields and factorization", "references": []}, {"date": "1987", "abstract": "This paper describes a new class of Hermite normal form solution procedures which perform modulo determinant arithmetic throughout the computation. This class of procedures is shown to possess a polynomial time complexity bound which is a function of the length of the input string. Computational results are also given.", "authors": ["Paul D. Domich", "R. Kannan", "Leslie E. Trotter"], "id": "3bdb5b85fa942bb962f899187c292a55d53f2c04", "title": "Hermite Normal Form Computation Using Modulo Determinant Arithmetic", "references": []}, {"date": "1982", "abstract": "1. The Factorization of Integers.- 1.1 Divisibility.- 1.2 Prime Numbers and Composite Numbers.- 1.3 Prime Numbers.- 1.4 Integral Modulus.- 1.5 The Fundamental Theorem of Arithmetic.- 1.6 The Greatest Common Factor and the Least Common Multiple.- 1.7 The Inclusion-Exclusion Principle.- 1.8 Linear Indeterminate Equations.- 1.9 Perfect Numbers.- 1.10 Mersenne Numbers and Fermat Numbers.- 1.11 The Prime Power in a Factorial.- 1.12 Integral Valued Polynomials.- 1.13 The Factorization of Polynomials.- Notes.- 2. Congruences.- 2.1 Definition.- 2.2 Fundamental Properties of Congruences.- 2.3 Reduced Residue System.- 2.4 The Divisibility of 2p-1-1 by p2.- 2.5 The Function ?(m).- 2.6 Congruences.- 2.7 The Chinese Remainder Theorem.- 2.8 Higher Degree Congruences.- 2.9 Higher Degree Congruences to a Prime Power Modulus.- 2.10 Wolstenholme's Theorem.- 3. Quadratic Residues.- 3.1 Definitions and Euler's Criterion.- 3.2 The Evaluation of Legendre's Symbol.- 3.3 The Law of Quadratic Reciprocity.- 3.4 Practical Methods for the Solutions.- 3.5 The Number of Roots of a Quadratic Congruence.- 3.6 Jacobi's Symbol.- 3.7 Two Terms Congruences.- 3.8 Primitive Roots and Indices.- 3.9 The Structure of a Reduced Residue System.- 4. Properties of Polynomials.- 4.1 The Division of Polynomials.- 4.2 The Unique Factorization Theorem.- 4.3 Congruences.- 4.4 Integer Coefficients Polynomials.- 4.5 Polynomial Congruences with a Prime Modulus.- 4.6 On Several Theorems Concerning Factorizations.- 4.7 Double Moduli Congruences.- 4.8 Generalization of Fermat's Theorem.- 4.9 Irreducible Polynomials mod p.- 4.10 Primitive Roots.- 4.11 Summary.- 5. The Distribution of Prime Numbers.- 5.1 Order of Infinity.- 5.2 The Logarithm Function.- 5.3 Introduction.- 5.4 The Number of Primes is Infinite.- 5.5 Almost All Integers are Composite.- 5.6 Chebyshev's Theorem.- 5.7 Bertrand's Postulate.- 5.8 Estimation of a Sum by an Integral.- 5.9 Consequences of Chebyshev's Theorem.- 5.10 The Number of Prime Factors of n.- 5.11 A Prime Representing Function.- 5.12 On Primes in an Arithmetic Progression.- Notes.- 6. Arithmetic Functions.- 6.1 Examples of Arithmetic Functions.- 6.2 Properties of Multiplicative Functions.- 6.3 The Mobius Inversion Formula.- 6.4 The Mobius Transformation.- 6.5 The Divisor Function.- 6.6 Two Theorems Related to Asymptotic Densities.- 6.7 The Representation of Integers as a Sum of Two Squares.- 6.8 The Methods of Partial Summation and Integration.- 6.9 The Circle Problem.- 6.10 Farey Sequence and Its Applications.- 6.11 Vinogradov's Method of Estimating Sums of Fractional Parts.- 6.12 Application of Vinogradov's Theorem to Lattice Point Problems.- 6.13 ?-results.- 6.14 Dirichlet Series.- 6.15 Lambert Series.- Notes.- 7. Trigonometric Sums and Characters.- 7.1 Representation of Residue Classes.- 7.2 Character Functions.- 7.3 Types of Characters.- 7.4 Character Sums.- 7.5 Gauss Sums.- 7.6 Character Sums and Trigonometric Sums.- 7.7 From Complete Sums to Incomplete Sums.- 7.8 Applications of the Character Sum $$\\sum\\limits_{x = 1}^p {\\left( {\\frac{{x^2 + ax + b}}{p}} \\right)} $$.- 7.9 The Problem of the Distribution of Primitive Roots.- 7.10 Trigonometric Sums Involving Polynomials.- Notes.- 8. On Several Arithmetic Problems Associated with the Elliptic Modular Function.- 8.1 Introduction.- 8.2 The Partition of Integers.- 8.3 Jacobi's Identity.- 8.4 Methods of Representing Partitions.- 8.5 Graphical Method for Partitions.- 8.6 Estimates for p(n).- 8.7 The Problem of Sums of Squares.- 8.8 Density.- 8.9 A Summary of the Problem of Sums of Squares.- 9. The Prime Number Theorem.- 9.1 Introduction.- 9.2 The Riemann ?-Function.- 9.3 Several Lemmas.- 9.4 A Tauberian Theorem.- 9.5 The Prime Number Theorem.- 9.6 Selberg's Asymptotic Formula.- 9.7 Elementary Proof of the Prime Number Theorem.- 9.8 Dirichlet's Theorem.- Notes.- 10. Continued Fractions and Approximation Methods.- 10.1 Simple Continued Fractions.- 10.2 The Uniqueness of a Continued Fraction Expansion.- 10.3 The Best Approximation.- 10.4 Hurwitz's Theorem.- 10.5 The Equivalence of Real Numbers.- 10.6 Periodic Continued Fractions.- 10.7 Legendre's Criterion.- 10.8 Quadradic Indeterminate Equations.- 10.9 Pell's Equation.- 10.10 Chebyshev's Theorem and Khintchin's Theorem.- 10.11 Uniform Distributions and the Uniform Distribution of n? (mod 1).- 10.12 Criteria for Uniform Distributions.- 11. Indeterminate Equations.- 11.1 Introduction.- 11.2 Linear Indeterminate Equations.- 11.3 Quadratic Indeterminate Equations.- 11.4 The Solution to ax2 + bxy + cy2=k.- 11.5 Method of Solution.- 11.6 Generalization of Soon Go's Theorem.- 11.7 Fermat's Conjecture.- 11.8 Markoff's Equation.- 11.9 The Equation x3 + y3 + z3 + ?3=0.- 11.10 Rational Points on a Cubic Surface.- Notes.- 12. Binary Quadratic Forms.- 12.1 The Partitioning of Binary Quadratic Forms into Classes.- 12.2 The Finiteness of the Number of Classes.- 12.3 Kronecker's Symbol.- 12.4 The Number of Representations of an Integer by a Form.- 12.5 The Equivalence of Formsmod q.- 12.6 The Character System for a Quadratic Form and the Genus.- 12.7 The Convergence of the Series K(d).- 12.8 The Number of Lattice Points Inside a Hyperbola and an Ellipse.- 12.9 The Limiting Average.- 12.10 The Class Number: An Analytic Expression.- 12.11 The Fundamental Discriminants.- 12.12 The Class Number Formula.- 12.13 The Least Solution to Pell's Equation.- 12.14 Several Lemmas.- 12.15 Siegel's Theorem.- Notes.- 13. Unimodular Transformations.- 13.1 The Complex Plane.- 13.2 Properties of the Bilinear Transformation.- 13.3 Geometric Properties of the Bilinear Transformation.- 13.4 Real Transformations.- 13.5 Unimodular Transformations.- 13.6 The Fundamental Region.- 13.7 The Net of the Fundamental Region.- 13.8 The Structure of the Modular Group.- 13.9 Positive Definite Quadratic Forms.- 13.10 Indefinite Quadratic Forms.- 13.11 The Least Value of an Indefinite Quadratic Form.- 14. Integer Matrices and Their Applications.- 14.1 Introduction.- 14.2 The Product of Matrices.- 14.3 The Number of Generators for Modular Matrices.- 14.4 Left Association.- 14.5 Invariant Factors and Elementary Divisors.- 14.6 Applications.- 14.7 Matrix Factorizations and Standard Prime Matrices.- 14.8 The Greatest Common Factor and the Least Common Multiple.- 14.9 Linear Modules.- 15. p-adic Numbers.- 15.1 Introduction.- 15.2 The Definition of a Valuation.- 15.3 The Partitioning of Valuations into Classes.- 15.4 Archimedian Valuations.- 15.5 Non-Archimedian Valuations.- 15.6 The ?-Extension of the Rationals.- 15.7 The Completeness of the Extension.- 15.8 The Representation of p-adic Numbers.- 15.9 Application.- 16. Introduction to Algebraic Number Theory.- 16.1 Algebraic Numbers.- 16.2 Algebraic Number Fields.- 16.3 Basis.- 16.4 Integral Basis.- 16.5 Divisibility.- 16.6 Ideals.- 16.7 Unique Factorization Theorem for Ideals.- 16.8 Basis for Ideals.- 16.9 Congruent Relations.- 16.10 Prime Ideals.- 16.11 Units.- 16.12 Ideal Classes.- 16.13 Quadratic Fields and Quadratic Forms.- 16.14 Genus.- 16.15 Euclidean Fields and Simple Fields.- 16.16 Lucas's Criterion for the Determination of Mersenne Primes.- 16.17 Indeterminate Equations.- 16.18 Tables.- Notes.- 17. Algebraic Numbers and Transcendental Numbers.- 17.1 The Existence of Transcendental Numbers.- 17.2 Liouville's Theorem and Examples of Transcendental Numbers.- 17.3 Roth's Theorem on Rational Approximations to Algebraic Numbers.- 17.4 Application of Roth's Theorem.- 17.5 Application of Thue's Theorem.- 17.6 The Transcendence of e.- 17.7 The Transcendence of ?.- 17.8 Hilbert's Seventh Problem.- 17.9 Gelfond's Proof.- Notes.- 18. Waring's Problem and the Problem of Prouhet and Tarry.- 18.1 Introduction.- 18.2 Lower Bounds for g(k) and G(k).- 18.3 Cauchy's Theorem.- 18.4 Elementary Methods.- 18.5 The Easier Problem of Positive and Negative Signs.- 18.6 Equal Power Sums Problem.- 18.7 The Problem of Prouhet and Tarry.- 18.8 Continuation.- 19. Schnirelmann Density.- 19.1 The Definition of Density and its History.- 19.2 The Sum of Sets and its Density.- 19.3 The Goldbach-Schnirelmann Theorem.- 19.4 Selberg's Inequality.- 19.5 The Proof of the Goldbach-Schnirelmann Theorem.- 19.6 The Waring-Hiibert Theorem.- 19.7 The Proof of the Waring-Hiibert Theorem.- Notes.- 20. The Geometry of Numbers.- 20.1 The Two Dimensional Situation.- 20.2 The Fundamental Theorem of Minkowski.- 20.3 Linear Forms.- 20.4 Positive Definite Quadratic Forms.- 20.5 Products of Linear Forms.- 20.6 Method of Simultaneous Approximations.- 20.7 Minkowski's Inequality.- 20.8 The Average Value of the Product of Linear Forms.- 20.9 Tchebotaref's Theorem.- 20.10 Applications to Algebraic Number Theory.- 20.11 The Least Value for |?|.", "authors": ["Loo Keng Hua"], "id": "9b8e6f026ba59e60b5f56e02b641ce0b748f2e03", "title": "Introduction to number theory", "references": []}, {"date": "1996", "abstract": "If two separated observers are supplied with entanglement, in the form of n pairs of particles in identical partly entangled pure states, one member of each pair being given to each observer, they can, by local actions of each observer, concentrate this entanglement into a smaller number of maximally entangled pairs of particles, for example, Einstein-Podolsky-Rosen singlets, similarly shared between the two observers. The concentration process asymptotically conserves entropy of entanglement---the von Neumann entropy of the partial density matrix seen by either observer---with the yield of singlets approaching, for large n, the base-2 entropy of entanglement of the initial partly entangled pure state. Conversely, any pure or mixed entangled state of two systems can be produced by two classically communicating separated observers, drawing on a supply of singlets as their sole source of entanglement. \\textcopyright{} 1996 The American Physical Society.", "authors": ["Bennett", "Bernstein", "Popescu", "Schumacher"], "id": "9664534349b8b166d7ac341eed2c5ffc52324d97", "title": "Concentrating partial entanglement by local operations.", "references": ["f6e8937d89ad84080f3181be87c4f19f3a29fa39", "e44193a72ea571878bc157e61a95a9e1dc218dae", "93abc3947cc81f18289a06b641dc0accdf9fc242"]}, {"date": "1974", "abstract": "A modification of Fermat's difference of squares method is used for factoring large integers. This modification permits factoring n in O(n I /3) elementary operations, where addition, subtraction, multiplication, division, or the extraction of a square root is considered as an elementary operation. A principal part is played by the use of a dissection of the continuum similar to the Farey dissection. This has been programmed for n < 1.05 X 1020 on the CDC 6400.", "authors": ["R. Sherman Lehman"], "id": "fe39a6fb771986af24fa02f0ab6c4334148bd1bd", "title": "Factoring large integers", "references": []}, {"date": "1979", "abstract": "Recently, Frumkin [9] pointed out that none of the well-known algorithms that transform an integer matrix into Smith [16] or Hermite [12] normal form is known to be polynomially bounded in its running time. In fact, Blankinship [3] noticed\u2014as an empirical fact\u2014that intermediate numbers may become quite large during standard calculations of these canonical forms. Here we present new algorithms in which both the number of algebraic operations and the number of (binary) digits of all intermediate numbers are bounded by polynomials in the length of the input data (assumed to be encoded in binary). These algorithms also find the multiplier-matrices K, $U'$ and $K'$ such that $AK$ and $U'AK'$ are the Hermite and Smith normal forms of the given matrix A. This provides the first proof that multipliers with small enough entries exist.", "authors": ["Ravi Kannan", "Achim Bachem"], "id": "6f1d7d966a2a24183919e3da3b7ade68e6c33118", "title": "Polynomial Algorithms for Computing the Smith and Hermite Normal Forms of an Integer Matrix", "references": ["97917d7ba767239960e59169db3c72b09e4a07c4"]}, {"date": "1985", "abstract": "a result first proved by Heilbronn [H] in 1934. The Disquisitiones also contains tables of binary quadratic forms with small class numbers (actually tables of imaginary quadratic fields of small class number with even discriminant which is a much easier problem to deal with) and Gauss conjectured that his tables were complete. In modern parlance, we can rewrite Gauss\u2019 tables (we are including both even and odd discriminants) in the following form.", "authors": ["Dorian Goldfeld"], "id": "5c34d232133ac50117a72904521c9caaef7a7e9b", "title": "Gauss' class number problem for imaginary quadratic fields", "references": ["099b9562e047e95d86fe5a782209363d50e19fbb", "345870b7d891f7343959271c4187f7e4e7e495cd", "7e75bd128bf40a78e8242434786e74a71ad548ef", "46664e40f214aec665b7de2802c4e42187c452b0", "69ae982b660592dc17e7f40da3b65d0eca1ac990", "85307f488d85a730edacb2f6e27b7778ceb8890e", "3f7277f0f3d9da9d353657b82937c4a6344c8ddb", "1bcea362be8bf535a7b71e389ccf681f3246b235", "fde926fb26a59fd05fec87ca0e4447a3f5250294", "70dde5d2141cc2075b389db78cb1068bb1376770"]}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"Class number, a theory of factorization, and genera\" by Daniel Shanks", "authors": ["Daniel Shanks"], "id": "95565db6e6afc1cff4b1e8a674d163bd01cecc07", "title": "Class number, a theory of factorization, and genera", "references": []}, {"date": "1936", "abstract": "Semantic Scholar extracted view of \"A problem in \"Factorisatio Numerorum\"\" by Einar Hille", "authors": ["Einar Hille"], "id": "ab1f2229bb7edbe667d78a31b75ea510efcdffe0", "title": "A problem in ", "references": []}, {"date": "1976", "abstract": "In this paper we present two algorithms for testing primality of an integer. The first algorithm runs in 0(n1/7) steps; while, the second runs in 0(log4n) step but assumes the Extended Riemann Hypothesis. We also show that a class of functions which includes the Euler phi function are computationally equivalent to factoring integers.", "authors": ["Gary L. Miller"], "id": "2f1a5034a2e1257b7ad910d0939114123cd2a676", "title": "Riemann's Hypothesis and Tests for Primality", "references": []}, {"date": "1970", "abstract": "Semantic Scholar extracted view of \"On Integers All of Whose Prime Factors are Small\" by Heini Halberstam", "authors": ["Heini Halberstam"], "id": "f1c215b3953a1e1c315cd4e16b219b395f336b3e", "title": "On Integers All of Whose Prime Factors are Small", "references": []}, {"date": "1978", "abstract": "Solovay and Strassen, and Miller and Rabin have discovered fast algorithms for testing primality which use coin-flipping and whose con", "authors": ["Gregory J. Chaitin", "Jacob T. Schwartz"], "id": "3e2c03da30f7824376f62a26246e95112fcaacb7", "title": "A note on monte carlo primality tests and algorithmic information theory", "references": ["a6054012f9be8dd8ae4403d4d778bea71881fd9d", "36a109672bf4ad2259d20813d43781f91c39dabc", "2f1a5034a2e1257b7ad910d0939114123cd2a676", "2842d53d0f2389618f8ad1c47d2ffef2c344d22f", "28d06ce92c97e18ebe8f61a9e2fe802f4d42d218", "02cfa4d4a598659977794d0e8645f14c6849b61d", "533ba306dbd8e4f483372d177d898ef4963c9a02", "e753bef6fd2b325dba0ac4667d88dbb71d3f9f5f", "153f57234618b4c3b19d8d7d12a09a16d0c2891a", "ade7a629eedd09d3c874a5f0ada3ead1d08ec52a"]}, {"date": "1940", "abstract": "In a previous paper, under the same title, I considered the problem of how far apart two consecutive primes can be. The present paper is concerned with the opposite question. How near together can large primes lie? The published literature on this subject is scanty and, though interesting, is mainly negative in character. It appears to be very difficult to give any answer that is not trivial, or that is at all illuminating.", "authors": ["Robert A. Rankin"], "id": "2ced9f29b4bd8193272cd8752fdc53d4979de6d1", "title": "The difference between consecutive prime numbers. II", "references": []}, {"date": "1964", "abstract": "Semantic Scholar extracted view of \"Hash-Coding Functions of a Complex Variable\" by William F. Martin et al.", "authors": ["William F. Martin", "Tim Hart"], "id": "0a9cbd1f68b2aa6a5781f1b5aedd1d22ad869da1", "title": "Hash-Coding Functions of a Complex Variable", "references": []}, {"date": "1954", "abstract": "Semantic Scholar extracted view of \"Elements of Number Theory\" by Ivan Matveevich Vinogradov et al.", "authors": ["Ivan Matveevich Vinogradov", "Saul Kravetz"], "id": "bade6c18ec70142ba54ef3c0fd885d982335bc08", "title": "Elements of Number Theory", "references": []}, {"date": "1974", "abstract": "1. Introduction . This paper is concerned with the problem of obtaining theoretical estimates for the number of arithmetical operations required to factorize a large integer n or test it for primality. One way of making these problems precise uses a multi-tape Turing machine (e.g. (1), although we require a version with an input tape). At the start of the calculation n is written in radix notation on one of the tapes, and the machine is to stop after writing out the factors in radix notation or after writing one of two symbols denoting \u2018prime\u2019 or \u2018composite\u2019. There are, of course, other definitions which could be used; but the differences between these are unimportant for our purpose.", "authors": ["John M. Pollard"], "id": "b39f2b6ce1014f90992f493d76c00fe06854ca0e", "title": "Theorems on factorization and primality testing", "references": []}, {"date": "1965", "abstract": "Semantic Scholar extracted view of \"Mathematical Methods of Physics.\" by M. Z. von Krzywoblocki et al.", "authors": ["M. Z. von Krzywoblocki", "Jon Mathews", "R. L. Walker"], "id": "974af5a7d08d0031ec62eb33c86c11fa3326bfff", "title": "Mathematical Methods of Physics.", "references": []}, {"date": "1971", "abstract": "This paper reviews some of the known algorithms for factoring polynomials over finite fields and presents a new deterministic procedure for reducing the problem of factoring an arbitrary polynomial over the Galois field GF(p m) to the problem of finding the roots in GF(p) of certain other polynomials over GF(p). The amount of computation and the storage space required by these algorithms are algebraic in both the degree of the polynomial to be factored and the logarithm of the order of the finite field.\n Certain observations on the application of these methods to the factorization of polynomials over the rational integers are also included.", "authors": ["Elwyn R. Berlekamp"], "id": "4f942cd0174a928b56245cd639d488e195baa60d", "title": "Factoring polynomials over large finite fields*", "references": []}, {"date": "1963", "abstract": "0. Introduction. In this paper we study a sequence of classes of computable functions for which a prediction of the complexity of the calculation may be made in a comparatively simple fashion. The class of recursive functions is accepted as being the class of just those functions for which there is a mechanical procedure for obtaining the values from the arguments. However, this class, since it involves all computations, must involve arbitrarily complex computations. Various more restricted classes of functions have been proposed as being somehow more easily computable, but the greater ease of computation has largely been left on the level of intuition. The notable exception to this is the class F of functions computable(') by finite automata. However, this class is too severely restricted; it does not even include multiplication. We present a hierarchy of more inclusive classes of functions, each defined as the class of functions whose computational complexity is \"predictable\" by a function in the preceding class. It is known that the class F of numerical functions computable by finite auto", "authors": ["Robert W. Ritchie"], "id": "d268511f8a8abe7a97a73b63b46dbc861fb94adc", "title": "CLASSES OF PREDICTABLY COMPUTABLE FUNCTIONS", "references": ["fc9f7de5c4fae65d901bbe2edbda4a7b5d365e56", "b449b779203f4392211e6db98720d46eecc0e4bd", "6e033c9dd8926a94aac7a025c5073ac68b0235c4", "2a6a67a3019c9fc81de3d2cf691a9265176a7fb0", "5ec7cd81a0b4cf798c30a5223342025d509fdba4"]}, {"date": "", "abstract": "Le disposizioni normative e in ultimo l\u2019art. 3 della Legge 11 gennaio 1996 n. 23, che disciplinando in maniera organica la materia di edilizia scolastica, hanno attribuito ai Comuni l\u2019onere della realizzazione, manutenzione e messa a norma degli immobili e relative pertinenze adibiti all\u2019uso scolastico, con riferimento ai nidi e alle scuole dell\u2019infanzia, primarie e secondarie di primo grado. Il patrimonio edilizio scolastico della Citt\u00e0, che si compone di circa 315 edifici di propriet\u00e0 per un totale di circa 1 milione e 350 mila mq, \u00e8 salvaguardato e mantenuto in efficienza dal Servizio Edilizia Scolastica per l\u2019aspetto edile ed impiantistico idraulico, mentre per quanto riguarda l\u2019impiantistica termica ed elettrica vi provvede la Soc. IREN Energia in virt\u00f9 di contratti di servizio stipulati con la Citt\u00e0. Tale patrimonio \u00e8 costituito da edifici risalenti a fine Ottocento, primi Novecento che rivestono interesse storico-architettonico, soggetti a vincolo da parte degli enti di tutela, e prevalentemente, da edifici scolastici pi\u00f9 recenti costruiti tra gli anni sessanta ed ottanta, nei quali si manifestano con cadenza periodica problemi manutentivi legati all\u2019invecchiamento fisiologico di numerosi materiali. Inoltre la continua evoluzione della normativa tecnica di riferimento impone sia la realizzazione di opere edili necessarie per adeguare e migliorare le strutture edilizie esistenti, sia l\u2019obbligo di effettuare verifiche semestrali o annuali sui dispositivi di sicurezza attivi e passivi. Occorre anche tenere in considerazione che il cambiamento che ha caratterizzato la nostra societ\u00e0 negli ultimi decenni ha accelerato il superamento del modello industriale di scolarizzazione, pertanto le esigenze pedagogiche e didattiche, dinamiche e in continua evoluzione, richiedono un costante sforzo di adeguamento delle strutture edilizie in esse ospitate. Gli interventi richiesti per migliorare ed adeguare i livelli prestazionali degli edifici scolastici nella loro totalit\u00e0 devono tener conto non solo quindi degli aspetti architettonici-", "authors": ["Citt\u00e0 DI Torino", "Determinazione Dirigenziale", "N. Cronologico"], "id": "444537108d9171272bfbb0eccd7a3855ada66b0d", "title": "N", "references": []}, {"date": "1986", "abstract": "This paper is devoted to the description and analysis of a new algorithm to factor positive integers. It depends on the use of elliptic curves. The new method is obtained from Pollard's (p - 1)-method (Proc. Cambridge Philos. Soc. 76 (1974), 521-528) by replacing the multiplicative group by the group of points on a random elliptic curve. It is conjectured that the algorithm determines a non-trivial divisor of a composite number n in expected time at most K( p)(log n)2, where p is the least prime dividing n and K is a function for which log K(x) = /(2 + o(1))log x log log x for x -x o. In the worst case, when n is the product of two primes of the same order of magnitude, this is", "authors": ["H. W. Lenstra"], "id": "307ab08c3d4f551019297d2480597c614af8069c", "title": "Factoring integers with elliptic curves", "references": ["8e7ba25aa34a583c79de304da8228f52925c854a", "a377c34ff3252c94ae864b7e0cfa64c05d01ef6d", "d5d7e306e7563ed23dd1b7382d26ca6b933817ec", "24f3cc4bd6f6df45016a3197539b83b79c1c655c", "770be78c27f3086e0f09837b70d6bcdfb436b3b1", "b3eabd361ed308049df1907f3995d19253f145a1", "4e09314d22260dc30db8e93fd26313feae402a28", "041091d3231ceac8691eb9975ff3b7f17043e06b"]}, {"date": "1972", "abstract": "Abstract A conjecture concerning linear forms in the logarithms of algebraic numbers is made. It is shown that this conjecture allows an effective determination of all imaginary quadratic fields of class number 2.", "authors": ["Larry Joel Goldstein"], "id": "70dde5d2141cc2075b389db78cb1068bb1376770", "title": "Imaginary quadratic fields of class number 2", "references": []}, {"date": "1962", "abstract": "As an attempt to investigate a general theory of real-time computability in digital computers, a subclass of Turing machines is formally introduced together with some classes of functions that are computable by them in real time. Then the existence is established of a class of recursive functions that are not computable in real time by use of a class of machines, no matter how general we make the machines subject to a given constraint.", "authors": ["Hisao Yamada"], "id": "35b74d763e74b0ec7e29f3f106e9402bc6ccc5ea", "title": "Real-Time Computation and Recursive Functions Not Real-Time Computable", "references": []}, {"date": "1995", "abstract": "A theorem is proven for quantum information theory that is analogous to the noiseless coding theorem of classical information theory. In the quantum result, the von Neumann entropy S of the density operator describing an ensemble of pure quantum signal states is equal to the number of spin-1/2 systems (quantum bits or \\qubits\") necessary to represent the signal faithfully. The theorem holds whether or not the signal states are orthogonal. Related results are also presented about the delity of quantum coding and about representing entangled quantum states.", "authors": ["Schumacher"], "id": "f6e8937d89ad84080f3181be87c4f19f3a29fa39", "title": "Quantum coding.", "references": ["a13bd077717cb28cace6fd97c2fedeab8ba2449c"]}, {"date": "1969", "abstract": "A review. Quantum detection theory is a reformulation, in quantum-mechanical terms, of statistical decision theory as applied to the detection of signals in random noise. Density operators take the place of the probability density functions of conventional statistics. The optimum procedure for choosing between two hypotheses, and an approximate procedure valid at small signal-to-noise ratios and called threshold detection, are presented. Quantum estimation theory seeks best estimators of parameters of a density operator. A quantum counterpart of the Cram\u00e9r-Rao inequality of conventional statistics sets a lower bound to the mean-square errors of such estimates. Applications at present are primarily to the detection and estimation of signals of optical frequencies in the presence of thermal radiation.", "authors": ["Carl W. Helstrom"], "id": "93abc3947cc81f18289a06b641dc0accdf9fc242", "title": "Quantum detection and estimation theory", "references": []}, {"date": "1971", "abstract": "* uschian groups of the first kind * Automorphic forms and functions * Hecke operators and the zeta-functions associated with modular forms * Elliptic curves * Abelian extensions of imaginary quadratic fields and complex multiplication of elliptic curves * Modular functions of higher level * Zeta-functions of algebraic curves and abelian varieties * The cohomology group assoicated with cusp forms * Arithmetic Fuschian groups", "authors": ["\u5fd7\u6751 \u4e94\u90ce"], "id": "fde926fb26a59fd05fec87ca0e4447a3f5250294", "title": "Introduction to the arithmetic theory of automorphic functions", "references": []}, {"date": "1975", "abstract": "L\u2019acc\u00e8s aux archives de la revue \u00ab Annali della Scuola Normale Superiore di Pisa, Classe di Scienze \u00bb (http://www.sns.it/it/edizioni/riviste/annaliscienze/) implique l\u2019accord avec les conditions g\u00e9n\u00e9rales d\u2019utilisation (http://www.numdam.org/legal.php). Toute utilisation commerciale ou impression syst\u00e9matique est constitutive d\u2019une infraction p\u00e9nale. Toute copie ou impression de ce fichier doit contenir la pr\u00e9sente mention de copyright.", "authors": ["Dorian Goldfeld"], "id": "1bcea362be8bf535a7b71e389ccf681f3246b235", "title": "An asymptotic formula relating the Siegel zero and the class number of quadratic fields", "references": ["f421f903b5e9f96fe388dcc956a3565835d26a02"]}, {"date": "1971", "abstract": "A survey is made of our state of knowledge concerning the effective determination of all imaginary quadratic fields with prescribed class numbers. Recent advances are discussed relating to linear forms in the logarithms of algebraic numbers fundamental to the theory, and applications are mentioned to other arithmetical questions. It is my privilege once again to address a meeting of the American Mathematical Society, and I am particularly pleased that the present session has been held here in this historic capital. I t may perhaps be regarded as an especially appropriate locality, for it was in this city just over half a century ago that Dickson's celebrated History of the theory of numbers was published, and these scholarly tomes have undoubtedly inspired much progress in the topic that forms the theme of my talk. The branch of research about which I shall speak is in fact one of the oldest in the realms of mathematics, originating indeed from the famous Disquisitiones Arithmeticae of Gauss* It was here that Gauss laid the foundations of the theory of binary quadratic forms which, with certain modifications in terminology, has become our modern theory of quadratic fields. Gauss showed how one could divide the set of all binary quadratic forms with a given discriminant into classes such that two forms belong to the same class if and only if there exists an integral unimodular substitution relating them, and he showed also how one could combine the classes into genera so that two forms are in the same genus if and only if they are rationally equivalent. He also raised a number of notorious problems; in particular, in Article 303, he conjectured that there are only finitely many negative discriminants associated with any given class number and moreover that the tables of discriminants which he had drawn up in the cases of relatively small class numbers were in fact complete. He added, however: \" Demons trationes autem rigorosae harum AMS 1970 subject classifications. Primary 12A25, 12A50; Secondary 10F35.", "authors": ["A. Baker"], "id": "3f7277f0f3d9da9d353657b82937c4a6344c8ddb", "title": "On the class number of imaginary quadratic fields", "references": []}, {"date": "1971", "abstract": "The purpose of this note is to state precisely and prove the following informal statement: If T is a theory and a is a new axiom such that r + n o n a is an undecidable theory then some theorems of T have much shorter proofs in T+a than in T. Notice that if T is an essentially undecidable theory, like e.g. arithmetic, this conclusion will be true provided a is a sentence which is not a theorem of T, since then T + n o n a is undecidable. Let T be a formalized theory which among its logical functors has the negation ], the implication \u2014\u00bb, and the alternative V . Let <r and r be variables ranging over sentences formulated in the language of T and a one fixed such sentence. We denote by V the G\u00f6del number of <r, although here r 1 is just any one-to-one map of the set of sentences into the set of positive integers. For any theorem r of T let W(T) be also a positive integer measuring in some way the length of the shortest proof of r in T. But all we need about r \" and W are the following conditions: (i) The set {2(2 r + 1 ) : r is valid in T and W(T) ^n} is recursive. (ii) There are recursive functions g and h such that for every <r", "authors": ["Andrzej Ehrenfeucht", "Jan Mycielski"], "id": "a6054012f9be8dd8ae4403d4d778bea71881fd9d", "title": "Abbreviating proofs by adding new axioms", "references": []}, {"date": "1974", "abstract": "This book is an introductory course in mathematical logic covering basic topics in quantification theory and recursive function theory, and is intended for the reader who is interested in artificial intelligence, computer linguistics, and other related areas. The text is theoretical, but organized with implementation in mind. Toward the end there are a few experimental subjects aiming toward systems that can examine their own behavior, and toward the semantics of programming languages. The arithmetization of metamathematics is carried out in LISP rather than in the natural numbers, following an axiomatic treatment of LISP.", "authors": ["Magnus Levin"], "id": "153f57234618b4c3b19d8d7d12a09a16d0c2891a", "title": "MATHEMATICAL LOGIC FOR COMPUTER SCIENTISTS", "references": []}, {"date": "1951", "abstract": "Semantic Scholar extracted view of \"Mathematics, queen and servant of science\" by George E. Forsythe", "authors": ["George E. Forsythe"], "id": "ade7a629eedd09d3c874a5f0ada3ead1d08ec52a", "title": "Mathematics, queen and servant of science", "references": []}, {"date": "1978", "abstract": "Improved diecuttable polyester urethane foams are provided by the inclusion in the foam formulation of specified minor amounts of certain low molecular weight polyols, typically having hydroxyl numbers of 100 or greater. Representative useful species of such polyols include aliphatic alcohols, such as glycerol and erythritol, polymethylols such as trimethylolpropane, alkanolamines such as triethanolamine and relatively low molecular weight alkylene oxide adducts such as propylene - glycerol adducts and the like. Especially effective diecuttable foam formulations are provided by further employing, as co-additives, certain alkylene oxide adducts of linear alcohols or phenols, sulfonated petroleum oils, ammonium salts and silicone-containing copolymers. If desired, the viscosity and compatibility characteristics of the additive and/or additive and co-additive mixture may be modified by the incorporation of a diluent.", "authors": ["Margaret J. Davis"], "id": "e753bef6fd2b325dba0ac4667d88dbb71d3f9f5f", "title": "What is a computation? in Mathematics Today | Twelve Informal Essays", "references": []}, {"date": "1967", "abstract": "From the contents: Primes in Arithmetic Progression.- Gauss' Sum.- Cyclotomy.- Primes in Arithmetic Progression: The General Modulus.- Primitive Characters.- Dirichlet's Class Number Formula.- The Distribution of the Primes.- Riemann's Memoir.- The Functional Equation of the L Function.- Properties of the Gamma Function.- Integral Functions of Order 1.- The Infinite Products for xi(s) and xi(s,Zero-Free Region for zeta(s).- Zero-Free Regions for L(s, chi).- The Number N(T).- The Number N(T, chi).- The explicit Formula for psi(x).- The Prime Number Theorem.- The Explicit Formula for psi(x,chi).- The Prime Number Theorem for Arithmetic Progressions (I).- Siegel's Theorem.- The Prime Number Theorem for Arithmetic Progressions (II).- The Polya-Vinogradov Inequality.- Further Prime Number Sums.", "authors": ["Harold Davenport"], "id": "85307f488d85a730edacb2f6e27b7778ceb8890e", "title": "Multiplicative number theory", "references": []}, {"date": "1975", "abstract": "The first is obviously constructed according to a simple rule; it consists of the number 01 repeated ten times. If one were asked to speculate on how the series might continue, one could predict with considerable confidence that the next two digits would be 0 and 1. Inspection of the second series of digits yields no such comprehensive pattern. There is no obvious rule governing the formation of the number, and there is no rational way to guess the succeeding digits. The arrangement seems haphazard; in other words, the sequence appears to be a random assortment of 0's and 1's.", "authors": ["Gregory J. Chaitin"], "id": "02cfa4d4a598659977794d0e8645f14c6849b61d", "title": "Randomness and Mathematical Proof", "references": []}, {"date": "1975", "abstract": "To prove that a number n is composite, it suffices to exhibit the working for the multiplication of a pair of factors. This working, represented as af string, is of length bounded by a polynomial in $\\log _2 n$. We show that the same property holds for the primes. It is noteworthy that almost no other set is known to have the property that short proofs for membership or nonmembership exist for all candidates without being known to have the property that such proofs are easy to come by. It remains an open problem whether a prime n can be recognized in only $\\log _2^\\alpha n$ operations of a Turing machine for any fixed $\\alpha $.The proof system used for certifying primes is as follows.Axiom. $(x,y,1)$.Inference Rules. \\[ R_1 :\\quad(p,x,a),q \\vdash (p,x,qa)\\quad\\text{provided }x^{(p - 1)/q} \\not\\equiv 1(\\bmod p)\\text{ and }q | (p - 1). \\]\\[ R_2 :\\quad(p,x,p - 1) \\vdash p\\quad\\text{provided } x^{p - 1} \\equiv 1(\\bmod p). \\]Theorem 1. pis a theorem$\\equiv p$is a prime.Theorem 2. pis a theorem$\\supset p$has a...", "authors": ["Vaughan R. Pratt"], "id": "533ba306dbd8e4f483372d177d898ef4963c9a02", "title": "Every Prime has a Succinct Certificate", "references": ["5b7f3a6efb4a1cde3b7f3a4521290cc87f230f0e", "e54bbbf8090cf11d6fd77c1827a1286753a39d3d"]}, {"date": "1964", "abstract": "Semantic Scholar extracted view of \"Some classes of recursive functions\" by Andrzej Grzegorczyk", "authors": ["Andrzej Grzegorczyk"], "id": "2a6a67a3019c9fc81de3d2cf691a9265176a7fb0", "title": "Some classes of recursive functions", "references": []}, {"date": "1974", "abstract": "An attempt is made to apply information-theoretic computational complexity to meta-mathematics. The paper studies the number of bits of instructions that must be given to a computer for it to perform finite and infinite tasks, and also the time it takes the computer to perform these tasks. This is applied to measuring the difficulty of proving a given set of theorems, in terms of the number of bits of axioms that are assumed, and the size of the proofs needed to deduce the theorems from the axioms.", "authors": ["Gregory J. Chaitin"], "id": "28d06ce92c97e18ebe8f61a9e2fe802f4d42d218", "title": "Information-Theoretic Limitations of Formal Systems", "references": []}, {"date": "1952", "abstract": "Semantic Scholar extracted view of \"Introduction to Metamathematics\" by Stephen Cole Kleene", "authors": ["Stephen Cole Kleene"], "id": "5ec7cd81a0b4cf798c30a5223342025d509fdba4", "title": "Introduction to Metamathematics", "references": []}, {"date": "1961", "abstract": "1. Motivation. Many variants of the notion of automaton have appeared in the literature. We find it convenient here to adopt the notion of E. F. Moore [7]. Inasmuch as Rabin-Scott [9] adopt this notion, too, it is convenient to refer to [9] for various results presumed here. In particular, Kleene's theorem [5, Theorems 3, 5] is used in the form in which it appears in [9]. It is often perspicacious to view regular expressions, and this notion is used in the sense of [3]. In general, we are concerned with the problems of automatically designing an automaton from a specification of a relation which is to hold between the automaton's input sequences and determined output sequences. These \"design requirements\" are given via a formula of some kind. The problems with which we are concerned have been described in [1]. With respect to particular formalisms for expressing \"design requirements\" as well as the notion of automaton itself, the problems are briefly and informally these: (1) to produce an algorithm which when it operates on an automaton and a design requirement produces the correct answer to the question \"Does this automaton satisfy this design requirement?\", or else show no such algorithm exists; (2) to produce an algorithm which operates on a design requirement and produces the correct answer to the question \"Does there exist an automaton which satisfies this design requirement?\", or else show no such algorithm exists; (3) to produce an algorithm which operates on a design requirement and terminates with an automaton which satisfies the requirement when one exists and otherwise fails to terminate, or else show no such algorithm exists. Interrelationships among problems (1), (2), (3) will appear in the paper [1]. This paper will also indicate the close connection between problem (1) and decision problems for truth of sentences of certain arithmetics. The paper [1 ] will also make use of certain results concerning weak arithmetics already obtained in the literature to obtain answers to problems (1) and (3). Thus", "authors": ["Calvin C. Elgot"], "id": "fc9f7de5c4fae65d901bbe2edbda4a7b5d365e56", "title": "Decision problems of finite automata design and related arithmetics", "references": []}, {"date": "1946", "abstract": "Semantic Scholar extracted view of \"Concatenation as a Basis for Arithmetic\" by W. V. Quine", "authors": ["W. V. Quine"], "id": "6e033c9dd8926a94aac7a025c5073ac68b0235c4", "title": "Concatenation as a Basis for Arithmetic", "references": []}, {"date": "2004", "abstract": "SummaryA method, given by D. E. Knuth for the computation of the greatest common divisor of two integers u, v and of the continued fraction for u/v is modified in such a way that only O(n(lg n)2(lglg n)) elementary steps are used for u,v<.2n.ZusammenfassungEin von D. E. Knuth angegebenes Verfahren, f\u00fcr ganze Zahlen u, v den gr\u00f6\u00dften gemeinsamen Teiler und den Kettenbruch f\u00fcr u/v zu berechnen, wird so modifiziert, da\u00df f\u00fcr n-stellige Zahlen nur O(n(lg n)2 (lglg n)) elementare Schritte gebraucht werden.", "authors": ["Arnold Sch\u00f6nhage"], "id": "770be78c27f3086e0f09837b70d6bcdfb436b3b1", "title": "Schnelle Berechnung von Kettenbruchentwicklungen", "references": []}, {"date": "1941", "abstract": "Semantic Scholar extracted view of \"Die Typen der Multiplikatorenringe elliptischer Funktionenk\u00f6rper\" by Max Deuring", "authors": ["Max Deuring"], "id": "d5d7e306e7563ed23dd1b7382d26ca6b933817ec", "title": "Die Typen der Multiplikatorenringe elliptischer Funktionenk\u00f6rper", "references": []}, {"date": "1961", "abstract": "Semantic Scholar extracted view of \"Lectures on advanced analytic number theory\" by Carl Ludwig Siegel et al.", "authors": ["Carl Ludwig Siegel", "Srikanth Raghavan"], "id": "f421f903b5e9f96fe388dcc956a3565835d26a02", "title": "Lectures on advanced analytic number theory", "references": ["1943287231eb7e7f82acb106fe698c3a7ac83901", "da6508b027b3951377f06f833050e33e55957bb2", "09c7af01be24cef10f2e101f12b03d9f3111488f", "ba2a8ac5ff9435d3057c52c753d1e187b8ce99fa", "c197ec679a7b1dcb76dc1689907964f48f856919", "634922d5cbd9ca2561b1984a3de5cde94271c012", "eac0dad8b61cd1f6e909046cd01eaa2559735a09"]}, {"date": "1985", "abstract": "In this paper we present a deterministic algorithm to compute the number of F^-points of an elliptic curve that is defined over a finite field Fv and which is given by a Weierstrass equation. The algorithm takes 0(log9 q) elementary operations. As an application wc give an algorithm to compute square roots mod p. For fixed .i e Z, it takes 0(log9p) elementary operations to compute fx mod p. 1. Introduction. In this paper we present an algorithm to compute the number of F(/-points of an elliptic curve defined over a finite field F , which is given by a Weierstrass equation. We restrict ourselves to the case where the characteristic of F^ is not 2 or 3. The algorithm is deterministic, does not depend on any unproved hypotheses and takes 0(log9 0. If one applies fast multiplication techniques, the algorithm will take 0((|x|1/2log p)6+f) elementary operations for any e > 0. Let \u00a3 be an elliptic curve defined over the prime field Fp and let an affine model of it be given by a Weierstrass equation Y2 = X3 + AX + B (A,BeFp). An explicit formula for the number of F^-points on \u00a3 is given by", "authors": ["Ren\u00e9 Schoof"], "id": "24f3cc4bd6f6df45016a3197539b83b79c1c655c", "title": "Elliptic curves over finite fields and the computation of square roots mod p", "references": ["0b84b3abc6e65745ba75c59eb27caa9c0bf469a4", "ce5339cdff76ababf789cc1462f214094d41618f", "95565db6e6afc1cff4b1e8a674d163bd01cecc07", "d5d7e306e7563ed23dd1b7382d26ca6b933817ec", "71937f0cd51dff739c91ad39c3af82b7341d3f39", "db63a2dcb83af2e0b531e3c208943b60cb0b71a3", "06bc48bd24457611a56146025801e19c1721c979", "92f1c943f00b57365082f1463deec7982227d7a5", "5fabc815e50bf77749ae36bf2eba126ab2fa08fd"]}, {"date": "1955", "abstract": "Mathematical Foundations of Quantum Mechanics was a revolutionary book that caused a sea change in theoretical physics. Here, John von Neumann, one of the leading mathematicians of the twentieth century, shows that great insights in quantum physics can be obtained by exploring the mathematical structure of quantum mechanics. He begins by presenting the theory of Hermitean operators and Hilbert spaces. These provide the framework for transformation theory, which von Neumann regards as the definitive form of quantum mechanics. Using this theory, he attacks with mathematical rigor some of the general problems of quantum theory, such as quantum statistical mechanics as well as measurement processes. Regarded as a tour de force at the time of publication, this book is still indispensable for those interested in the fundamental issues of quantum mechanics.", "authors": ["John von Neumann"], "id": "a13bd077717cb28cace6fd97c2fedeab8ba2449c", "title": "Mathematical Foundations of Quantum Mechanics", "references": []}, {"date": "1976", "abstract": "Methods and apparatus for fabricating a hollow article from sheets of juxtaposed, deformable, thermoplastic material wherein mold apparatus is provided for thermoforming a shape in at least one of the sheets. In one aspect of the invention, apparatus is provided for depositing a reinforcing member on one of the sheets, and the sheets are then fused together to sandwich the reinforcing member between the sheets. The reinforcing member may move a portion of one of the sheets into a mold cavity before it is released. In another aspect of the invention, a plug assist member is moved to a position between a pair of molds and is then swung into a cavity in one of the molds to assist in moving a portion of a plastic sheet into the mold cavity. The plug assist member is then removed and the molds are brought together to fuse the edge of the sheets.", "authors": ["Dorian Goldfeld"], "id": "46664e40f214aec665b7de2802c4e42187c452b0", "title": "The class number of quadratic fields and the conjectures of Birch and Swinnerton-Dyer", "references": ["3ecdd875fa117d9f9f3c821d728e9b11d5f856ef", "275d5ebdedf1c6076c235055b0cce1eeddff0450", "5511792b09d36d6cd7865e43af2b4f46a76afa27", "4cebcf15a6effac2142c308fd37017f02871c9f9", "637453660c8d26220690cd5a741eba3dac57adaf", "a7a7636658bb2baa1d4fd71c5cc353bbc4fb87fb", "991247f3c96f77915b63626ee50444c1706b8b3b", "1bcea362be8bf535a7b71e389ccf681f3246b235", "410c7a0ff8f28772c8a1fc9377f551f7361cf043"]}, {"date": "1984", "abstract": "\u00a9 Soci\u00e9t\u00e9 math\u00e9matique de France, 1985, tous droits r\u00e9serv\u00e9s. L\u2019acc\u00e8s aux archives de la collection \u00ab Ast\u00e9risque \u00bb (http://smf4.emath.fr/ Publications/Asterisque/) implique l\u2019accord avec les conditions g\u00e9n\u00e9rales d\u2019utilisation (http://www.numdam.org/conditions). Toute utilisation commerciale ou impression syst\u00e9matique est constitutive d\u2019une infraction p\u00e9nale. Toute copie ou impression de ce fichier doit contenir la pr\u00e9sente mention de copyright.", "authors": ["Joseph Oesterl\u00e9"], "id": "69ae982b660592dc17e7f40da3b65d0eca1ac990", "title": "Nombres de classes des corps quadratiques imaginaires", "references": ["a4afacae43427a8a245cf5271eb1b41a89cb1e6f", "d26bf083078e43d5db5697a744d00cc7f03638c6", "0a9052a44780b22027332dc3f5330bf283ebd1e4", "3f7277f0f3d9da9d353657b82937c4a6344c8ddb", "ee8a955dc6a2132e8e464bdcee22849a81a2a055", "cccfb5fa8c4c7c9f7ba0aba41e3be105e80a08a8", "ff596dfb5c3c1f5746aa6278f7793752fd423de7"]}, {"date": "1986", "abstract": "One can associate with an arbitrary algebroid formal group law F, defined over F\"p, a sequence [n]\"F(x@?) (= [n - 1]\"F(x@?) @?\"Fx@?). These sequences for various F (multiplicative group, reduced elliptic curves and Abelian varieties) provide a variety of new primality tests like Lucas' test for Mersenne primes. Implementations and relations with factorization algorithms are presented.", "authors": ["D. V. Chudnovsky", "G. V. Chudnovsky"], "id": "8e7ba25aa34a583c79de304da8228f52925c854a", "title": "Sequences of numbers generated by addition in formal groups and new primality and factorization tests", "references": []}, {"date": "1987", "abstract": "Since 1974, several algorithms have been developed that attempt to factor a large number N by doing extensive computations module N and occasionally taking GCDs with N. These began with Pollard's p 1 and Monte Carlo methods. More recently, Williams published a p + 1 method, and Lenstra discovered an elliptic curve method (ECM). We present ways to speed all of these. One improvement uses two tables during the second phases of p ? 1 and ECM, looking for a match. Polynomial preconditioning lets us search a fixed table of size n with n/2 + o(n) multiplications. A parametrization of elliptic curves lets Step 1 of ECM compute the x-coordinate of nP from that of P in about 9.3 1og2 n multiplications for arbitrary P.", "authors": ["Peter L. Montgomery"], "id": "a377c34ff3252c94ae864b7e0cfa64c05d01ef6d", "title": "Speeding the Pollard and elliptic curve methods of factorization", "references": ["b39f2b6ce1014f90992f493d76c00fe06854ca0e", "f0215c6ef8a15cc89daaac9be04089ddf480f6ea", "5647a71c0d4e4d2616ac171a89b61aca8c0b63a8", "b913cf330852035f49b4ec5fe2db86c47d8a98fd", "32a7bb5212b741f89e4acf7b51812955a0963e8f", "bd9356733929d4bdac9bcef87c4406709f4596cd", "832d9e1ac57c79b5b4943605813dd169bd73feba", "742826a56c402c68e81d2cc4db4edae3126fde8c", "4bc1656a66f55e68c738a057d8b9e49d2308eb23", "1318eeab4352b81927792347a374af86f0683a91"]}, {"date": "", "abstract": "In einer Stra\u00dfburger Dissertation hat Herr Jakob Schatunowski**) gezeigt, da\u00df der Euklidische Algorithmus des gr\u00f6\u00dften gemeinschaftlichen Teilers in quadratischen Zahlk\u00f6rpern mit einer Diskriminante der Form D=l \u2014 4m, wo m > 3 ist, im allgemeinen nicht mehr zum Ziele f\u00fchrt; er hat dort einen anderen Algorithmus angegeben, welcher in allen den quadratischen Zahlk\u00f6rpern anwendbar ist, in denen jede Zahl nur auf eine Weise in Primzahlfaktorqji***) zerlegt werden kann (ich nenne im folgenden solche Zahlk\u00f6rper einfache K\u00f6rper). Durch diese Arbeit angeregt, versuchte ich auch einen solchen Algorithmus zu finden, indem ich Euklids Algorithmus verallgemeinerte. Ein Schritt des Euklidischen Algorithmus besteht bekanntlich darin, da\u00df das Zahlenpaar a, 6 durch das Zahlenpaar 6, r ersetzt wird, wo r der Rest der Division von durch 6 ist, d. h. eine Zahl der Form a \u2014 by, welche absolut kleiner als 6 ist. Dabei haben beide Paare dieselben gemeinsamen Teiler. In manchen quadratischen K\u00f6rpern ist es aber nicht immer m\u00f6glich, wenn a und gegeben sind, eine Zahl so zu finden,", "authors": ["Georg Rabinowitsch"], "id": "345870b7d891f7343959271c4187f7e4e7e495cd", "title": "Eindeutigkeit der Zerlegung in Primzahlfaktoren in quadratischen Zahlk\u00f6rpern*).", "references": []}, {"date": "1961", "abstract": "Let R be a c o m m u t a t i v e r ing with 1. \u00c0 bilinear fo rm over R is a pa i r ( M , b) where M is a finitely gene ra t ed pro jec t ive A m o d u l e and b : M x M \u2014\u2022 R is s y m m e t r i c bilinear. If M is f r ee and e i , . . . , t n is a basis for M then b can be descr ibed by the symmet r i c n x n ina t r i x B = ( b ( t t j ) ) over R.. Couversely any B G S y m ( n , R) defines t h e bilinear fo rm ( R n , B ) . T h e d\u00e9terminant . <U:l{M,h) is defined as d t t ( M , b ) = d e t B . ( M , b ) is called r\u00e9gulaiif d t t ( M , b ) is a unit iu R. We call two forms ( M , b) and ( M ' , b ' ) with ma t r i c e s B , B ' G S y m ( n , R) isom\u00e9trie if the re exists an inver t ible M G M a t ( n , R) such t h a t B = M * B ' M . If f i : R. \u2014\u2022 A is a r ingmorph ism such tha t A is a finitely gene ra t ed p ro jec t ive R m o d u l e ancl s G HoniFt(A, i?.), then the m a p [ x , y ) \u20ac A x A \u2014* s ( x y ) defines a bilinear fo rm (A, s ) over R. More generally, if J is an id\u00e9al in A such t h a t J is a finitely gene ra t ed p ro j ec t ive /?.-module then (x, y) \u00c7 \u00ee x l \u00bb s (xy ) defines a bil inear form ( J , s ) over R . We call (Z\\.s) scaled t race form of A / R . Let f ( X ) 6 %[X] be a mouic separable polynomial of degree n (in one variable X ) . T h e n A : = Z [ X ] / ( f ( X ) ) is a free i \u00a3 m o d u l e of rank n wi th basis 1 , X , . . . , X n ~ 1 . We set A q A \u00ae Q. If f is i rreducible then A q = Q(A) d\u00e9not\u00e9s t h e quot ien t field of A. Let T r : A q Q d\u00e9no t\u00e9 the t race m a p which is non-zero . Eu le r ' s l e m m a (see [L] III\u2014I, propos i t ion 2, corollary) implies t h a t we have: A * : = {c G Aq | T r ( c A ) C Z } = l / ( f ' ( X ) ) A . Any c G A * defines a symmet r i c bilinear form (A, T r c ) where T r c m a p s ( a i , 02) G A x A to Tr (ca in i ) . Let N : A q \u2014* Q d\u00e9not\u00e9 the n o r m m a p . It is well known (compare [L] I I I l ) t h a t if c = c u / f ' [ X ) G A # , where cu G A, then we have d e t ( A , T r c ) = ( _ l ) \u00ab ( \u00ab i ) / ^ V ( c u ) . In par t icular , if c ^ 0 then d t t ( A , T r c ) \u00a3 0 a n d the f o r m ( A , T r c ) is regu la r if Co is a uni t in the int\u00e9gral closure. More generally, let J C A be an id\u00e9al in A and let c G (T ' ! ) # . Tl ieu we obta in a s y m m e t r i c bi l inear f o r m ( J , T r c ) if T r c m a p s ( a i , \u00ab a ) \u20ac J x \u00ee to '7V(c\u00abi\u00ab2). Note t h a t if B is an id\u00e9al in A then B * = B ~ X A * = l / ( f ' ( X ) ) B ~ l . If c = c u / f ' { X ) where c\u201e G 1 ~ 2 we have d e t ( l , T r c ) = ( l ) \u00ab l \u00bb W N { c u ) N { l ) i . (Compare [CS] page 220.) An obvions ques t ion is the l'ollowiug: Let (MJ>) be a scaled t r ace fo rm of A/Zi\u00ef. If ( N , b') is s y m m e t r i c bil inear such tha t ( M , h) 0 Q is isom\u00e9tr ie to ( N , b') <g> Q, then is", "authors": ["Edouard Lucas"], "id": "099b9562e047e95d86fe5a782209363d50e19fbb", "title": "Th\u00e9orie des nombres", "references": []}, {"date": "1972", "abstract": "Semantic Scholar extracted view of \"Reducibility among Combinatorial Problems In Complexity of Computer Compu-tations Plenum Press\" by Richard M. Karp", "authors": ["Richard M. Karp"], "id": "e54bbbf8090cf11d6fd77c1827a1286753a39d3d", "title": "Reducibility among Combinatorial Problems In Complexity of Computer Compu-tations Plenum Press", "references": []}, {"date": "1977", "abstract": "\u00a9 Publications math\u00e9matiques de l\u2019I.H.\u00c9.S., 1977, tous droits r\u00e9serv\u00e9s. L\u2019acc\u00e8s aux archives de la revue \u00ab Publications math\u00e9matiques de l\u2019I.H.\u00c9.S. \u00bb ( http://www. ihes.fr/IHES/Publications/Publications.html), implique l\u2019accord avec les conditions g\u00e9n\u00e9rales d\u2019utilisation (http://www.numdam.org/legal.php). Toute utilisation commerciale ou impression syst\u00e9matique est constitutive d\u2019une infraction p\u00e9nale. Toute copie ou impression de ce fichier doit contenir la pr\u00e9sente mention de copyright.", "authors": ["Barry Mazur"], "id": "7e75bd128bf40a78e8242434786e74a71ad548ef", "title": "Modular curves and the eisenstein ideal", "references": ["643297c8a6e681dee44cc78acb2fed5c7dffb5bc", "78d82a6cdb268311cbd9e38b7591f6761c6cc7b1", "18fb1053fc3bc405ef19e1e73817e1dce7c84a59"]}, {"date": "1967", "abstract": "We present here an algorithm for factoring a given polynomial over GF(q) into powers of irreducible polynomials. The method reduces the factorization of a polynomial of degree m over GF(q) to the solution of about m(q \u2212 1)/q linear equations in as many unknowns over GF(q).", "authors": ["Elwyn R. Berlekamp"], "id": "5b7f3a6efb4a1cde3b7f3a4521290cc87f230f0e", "title": "Factoring polynomials over finite fields", "references": []}, {"date": "1973", "abstract": "This paper gives specific computational details and experience with a group theoretic integer programming algorithm. Included among the subroutines are a matrix reduction scheme for obtaining group representations, network algorithms for solving group optimization problems, and a branch and bound search for finding optimal integer programming solutions. The innovative subroutines are shown to be efficient to compute and effective in finding good integer programming solutions and providing strong lower bounds for the branch and bound search.", "authors": ["G. Anthony Gorry", "William D. Northup", "Jeremy F. Shapiro"], "id": "97917d7ba767239960e59169db3c72b09e4a07c4", "title": "Computational experience with a group theoretic integer programming algorithm", "references": ["a4da8e7899989fcdc742a0de23778d6b2d16664d", "e9c32a75c5f23754740f49089e65e72f84a2e881"]}, {"date": "1979", "abstract": "as x --, oc. In [7] two versions of the Chebotarev density theorem were proved, one unconditional and the other on the assumption of the Generalized Riemann Hypothesis (GRH), each of which expressed ~Zc(X ) as the sum of the main term ICI IC]_lGt Li(x) and an error term which is an effectively computable function of x, i-~-' and the associated field constants n K = [ K : Q ], h i = [L :Q] and dK,dz, (the absolute values of the discriminants of the two fields). Assuming the truth of the GRH for ~L(s), that paper also proved the existence of an effectively computable constant b (independent of K and L) such that for any conjugacy class C, there exists a prime", "authors": ["Jeffrey C. Lagarias", "Hugh L. Montgomery", "Andrew M. Odlyzko"], "id": "6a4ef42f1f2d709c24c44e9f9594f7069274d4b5", "title": "A bound for the least prime ideal in the Chebotarev Density Theorem", "references": []}, {"date": "1993", "abstract": "We present a wave-function approach to the study of the evolution of a small system when it is coupled to a large reservoir. Fluctuations and dissipation originate in this approach from quantum jumps that occur randomly during the time evolution of the system. This approach can be applied to a wide class of relaxation operators in the Markovian regime, and it is equivalent to the standard master-equation approach. For systems with a number of states N much larger than unity this Monte Carlo wave-function approach can be less expensive in terms of calculation time than the master-equation treatment. Indeed, a wave function involves only N components, whereas a density matrix is described by N2 terms. We evaluate the gain in computing time that may be expected from such a formalism, and we discuss its applicability to several examples, with particular emphasis on a quantum description of laser cooling.", "authors": ["Klaus M\u00f8lmer", "Yvan Castin", "Jean Dalibard"], "id": "eb747b753af634d3f58ee6e9cc170287f81c372a", "title": "Monte Carlo wave-function method in quantum optics", "references": []}, {"date": "1975", "abstract": "The class groups of certain elliptic function fields without complex multiplications are computed. Questions about the structure of these groups and the arithmetical nature of their orders are considered.", "authors": ["I. Borosh", "Carlos J. Moreno", "H. Porta"], "id": "5fabc815e50bf77749ae36bf2eba126ab2fa08fd", "title": "Elliptic curves over finite fields. II", "references": ["8b138ef2699ff84c52cf2348bc20f176393d7b4f", "10186f5ac2d8dcb95764b2f4e9e79675d3ecc2c8", "d3ffd48d8d21ff966e3db33bd1fdd368ab5e7880", "643297c8a6e681dee44cc78acb2fed5c7dffb5bc"]}, {"date": "1992", "abstract": "A novel treatment of dissipation of energy from a \"small\" quantum system to a reservoir is presented. %'e replace the usual master equation for the small-system density matrix by a wave function evolution including a stochastic element. This wave-function approach provides new insight and it allows calculations on problems which would otherwise be exceedingly complicated. The approach is applied here to a two- or three-level atom coupled to a laser field and to the vacuum modes of the quantized electromagnetic field. PACS numbers: 42.50.\u2014 p, 32.80. \u2014 t A typical model system in quantum optics consists of an atom coupled simultaneously to a laser field and to the vacuum modes of the quantized electromagnetic field. The atom-laser interaction, responsible for absorption and stimulated emission processes, is coherent, whereas the coupling between the atom and the vacuum modes, responsible for spontaneous emission processes, is fundamentally incoherent. This leads to a dissipation of energy from the \"small system\" (atom+ laser) to the \"reservoir\" (vacuum field), which is usually treated by a density operator approach. A master equation, usually referred to as the optical Bloch equations (OBE's), is written for the reduced atomic density matrix pz [1]. We present here an alternative approach using a wave-function treatment to describe the atomic system. The apparent incompatibility between such a wavefunction approach and the inherent irreversibility of the spontaneous processes is lifted by introducing repeated \"gedanken measurements\" on the atomic system simulating the detection of the spontaneous photons. The random result of each of these measurements determines the atomic state afterwards, and is at the origin of the irreversibility. We show that this treatment is equivalent to the standard density matrix approach leading to the OBE's. There are two main interests in this approach. First, it gives new insights into the processes taking place in the interaction of an atom with light. Here we will give two examples concerning the Rabi transitory regime for a two-level atom and the \"dark resonance\" problem. Second, it provides an efficient computational tool. If we consider an atomic system with N states, the master equation treatment requires the simultaneous solution of W OBE's, while in this new approach, we deal with wave functions and we have to look for the evolution of no more than W variables. For instance, in problems related to a quantum treatment of laser cooling, N includes both the number of internal and external atomic states and can be very large. This makes the density matrix approach unwieldy except for simple one-dimensional configuraI ttI(t+dt)) =I vr\"'(t+dt))+", "authors": ["Dalibard", "Castin", "Molmer"], "id": "f9f880bb8c8edb1484d2c8efe6262ce2c08f73dd", "title": "Wave-function approach to dissipative processes in quantum optics.", "references": []}, {"date": "", "abstract": "Semantic Scholar extracted view of \"\u00dcber die Entwicklungskoeffizienten der automorphen Formen\" by Hans Petersson", "authors": ["Hans Petersson"], "id": "06bc48bd24457611a56146025801e19c1721c979", "title": "\u00dcber die Entwicklungskoeffizienten der automorphen Formen", "references": []}, {"date": "1996", "abstract": "A new type of uncertainty relation is presented, concerning the information-bearing properties of a discrete quantum system. A natural link is then revealed between basic quantum theory and the linear error correcting codes of classical information theory. A subset of the known codes is described, having properties which are important for error correction in quantum communication. It is shown that a pair of states which are, in a certain sense, \u201cmacroscopically different,\u201d can form a superposition in which the interference phase between the two parts is measurable. This provides a highly stabilized \u201cSchrodinger cat\u201d state. [S0031-9007(96)00779-X]", "authors": ["Steane"], "id": "db5c47769e7b1a852303220fb75b8e9d6e2086ce", "title": "Error Correcting Codes in Quantum Theory.", "references": ["36ff067850e89ec952ea4fb31d6076ef3e1dabd9"]}, {"date": "1978", "abstract": "I. General Algebraic Theory.- I. Elliptic Functions.- II. The Division Equation.- III. p-Adic Addition.- IV. Heights.- V. Kummer Theory.- V1. Integral Points.- II. Approximation of Logarithms.- VII. Auxiliary Results.- VIII. The Baker-Feldman Theorem.- IX. Linear Combinations of Elliptic Logarithms.- X. The Baker-Tijdeman Theorem.- XI. Refined Inequalities.", "authors": ["Serge Lang"], "id": "92f1c943f00b57365082f1463deec7982227d7a5", "title": "Elliptic Curves: Diophantine Analysis", "references": []}, {"date": "1990", "abstract": "It is demonstrated that the premisses of the Einstein\u2013Podolsky\u2013Rosen paper are inconsistent when applied to quantum systems consisting of at least three particles. The demonstration reveals that the EPR program contradicts quantum mechanics even for the cases of perfect correlations. By perfect correlations is meant arrangements by which the result of the measurement on one particle can be predicted with certainty given the outcomes of measurements on the other particles of the system. This incompatibility with quantum mechanics is stronger than the one previously revealed for two\u2010particle systems by Bell\u2019s inequality, where no contradiction arises at the level of perfect correlations. Both spin\u2010correlation and multiparticle interferometry examples are given of suitable three\u2010 and four\u2010particle arrangements, both at the gedanken and at the real experiment level.", "authors": ["Daniel Mordecai Greenberger", "Michael A. Horne", "Abner Shimony", "Anton Zeilinger"], "id": "7c5e13efa803a8acf5e5e1b547bb427b6c0ffc02", "title": "Bell\u2019s theorem without inequalities", "references": []}, {"date": "", "abstract": "The field of channel coding started with Claude Shannon's 1948 landmark paper. Fifty years of efforts and invention have finally produced coding schemes that closely approach Shannon's channel capacity limit on AWGN channels, both power-limited and band-limited. Similar gains are being achieved in other important applications, such as wireless channels. This course is divided in two parts. In the first part, we remind students of the basics of the theory of linear codes for conventional memoryless ergodic channels. We then introduce more advanced notions so as to make comprehensible some of the most recent coding schemes proposed in the literature. In the second part, we expound the principles of coded modulations for the Gaussian channel and, if time permits, for Rician and Rayleigh fading channels (fully interleaved). We will conclude the course by evoking some aspects of code design for non-ergodic block-fading channels. Basic definitions \u2013 Classification of channels, random codes Coding theorem for DMC \u2013 Upper bounds on error probability Coding theorem for DMC \u2013 Lower bounds on error probability Strong converse theorem for discrete channels [Wolfowitz] Generalization of results to BIOS memoryless channels Hard or soft decoding, information loss Cutoff rate Lecture 2. Codes with algebraic structures [4][5] Detection and correction capabilities of block codes Lower and upper bounds on code parameters \u2013 General case Linear block codes \u2013 Minimum distance, duality, elementary transformations Lower and upper bounds on code parameters \u2013 GV, Hamming, Plotkin, Singleton Convolutional codes", "authors": ["Odette Antoine", "Berthet"], "id": "9cbc6bc1bec461f1bb7ddc4b68509ae9a4c6ef6b", "title": "Theory of Error-correcting Codes", "references": ["893caf4196d0fce0c287e7c8099beda28abf0ada", "52a8776f2435b7ec552e37d6b772c27cb5ae1932", "78d1b7abae97f53b7c9b8232c59720cb6c15a289", "f7ddcf396c3e5adbbfc3f8c0d06549a65bfb591c", "ea4916feb7d36cbaaf704ea11fcf8519b021ac8d", "03fd4bbbce57b08fb7ff90f78de391086af2e440", "7dbdb4209626fd92d2436a058663206216036e68", "d5c20ac7da03b37c211dcb2cc0348a66fd5c2f82", "679444233da73944cba37122cb7ff321e150fd81", "ede9e601e3070e8ff4ad9ff5687e43c930a293cb"]}, {"date": "1975", "abstract": "Semantic Scholar extracted view of \"On the Holomorphy of Certain Dirichlet Series\" by Goro Shimura", "authors": ["Goro Shimura"], "id": "cccfb5fa8c4c7c9f7ba0aba41e3be105e80a08a8", "title": "On the Holomorphy of Certain Dirichlet Series", "references": []}, {"date": "1975", "abstract": "Semantic Scholar extracted view of \"Modular Functions of One Variable IV\" by Bryan John Birch et al.", "authors": ["Bryan John Birch", "Willem Kuyk"], "id": "ff596dfb5c3c1f5746aa6278f7793752fd423de7", "title": "Modular Functions of One Variable IV", "references": []}, {"date": "1980", "abstract": "Pollard's Monte Carlo factorization algorithm usually finds a factor of a composite integerN inO(N1/4) arithmetic operations. The algorithm is based on a cycle-finding algorithm of Floyd. We describe a cycle-finding algorithm which is about 36 percent faster than Floyd's (on the average), and apply it to give a Monte Carlo factorization algorithm which is similar to Pollard's but about 24 percent faster.", "authors": ["Richard P. Brent"], "id": "1318eeab4352b81927792347a374af86f0683a91", "title": "An improved Monte Carlo factorization algorithm", "references": ["832d9e1ac57c79b5b4943605813dd169bd73feba", "7b3811c1f378003be1e0d0c2e0f9277e8600a5e8", "8052adc20ffb2f284bc31160874721d6943bcb78"]}, {"date": "1987", "abstract": "A modification, due to Peter Montgomery, of Pomerance's Quadratic Sieve for factoring large integers is discussed along with its implementation. Using it, allows factorization with over an order of magnitude less sieving than the basic algorithm. It enables one to factor numbers in the 60-digit range in about a day, using a large minicomputer. The algorithm has features which make it well adapted to parallel implementation.", "authors": ["Robert D. Silverman"], "id": "4bc1656a66f55e68c738a057d8b9e49d2308eb23", "title": "The multiple polynomial quadratic sieve", "references": ["591b9df0839280470310d5bdd100170b86050151", "6eb53aebf5000091d191976a5c042d65a39f7b30", "bd9356733929d4bdac9bcef87c4406709f4596cd", "134b7b065a73d4ca00bb16c7b8bebbde951b0ba0"]}, {"date": "1983", "abstract": "The quadratic sieve algorithm was used to factor a 47-digit number into primes. A comparison with Wagstaff's results using the continued fraction early abort algorithm suggests that QS should be faster than CFEA when the number being factored exceeds 60 digits (plus or minus ten or more digits, depending on details of the hardware and software).", "authors": ["Joseph L. Gerver"], "id": "bd9356733929d4bdac9bcef87c4406709f4596cd", "title": "Factoring large numbers with a quadratic sieve", "references": ["41c5341ea5409468e4d68ad14fce820c78bc51b2", "6eb53aebf5000091d191976a5c042d65a39f7b30", "291d2956c10b4911275dd1bb35579a6ebcae43f8"]}, {"date": "1973", "abstract": "Semantic Scholar extracted view of \"Rational points on certain elliptic modular curves\" by Andrew P. Ogg", "authors": ["Andrew P. Ogg"], "id": "643297c8a6e681dee44cc78acb2fed5c7dffb5bc", "title": "Rational points on certain elliptic modular curves", "references": []}, {"date": "1981", "abstract": "We describe a Monte Carlo factorization algorithm which was used to factorize the Fermat number F8 = 2256+1. Previously, F8 was known to be composite, but its factors were unknown. Comments Only the Abstract is given here. The full paper appeared as [2]. For a succinct proof of primality of the larger factor q8 of F8, see [3]. At the time of this paper, Lenstra\u2019s elliptic curve method (ECM) had not been invented. Thus, a modification of Pollard\u2019s \u201crho\u201d method [1] was used to factor F8 and several Mersenne numbers [4]. For the factorization of a larger Fermat number by ECM, see [5]. The smaller factor of F8 is 1238926361552897. The epigram I am now entirely persuaded to employ the method, a handy trick, on gigantic composite numbers may appeal to readers who wish to memorize this factor.", "authors": ["R. P. Brent", "John M. Pollard"], "id": "832d9e1ac57c79b5b4943605813dd169bd73feba", "title": "Factorization of the eighth Fermat number", "references": ["0932af420e4d4d13e6eeb087556eaace68c3d387", "1318eeab4352b81927792347a374af86f0683a91"]}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"Group theoretic structures in the fixed charge transportation problem\" by Curtis Johnston Tompkins", "authors": ["Curtis Johnston Tompkins"], "id": "a4da8e7899989fcdc742a0de23778d6b2d16664d", "title": "Group theoretic structures in the fixed charge transportation problem", "references": []}, {"date": "1973", "abstract": "Semantic Scholar extracted view of \"On the fields generated by certain points of finite order on Shimura's elliptic curves\" by \u5c71\u5185 \u6b63\u654f", "authors": ["\u5c71\u5185 \u6b63\u654f"], "id": "78d82a6cdb268311cbd9e38b7591f6761c6cc7b1", "title": "On the fields generated by certain points of finite order on Shimura's elliptic curves", "references": []}, {"date": "1969", "abstract": "1,4-bis-Mercaptoacetyl and alpha -substituted mercaptoacetyl-piperazines are potent mucolytic agents which are effective both topically and systemically on oral or parenteral administration to animals.", "authors": ["Tadao Oda"], "id": "18fb1053fc3bc405ef19e1e73817e1dce7c84a59", "title": "The first de Rham cohomology group and Dieudonn\u00e9 modules", "references": ["54830ce7d7cae48b10eb402b78e00e68d52cb9f5", "7c7dc7639db7072a7376fef8cf58723d5186071f", "65d4849328104d2027b511538c7c54b172ff7242", "047d4cf3d9502f15448a17426c44d9995d9a29d4", "3ae65e1ea511643c6f6cec629ab56e73fd19ca31", "0e6ab3a19b807b997fdb8c54bf96a3e1e56ab7b9", "118db5420a4a67a1f6035d6975ee706caa2040cb", "ec070b4ad7e5dc774a741ea6bf7098e354c9befc", "b15c8ea47534761a16e0752c70c13474e7c568a8"]}, {"date": "1969", "abstract": "This paper treats five discrete shortest-path problems: 1 determining the shortest path between two specified nodes of a network; 2 determining the shortest paths between all pairs of nodes of a network; 3 determining the second, third, etc., shortest path; 4 determining the fastest path through a network with travel times depending on the departure time; and 5 finding the shortest path between specified endpoints that passes through specified intermediate nodes. Existing good algorithms are identified while some others are modified to yield efficient procedures. Also, certain misrepresentations and errors in the literature are demonstrated.", "authors": ["Stuart E. Dreyfus"], "id": "e9c32a75c5f23754740f49089e65e72f84a2e881", "title": "An Appraisal of Some Shortest-Path Algorithms", "references": []}, {"date": "1971", "abstract": "Semantic Scholar extracted view of \"On the Prime Divisors of Polynomials\" by Irving Gerst et al.", "authors": ["Irving Gerst", "John Brillhart"], "id": "10186f5ac2d8dcb95764b2f4e9e79675d3ecc2c8", "title": "On the Prime Divisors of Polynomials", "references": []}]